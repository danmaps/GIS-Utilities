

# ====================
# Notebook Image Purge and Markdown Preservation.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# In[16]:


import nbformat
from nbconvert import PythonExporter
import os
import re

def remove_images_from_markdown(cell):
    if cell['cell_type'] == 'markdown':
        # Regular expression to find <img> tags with base64 encoded images
        pattern = r"(#?\s*<img [^>]*>)|(#?\s*<image [^>]*>)|(#?\s*\!\[.*?\]\(data:image\/(png|jpeg);base64,[^)]*\))"
        cell['source'] = re.sub(pattern, '', cell['source'])
    return cell

def process_notebook(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        notebook = nbformat.read(file, as_version=4)

    # Process each cell
    processed_cells = [remove_images_from_markdown(cell) for cell in notebook['cells']]
    notebook['cells'] = processed_cells

    # Convert to Python file
    exporter = PythonExporter()
    python_code, _ = exporter.from_notebook_node(notebook)

    return python_code

def main():
    directory = r"C:\Users\mcveydb\Downloads\arcgis-python-api-master\samples\04_gis_analysts_data_scientists"
    for file in os.listdir(directory):
        if file.endswith('.ipynb'):
            file_path = os.path.join(directory, file)
            python_code = process_notebook(file_path)
            # Write the Python code to a .py file
            with open(file_path.replace('.ipynb', '.py'), 'w', encoding='utf-8') as py_file:
                py_file.write(python_code)
main()


# In[17]:


import os
import zipfile

def zip_py_files(directory, zip_filename):
    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(directory):
            for file in files:
                if file.endswith('.py'):
                    file_path = os.path.join(root, file)
                    zipf.write(file_path, os.path.relpath(file_path, start=directory))

def main():
    directory = r"C:\Users\mcveydb\Downloads\arcgis-python-api-master\samples\04_gis_analysts_data_scientists"
    zip_filename = os.path.join(directory,"pyfiles.zip")

    zip_py_files(directory, zip_filename)
    print(f"All .py files in {directory} have been zipped into {zip_filename}")

if __name__ == "__main__":
    main()



# ====================
# address-standardization-and-correction-with-sequencetosequence.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Address Standardization and Correction using SequenceToSequence model

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc">
# <ul class="toc-item">
# <li><span><a href="#Introduction" data-toc-modified-id="Introduction-1">Introduction</a></span></li>
# <li><span><a href="#Prerequisites" data-toc-modified-id="Prerequisites-2">Prerequisites</a></span></li>
# <li><span><a href="#Imports" data-toc-modified-id="Imports-3">Imports</a></span></li>
# <li><span><a href="#Data-preparation" data-toc-modified-id="Data-preparation-4">Data preparation</a></span></li>
# <li><span><a href="#SequenceToSequence-model" data-toc-modified-id="SequenceToSequence-model-5">SequenceToSequence model</a></span></li>
# <ul class="toc-item">
# <li><span><a href="#Load-model-architecture" data-toc-modified-id="Load-model-architecture-5.1">Load model architecture</a></span></li>
# <li><span><a href="#Model-training" data-toc-modified-id="Model-training-5.2">Model training</a></span></li>    
# <li><span><a href="#Validate-results" data-toc-modified-id="Validate-results-5.3">Validate results</a></span></li>
# <li><span><a href="#Model-metrics" data-toc-modified-id="Model-metrics-5.4">Model metrics</a></span></li>    
# <li><span><a href="#Saving-the-trained-model" data-toc-modified-id="Saving-the-trained-model-5.5">Saving the trained model</a></span></li>
# </ul>
# <li><span><a href="#Model-inference" data-toc-modified-id="Model-inference-6">Model inference</a></span></li>
# <li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-7">Conclusion</a></span></li>
# <li><span><a href="#References" data-toc-modified-id="References-8">References</a></span></li>
# </ul></div>

# # Introduction

# **Address Standardization** is the process of changing addresses to adhere to USPS standards. In this notebook, we will aim at abbreviating the addresses as per standard USPS abbreviations. 
# 
# **Address Correction** will aim at correcting  miss-spelled place names.
# 
# We will train a model using `SequenceToSequence` class of `arcgis.learn.text` module to translate the non-standard and erroneous address to their standard and correct form. 
# 
# The dataset consists of a pair of non-standard, incorrect(synthetic errors) house addresses and corresponding correct, standard house addresses from the United States. The correct addresses are taken from [OpenAddresses data](http://results.openaddresses.io/).
# 
# *Disclaimer: The correct addresses were synthetically corrupted to prepare the training dataset, this could have lead to some unexpected corruptions in addresses, which will affect the translation learned by the model.* 
# 
# **A note on the dataset**
# - The data is collected around 2020-04-29 by [OpenAddresses](http://openaddresses.io).
# - The data licenses can be found in `data/address_standardization_correction_data/LICENSE.txt`.

# # Prerequisites

# - Data preparation and model training workflows using arcgis.learn have a dependency on [transformers](https://huggingface.co/transformers/v3.0.2/index.html). Refer to the section **"Install deep learning dependencies of arcgis.learn module"** [on this page](https://developers.arcgis.com/python/guide/install-and-set-up/#Install-deep-learning-dependencies) for detailed documentation on the installation of the dependencies.
# 
# - **Labeled data**: For `SequenceToSequence` model to learn, it needs to see documents/texts that have been assigned a label. Labeled data for this sample notebook is located at `data/address_standardization_correction_data/address_standardization_correction.csv`
# 
# - To learn more about how `SequenceToSequence` works, please see the guide on [How SequenceToSequence works](https://developers.arcgis.com/python/guide/how-sequencetosequence-works).

# In[8]:


get_ipython().system('pip install transformers==3.3.0')


# Note: Please restart the kernel before running the cells below.

# # Imports

# In[1]:


import os
import zipfile
from pathlib import Path
from arcgis.gis import GIS
from arcgis.learn import prepare_textdata
from arcgis.learn.text import SequenceToSequence


# In[2]:


gis = GIS('home')


# # Data preparation
# 
# Data preparation involves splitting the data into training and validation sets, creating the necessary data structures for loading data into the model and so on. The `prepare_textdata()` function can directly read the training samples and automate the entire process.

# In[3]:


training_data = gis.content.get('06200bcbf46a4f58b2036c02b0bff41e')
training_data


# Note: This address dataset is a subset (~15%) of the dataset available at "ea94e88b5a56412995fd1ffcb85d60e9" item id.

# In[4]:


filepath = training_data.download(file_name=training_data.name)


# In[5]:


with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[6]:


data_root = Path(os.path.join(os.path.splitext(filepath)[0]))


# In[7]:


data = prepare_textdata(path=data_root, batch_size=16, task='sequence_translation', 
                        text_columns='non-std-address', label_columns='std-address', 
                        train_file='address_standardization_correction_data_small.csv')


# The `show_batch()` method can be used to see the training samples, along with labels.

# In[8]:


data.show_batch()


# # SequenceToSequence model

# `SequenceToSequence` model in `arcgis.learn.text` is built on top of [Hugging Face Transformers](https://huggingface.co/transformers/v3.0.2/index.html) library. The model training and inferencing workflows are similar to computer vision models in `arcgis.learn`. 
# 
# Run the command below to see what backbones are supported for the sequence translation task.

# In[9]:


SequenceToSequence.supported_backbones


# Call the model's `available_backbone_models()` method with the backbone name to get the available models for that backbone. The call to **available_backbone_models** method will list out only a few of the available models for each backbone. Visit [this](https://huggingface.co/transformers/pretrained_models.html) link to get a complete list of models for each backbone.

# In[10]:


SequenceToSequence.available_backbone_models("T5")


# ## Load model architecture

# Invoke the `SequenceToSequence` class by passing the data and the backbone you have chosen. The dataset consists of house addresses in non-standard format with synthetic errors, we will finetune a [t5-base](https://huggingface.co/t5-base) pretrained model. The model will attempt to learn how to standardize and correct the input addresses.

# In[11]:


model = SequenceToSequence(data,backbone='t5-base')


# ## Model training

# The `learning rate`[[1]](#References) is a **tuning parameter** that determines the step size at each iteration while moving toward a minimum of a loss function, it represents the speed at which a machine learning model **"learns"**. `arcgis.learn` includes a learning rate finder, and is accessible through the model's `lr_find()` method, which can automatically select an **optimum learning rate**, without requiring repeated experiments.

# In[12]:


lr = model.lr_find()
lr


# Training the model is an iterative process. We can train the model using its `fit()` method till the validation loss (or error rate) continues to go down with each training pass also known as epoch. This is indicative of the model learning the task.

# In[13]:


model.fit(1, lr=lr)


# By default, the earlier layers of the model (i.e. the backbone) are frozen. Once the later layers have been sufficiently trained, the earlier layers are unfrozen (by calling `unfreeze()` method of the class) to further fine-tune the model.

# In[14]:


model.unfreeze()


# In[15]:


lr = model.lr_find()
lr


# In[16]:


model.fit(5, lr)


# In[17]:


model.fit(3, lr)


# ## Validate results

# Once we have the trained model, we can see the results to see how it performs.

# In[18]:


model.show_results()


# ## Model metrics
# 
# To get a sense of how well the model is trained, we will calculate some important metrics for our `SequenceToSequence` model. To see what's the model accuracy [[2]](#References) and bleu score [[3]](#References) on the validation data-set. We will call the model's `get_model_metrics()` method.

# In[19]:


model.get_model_metrics()


# ## Saving the trained model
# 
# Once you are satisfied with the model, you can save it using the save() method. This creates an Esri Model Definition (EMD file) that can be used for inferencing unseen data.

# In[20]:


model.save('seq2seq_unfrozen8E_bleu_98', publish=True)


# # Model inference
# 
# The trained model can be used to translate new text documents using the predict method. This method accepts a string or a list of strings to predict the labels of these new documents/text.

# In[21]:


txt=['940, north pennsylvania avneue, mason icty, iowa, 50401, us',
     '220, soyth rhodeisland aveune, mason city, iowa, 50401, us']


# In[22]:


model.predict(txt, num_beams=6, max_length=50)


# # Conclusion

# In this notebook we will build an address standardization and correction model using `SequenceToSequence` class of `arcgis.learn.text` module. The dataset consisted of a pair of non-standard, incorrect (synthetic errors) house addresses and corresponding correct, standard house addresses from the United States. To achieve this we used a [t5-base](https://huggingface.co/t5-base) pretrained transformer to build a SequenceToSequence model to standardize and correct the input house addresses. Below are the results on sample inputs.

# <span style="background-color: LightSalmon">Non-Standard</span> &rarr;  <span style="background-color: LightGreen">Standard
# </span>  ,  <span style="background-color: LightCoral">Error</span> &rarr; <span style="background-color: LightGreen">Correction
# </span>
# * <span style="font-size:1.5em;">940, <span style="background-color: LightSalmon;">north</span> pennsylvania <span style="background-color: LightSalmon">avneue</span>, mason <span style="background-color: LightCoral">icty</span>, <span style="background-color: LightSalmon">iowa</span>, 50401, us &rarr; 940, <span style="background-color: LightGreen">n</span> pennsylvania <span style="background-color: LightGreen">ave</span>, mason <span style="background-color: LightGreen">city</span>, <span style="background-color: LightGreen">ia</span>, 50401, us</span>
# 
# 
# * <span style="font-size:1.5em;">220, <span style="background-color: LightCoral">soyth rhodeisland</span> <span style="background-color: LightSalmon">aveune</span>, mason city, <span style="background-color: LightSalmon">iowa</span>, 50401, us &rarr; 220, <span style="background-color: LightGreen">s rhode island ave</span>, mason city, <span style="background-color: LightGreen">ia</span>, 50401, us</span>
# 
# 

# # References

# [1] [Learning Rate](https://en.wikipedia.org/wiki/Learning_rate)
# 
# [2] [Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision)
# 
# [3] [Bleu score](https://en.wikipedia.org/wiki/BLEU)


# ====================
# analyze_new_york_city_taxi_data.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Analyzing New York City taxi data using big data tools
# 
# At 10.5 and later releases, ArcGIS Enterprise introduces [ArcGIS GeoAnalytics Server](http://server.arcgis.com/en/server/latest/get-started/windows/what-is-arcgis-geoanalytics-server-.htm) which provides you the ability to perform big data analysis on your infrastructure. This sample demonstrates the steps involved in performing an aggregation analysis on New York city taxi point data using ArcGIS API for Python.
# 
# The data used in this sample can be downloaded from [NYC Taxi & Limousine Commission website](http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml). For this sample, data for the months January & Febuary of 2015 were used, each averaging 12 million records.
# 
# **Note**: The ability to perform big data analysis is only available on ArcGIS Enterprise 10.5 licensed with a GeoAnalytics server and not yet available on ArcGIS Online.

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Analyzing-New-York-City-taxi-data-using-big-data-tools" data-toc-modified-id="Analyzing-New-York-City-taxi-data-using-big-data-tools-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Analyzing New York City taxi data using big data tools</a></span><ul class="toc-item"><li><span><a href="#The-NYC-taxi-data" data-toc-modified-id="The-NYC-taxi-data-1.1"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>The NYC taxi data</a></span></li><li><span><a href="#Searching-for-big-data-file-shares" data-toc-modified-id="Searching-for-big-data-file-shares-1.2"><span class="toc-item-num">1.2&nbsp;&nbsp;</span>Searching for big data file shares</a></span></li><li><span><a href="#Registering-big-data-file-shares" data-toc-modified-id="Registering-big-data-file-shares-1.3"><span class="toc-item-num">1.3&nbsp;&nbsp;</span>Registering big data file shares</a></span></li><li><span><a href="#Performing-data-aggregation" data-toc-modified-id="Performing-data-aggregation-1.4"><span class="toc-item-num">1.4&nbsp;&nbsp;</span>Performing data aggregation</a></span><ul class="toc-item"><li><span><a href="#Aggregate-points-tool" data-toc-modified-id="Aggregate-points-tool-1.4.1"><span class="toc-item-num">1.4.1&nbsp;&nbsp;</span>Aggregate points tool</a></span></li><li><span><a href="#Inspect-the-results" data-toc-modified-id="Inspect-the-results-1.4.2"><span class="toc-item-num">1.4.2&nbsp;&nbsp;</span>Inspect the results</a></span></li></ul></li></ul></li></ul></div>

# ## The NYC taxi data
# 
# To give you an overview, let us take a look at a subset with 2000 points published as a feature service.

# In[1]:


import arcgis
from arcgis.gis import GIS

ago_gis = GIS() # Connect to ArcGIS Online as an anonymous user
search_subset = ago_gis.content.search("NYC_taxi_subset", item_type = "Feature Layer")
subset_item = search_subset[0]
subset_item


# Let us bring up a map to display the data.

# In[1]:


subset_map = ago_gis.map("New York, NY", zoomlevel=11)
subset_map


# In[3]:


subset_map.add_layer(subset_item)


# Let us access the feature layers using the `layers` property. We can select a specific layer from the laters list and explore its attribute table to understand the structure of our data. In the cell below, we use the feature layer's [`query()`](https://developers.arcgis.com/python/api-reference/arcgis.features.toc.html#arcgis.features.FeatureLayer.query) method to return the layer attribute information. The `query()` method returns a [`FeatureSet`](https://developers.arcgis.com/python/api-reference/arcgis.features.toc.html#featureset) object, which is a collection of individual [`Feature`](https://developers.arcgis.com/python/api-reference/arcgis.features.toc.html#feature) objects.
# 
# You can mine through the `FeatureSet` to inspect each individual `Feature`, read its attribute information and then compose a table of all features and their attributes. However, the `FeatureSet` object provides a much easier and more direct way to get that information. Using the [`df`](https://developers.arcgis.com/python/api-reference/arcgis.features.toc.html#arcgis.features.FeatureSet.df) property of a `FeatureSet`, you can load the attribute information as a [`pandas`](https://pandas.pydata.org/) [`dataframe`](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe) object.
# 
# If you installed the ArcGIS API for Python through ArcGIS Pro or with the `conda install` command, you have the api and its dependencies, including the `pandas` package. The [`df`](https://developers.arcgis.com/python/api-reference/arcgis.features.toc.html#arcgis.features.FeatureSet.df) property will return a `dataframe`.  If you [installed without dependences](https://developers.arcgis.com/python/guide/install-and-set-up/#Install-without-Dependencies), you need to install the `pandas` Python package for the `df` property to return a dataframe. If you get an error that pandas cannot be found, you can install it by typing the following in your terminal that is running the jupyter notebook:
# 
#     conda install pandas

# In[2]:


subset_feature_layer = subset_item.layers[0]

# query the attribute information. Limit to first 5 rows.
query_result = subset_feature_layer.query(where = 'OBJECTID < 5',
                                          out_fields = "*", 
                                          returnGeometry = False)

att_data_frame = query_result.sdf # get as a Pandas dataframe
att_data_frame


# The table above represents the attribute information available from the NYC dataset. Columns provide a wealth of infomation such as pickup and dropoff_locations, fares, tips, tolls, and trip distances which you can analyze to observe many interesting patterns. The full data dataset contains over 24 million points. To discern patterns out of it, let us aggregate the points into blocks of 1 square kilometer.

# ## Searching for big data file shares

# To process data using GeoAnalytics Server, you need to have registered the data with your Geoanalytics Server. In this sample the data is in multiple csv files, which have been previously registered as a big data file share.
# 
# Let us connect to an ArcGIS Enterprise.

# In[1]:


gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# Ensure that the Geoanalytics [is supported](https://developers.arcgis.com/python/api-reference/arcgis.geoanalytics.toc.html#arcgis.geoanalytics.is_supported) with our GIS.

# In[4]:


arcgis.geoanalytics.is_supported()


# Get the geoanalytics datastores and search it for the registered datasets:

# In[5]:


datastores = arcgis.geoanalytics.get_datastores()


# In[18]:


bigdata_fileshares = datastores.search(id='0e7a861d-c1c5-4acc-869d-05d2cebbdbee')
bigdata_fileshares


# GA_Data is registered as a `big data file share` with the Geoanalytics datastore, so we can reference it:

# In[20]:


data_item = bigdata_fileshares[0]


# ## Registering big data file shares
# 
# The code below shows how a big data file share can be registered with the geoanalytics datastores, in case it's not already registered.

# In[10]:


# data_item = datastores.add_bigdata("NYCdata", r"\\pathway\to\data")


# Once a big data file share is created, the GeoAnalytics server processes all the valid file types to discern the schema of the data. This process can take a few minutes depending on the size of your data. Once processed, querying the [`manifest`](https://developers.arcgis.com/python/api-reference/arcgis.gis.toc.html?highlight=manifest#arcgis.gis.Datastore.manifest) property returns the schema. As you can see from below, the schema is similar to the subset we observed earlier in this sample.

# In[22]:


data_item.manifest


# Since this big data file share has multiple datasets, let's check the manifest for the taxi dataset.

# In[37]:


data_item.manifest['datasets'][3]


# ## Performing data aggregation

# When you add a big data file share datastore, a corresponding item gets created on your portal. You can search for it like a regular item and query its layers.

# In[40]:


search_result = gis.content.search("bigDataFileShares_GA_Data", item_type = "big data file share")
search_result


# In[41]:


data_item = search_result[0]
data_item


# In[42]:


data_item.layers


# In[43]:


year_2015 = data_item.layers[3]
year_2015


# ### Aggregate points tool
# You access the [`aggregate_points()`](https://developers.arcgis.com/python/api-reference/arcgis.geoanalytics.summarize_data.html#aggregate-points) tool in the [`summarize_data`](https://developers.arcgis.com/python/api-reference/arcgis.geoanalytics.summarize_data.html#) submodule of the [`geoanalytics`](https://developers.arcgis.com/python/api-reference/arcgis.geoanalytics.toc.html) module. In this example, we are using this tool to aggregate the numerous points into 1 kilometer square blocks. The tool creates a polygon feature layer in which each polygon contains aggregated attribute information from all the points in the input dataset that fall within that polygon. The output feature layer contains only polygons that contain at least one point from the input dataset. See [Aggregate Points](https://enterprise.arcgis.com/en/portal/latest/use/geoanalyticstool-aggregatepoints.htm) for details on using this tool.

# In[30]:


from arcgis.geoanalytics.summarize_data import aggregate_points


# The aggregate points tool requires that either:
#   * the point layer is projected, or
#   * the output or processing coordinate system is set to a Projected Coordinate System
# 
# We can query the layer `properties` to investigate the coordinate system of the point layer:

# In[31]:


year_2015.properties['spatialReference']


# Since WGS84 (the coordinate system referred to by wkid 4326) is unprojected, we can use the [`arcgis.env`]() module to set the environment used in the tool processing. The [`process_spatial_reference`](https://developers.arcgis.com/python/api-reference/arcgis.env.html#arcgis.env.process_spatial_reference) environment setting controls the geometry processing of tools used by the API for Python. We can set this parameter to a projected coordinate system for tool processing:
# 
# > **NOTE:** Aggregate Points requires that your area layer is in a projected coordinate system. See the `Usage notes` section of the [`help`](https://enterprise.arcgis.com/en/portal/latest/use/geoanalyticstool-aggregatepoints.htm#Usage-notes) for more information.

# In[32]:


arcgis.env.process_spatial_reference=3857


# We can use the [`arcgis.env`](https://developers.arcgis.com/python/api-reference/arcgis.env.html) module to modify environment settings that geoprocessing and geoanalytics tools use during execution. Set [`verbose`](https://developers.arcgis.com/python/api-reference/arcgis.env.html#verbose) to `True` to return detailed messaging when running tools:

# In[33]:


arcgis.env.verbose = True


# Let's run the tool, specifying 1 kilometer squares as the polygons for which we want to aggregate information about all the NYC taxi information in each of those polygons:

# In[50]:


agg_result = aggregate_points(year_2015, 
                              bin_type='square',
                              bin_size=1, 
                              polygon_layer='', 
                              bin_size_unit='Kilometers')


# ### Inspect the results
# 
# Let us create a map and load the processed result which is a feature layer item.

# In[2]:


processed_map = gis.map('New York, NY', 11)
processed_map


# In[36]:


processed_map.add_layer(agg_result)


# By default the item we just created is not shared, so additinal processing requires login credentials. Let's [`share()`] the item to avoid this.

# In[40]:


agg_result.share(org=True)


# Let us inspect the analysis result using smart mapping. To learn more about this visualization capability, refer to the guide on [Smart Mapping](https://developers.arcgis.com/python/guide/smart-mapping/) under the 'Mapping and Visualization' section.

# In[3]:


map2 = gis.map("New York, NY", 11)
map2


# In[ ]:


map2.add_layer(agg_result, {
                "renderer":"ClassedColorRenderer",
                "field_name":"MAX_tip_amount", 
                "normalizationField":'MAX_trip_distance',
                "classificationMethod":'natural-breaks',
                "opacity":0.75
              })


# We can now start seeing patterns, such as which pickup areas resulted in higher tips for the cab drivers.


# ====================
# analyze_patterns_in_construction_permits_part1.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Data Visualization - Construction permits, part 1/2

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Overview" data-toc-modified-id="Overview-1">Overview</a></span></li><li><span><a href="#Explore-the-data" data-toc-modified-id="Explore-the-data-2">Explore the data</a></span><ul class="toc-item"><li><span><a href="#Data-Exploration" data-toc-modified-id="Data-Exploration-2.1">Data Exploration</a></span></li></ul></li><li><span><a href="#Permits-by-Status" data-toc-modified-id="Permits-by-Status-3">Permits by Status</a></span></li><li><span><a href="#Permits-by-Type" data-toc-modified-id="Permits-by-Type-4">Permits by Type</a></span><ul class="toc-item"><li><span><a href="#Clean-up-the-data" data-toc-modified-id="Clean-up-the-data-4.1">Clean up the data</a></span></li><li><span><a href="#Filter-the-permits" data-toc-modified-id="Filter-the-permits-4.2">Filter the permits</a></span></li><li><span><a href="#Visualize-filtered-dataset" data-toc-modified-id="Visualize-filtered-dataset-4.3">Visualize filtered dataset</a></span></li></ul></li><li><span><a href="#Visualize-temporal-and-spatial-trends" data-toc-modified-id="Visualize-temporal-and-spatial-trends-5">Visualize temporal and spatial trends</a></span><ul class="toc-item"><li><span><a href="#Visualize-permits-by-time-of-issue" data-toc-modified-id="Visualize-permits-by-time-of-issue-5.1">Visualize permits by time of issue</a></span></li></ul></li></ul></div>

# ## Overview

# One indicator of a region's growth is the number of permits issued for new construction. Exploring and analyzing permit activity can help regional planners ensure that development occurs in accordance to the area's long-term goals. One area that has recently experienced rapid growth is Montgomery County, Maryland, a suburban county near Washington, D.C. County planners want to observe spatial and temporal growth trends, find out why certain areas are growing faster than others, and communicate key information about the county's growth to the public.
# 
# In this notebook, you'll explore Montgomery County permit data. First, you'll add the permit data from ArcGIS Living Atlas of the World. You'll explore the data and become familiar with exactly what kind of information it contains. Then, you'll analyze the data to detect patterns and find out why growth is occurring. Once you've gathered your findings from your exploration and analysis, you'll share your work online.

# ## Explore the data
# 
# To better understand trends in permit activity in Montgomery County, you'll add a dataset of permits issued since 2010. Before you begin your analysis, however, it's important to explore your data and understand what it shows and does not show. You'll familiarize yourself with the data's attributes, sort the data by type, and visualize spatial and temporal trends. In doing so, you'll gain context for your analysis and know exactly which questions you still need to ask to find out why, where, and when growth is occurring.

# Connect to your ArcGIS online organization.

# In[2]:


from arcgis.gis import GIS
import pandas as pd

from arcgis.features import GeoAccessor, GeoSeriesAccessor


# In[3]:


agol_gis = GIS()


# Search for the **Commercial Permits since 2010** layer. You can specify the owner's name to get more specific results. To search for content from the Living Atlas, or content shared by other users on ArcGIS Online, set `outside_org=True`.

# In[4]:


data = agol_gis.content.search('title: Commercial Permits since 2010', 'Feature layer', 
                               outside_org=True)
data[0]


# Get the first item from the results.

# In[5]:


permits = data[0]


# Since the item is a Feature Layer Collection, accessing the layers property gives us a list of FeatureLayer objects. The permit layer is the first layer in this item. Visualize this layer on a map of Montgomery County, Maryland.

# In[6]:


permit_layer = permits.layers[0]


# In[7]:


permit_map = agol_gis.map('Montgomery County, Maryland', zoomlevel=9)
permit_map


# You can add a number of different layer objects such as FeatureLayer, FeatureCollection, ImageryLayer, MapImageLayer to the map by calling the `add_layer()` method.

# In[8]:


permit_map.add_layer(permit_layer)


# ### Data Exploration

# Now that you've added the permit data, you'll explore its contents. Geographic data doesn't only contain information about location; it can also include other attributes not seen on a map. 
# 
# Convert the layer into a spatially-enabled dataframe to explore these attributes.

# In[9]:


permit_layer


# In[10]:


sdf = pd.DataFrame.spatial.from_layer(permit_layer)


# `tail()` method gives the last 5 rows of the dataframe.

# In[11]:


sdf.tail()


# The permit data contains a long list of attributes. Some attributes have self-explanatory names, while others may have names that can be difficult to understand without context. The list of attributes can be obtained using the columns of the dataframe.

# In[12]:


sdf.columns


# In[13]:


sdf.describe().T


# Query the types of attributes and explore the data.

# In[14]:


sdf.dtypes


# In[15]:


sdf['work_type'].unique()


# In[16]:


sdf['status'].unique()


# In[17]:


sdf['use_code'].unique()


# ## Permits by Status

# The `groupby()` method groups the rows per the column and does calculations, such as finding their counts, as shown in the following code.

# In[18]:


permits_by_status = sdf.groupby(sdf['status']).size()
permits_by_status


#  There are only four permit statuses: Issued, Finaled, Open, and Stop Work. To visualize the number of permits for each status, you'll create a pie chart.

# Since the dataframe attributes just show the count of status, you can consider any attribute to graph the status count.

# In[19]:


get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib.pyplot as plt


# In[20]:


plt.axis('equal') 
permits_by_status.plot(kind='pie', legend=False, label='Permits by Status');


# The pie chart above shows the four permit statuses, with the size of each status determined by the number of permits. The vast majority of permits are either Issued or Finaled. Finaled permits are issued permits that have also had the requisite inspections performed.

# It's helpful to visualize the spatial distribution of permit attributes on a map. You'll change the map so that each permit's symbol represents its status.

# In[21]:


permits_by_status_map = agol_gis.map('Montgomery County, Maryland')
permits_by_status_map


# ![](insights_images/image_6.png)

# In[22]:


sdf.spatial.plot(kind='map', map_widget=permits_by_status_map,
        renderer_type='u', # specify the unique value renderer using its notation 'u'
        col='status')  # column to get unique values from


# ## Permits by Type

# In[23]:


permits_by_type = sdf.groupby(['use_code']).size()
permits_by_type


# The series is not sorted properly. Use the `sort()` method to sort it from highest count to lowest count. The most common use code, **Business Buildings**, has almost twice as many permits as the second highest, **Multi-family Dwelling**. The top four use codes together comprise the majority of all permits, so these use codes may be the most important to focus on in your analysis later.

# In[24]:


permits_by_type.sort_values(ascending=False, inplace=True)
permits_by_type.head()


# ### Clean up the data

# Before you begin analysis of your data, you'll hide attribute fields you don't intend to use, rename fields with unclear names, and filter your dataset to only show permits with the four most common use codes. These changes won't permanently affect the original dataset, but they will make the data easier to work with and understand.

# **'Declared_V'**, **'Building_A'**, **'Applicatio'** attribute fields describe aspects of the data that aren't important for your analysis. You'll drop these fields.

# In[25]:


sdf.drop(['declared_v', 'building_a', 'applicatio'], axis=1, inplace=True)


# In[26]:


sdf.columns


# The fields are no longer listed. 

# Next, you'll rename some of the attribute fields with shortened or unclear names so that their names are more descriptive.

# In[27]:


sdf.rename(columns={"descriptio": "Description", "bldgareanu": "Building_Area", "declvalnu": "Declared_Value"}, inplace=True)


# In[28]:


sdf.columns


# There are other fields that you may want to either rename or remove, but for the purposes of this lesson, these are enough. 

# ### Filter the permits
# 
# Next, you'll filter the permits to reduce the number of records in your analysis. As you saw previously, there are four types of permits that comprise over half the total number of permits. Focusing your analysis on just these four types will reduce the amount of data to analyze without ignoring the most important types of development. To remove the other use codes, you'll create a filter.

# In[29]:


permits_by_type.head(4) # top 4 Use_Codes


# In[30]:


filtered_permits = list(permits_by_type.head(4).index)
filtered_permits


# To visualize the top 4 Use Codes on a map, you can filer the dataframe with Use_Code containing only the top 4 attribute value.

# In[31]:


filtered_df = sdf.loc[sdf['use_code'].isin(filtered_permits)]


# In[32]:


filtered_df.head()


# In[33]:


sdf.shape, filtered_df.shape


# The dataset is filtered. Instead of more than 11,000 permits, the filtered dataframe has about 7,500.

# ### Visualize filtered dataset

# In[34]:


filtered_map = agol_gis.map('Montgomery County, Maryland')


# In[35]:


filtered_map


# ![](insights_images/image_7.png)

# In[36]:


filtered_df.spatial.plot(kind='map', map_widget=filtered_map,
        renderer_type='u', # specify the unique value renderer using its notation 'u'
        col='use_code')  # column to get unique values from


# ## Visualize temporal and spatial trends

# Your data show permits, but what do these permits say about when and where growth is happening in the county? Your data also contains temporal attribute fields, such as **Added_Date**, which indicates when a permit was first added to the system. The field has several values that break down the data by year, month, and even hour.

# Split the **Added_date** to get year, month, week_of_day

# In[37]:


sdf['datetime'] = pd.to_datetime(sdf['added_date'], unit='ms')
sdf['year'], sdf['month'], sdf['day_of_week'] = sdf.datetime.dt.year, sdf.datetime.dt.month, sdf.datetime.dt.dayofweek


# ### Visualize permits by time of issue
# You'll create chart cards for the year, month, and day subfields to visualize patterns in permit activity over time.

# In[38]:


import seaborn as sns


# In[39]:


sns.countplot(x="year", data=sdf);


# The chart shows the number of permits issued each year since 2010. (The year 2017 has significantly fewer permits because the dataset only covers part of 2017.) You can compare the number of permits visually by the size of each bar. Although some fluctuation occurs from year to year, most years had similar permit activity.
# 
# Similarly you can visualize it by month as well as day_of_week

# In[40]:


sns.countplot(x="month", data=sdf);


# This bar chart changes to show the number of permits issued by month. Based on the chart, the highest permit activity occurs in June and July.

# In[41]:


sns.countplot(x="day_of_week", data=sdf);


# Almost all permit activity occurs on weekdays. Government offices are closed on weekends, so few permits are issued then.

# In[42]:


ddf = sdf.set_index('datetime')


# In[43]:


ddf['num'] = 1
ddf['num'].resample('M').sum().plot();


# A huge spike in permit activity occurred in mid-2011. What caused this spike? Is it an increase in overall permit activity, or is it mostly an increase in a certain type of permit? You'll plot the number of permits based on Use_Code to find which one cased the spike.

# In[44]:


fig = plt.figure(figsize=(15,5))
ax = fig.add_subplot(1, 1, 1)

ax.plot(ddf['num'].resample('M').sum(), 'k', label='Total permits')
for use_code in filtered_permits:
    x = ddf[ddf.use_code == use_code]['num'].resample('M').sum()
    ax.plot(x, label=use_code)
ax.legend();


# Based on the legend, permit activity spiked in 2011 due to a sharp increase in the number of multifamily dwelling permits issued. This likely means that there was large residential growth in 2011.
# 
# You've investigated some temporal patterns in your data. Next, you'll look at spatial patterns. Are there certain areas in the county that have experienced a relatively high degree of permit activity? Was the 2011 spike in residential permits in a specific location? To find out, you'll change the symbology of the map card to show hot spots, or areas with concentrations of points.

# In[45]:


hotspot_map = agol_gis.map('Germantown, Montgomery County, Maryland')
hotspot_map


# In[46]:


sdf.spatial.plot(kind='map', map_widget=hotspot_map,
        renderer_type='h', 
        col='status') 


# ![](insights_images/hotspot.png)

# The hot spots show up  where there is a high concentration of permits. The highest concentration areas are in the southeast and northwest corners of the county, which correspond to the major population centers of Germantown and the suburban communities near Washington, D.C.
# 
# Next, you'll see if the 2011 permit spike corresponds to a specific area of the map. The code below filters the dataframe to only show permits from 2011 and highlights related data in the map. In this case, the heat map changes to show the hot spot in the northwest part of the county, near Germantown.

# In[47]:


hotspot_2011_map = agol_gis.map('Germantown, Montgomery County, Maryland')
hotspot_2011_map


# In[48]:


sdf[sdf.year==2011].spatial.plot(kind='map', map_widget=hotspot_2011_map,
        renderer_type='h',
        col='status')  # column to get unique values from


# ![](insights_images/filtered_hotspot.png)


# ====================
# analyze_patterns_in_construction_permits_part2.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Data Summarization - Construction permits, part 2/2

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Aggregate-points" data-toc-modified-id="Aggregate-points-1">Aggregate points</a></span></li><li><span><a href="#Aggregation-results" data-toc-modified-id="Aggregation-results-2">Aggregation results</a></span></li><li><span><a href="#Enrich-the-data" data-toc-modified-id="Enrich-the-data-3">Enrich the data</a></span></li><li><span><a href="#Share-your-work" data-toc-modified-id="Share-your-work-4">Share your work</a></span></li></ul></div>

# In the "Explore and analyze construction permits" notebook, we explored your data and learned a little about the spatial and temporal trends of permit activity in Montgomery County. In this lesson, we'll move beyond exploration and run spatial analysis tools to answer specific questions that can't be answered by the data itself. In particular, we want to know why permits spiked in Germantown in 2011 and predict where future permit spikes - and, by extension, future growth - are likely to occur.
# 
# First, we'll aggregate the points by ZIP Code. We'll enrich each ZIP Code with demographic information and learn more about the demographic conditions that led to such rapid growth in such a short time. Once you determine why growth occurred where and when it did, we'll locate other ZIP Codes with similar demographic characteristics to predict future growth.

# ## Aggregate points

# In[1]:


from arcgis import GIS


# In[2]:


gis = GIS("home")


# In[3]:


data = gis.content.search("Commercial_Permits_since_2010 owner:api_data_owner",
                          'Feature layer',
                           outside_org=True)
data[0]


# In[4]:


permits = data[0]
permit_layer = permits.layers[0]


# Use authoritative data from Living Atlas to assign a variable to the item.

# In[5]:


zip_item = gis.content.get('a1569e93ecd2408d89f42e8770a90f76')
zip_item


# Since the item is a feature layer collection, using the layers property will give us a list of layers.

# In[6]:


for lyr in zip_item.layers:
    print(lyr.properties.name)


# In[7]:


zip_code_layer = zip_item.layers[3]


# Next, you'll use this layer to aggregate permit points. By default, the parameters are set to use the ZIP Codes as the area layer, the permits as the layer to be aggregated, and the layer style to be based on permit count. These parameters are exactly what you want.

# In[8]:


from arcgis.features.summarize_data import aggregate_points
from datetime import datetime as dt


# In[9]:


permit_agg_by_zip = aggregate_points(permit_layer, zip_code_layer, 
                                     keep_boundaries_with_no_points=False,
                                     output_name='zipcode_aggregate_' + str(dt.now().microsecond))


# In[10]:


permit_agg_by_zip


# ## Aggregation results

# In[11]:


agg_map = gis.map("Montgomery County, Maryland")
agg_map.add_layer(permit_agg_by_zip)


# In[12]:


agg_map


# ![aggregatemap2.PNG](attachment:aggregatemap2.PNG)

# The new layer looks like a point layer, but it's actually a polygon layer with a point symbology. Each point represents the number of permits per ZIP Code area. Larger points indicate ZIP Codes with more permits.

# In[13]:


import pandas as pd


# In[14]:


sdf = pd.DataFrame.spatial.from_layer(permit_agg_by_zip.layers[0])


# In[15]:


sdf.head(10)


# Review some basic statistics about the data.

# In[16]:


sdf['Point_Count'].mean()


# In[17]:


sdf['Point_Count'].max()


# In[18]:


sdf['Point_Count'].min()


# In[19]:


agg_layer = permit_agg_by_zip.layers[0]


# Although most of the large point symbols on the map are in the southeast corner, near Washington, D.C., there are a few large points in the northwest. In particular, there is a very large circle in the ZIP Code located in Clarksburg. (If you're using different ZIP Code data, this area may be identified as ZIP Code 20871 instead.) The ZIP code has 948 permits. Additionally, this area geographically corresponds to the hot spot you identified in the previous lesson. This ZIP Code is one that you'll focus on when you enrich your layer with demographic data.

# ## Enrich the data
# 
# Are there demographic characteristics about the Clarksburg ZIP Code that contributed to its high growth? If so, are there other areas with those characteristics that may experience growth in the future? To answer these questions, you'll use the **Enrich Data** analysis tool. This tool adds demographic attributes of your choice to your data. Specifically, you'll add Tapestry information to each ZIP Code. <a href="https://doc.arcgis.com/en/esri-demographics/latest/regional-data/tapestry-segmentation.htm">Tapestry</a> is a summary of many demographic and socioeconomic variables, including age groups and lifestyle choices. It'll teach you more about the types of people who live in your area of interest and help you better understand the reasons why growth happened where it did.

# In[20]:


from arcgis.features.enrich_data import enrich_layer


# The analysis variable **TSEGNAME** is 2022 Dominant Tapestry Segment Name such as Exurbanites, City Lights, Metro Renters, etc. The detailed summary of various segments can be found in <a href="https://doc.arcgis.com/en/esri-demographics/latest/regional-data/tapestry-segmentation.htm#ESRI_SECTION1_209EDB8ACF264A4785CE8D7C9210949D:~:text=Segmentation%20Methodology%20Statement-,Tapestry%20segment%20summaries,-The%2067%20distinct">here</a>.

# In[21]:


enrich_aggregate = enrich_layer(agg_layer, 
                                analysis_variables=["AtRisk.TSEGNAME"],
                                output_name="added_tapestry_var_" + str(dt.now().microsecond))


# In[22]:


enrich_aggregate


# In[23]:


agg_lyr = enrich_aggregate.layers[0]


# In[24]:


sdf = pd.DataFrame.spatial.from_layer(agg_lyr)


# In[25]:


sdf.head()


# In[26]:


enrich_aggregate_map = gis.map('Montgomery County, Maryland')


# In[27]:


enrich_aggregate_map


# ![MicrosoftTeams-image.png](attachment:MicrosoftTeams-image.png)

# In[28]:


sdf.spatial.plot(kind='map', map_widget=enrich_aggregate_map,
        renderer_type='u',
        col='TSEGNAME') 


# Click some of the ZIP Codes.
# 
# The Tapestry segment is displayed when you click a ZIP Code. The Tapestry segments have names such as Enterprising Professionals and Savvy Suburbanites. You can look up more information about each segment, including its specific demographic characteristics, on the<a href="http://doc.arcgis.com/en/esri-demographics/data/tapestry-segmentation.htm"> Tapestry Segmentation help page</a>.

# What Tapestry segment is dominant for the Clarksburg ZIP Code where major growth occurred? Click the Clarksburg ZIP Code to find out. According to the pop-up, Boomburbs is the dominant Tapestry segment for the ZIP Code. Boomburbs have many young professionals with families living in affordable new housing. This description may explain why the area saw such rapid residential growth in 2011. It's possible that other ZIP Codes with similar demographic profiles may experience rapid growth in the near future.

# Click the ZIP Code directly southwest of Clarksburg.
# 
# This ZIP Code is in Boyds. It also has the Boomburbs Tapestry segment. However, its number of permits has been relatively low since 2010. The county may be able to anticipate a similar spike in permit activity in this area.
# 
# Although Tapestry segments are based on several demographic characteristics, you could also perform this analysis with other variables. For instance, you could determine if there is a correlation between high permit activity and high population growth. Is a young population or a high income level a stronger indicator of growth? You can answer these questions and others with the analysis tools at your disposal. For the purposes of this lesson, however, your results are satisfactory.

# In[29]:


enrich_aggregate_map.add_layer(agg_lyr, {'renderer':'ClassedSizeRenderer',
                                             'field_name':'POPULATION',
                                             'opacity':0.75})


# ## Share your work

# We've analyzed your data and come to a couple conclusions about your data. Next, we'll share your results online. Currently, our result layers are layers that are accessible only to you. Sharing our data will make it easier for county officials to use your data in other ArcGIS applications and communicate key information to the public. In particular, we'll share your work to ArcGIS Online. We'll share your enriched ZIP Codes dataset as feature layers that can be added to any web map. 
# 
# The layer contains fields for both the count of permits per ZIP Code and the dominant Tapestry segment—basically all of the result data we created in your analysis. We'll only need to share this layer, not the original aggregation layer.
# 
# Using the `share()` method you can share your work with others.

# In[30]:


enrich_aggregate.share(everyone=True)


# In this notebook, we used ArcGIS API for Python to explore and analyze permit data for Montgomery County, Maryland. You answered questions about your data's spatial and temporal trends and located areas of the county with rapid growth. We compared your findings with demographic data, came to conclusions about the possible causes of growth, and even predicted an area that may experience similar growth in the future based on shared demographic characteristics. With ArcGIS API for Python, we can perform a similar workflow on any of your data to better understand what it contains and what questions it can answer.


# ====================
# analyze_us_tornadoes.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Analyzing United States tornadoes

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Connect-your-GIS" data-toc-modified-id="Connect-your-GIS-1">Connect your GIS</a></span></li><li><span><a href="#Who-has-suffered-the-most?" data-toc-modified-id="Who-has-suffered-the-most?-2">Who has suffered the most?</a></span></li><li><span><a href="#Visualize-which-states-have-had-the-most-tornadoes." data-toc-modified-id="Visualize-which-states-have-had-the-most-tornadoes.-3">Visualize which states have had the most tornadoes.</a></span></li><li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-4">Conclusion</a></span></li></ul></div>

# Tornadoes occur in many parts of the world, including Australia, Europe, Africa, Asia, and South America, but they are found most frequently in the United States. Outside the United States, two of the highest concentrations of tornadoes are found in Argentina and Bangladesh.
# 
# Tornadoes are the most violent of all atmospheric storms and in the United States alone have caused an average of more than 80 deaths and 1,400 injuries each year (based on 1950–2011 data). A tornado is a narrow, violently rotating column of air that extends from the base of a thunderstorm to the ground. Tornado formation is complex, and no two tornadoes are the same; however, they need certain conditions to form, including intense or unseasonable heat. Wind speed within a tornado can vary from just above 0 mph up to 70 mph, with an average of 30 mph (NOAA). The Fujita damage scale is used to quantify the intensity of a tornado.

# 

# Explore the map of tornadoes across the United States: Twister! United States tornadoes from 1950 to 2012. Which states have had the most tornadoes? Using tornado location data from the United States severe weather report database, provided by the National Oceanic and Atmospheric Administration (NOAA)/National Weather Service Storm Prediction Center (http://www.spc.noaa.gov/gis/svrgis/), you can find the total number of tornadoes by state.

# ## Connect your GIS

# In[1]:


from arcgis.gis import GIS


# To create the GIS object, we pass in the url and our login credentials as shown below

# In[2]:


gis = GIS('home')


# `Search`
# for the title: **tornado_lyr** layer.You can search the GIS for feature layer collections by specifying the item type as 'Feature Layer Collection' or 'Feature Layer'. You can also mention the owner name of the layer to get better search results.

# In[3]:


data = gis.content.search('Tornadoes_and_Tracks owner: api_data_owner',
                          'Feature layer', outside_org=True)


# Display the list of results.

# In[4]:


from IPython.display import display

for item in data:
    display(item)


# Get the first item from the results.

# In[5]:


item = data[0] #tornado_lyr 


# The code below cycles through the layers and print their names.

# In[6]:


for lyr in item.layers:
    print(lyr.properties.name)


# Since the item is a Feature Layer Collection, accessing the layers property gives us a list of FeatureLayer objects. The StatePop_5011 layer is the first layer in this item.

# In[7]:


#StatePop_5011
boundary = item.layers[4]


# In[8]:


#Tornadoes_5011
tornado_lyr = item.layers[0]


# `Aggregate_points` tool summarizes data from spot measurements by area. To learn more about this tool and the formula it uses, refer to the documentation <a href="http://doc.arcgis.com/en/arcgis-online/analyze/aggregate-points.htm">here</a>.

# In[9]:


from arcgis.features.summarize_data import aggregate_points
from datetime import datetime as dt


# Please change the output_name if this is not the first you run the cell below.

# In[10]:


agg_points = aggregate_points(point_layer=tornado_lyr, 
                              polygon_layer=boundary, 
                              keep_boundaries_with_no_points=True,
                              output_name="agg_tornado_points" + str(dt.now().microsecond))


# In[11]:


agg_points


# In[12]:


agg_lyr = agg_points.layers[0]


# The GIS object includes a map widget for displaying geographic locations, visualizing GIS content, as well as the results of your analysis. To use the map widget, call ``gis.map()`` and assign it to a variable.

# In[12]:


m1 = gis.map('US')
m1


# In[13]:


m1.add_layer(agg_lyr)


# ## Who has suffered the most?

# There can be many devastating effects from a tornado, including loss of life, injuries, property damage, and financial losses. To identify populations that have been affected by tornadoes, you can aggregate the number of tornadoes to the state level and normalize by population.

# In[14]:


m2 = gis.map('US', zoomlevel=4)
m2


# ![](tornado_img/5.PNG)

# In[15]:


m2.add_layer(agg_lyr, {
               "renderer":"ClassedColorRenderer",
               "field_name":"AvgPop",
               "opacity":0.7
              })


# In[16]:


m2.legend = True


# Feature layers hosted can be easily read into a Spatially Enabled DataFrame using the from_layer method. Once you read it into a SEDF object, you can create reports, manipulate the data, or convert it to a form that is comfortable and makes sense for its intended purpose.

# In[13]:


import pandas as pd
sdf = pd.DataFrame.spatial.from_layer(agg_lyr)


# In[14]:


sdf.head()


# In[15]:


sdf.shape


# ## Visualize which states have had the most tornadoes.

# In[16]:


sdf.sort_values(by='Point_Count', ascending=False, axis=0, inplace=True)


# In[17]:


df = sdf[0:10]


# In[18]:


import matplotlib.pyplot as plt


# In[19]:


plt.figure(figsize=(10,7))
plt.bar(df.NAME, df.Point_Count)
plt.xlabel('NAME', fontsize=7)
plt.ylabel('point_count', fontsize=7)
plt.xticks(fontsize=8, rotation=45)
plt.title('The ten states with the highest number of tornadoes')
plt.show()


# Number of tornadoes by state, 1950–2011
# 
# Some states are subject to many more tornadoes than others. Over a 62-year period (1950–2011), Texas had by far the most tornadoes (with 7,935), followed by Kansas (with 3,713), while others such as Vermont, Rhode Island, and the District of Columbia had fewer than 50. The ten states shown in the graph below had 20 percent of the total number of tornadoes.

# ## Conclusion
# 
# In this notebook, we demonstrate how to use aggregation analysis to summarize the number of data points within each polygon. Thus, using `aggregate_points()` method, we arrive at the number of tornadoes that hit each state. and published the aggregation results as an online service. Mapping results often show new insights that lead to deeper understanding and more clearly defined analysis.


# ====================
# analyzing_growth_factors_of_airbnb_properties_in_new_york_city.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Analysing the factors of growth and spatial distribution of Airbnb properties across New York City

# ## Table of Contents
# * [Introduction](#1) 
# * [Necessary Imports](#117)
# * [Accessing the Airbnb data as of 2019 and the NYC tracts dataset](#69) 
# * [Visualizing dataset](#50) 
# * [Aggregating Airbnb count by Tracts for NYC](#81)   
# * [Importing demographic data using geoenrichment service](#133)  
# * [Estimating distances of tracts from various city features](#347)   
# * [Importing Borough Info for each Tract](#508)  
# * [Merging all the above estimated data set of features](#504) 
# * [Adding census data 2019 obtained using data enrich tool](#612)    
# * [Model Building](#544)
#     * [Random Forest Regressor Model](#643)  
#     * [Feature importance of Random Forest model](#641) 
#     * [Gradient Boosting Regressor Model](#645)   
#     * [Feature Importance of Gradient Boosting Model](#647)   
# * [Running cross validation](#651)
# * [Result Visualization](#652)
# * [Conclusion](#655)
# * [Data resources](#656) 
# * [Summary of methods used](#657)

# ## Introduction <a class="anchor" id="1"></a>
# 
# 
# Airbnb properties across cities are a great alternative for travellers to find comparatively cheaper accommodation. It also provides homeowners opportunities to utilize spare or unused rooms as an additional income source. However in recent times the alarming spread of Airbnb properties has become a topic of debate among the public and the city authorities across the world.
# 
# Considering the above, a study is carried out in this sample notebook to understand the factors that are fuelling widespread growth in the number of Airbnb listings. These might include location characteristics of concerned neighbourhoods (which in this case, NYC census tracts) and as well as qualitative information about the inhabitants residing in them. The goal is to help city planners deal with the negative externalities of the Airbnb phenomenon (and similar short term rentals) by making informed decision on framing suitable policies. 
# 
# The primary data is downloaded from the [Airbnb website](http://insideairbnb.com/get-the-data.html) for the city of New York. Other data includes 2019 and 2017 census data using Esri's enrichment services, and various other datasets from the [NYCOpenData](https://opendata.cityofnewyork.us/) portal.

# ## Necessary Imports <a class="anchor" id="117"></a>

# In[1]:


get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib.pyplot as plt


from datetime import datetime as dt
import pandas as pd
import numpy as np
from IPython.display import display, HTML
from IPython.core.pylabtools import figsize
import seaborn as sns


# Machine Learning models
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
import sklearn.metrics as metrics
from sklearn import preprocessing

# Arcgis api imports
import arcgis
from arcgis.geoenrichment import Country
from arcgis.features import summarize_data
from arcgis.features.enrich_data import enrich_layer
from arcgis.features import SpatialDataFrame
from arcgis.features import use_proximity 
from arcgis.gis import GIS
from arcgis.features import summarize_data


# In[2]:


gis = GIS('home')


# ### Access the NYC Airbnb and Tracts dataset <a class="anchor" id="69"></a>
# 
# Airbnb Data - It contains information about 48,000 Airbnb properties available in New York as of 2019. These include location of the property, its neighbourhood characters and transit facilities available, information about the owner, details of the room including number of bedrooms etc., and rental price per night.
# 
# NYC Tracts - It is a polygon shapefile consisting 2167 tracts of New York City, including area of the tracts along with unique id for each tract.
#      

# In[3]:


# Accessing NYCTracts
nyc_tract_full = gis.content.search('NYCTractData owner:api_data_owner', 'feature layer')[0]
nyc_tract_full


# In[4]:


nyc_tracts_layer = nyc_tract_full.layers[0]


# In[5]:


# Accessing airbnb NYC
airbnb_nyc2019 = gis.content.search('AnBNYC2019 owner:api_data_owner', 'feature layer')[0]
airbnb_nyc2019


# In[6]:


airbnb_layer = airbnb_nyc2019.layers[0]


# ### Visualizing dataset <a class="anchor" id="50"></a>

# In[7]:


# NYC Tracts
m1 = gis.map('New York City')
m1.add_layer(nyc_tracts_layer)
m1


# In[8]:


# NYC Airbnb Properties
m = gis.map('Springfield Gardens, NY')
m.add_layer(airbnb_layer)
m


# In[9]:


# extracting the dataframe from the layer and visualize it as a pandas dataframe
pd.set_option('display.max_columns', 110)
sdf_airbnb_layer = pd.DataFrame.spatial.from_layer(airbnb_layer)
sdf_airbnb_layer.head(2)


# ### Aggregating number of Airbnb properties by Tracts for NYC <a class="anchor" id="81"></a>
# Number of Airbnb properties per tract is to be estimated using the polygon tract layer and the Airbnb point layer.
# 
# The Aggregate Points tool uses area features to summarize a set of point features. The boundaries from the area feature are used to collect the points within each area and use them to calculate statistics. The resulting layer displays the count of points within each area. Here, the polygon tract layer is used as the area feature, and the Airbnb point layer is used as the point feature.

# In[10]:


agg_result = summarize_data.aggregate_points(point_layer=airbnb_layer,
                                             polygon_layer=nyc_tracts_layer,
                                             output_name='airbnb_counts'+ str(dt.now().microsecond))


# In[11]:


agg_result


# In[12]:


# mapping the aggregated airbnb data with darker areas showing more airbnb properties per tract
aggr_map = gis.map('NY', zoomlevel=10)
aggr_map.add_layer(agg_result,{"renderer":"ClassedColorRenderer", "field_name": "Point_Count"})
aggr_map


# In[13]:


airbnb_count_by_tract = agg_result.layers[0]


# In[14]:


sdf_airbnb_count_by_tract = airbnb_count_by_tract.query().sdf


# In[15]:


sdf_airbnb_count_by_tract = sdf_airbnb_count_by_tract.sort_values('geoid')
sdf_airbnb_count_by_tract.head()


# Here the Point_Count field from the above aggregated dataframe returns the number of Airbnb properties per tract. This would form the target variable for this problem.

# ### Enriching tracts with demographic data using geoenrichment service from Esri <a class="anchor" id="133"></a>
# The feature data is now created using selected demographics information for each tracts. This is accomplished accessing the geoenrichment services from Esri, which consists the latest census data. The entire data repository is first visualized, out of which the relevant variables are finalized from a literature study. These selected variables are searched for adding in the feature set.

# In[16]:


# Displaying the various data topic available for geoenrichment for USA in the Esri database
usa = Country.get('US')
type(usa)
usa_data = usa.data_collections
df_usa_data = pd.DataFrame(usa_data)
df_usa_data.head()


# All the data topics are visualized that are available in the geoenrichment services.

# In[17]:


# Filtering the unique topic under dataCollectionID
df_usa_data.reset_index(inplace=True)
list(df_usa_data.dataCollectionID.unique())


# Items can be searched using alias field, for the related analysis variable name --  here as an example a variable with 'Nonprofit' is searched. Out of the these the relevant 'Nonprofit' data is to be selected.

# In[18]:


df_usa_data[df_usa_data['alias'].str.contains('Nonprofit')]                        


# Adding data using enrichment - At this stage a literature study is undertaken to narrow down the various factors that might impact opening of new Airbnb properties in NYC. 
# 
# Subsequently these factors are identified from the USA geoenrichment database as shown above. These variable names are then compiled in a dictionary for passing them to the enrichment tool.

# In[19]:


enrichment_variables = {'classofworker.ACSCIVEMP':      'Employed Population Age 16+',
 'classofworker.ACSMCIVEMP':                      'Employed Male Pop Age 16+',
 'classofworker.ACSMPRIVNP':                      'Male 16+Priv Nonprofit',
 'classofworker.ACSMEPRIVP':                      'Male 16+:Priv Profit Empl',
 'classofworker.ACSMSELFI':                       'Male 16+:Priv Profit Self Empl',
 'classofworker.ACSMSTGOV':                       'Male 16+:State Govt Wrkr',
 'classofworker.ACSMFEDGOV':                      'Male 16+:Fed Govt Wrkr',
 'classofworker.ACSMSELFNI':                      'Male 16+:Self-Emp Not Inc',
 'classofworker.ACSMUNPDFM':                      'Male 16+:Unpaid Family Wrkr',              
 'classofworker.ACSFCIVEMP':                      'Female Pop Age 16+',
 'classofworker.ACSFEPRIVP':                      'Female 16+:Priv Profit Empl',
 'classofworker.ACSFSELFI':                       'Female 16+:Priv Profit Self Empl',                      
 'classofworker.ACSFPRIVNP':                      'Female 16+:Priv Nonprofit',
 'classofworker.ACSFLOCGOV':                      'Female 16+:Local Govt Wrkr',
 'classofworker.ACSFSTGOV':                       'Female 16+:State Govt Wrkr',
 'classofworker.ACSFFEDGOV':                      'Female 16+:Fed Govt Wrkr',                      
 'classofworker.ACSFSELFNI':                      'Female 16+:Self-Emp Not Inc',                      
 'classofworker.ACSFUNPDFM':                      'Female 16+:Unpaid Family Wrkr',                      
 'gender.MEDAGE_CY':                              '2019 Median Age',
 'Generations.GENALPHACY':                        '2019 Generation Alpha Population',
 'Generations.GENZ_CY':                           '2019 Generation Z Population',
 'Generations.MILLENN_CY':                        '2019 Millennial Population',
 'Generations.GENX_CY':                           '2019 Generation X Population',
 'Generations.BABYBOOMCY':                        '2019 Baby Boomer Population',
 'Generations.OLDRGENSCY':                        '2019 Silent & Greatest Generations Population',
 'Generations.GENBASE_CY':                        '2019 Population by Generation Base',
 'populationtotals.POPDENS_CY':                   '2019 Population Density',
 'DaytimePopulation.DPOP_CY':                     '2019 Total Daytime Population',
 'raceandhispanicorigin.WHITE_CY':                '2019 White Population',
 'raceandhispanicorigin.BLACK_CY':                '2019 Black Population',
 'raceandhispanicorigin.AMERIND_CY':              '2019 American Indian Population',
 'raceandhispanicorigin.ASIAN_CY':                '2019 Asian Population',
 'raceandhispanicorigin.PACIFIC_CY':              '2019 Pacific Islander Population',
 'raceandhispanicorigin.OTHRACE_CY':              '2019 Other Race Population',
 'raceandhispanicorigin.DIVINDX_CY':              '2019 Diversity Index',
 'households.ACSHHBPOV':                          'HHs: Inc Below Poverty Level',
 'households.ACSHHAPOV':                          'HHs:Inc at/Above Poverty Level',
 'households.ACSFAMHH':                           'ACS Family Households',
 'businesses.S01_BUS':                            'Total Businesses (SIC)',
 'businesses.N05_BUS':                            'Construction Businesses (NAICS)',
 'businesses.N08_BUS':                            'Retail Trade Businesses (NAICS)',
 'businesses.N21_BUS':                            'Transportation/Warehouse Bus (NAICS)',
 'ElectronicsInternet.MP09147a_B':                'Own any tablet',
 'ElectronicsInternet.MP09148a_B':                'Own any e-reader',
 'ElectronicsInternet.MP19001a_B':                'Have access to Internet at home',                
 'ElectronicsInternet.MP19070a_I':                'Index: Spend 0.5-0.9 hrs online(excl email/IM .',               
 'ElectronicsInternet.MP19071a_B':                'Spend <0.5 hrs online (excl email/IM time) daily',
 'populationtotals.TOTPOP_CY':                    '2019 Total Population',              
 'gender.MALES_CY':                               '2019 Male Population',
 'gender.FEMALES_CY':                             '2019 Female Population',
 'industry.EMP_CY':                               '2019 Employed Civilian Pop 16+',
 'industry.UNEMP_CY':                             '2019 Unemployed Population 16+',                     
 'industry.UNEMPRT_CY':                           '2019 Unemployment Rate',
 'commute.ACSWORKERS':                            'ACS Workers Age 16+',
 'commute.ACSDRALONE':                            'ACS Workers 16+: Drove Alone',
 'commute.ACSCARPOOL':                            'ACS Workers 16+: Carpooled',
 'commute.ACSPUBTRAN':                            'ACS Workers 16+: Public Transportation',
 'commute.ACSBUS':                                'ACS Workers 16+: Bus',
 'commute.ACSSTRTCAR':                            'ACS Workers 16+: Streetcar',
 'commute.ACSSUBWAY':                             'ACS Workers 16+: Subway',
 'commute.ACSRAILRD':                             'ACS Workers 16+: Railroad',
 'commute.ACSFERRY':                              'ACS Workers 16+: Ferryboat',
 'commute.ACSTAXICAB':                            'ACS Workers 16+: Taxicab',           
 'commute.ACSMCYCLE':                             'ACS Workers 16+: Motorcycle',
 'commute.ACSBICYCLE':                            'ACS Workers 16+: Bicycle',                             
 'commute.ACSWALKED':                             'ACS Workers 16+: Walked',
 'commute.ACSOTHTRAN':                            'ACS Workers 16+: Other Means',
 'commute.ACSWRKHOME':                            'ACS Wrkrs 16+: Worked at Home',
 'OwnerRenter.OWNER_CY':                          '2019 Owner Occupied HUs', 
 'OwnerRenter.RENTER_CY':                         '2019 Renter Occupied HUs', 
 'vacant.VACANT_CY':                              '2019 Vacant Housing Units', 
 'homevalue.MEDVAL_CY':                           '2019 Median Home Value',
 'housingunittotals.TOTHU_CY':                    '2019 Total Housing Units',
 'yearbuilt.ACSMEDYBLT':                          'ACS Median Year Structure Built: HUs',
 'SpendingTotal.X1001_X':                         '2019 Annual Budget Exp',
 'transportation.X6001_X':                        '2019 Transportation',
 'households.ACSTOTHH':                           'ACS Total Households',
 'DaytimePopulation.DPOPWRK_CY':                  '2019 Daytime Pop: Workers',
 'DaytimePopulation.DPOPRES_CY':                  '2019 Daytime Pop: Residents',
 'DaytimePopulation.DPOPDENSCY':                  '2019 Daytime Pop Density',
 'occupation.OCCPROT_CY':                         '2019 Occupation: Protective Service',
 'occupation.OCCFOOD_CY':                         '2019 Occupation: Food Preperation',
 'occupation.OCCPERS_CY':                         '2019 Occupation: Personal Care',
 'occupation.OCCADMN_CY':                         '2019 Occupation: Office/Admin',
 'occupation.OCCCONS_CY':                         '2019 Occupation: Construction/Extraction',
 'occupation.OCCPROD_CY':                         '2019 Occupation: Production'
                  }


# In[20]:


# Enrichment operation using ArcGIS API for Python 
enrichment_variables_df = pd.DataFrame.from_dict(enrichment_variables, orient='index',columns=['Variable Definition'])
enrichment_variables_df.reset_index(level=0, inplace=True)
enrichment_variables_df.columns = ['AnalysisVariable','Variable Definition']
enrichment_variables_df.head()


# In[21]:


# Convertng the variables names to list for passing them to the enrichment tool
variable_names = enrichment_variables_df['AnalysisVariable'].tolist()

# checking the firt few values of the list
variable_names[1:5]


# In[22]:


# Data Enriching operation
airbnb_count_by_tract_enriched = enrich_layer(airbnb_count_by_tract,
                                              analysis_variables = variable_names,
                                              output_name='airbnb_tract_enrich1'+ str(dt.now().microsecond))


# In[23]:


# Extracting the resulting enriched dataframe after the geoenrichment method
sdf_airbnb_count_by_tract_enriched = airbnb_count_by_tract_enriched.layers[0].query().sdf


# In[24]:


# Visualizing the data as a pandas dataframe
print(sdf_airbnb_count_by_tract_enriched.columns)
sdf_airbnb_count_by_tract_enriched_sorted = sdf_airbnb_count_by_tract_enriched.sort_values('geoid')
sdf_airbnb_count_by_tract_enriched_sorted.head()


# The field name of the enriched dataframe are code words which needs to be elaborated. Hence these are renamed with their actual definition from the variable definition of the list that was first created during selection of the variables.  

# In[25]:


enrichment_variables_df.head()


# In[26]:


enrichment_variables_copy = enrichment_variables_df.copy()
enrichment_variables_copy.head(2)


# In[27]:


enrichment_variables_copy['AnalysisVariable'] = enrichment_variables_copy.AnalysisVariable.str.split(pat='.', expand=True)[1]
enrichment_variables_copy


# In[28]:


enrichment_variables_copy.set_index("AnalysisVariable", drop=True, inplace=True)
dictionary = enrichment_variables_copy.to_dict()
new_columns = dictionary['Variable Definition']


# In[29]:


# Field renamed and new dataframe visualized
pd.set_option('display.max_columns', 150)
sdf_airbnb_count_by_tract_enriched_sorted.rename(columns=new_columns, inplace=True)
sdf_airbnb_count_by_tract_enriched_sorted.head()


# The renamed data frame above is now self explanatory hence more interpretable.

# ### Estimating distances of tracts from various city features <a class="anchor" id="347"></a>
# The next set of feature data set will be the distances of each of the tract from various city features. These distance variables accomplishes two important tasks. 
# 
# First is that they include the spatial components of the Airbnb development phenomenon into the model.
# 
# Secondly each Airbnb properties are impacted by unique locational factors. This is reflected from the Airbnb reviews where the most highly rated in demand Airbnb property are located in neighbourhood with good transit accessibility. Hence these are accounted into the model by including the distances of different public transit options from the tracts. 
# 
# The hypothesis formed here is that tracts located near transit hubs which could be subway station, bus stops, railroad lines, subway routes etc., might attract more Airbnb property. Similarly the central business district which for New York is located at lower Manhattan might also influence Airbnb properties, since this is the city's main business hub. In the following these various distances are estimated using ArcGIS API for Python proximity method.

# In[30]:


# accessing the various city feature shapefile from arcgis portal
busi_distr = gis.content.search('BusinessDistricts owner:api_data_owner', 'feature layer')[0]
cbd = gis.content.search('NYCBD owner:api_data_owner', 'feature layer')[0]
bus_stop = gis.content.search('NYCBusStop owner:api_data_owner', 'feature layer')[0]
hotels = gis.content.search('NYCHotels owner:api_data_owner', 'feature layer')[0]
railroad = gis.content.search('NYCRailroad owner:api_data_owner', 'feature layer')[0]
subwy_rt = gis.content.search('NYCSubwayRoutes owner:api_data_owner', 'feature layer')[0]
subwy_stn = gis.content.search('NYCSubwayStation owner:api_data_owner', 'feature layer')[0]


# In[31]:


bus_stop_lyr = bus_stop.layers[0]
cbd_lyr = cbd.layers[0] 
hotels_lyr = hotels.layers[0] 
subwy_stn_lyr =subwy_stn.layers[0]
subwy_rt_lyr = subwy_rt.layers[0] 
railroad_lyr = railroad.layers[0]
busi_distrs_lyr = busi_distr.layers[0] 


# In[32]:


# Avoid warning for chain operation
pd.set_option('mode.chained_assignment', None) 

# Estimating Tract to hotel distances
tract_hotel_dist = use_proximity.find_nearest(nyc_tracts_layer,
                                              hotels_lyr,
                                              measurement_type='StraightLine',
                                              max_count=1,
                                              output_name='ny_tract_hotel_dist1' + str(dt.now().microsecond))


# In[33]:


tract_hotel_dist.layers


# In[34]:


tract_hotel_dist_lyr = tract_hotel_dist.layers[1]
sdf_tract_hotel_dist_lyr = pd.DataFrame.spatial.from_layer(tract_hotel_dist_lyr)
sdf_tract_hotel_dist_lyr.head()


# In the above dataframe the Total_Miles field returns the distances of the tract from hotels in miles. Hence this field is converted into feet and retained. This is then repeated for each of the other distance estimation.

# In[35]:


# Final hotel Distances in feet — Here in each row column "hotel_dist" returns the distance of the nearest hotel from that tract indicated by its geoids.
# For example in the first row the tract with ID 36005000100 has a nearest hotel at 5571.75 feet away from it. 
sdf_tract_hotel_dist_lyr_new = sdf_tract_hotel_dist_lyr[['From_geoid', 'Total_Kilometers']]
sdf_tract_hotel_dist_lyr_new['hotel_dist'] = round(sdf_tract_hotel_dist_lyr_new['Total_Kilometers'] * 3280.84, 2)
sdf_tract_hotel_dist_lyr_new.sort_values('From_geoid').head()


# In[36]:


# Estimating Busstop distances from tracts
tract_bustop_dist = use_proximity.find_nearest(nyc_tracts_layer,
                                               bus_stop_lyr,
                                               measurement_type='StraightLine',
                                               max_count=1,
                                               output_name='ny_tract_bus_stop_dist'+ str(dt.now().microsecond))
tract_bustop_dist_lyr = tract_bustop_dist.layers[1]
sdf_tract_bustop_dist_lyr =tract_bustop_dist_lyr.query().sdf


# In[37]:


# Final Bustop Distances in feet — Here in each row column "busstop_dist" returns the distance of the nearest bus stop 
# from that tract indicated by its geoids 
sdf_tract_bustop_dist_lyr_new = sdf_tract_bustop_dist_lyr[['From_geoid', 'Total_Kilometers']]
sdf_tract_bustop_dist_lyr_new['busstop_dist'] = round(sdf_tract_bustop_dist_lyr_new['Total_Kilometers'] * 3280.84, 2)
sdf_tract_bustop_dist_lyr_new.sort_values('From_geoid').head()


# In[38]:


# estimating number of bus stops per tract
num_bustops_tracts = summarize_data.aggregate_points(point_layer=bus_stop_lyr,
                                                   polygon_layer=nyc_tracts_layer,
                                                   output_name='bustops_by_tracts'+ str(dt.now().microsecond)) 


# In[39]:


num_bustops_tracts_lyr = num_bustops_tracts.layers[0]
sdf_num_bustops_tracts_lyr = pd.DataFrame.spatial.from_layer(num_bustops_tracts_lyr)
sdf_num_bustops_tracts_lyr.head()


# In[40]:


# Number of Bus stops per tract — Here in each row column "num_bustop" returns the number of bus stops inside respective tracts 
sdf_num_bustops_tracts_lyr_new = sdf_num_bustops_tracts_lyr[['geoid', 'Point_Count']] 
sdf_num_bustops_tracts_lyr_new = sdf_num_bustops_tracts_lyr_new.rename(columns={'Point_Count':'num_bustop'})
sdf_num_bustops_tracts_lyr_new.sort_values('geoid').head()


# In[41]:


# estimating tracts distances from CBD 
tract_cbd_dist=use_proximity.find_nearest(nyc_tracts_layer,
                                          cbd_lyr,
                                          measurement_type='StraightLine',
                                          max_count=1,
                                          output_name='ny_tract_cbd_dist'+ str(dt.now().microsecond))
tract_cbd_dist_lyr = tract_cbd_dist.layers[1]
sdf_tract_cbd_dist_lyr = tract_cbd_dist_lyr.query().sdf
sdf_tract_cbd_dist_lyr.head()


# In[42]:


# Final CBD distances in feet — Here in each row the column "cbd_dst" returns the distance of the CBD from respective tracts
sdf_tract_cbd_dist_lyr_new = sdf_tract_cbd_dist_lyr[['From_geoid', 'Total_Kilometers']]
sdf_tract_cbd_dist_lyr_new['cbd_dist'] = round(sdf_tract_cbd_dist_lyr_new['Total_Kilometers'] * 3280.84, 2) 
sdf_tract_cbd_dist_lyr_new.sort_values('From_geoid').head()


# In[43]:


# Estimating NYCSubwayStation distances from tracts 
tract_subwy_stn_dist = use_proximity.find_nearest(nyc_tracts_layer,
                                                  subwy_stn_lyr,
                                                  measurement_type='StraightLine',
                                                  max_count=1,
                                                  output_name='ny_tract_subway_station_dist'+ str(dt.now().microsecond))
tract_subwy_stn_dist_lyr = tract_subwy_stn_dist.layers[1]
sdf_tract_subwy_stn_dist_lyr = pd.DataFrame.spatial.from_layer(tract_subwy_stn_dist_lyr)
sdf_tract_subwy_stn_dist_lyr.head()


# In[44]:


# Final Tract to NYC Subway Station distances in feet — Here in each row, column "subwy_stn_dist" returns the distance of
# the nearest subway station from that tract
sdf_tract_subwy_stn_dist_lyr_new = sdf_tract_subwy_stn_dist_lyr[['From_geoid', 'Total_Kilometers']]
sdf_tract_subwy_stn_dist_lyr_new['subwy_stn_dist'] = round(sdf_tract_subwy_stn_dist_lyr_new['Total_Kilometers'] * 3280.84, 2) 
sdf_tract_subwy_stn_dist_lyr_new.sort_values('From_geoid').head()


# In[45]:


# Estimating distances to NYCSubwayRoutes
tract_subwy_rt_dist=use_proximity.find_nearest(nyc_tracts_layer,
                                               subwy_rt_lyr,
                                               measurement_type='StraightLine',
                                               max_count=1,
                                               output_name='ny_tract_subway_routes_dist'+ str(dt.now().microsecond))
tract_subwy_rt_dist_lyr = tract_subwy_rt_dist.layers[1]
sdf_tract_subwy_rt_dist_lyr = tract_subwy_rt_dist_lyr.query().sdf
sdf_tract_subwy_rt_dist_lyr.head()


# In[46]:


# Final Tract to NYCSubwayRoutes distances in feet — Here in each row, column "subwy_rt_dist" returns the distance of
# the nearest subway route from that tract
sdf_tract_subwy_rt_dist_lyr_new = sdf_tract_subwy_rt_dist_lyr[['From_geoid', 'Total_Kilometers']]
sdf_tract_subwy_rt_dist_lyr_new['subwy_rt_dist'] = round(sdf_tract_subwy_rt_dist_lyr_new['Total_Kilometers'] * 3280.84, 2) 
sdf_tract_subwy_rt_dist_lyr_new.sort_values('From_geoid').head()


# In[47]:


# Estimating distances to NYCRailroad
tract_railroad_dist = use_proximity.find_nearest(nyc_tracts_layer,
                                           railroad_lyr,
                                           measurement_type='StraightLine',
                                           max_count=1,
                                           output_name='tract_railroad_dist'+ str(dt.now().microsecond))
tract_railroad_dist_lyr = tract_railroad_dist.layers[1]
sdf_tract_railroad_dist_lyr = pd.DataFrame.spatial.from_layer(tract_railroad_dist_lyr)
sdf_tract_railroad_dist_lyr.head()


# In[48]:


# Final Tract to NYCRailroad distances in feet — Here in each row, column "railroad_dist" returns the distance of
# the nearest rail road route from that tract
sdf_tract_railroad_dist_lyr_new = sdf_tract_railroad_dist_lyr[['From_geoid', 'Total_Kilometers']]
sdf_tract_railroad_dist_lyr_new['railroad_dist'] = round(sdf_tract_railroad_dist_lyr_new['Total_Kilometers'] * 3280.84, 2) 
sdf_tract_railroad_dist_lyr_new.sort_values('From_geoid').head()


# In[49]:


# Estimating distances to NYC Businesss Districts
tract_busi_distrs_dist = use_proximity.find_nearest(nyc_tracts_layer,
                                                      busi_distrs_lyr,
                                                      measurement_type='StraightLine',
                                                      max_count=1,
                                                      output_name='tract_busi_distrs_dist'+ str(dt.now().microsecond))
tract_busi_distrs_dist_lyr = tract_busi_distrs_dist.layers[1]
sdf_tract_busi_distrs_dist_lyr = pd.DataFrame.spatial.from_layer(tract_busi_distrs_dist_lyr)
sdf_tract_busi_distrs_dist_lyr.head()


# In[50]:


# Final Tract to NYC Businesss Districts distances in feet — Here in each row, column "busi_distr_dist" returns the distance of the CBD from respective tracts
sdf_tract_busi_distrs_dist_lyr_new = sdf_tract_busi_distrs_dist_lyr[['From_geoid', 'Total_Kilometers']]
sdf_tract_busi_distrs_dist_lyr_new['busi_distr_dist'] = round(sdf_tract_busi_distrs_dist_lyr_new['Total_Kilometers'] * 3280.84, 2) 
sdf_tract_busi_distrs_dist_lyr_new.sort_values('From_geoid').head()


# ### Importing Borough Info for each Tracts <a class="anchor" id="508"></a>

# In[51]:


# Name of the borough, inside which the tracts are located 
ny_tract_boro = gis.content.search('NYCTractBorough owner:api_data_owner', 'feature layer')[0]
ny_tract_boro_lyr = ny_tract_boro.layers[0]
sdf_ny_tract_boro_lyr = pd.DataFrame.spatial.from_layer(ny_tract_boro_lyr)
sdf_ny_tract_boro_lyr_new = sdf_ny_tract_boro_lyr[['geoid', 'boro_name']]
sdf_ny_tract_boro_lyr_new.sort_values('geoid').head()


# ### Merging all the above estimated data set of features <a class="anchor" id="504"></a>

# In[52]:


tract_merge_dist = sdf_tract_hotel_dist_lyr_new.merge(sdf_tract_subwy_rt_dist_lyr_new,
                                                           on='From_geoid').merge(sdf_tract_railroad_dist_lyr_new,
                                                           on='From_geoid').merge(sdf_tract_subwy_stn_dist_lyr_new,
                                                           on='From_geoid').merge(sdf_tract_busi_distrs_dist_lyr_new,
                                                           on='From_geoid').merge(sdf_tract_cbd_dist_lyr_new, on='From_geoid')
tract_merge_dist_new = tract_merge_dist[['From_geoid',
                                         'hotel_dist',
                                         'subwy_rt_dist',
                                         'railroad_dist',
                                         'subwy_stn_dist',
                                         'busi_distr_dist',
                                         'cbd_dist']]
tract_merge_dist_new = tract_merge_dist_new.rename(columns={'From_geoid':'geoid'})
tract_merge_dist_new.sort_values('geoid').head()


# In[53]:


# merging number of bus stop and borough name
tract_merge_dist_new = tract_merge_dist_new.merge(sdf_num_bustops_tracts_lyr_new,
                                                 on='geoid').merge(sdf_ny_tract_boro_lyr_new,
                                                 on='geoid') 
tract_merge_dist_new = tract_merge_dist_new.sort_values('geoid')
tract_merge_dist_new.head()


# In[54]:


# Accessing the airbnb count for each tract
sdf_airbnb_count_by_tract_new = sdf_airbnb_count_by_tract[['geoid','Point_Count']]
sdf_airbnb_count_by_tract_new = sdf_airbnb_count_by_tract_new.rename(columns={'Point_Count':'total_airbnb'})
sdf_airbnb_count_by_tract_new.head()


# In[55]:


# preparing the final distance table with airbnb count by tract
tract_merge_dist_all = sdf_airbnb_count_by_tract_new.merge(tract_merge_dist_new, on='geoid')
tract_merge_dist_all.head()


# In[56]:


tract_merge_dist_all.info()


# Borough column being an important location indicator is converted into numerical variable and inlcuded in the feature data

# In[57]:


tract_merge_dist_final = pd.get_dummies(tract_merge_dist_all, columns=['boro_name'])
tract_merge_dist_final.head()


# ### Adding census data 2019 obtained using geoenrichment <a class="anchor" id="612"></a>
# The above distance data set is now added with the census data to form the final feature set for the model

# In[58]:


sdf_airbnb_count_by_tract_enriched_sorted_new = sdf_airbnb_count_by_tract_enriched_sorted.drop(['AnalysisArea',
                                                                                                'ENRICH_FID',
                                                                                                'HasData',
                                                                                                'ID',
                                                                                                'OBJECTID',
                                                                                                'Point_Count',
                                                                                                'SHAPE',                      
                                                                                                'aggregationMethod',
                                                                                                'aland',
                                                                                                'apportionmentConfidence',
                                                                                                'awater',
                                                                                                'countyfp',
                                                                                                'funcstat',
                                                                                                'intptlat',
                                                                                                'intptlon',
                                                                                                'mtfcc',
                                                                                                'name',
                                                                                                'namelsad',
                                                                                                'populationToPolygonSizeRating',
                                                                                                'sourceCountry',
                                                                                                'statefp','tractce'], axis=1)
sdf_airbnb_count_by_tract_enriched_sorted_new.shape


# In[59]:


# checking the rows of the table for nan values
row_with_null = sdf_airbnb_count_by_tract_enriched_sorted_new.isnull().any(axis=1)

# printing the row which has nan values
sdf_airbnb_count_by_tract_enriched_sorted_new[row_with_null]


# In[60]:


# checking total number of nan values
nan_test = sdf_airbnb_count_by_tract_enriched_sorted_new.drop(['geoid'], axis=1)
np.isnan(nan_test).sum().sum()


# These two tracts area actually are water areas within NYC, hence have nan values and are filled with zeros

# In[61]:


sdf_airbnb_count_by_tract_enriched_sorted_fill = sdf_airbnb_count_by_tract_enriched_sorted_new.fillna(0)

#nan rechecked
nan_test = sdf_airbnb_count_by_tract_enriched_sorted_fill.drop(['geoid'], axis=1)
np.isnan(nan_test).sum().sum()


# Merging the distance data with the enriched data

# In[62]:


final_df = pd.merge(tract_merge_dist_final,
                    sdf_airbnb_count_by_tract_enriched_sorted_fill,
                    left_on = 'geoid',
                    right_on = 'geoid',
                    how = 'left')

print(final_df.shape)
final_df.head()


# In[63]:


# rechecking nan values of the final dataframe
final_nan_test = final_df.drop('geoid', axis=1)
np.isnan(final_nan_test).sum().sum()


# ### Model Building <a class="anchor" id="544"></a>
# 
# The goal here is to find the factors contributing towards the development of new Airbnb properties in New York City. Thus a model is fitted predicting the number of Airbnb properties per tract with the feature set composed of the distance and demographics characteristics of each tract. Once a good fit is obtained the most important predictors of the model are estimated which is our main ask.

# In[64]:


# Creating feature data 
X = final_df.drop(['geoid','total_airbnb'], axis=1)

# Creating target data  -- the number airbnb per tract
y = pd.DataFrame(final_df['total_airbnb'])


# split the dataframe into train - test of 90% to 10%

# In[65]:


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.10, random_state = 20)

print(X_train.shape)
print(y_train.shape)

print(X_test.shape)
print(y_test.shape)

# Converting the target into 1d array
y_train_array = y_train.values.flatten()
y_test_array = y_test.values.flatten() 

print(y_train_array.shape)
print(y_test_array.shape)


# As a best practice since scaled data performs well for model fitting, the features are normalized using Robust scaler 

# In[66]:


scaler = preprocessing.RobustScaler()

X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns) 
X_test_scaled = pd.DataFrame(scaler.fit_transform(X_test), columns=X_test.columns)


# ### RandomForest Regressor Model  <a class="anchor" id="643"></a>
# 
# The modelling is first started using a linear regression. However the linear model was failing to fit the data well. Hence it was carried out with a non linear algorithm as follows. This could be tested by the user to see the improvement of using Random Forest over a linear regression.
# 
# The accuracy metrics of mean absoute error and r-square is used

# In[67]:


# Random forest with scaled data
# for the best parameters a grid search could be done which could take some time
# however this model uses the default parameters of RF algorithm, while the estimators are changed till the best fit is obtained
model_RF = RandomForestRegressor(n_estimators = 500, random_state=43)

# Train the model
model_RF.fit(X_train_scaled, y_train_array)

# Training metrics for Random forest model
print('Training metrics for Random forest model using scaled data')
ypred_RF_train = model_RF.predict(X_train_scaled)
print('r-square_RF_Train: ', round(model_RF.score(X_train_scaled, y_train_array), 2))

mse_RF_train = metrics.mean_squared_error(y_train_array, ypred_RF_train)  
print('RMSE_RF_train: ', round(np.sqrt(mse_RF_train),4))

mean_absolute_error_RF_train = metrics.mean_absolute_error(y_train_array, ypred_RF_train)
print('MAE_RF_train: ', round(mean_absolute_error_RF_train, 4)) 

# Test metrics for Random Forest model
print('\nTest metrics for Random Forest model scaled data')
ypred_RF_test = model_RF.predict(X_test_scaled)
print('r-square_RF_test: ', round(model_RF.score(X_test_scaled, y_test_array), 2))

mse_RF_test = metrics.mean_squared_error(y_test_array, ypred_RF_test) 
print('RMSE_RF_test: ', round(np.sqrt(mse_RF_test), 4))

mean_absolute_error_RF_test = metrics.mean_absolute_error(y_test_array, ypred_RF_test)
print('MAE_RF_test: ', round(mean_absolute_error_RF_test, 4))


# The result shows that the model is returning an r-square of 0.85 with a mean absolute error of 9.28

# ### Feature importance for the RF model <a class="anchor" id="641"></a>

# In[68]:


feature_imp_RF = model_RF.feature_importances_

#relative feature importance  
rel_feature_imp = 100 * (feature_imp_RF / max(feature_imp_RF)) 
rel_feature_imp = pd.DataFrame({'features':list(X_train.columns),
                                'rel_importance':rel_feature_imp })

rel_feature_imp = rel_feature_imp.sort_values('rel_importance', ascending=False)


#plotting the top twenty important features
top20_features = rel_feature_imp.head(20) 

plt.figure(figsize=[20,10])
plt.yticks(fontsize=15)
ax = sns.barplot(x="rel_importance", y="features",
                 data=top20_features,
                 palette="Accent_r")

plt.xlabel("Relative Importance", fontsize=25)
plt.ylabel("Features", fontsize=25)
plt.show()


# In[69]:


rel_feature_imp.head()


# The feature importance plot reveals that distance from the city centre (cbd_dist) is the most important predictor of the number of Airbnb formation in NYC. This is expected since hotel rents near the cbd are quite high, rental income from Airbnb properties would be high as well, hence setting up Airbnb property would be a lucrative option, compared to long term rental income in areas near the cbd.
# 
# This is followed by the number of millennial population, or the tracts having most number of people in the age group of 25 to 40 years old. One reason might be that these group of population are more active online and are comfortable with internet technologies which is in a way a necessary prerequisite for setting up Airbnb properties. This is supported by the presence of another interesting predictor variable of -- 0.5-0.9 hrs online activity, in the top twenty.
# 
# This is followed by the tracts having workers who commute by bicycle and is the third most important predictor, which is followed by the number of generation alpha population, who are person born after 2011, and then by tracts having people commuting by subway, and so on. The median home value of the tracts is also an interesting predictor.

# ### Gradient Boosting Regressor Model  <a class="anchor" id="645"></a>
# Here trial shows that the gradient boosting model performs better with non scale data

# In[70]:


# GradientBoosting with non scaled data
# this model uses the default parameters of GB algorithm, while the estimators are changed to obtain the best fit 
model_GB_nonscale = GradientBoostingRegressor(n_estimators=500, random_state=60)

# Train the model
model_GB_nonscale.fit(X_train, y_train_array)

# Training metrics for Gradient Boosting Regressor model
print('Training metrics for Gradient Boosting Regressor model using scaled data')

ypred_GB_train = model_GB_nonscale.predict(X_train)
print('r-square_GB_Train: ', round(model_GB_nonscale.score(X_train, y_train_array), 2))

mse_RF_train = metrics.mean_squared_error(y_train_array, ypred_GB_train)
print('RMSE_GB_Train: ', round(np.sqrt(mse_RF_train), 4))

mean_absolute_error_RF_train = metrics.mean_absolute_error(y_train_array, ypred_GB_train)
print('MAE_GB_Train: ', round(mean_absolute_error_RF_train, 4))

#Test metrics for Gradient Boosting Regressor model
print('\nTest metrics for Gradient Boosting Regressor model using scaled data')

ypred_GB_test = model_GB_nonscale.predict(X_test)
print('r-square_GB_Test: ', round(model_GB_nonscale.score(X_test, y_test_array),2))

mse_RF_Test = metrics.mean_squared_error(y_test_array, ypred_GB_test)  
print('RMSE_GB_Test: ', round(np.sqrt(mse_RF_Test),4))

mean_absolute_error_GB_Test = metrics.mean_absolute_error(y_test_array, ypred_GB_test)
print('MAE_GB_Test: ', round(mean_absolute_error_GB_Test, 4))


# The result shows that the Gradient boosting regressor model is performing slightly better both in terms of Mean Absolute error and r-square than the random forest model.

# ### Feature Importance of Gradient Boosting Model <a class="anchor" id="647"></a>

# In[71]:


#checking the feature importance for the Gradient Boosting regressor
feature_imp_GB = model_GB_nonscale.feature_importances_
rel_feature_imp_GB = 100 * feature_imp_GB / max(feature_imp_GB)
rel_feature_imp_GB = pd.DataFrame({'features':list(X_train.columns),
                                   'rel_importance':rel_feature_imp_GB})
rel_feature_imp_GB = rel_feature_imp_GB.sort_values('rel_importance', ascending=False)
rel_feature_imp_GB.head()


# In[72]:


# Plot  feature importance for the Gradient Boosting regressor
top20_features_GB = rel_feature_imp_GB.head(20) 

plt.figure(figsize=[20,10])
plt.yticks(fontsize=15)
ax = sns.barplot(x="rel_importance", y="features", data = top20_features_GB, palette="Accent_r")
plt.xlabel("Relative Importance", fontsize=25)
plt.ylabel("Features", fontsize=25)
plt.show()


# The feature importance shown by the Gradient boosting model are almost identical to the one returned by the random forest model, which is expected.

# ### Running cross validation <a class="anchor" id="651"></a>
# 
# The above model is fitted and accuracy measured on a particular train and test split of the data. However the model accuracy for multiple split of the data remains to be seen. This is accomplished using k fold cross validation which splits the data into k different train-test splits and fit the model for each of them. Hence a 10 fold cross validation is run to check the overall model accuracy which is measured here as the mean absolute error for model fit accross the 10 different splits.

# In[73]:


# Validating with a 10 fold cross validation for the Gradient Boosting models
y_array = y.values.flatten()

modelGB_cross_val = GradientBoostingRegressor(n_estimators=500, random_state=60) 

modelGB_cross_val_scores = cross_val_score(modelGB_cross_val,
                                           X, 
                                           y_array,
                                           cv=10,
                                           scoring='neg_mean_absolute_error')

print("All Model Scores: ", modelGB_cross_val_scores)

print("Negative Mean Absolute Error: {}".format(np.mean(modelGB_cross_val_scores)))


# In[74]:


# Validating with a 10 fold cross validation for the Random forest models
y_array = y.values.flatten()

modelRF_cross_val = RandomForestRegressor(n_estimators=500, random_state=43)

modelRF_cross_val_scores = cross_val_score(modelRF_cross_val,
                                           X, 
                                           y_array,
                                           cv=10,
                                           scoring='neg_mean_absolute_error')

print("All Model Scores: ", modelRF_cross_val_scores)

print("Negative Mean Absolute Error: {}".format(np.mean(modelRF_cross_val_scores)))


# ### Final Result Visualization <a class="anchor" id="652"></a>

# In[75]:


# Plotting a kernel density map of the predicted vs. observed data
plt.figure(figsize=[15,5])

# plotting the prediction
sns.kdeplot(ypred_RF_test, label = 'Predictions', color = 'orange')
y_observed = np.array(y_test).reshape((-1, ))
sns.kdeplot(y_observed, label = 'Observation', color = 'green')

# label the plot
plt.xlabel('No. of Airbnb listings per census tract', fontsize=15)
plt.ylabel('Density', fontsize=15)
plt.title('Density Plot: Predicted vs Observed', fontsize=15)
plt.xticks(range(0,500,25), fontsize=10)
plt.yticks(fontsize=10)
plt.legend(fontsize=15)
plt.show()


# In[76]:


# Converting the predicted and observed values to dataframe and plotting the observed vs predicted
y_test_df = y_test.copy()
y_test_df['Predicted'] = (ypred_RF_test)  
y_test_df.head()


# In[77]:


# plotting the actual observed vs predicted airbnb properties by tract
plt.figure(figsize = [25,12])
sns.set(style = 'whitegrid')
sns.lineplot(data = y_test_df, markers=True) 

#label the plot
plt.xlabel('Tract ID', fontsize=15)
plt.ylabel('Total No. of Airbnb', fontsize=15)
plt.title('Actual No. of Airbnb by Tract: Predicted vs Observed', fontsize=15)
plt.xticks(range(0,2000,100), fontsize=15)
plt.yticks(fontsize=15)
plt.legend(fontsize='x-large', title_fontsize='10')
plt.legend(fontsize=15)
plt.show()


# The plot shows that the predicted values closely matches the observed values. However there are instances of underprediction for tracts with extremely high number of airbnb properties, and also overprediction instances for some tracts with comparatively lower number of airbnb properties.

# ### Conclusion <a class="anchor" id="655"></a>
# 
# The study shows that the location factor of distance from CBD is the foremost important factor which stimulates creation of Airbnb properties.
# 
# The proximity tool from the ArcGIS API for Python was used to perform this significant task for all the distance estimation. Other factors as returned by the feature importance result could be dealt individually. Another interesting capability of Esri utilized in the study is that of Esri's data repository, elaborated here via the geoenrichment services. The data enrichment service could provide the analyst an wide array of data that could be used for critical analysis. Further analysis would be done in the next study on this dataset.  

# ### Summary of methods used <a class="anchor" id="657"></a>

# | Method | Question | Examples |
# | -| - |-|
# | aggregate_points| How many points within each polygon?  |Counting the number of airbnb rentals within each NYC tracts|
# | Data Enrichment| Which demographic attribute are relevant for the problem?  |Population of Millennials for each tract|
# | find_nearest| Which distances from city features are relevant for the problem?  |Distance of the CBD from each tract|

# ### Data resources <a class="anchor" id="656"></a>

# | Shapefile | Source | Link |
# | -| - |-|
# | airbnb_nyc2019| NYC Airbnb Data Inside Airbnb:Get the Data  |http://insideairbnb.com/get-the-data.html|
# | nyc_tract_fulll|NYC Open Data: 2010 Census Tracts (water areas included)  |https://data.cityofnewyork.us/City-Government/2010-Census-Tracts-water-areas-included-/gx7x-82rk|
# | busi_distr | NYC Open Data: Business Improvement Districts  |https://data.cityofnewyork.us/Business/Business-Improvement-Districts/ejxk-d93y|
# | cbd|NYC Open Data: Business Improvement Districts  |https://data.cityofnewyork.us/Business/Business-Improvement-Districts/ejxk-d93y|
# | bus_stop|NYC Open Data: Bus Stop Shelters  |https://data.cityofnewyork.us/Transportation/Bus-Stop-Shelters/qafz-7myz|
# | hotels|NYC Open Data: Facilities Database  |https://data.cityofnewyork.us/City-Government/Facilities-Database-Shapefile/2fpa-bnsx|
# | railroad|NYC Open Data: Railroad Line  |https://data.cityofnewyork.us/Transportation/Railroad-Line/i7a5-bsik|
# | subwy_rt|NYC Open Data: Subway Lines  |https://data.cityofnewyork.us/Transportation/Subway-Lines/3qz8-muuu|
# | subwy_stn|NYC Open Data: Subway Stations  |https://data.cityofnewyork.us/Transportation/Subway-Stations/arq3-7z49|
# 


# ====================
# analyzing_violent_crime.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Analyzing violent crime

# Many of the poorest neighborhoods in the City of Chicago face violent crimes. Since some studies have linked alcohol to different crimes, there is pressure on the city officials to close down liquor establishments. The local business owners, on the other hand, want to block such restrictions as it could negatively impact business and the social fabric of the City. In this sample, we will perform an illustrative analysis to find a find a possible relation between violent crimes and liquor establishments.  This will also help us figure out if poverty and unemployment rate are factors which contribute to more crimes in a specific area of the city.   We will be using the Crime Analysis data from ArcGIS Living Atlas of the World in order to perform this analysis. The data was orginally obtained from the 2014 Violent Crime Data in the City of Chicago data portal (https://data.cityofchicago.org). 
# 
# Through this sample, we will demonstrate the utility of a number of spatial analysis methods including hot spot analysis, feature overlay, data enrichment and spatial selection using ArGIS API for Python.
# 
# Further, based on the results of the analysis, this sample will try to assist with an effective solution to this problem.
# 

# 

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# 
# <div class="toc">
#     <ul class="toc-item">
#         <li><span><a href="#Outline-of-Steps" data-toc-modified-id="Steps-1">Outline of Steps</a></span></li>
#         <li><span><a href="#Necessary-Imports" data-toc-modified-id="Necessary-Imports-2">Necessary Imports</a></span></li>
#         <li><span><a href="#Connect-to-your-GIS" data-toc-modified-id="Connect-to-your-GIS-3">Connect to your GIS</a></span></li>
#         <li><span><a href="#Get-the-data-for-the-analysis" data-toc-modified-id="Get-the-data-for-the-analysis-4">Get the data for the analysis</a></span></li>
#         <li><span><a href="#Create-a-hot-spot-map-of-violent-crime-densities" data-toc-modified-id="Create-hot-spot-map-of-violent-crime-densities-5">Create a hot spot map of violent crime densities</a></span></li>
#         <li><span><a href="#Create-a-hot-spot-map-of-liquor-vendors-to-compare-to-the-violent-crime-hot-spot-map" data-toc-modified-id="Create-a-hot-spot-map-of-liquor-vendors-to-compare-to-the-violent-crime-hot-spot-map-6">Create a hot spot map of liquor vendors to compare to the violent crime hot spot map</a></span></li>
#         <li><span><a href="#Get-poverty-data-for-each-of-the-polygon-grids-in-the-violent-crime-hot-spot-analysis-layer" data-toc-modified-id="Get-poverty-data-for-each-of-the-polygon-grids-in-the-violent-crime-hot-spot-analysis-layer-7">Get poverty data for each of the polygon grids in the violent crime hot spot analysis layer</a></span></li>
#         <li><span><a href="#Create-a-hot-spot-map-of-poverty" data-toc-modified-id="Create-a-hot-spot-map-of-poverty-8">Create a hot spot map of poverty</a></span></li>
#         <li><span><a href="#Overlay-the-violent-crime-and-liquor-vendor-to-find-where-they-overlap." data-toc-modified-id="Overlay-the-violent-crime-and-liquor-vendor-to-find-where-they-overlap.-9">Overlay the violent crime and liquor vendor to find where they overlap.</a></span></li>
#         <li><span><a href="#Overlay-the-violent-crime,-liquor-vendor-and-poverty-hot-spot-maps-to-find-where-they-overlap" data-toc-modified-id="Overlay-the-violent-crime,-liquor-vendor-and-poverty-hot-spot-maps-to-find-where-they-overlap-10">Overlay-the-violent-crime,-liquor-vendor-and-poverty-hot-spot-maps-to-find-where-they-overlap</a></span></li>
#         <li><span><a href="#Get-the-unemployment-rate-data-matching-the-violent-crime-trends-layer" data-toc-modified-id="Get-the-unemployment-rate-data-matching-the-violent-crime-trends-layer-11">Get the unemployment rate data matching the violent crime trends layer</a></span></li>
#         <li><span><a href="#Create-a-hot-spot-map-of-the-unemployment-rate-data" data-toc-modified-id="Create-a-hot-spot-map-of-the-unemployment-rate-data-12">Create a hot spot map of the unemployment rate data</a></span></li>
#         <li><span><a href="#Overlay-the-crime-trends-map-with-the-unemployment-hot-spot-map" data-toc-modified-id="Overlay-the-crime-trends-map-with-the-unemployment-hot-spot-map-13">Overlay the crime trends map with the unemployment hot spot map</a></span></li>
#         <li><span><a href="#Select-the-high-schools-falling-within-a-quarter-mile-of-the-proposed-remediation-areas" data-toc-modified-id="Select-the-high-schools-falling-within-a-quarter-mile-of-the-proposed-remediation-areas-14">Select the high schools falling within a quarter mile of the proposed remediation areas</a></span></li>
#         <li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-15">Conclusion</a></span></li>
#         <li><span><a href="#Summary-of-tools" data-toc-modified-id="Summary-of-tools-16">Summary of tools</a></span></li> 
#         <li><span><a href="#Terms-of-use" data-toc-modified-id="Terms-of-use-17">Terms of use</a></span></li>
#     </ul>
# </div>

# ### Outline of Steps

# We will use the following workflow for the analysis:
# 
#  1. Retrieve Crime Analysis data.
#  2. Plot the data on map for visualzation
#  3. Perform spatial analysis:
#     * We will first create hotspots for the crime densities and liquor vendor layers. This will help is in visualizing the significant areas of crime incidences and liquor vendors.
#     * Enrich the crime layer with poverty and unemployment rate data using the enrich_layer tool.
#     * Visualize hotspots on the enriched layers.
#     * Use overlay tool to find relationship between crimes and liquor establishments. This will also help to know whether or not poverty and unemployment rate are factors contributing to more crimes in any specific area of the city.
#  4. Suggest ways for reducing crime based on the results of our analysis
#     

# ### Necessary Imports

# In[1]:


from datetime import datetime as dt
from IPython.display import display

from arcgis import GIS
from arcgis.features.analyze_patterns import find_hot_spots
from arcgis.features.enrich_data import enrich_layer
from arcgis.features.manage_data import overlay_layers
from arcgis.features.find_locations import find_existing_locations 


# ### Connect to your GIS

# In[2]:


gis = GIS('home')


# ### Get the data for the analysis

# Search for **CrimeAnalysisData** layer in ArcGIS Online. We can search for content shared by users outside our organization by setting **outside_org** to True.

# In[3]:


items = gis.content.search('title:CrimeAnalysisData owner:api_data_owner', 'feature layer')


# In[4]:


for item in items:
    display(item)


# We will use the first item for our analysis. Since the item is a Feature Layer Collection, accessing the layers property will give us a list of FeatureLayer objects.

# In[5]:


crime_item = items[0]


# In[6]:


lyrs = crime_item.layers


# The code below cycles through the layers and prints their names.

# In[7]:


for lyr in lyrs:
    print(lyr.properties.name)


# We'll get the second layer and assign it to the `violent_crimes` variable. Similarly, get the `analysis_boundary` layer.

# In[8]:


violent_crimes = lyrs[2] # Violent Crime 2014


# In[9]:


analysis_boundary = lyrs[3] # Analysis Boundary


# Let's visualize the crime incidents on a map of Chicago.

# In[10]:


crime_map = gis.map('Chicago')
crime_map


# In[11]:


crime_map.add_layer(violent_crimes)


# It is difficult to discern spatial patterns with so many points on the map. To make sense of the more than 22,000 crime points, and over 1,500 business points, we will map them using hot spot analysis. 

# We can add a number of different layer objects such as FeatureLayer, FeatureCollection, ImageryLayer, MapImageLayer to the map by calling the add_layer() method.

# ### Create a hot spot map of violent crime densities

# ArcGIS has a set of tools to help us identify, quantify and visualize satial patterns in our data by identifying areas of statistically significant clusters. 
# 
# The  `find_hot_spots` tool allows us to visualize areas having such clusters.

# In[12]:


crime_hot_spots = find_hot_spots(violent_crimes,
                                 output_name='ViolentCrimeHotSpots' + str(dt.now().microsecond),
                                 bounding_polygon_layer=analysis_boundary)


# In[13]:


crime_hot_spots


# In[14]:


crime_spots_map = gis.map('Chicago')
crime_spots_map


# This map shows us the statistically significant hot spots (red) and cold spots (blue) for violent crime.

# In[15]:


crime_spots_map.add_layer(crime_hot_spots)


# Next, we will use the `liquor_vendors` layer.

# In[16]:


liquor_vendors = lyrs[1]


# ### Create a hot spot map of liquor vendors to compare to the violent crime hot spot map

# In[17]:


liquor_vendor_hot_spots = find_hot_spots(liquor_vendors,
                                         output_name='LiquorVendorHotSpots' + str(dt.now().microsecond),
                                         aggregation_polygon_layer=crime_hot_spots)


# In[18]:


liquor_vendor_hot_spots


# In[19]:


liquor_vendor_hot_spots_lyr = liquor_vendor_hot_spots.layers[0]


# In[20]:


liquor_hot_spots_map =  gis.map('Chicago')
liquor_hot_spots_map


# This map shows us the statistically significant hot spots (red) and cold spots (blue) for liquor establishments..

# In[21]:


liquor_hot_spots_map.add_layer(liquor_vendor_hot_spots_lyr)


# In order to identify the city's most vulnerable neighborhoods, we will obtain the data needed to create a hot spot map of poverty.

# ### Get poverty data for each of the polygon grids in the violent crime hot spot analysis layer

# The `enrich_layer` tool gives us demographic and landascape data for the people, places, and businesses in a specific area, or within a selected travel time or distance from a location. 

# We will add analysis variables, i.e, poverty, to our layer. This will enrich our layer with the poverty data. 

# In[22]:


poverty_enrich= enrich_layer(crime_hot_spots, 
                             analysis_variables=["households.ACSHHBPOV"], 
                             output_name='PovertyDataEnrichedLayer' + str(dt.now().microsecond))


# In[23]:


poverty_enrich


# In[24]:


poverty_enrich_lyr = poverty_enrich.layers[0]


# ### Create a hot spot map of poverty

# We can find hot spots of the enriched_layer by assigning the field name of the analysis variable to parameter 'analysis_field'.

# In[25]:


poverty_data_hot_spots = find_hot_spots(poverty_enrich_lyr,
                                        analysis_field='ACSHHBPOV',
                                        output_name='povertyEnrichedHotSpots' + str(dt.now().microsecond))


# Load the map again to visualize the poverty_data_hot_spots

# In[26]:


poverty_data_hot_spots_map = gis.map('Chicago')
poverty_data_hot_spots_map


# In[27]:


poverty_data_hot_spots_map.add_layer(poverty_data_hot_spots)


# The red areas are statistically significant hot spots for poverty.

# We will now filter the three variables, i.e., violent crime, existing liquor establishments, and poverty in order to get statistically significant hot spots.

# ### Overlay the violent crime and liquor vendor to find where they overlap.

# We will assign the Gi_Bin attribute a value of 3 in order to get statistically significant hot spots at 99 percent confidence level.

# In[28]:


crimelayer = crime_hot_spots.layers[0]
crimelayer.filter = "Gi_Bin = 3"


# In[29]:


liquorlayer = liquor_vendor_hot_spots.layers[0]
liquorlayer.filter = "Gi_Bin = 3"


# In[30]:


povertylayer = poverty_data_hot_spots.layers[0]
povertylayer.filter = "Gi_Bin = 3"


# The `overlay_layers` function combines two or more layers into one single layer using an intersect, union, or erase method. We can think of overlay as peering through a stack of maps and creating a single map containing all the information found in the stack.
# We will use the crimelayer as the input layer and intersect it with the liquorlayer. 

# In[31]:


intersect_of_crime_vendor = overlay_layers(crimelayer,
                                           liquorlayer,
                                           tolerance=0,
                                           context={},
                                           output_name="IntersectOfCrimeVendor" + str(dt.now().microsecond))


# In[32]:


intersect_of_crime_vendor


# ### Overlay the violent crime, liquor vendor and poverty hot spot maps to find where they overlap

# In[33]:


intersect_cri_pov_liq = overlay_layers(intersect_of_crime_vendor,
                                       poverty_data_hot_spots,
                                       output_name="intersectOfCrimeVendorPoverty" + str(dt.now().microsecond))


# In[34]:


intersect_cri_pov_liq


# Next, we will load the overlay layer on the map.

# In[35]:


intersected_map = gis.map('Chicago')
intersected_map


# In[36]:


intersected_map.add_layer(intersect_cri_pov_liq)


# With the exception of the small overlapping areas identified above, we find no spatial correlation between violent crime and businesses that sell or serve alcohol.
# 
# However since violent crimes are serious problem in the city, we will look into the current research in this area in order to provide a possible solution to this problem.

# A recent research indicates that two years ago the City implemented a summer jobs program that proved tremendously effective in reducing violent crime. So we will obtain unemployment data and repeat our hot spot analysis to see if we find a stronger spatial correlation between unemployment and violent crime than we did between liquor establishments and violent crime. 

# ### Get the unemployment rate data matching the violent crime trends layer
# 

# The space-time pattern mining tools are not currently available in ArcGIS API for Python which are required to create crime trend map. Although we can create the crime trend map using <a href='http://desktop.arcgis.com/en/analytics/case-studies/broken-bottles-2-arcmap-workflow.htm#ESRI_STEPS_4EF9CB8AB43145B99E9E343CB9C10917'>ArcMap</a> or using <a href='http://desktop.arcgis.com/en/analytics/case-studies/broken-bottles-3-pro-workflow.htm#ESRI_STEPS_5B9E855169CA4B818AF51D0FF69CE66A'>ArcGIS Pro </a>. 
# For our analysis we will use the layer already published in ArcGIS Online. To access the layer, search for **ViolentCrimeTrend** in ArcGIS Online. We can search the GIS for feature layer collections by specifying the item type as 'Feature Layer Collection' or 'Feature Layer'.

# In[37]:


crime_trend = gis.content.search('title:ViolentCrimeTrend owner: api_data_owner', 'feature layer')
crime_trend_item = crime_trend[0]

crime_trend_layer = crime_trend_item.layers[0]


# 
# We will use the `enrich_layer` tool to add more demographic information to the layer.

# In[38]:


crime_trend_unemp_enrich = enrich_layer(crime_trend_layer, 
                                        analysis_variables=["industry.UNEMPRT_CY"], 
                                        output_name='UnemploymentEnrichedLayer' + str(dt.now().microsecond))


# ### Create a hot spot map of the unemployment rate data

# We use find_hot_spots tool to spot hot spots of **crime_trend_enriched_unemployment** layer.

# In[39]:


unemployment_rate_hot_spots = find_hot_spots(crime_trend_unemp_enrich,
                                             analysis_field='UNEMPRT_CY',
                                             output_name='UnemploymentRateHotspots' + str(dt.now().microsecond))


# Now let's load and visualize the hot spots on the map.

# In[40]:


unemployment_rate_hot_spots_map = gis.map('Chicago')
unemployment_rate_hot_spots_map


# In[41]:


unemployment_rate_hot_spots_map.add_layer(unemployment_rate_hot_spots)


# ### Overlay the crime trends map with the unemployment hot spot map
# 

# We will filter unemployment_rate_hot_spots to select locations where the Gi_Bin Fixed 4554_FDR field is 3 (3 is the code for statistically significant hot spots at the 99 percent confidence level, as we mentioned earlier). Then we will filter the most intense unemployment hot spots

# In[42]:


unemployment_rate_hot_spots.layers[0].filter = "Gi_Bin = 3"


# For the crime trends map, we are interested in specific Pattern Type values: Intensifying, Consecutive, and Persistent hot spots. 

# In[43]:


crime_trend_layer.filter = "(PATTERN = 'Consecutive Hot Spot') OR (PATTERN ='Intensifying Hot Spot') OR (PATTERN = 'Persistent Hot Spot')"


# We use `overlay_layers` tool to find areas that are common to crime trend layer and unemployment_rate_hot_spots.

# In[44]:


overlay_unemp_crime = overlay_layers(crime_trend_layer,
                                     unemployment_rate_hot_spots,
                                     output_name="OverlayUnEmploymentCrimetrend" + str(dt.now().microsecond))


# In[45]:


overlay_unemp_crime


# Now let's load and visualize on map.

# In[46]:


overlay_map = gis.map('Chicago')
overlay_map


# In[47]:


overlay_map.add_layer(overlay_unemp_crime)


# We indeed find that there are a number of locations where the violent crime and unemployment hot spots overlap.
# 
# The blue areas are the locations where intensifying, persistent, and consecutive hot spot trends overlap with the most intense unemployment hot spots.

# High schools within a quarter mile of the remediation areas where high violent crime and high unemployment overlap should be targeted for an expanded summer jobs program.So we will identify such schools.

# ### Select the high schools falling within a quarter mile of the proposed remediation areas

# In[48]:


public_high_school_layer = lyrs[0]


# In[49]:


selected_schools = find_existing_locations(input_layers=[{'url': public_high_school_layer.url},
                                                         {'url': crime_trend_unemp_enrich.layers[0].url}],
                                           expressions=[{"operator":"",
                                                         "layer":0,
                                                         "selectingLayer":1,
                                                         "spatialRel":"withinDistance",
                                                         "distance":0.25,
                                                         "units":"Miles"}],
                                           output_name='SelectedSchools' + str(dt.now().microsecond))


# In[50]:


selected_schools


# In[51]:


selected_high_schools_map = gis.map('Chicago')
selected_high_schools_map


# In[52]:


selected_high_schools_map.add_layer(selected_schools)


# ## Conclusion

# In this study, we performed hotspot analysis on violent crimes layer and liquor vendors layer. We enriched the crime hotspots layer with poverty indicators and produced a poverty hotspot layer. When all 3 layers were overlaid, we found no spatial correlation. In the next step, we enriched the crime layer with unemployment information and produced an unemployment hotspot layer. However, this layer was spatially correlated with the crimes layer when overlaid. As a solution, we used find_existing_locations tools to identify high schools within 0.25 miles of places with high crime and high unemployment as candidates for summer programs as an abatement measure.
# 

# ## Summary of tools

# <style type="text/css">
# .tg  {border-collapse:collapse;border-spacing:0;}
# .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
# .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
# .tg .tg-s268{text-align:left}
# .tg .tg-0lax{text-align:left;vertical-align:top}
# </style>
# <table class="tg">
#   <tr>
#     <th class="tg-s268">Method</th>
#     <th class="tg-s268">Examples</th>
#   </tr>
#   <tr>
#     <td class="tg-0lax">Hot Spot Analysis of feature attributes</td>
#     <td class="tg-0lax">Where are the statistically significant clusters of poverty, unemployment, wealth, beer drinkers, lead levels, or college graduates?</td>
#   </tr>
#   <tr>
#     <td class="tg-0lax">Feature Overlay</td>
#     <td class="tg-0lax">Where are the intersections among high crime areas, high liquor vendor areas, and high poverty areas? Where are high lead levels and poor educational outcomes spatially congruent?</td>
#   </tr>
#   <tr>
#     <td class="tg-0lax">Spatial Selection</td>
#     <td class="tg-0lax">Which schools are close to the remediation areas? Which homes fall within the flood zone? Which ZIP Codes are within the county?</td>
#   </tr>
#   <tr>
#     <td class="tg-0lax">Data Enrich</td>
#     <td class="tg-0lax">Which aeas have high poverty and unemployment rate?</td>
#   </tr>
# </table>

# ## Terms of use
# 
# In keeping with the requirements of the City of Chicago Data Portal terms of data use, note the following: This case study describes analyses using data that have been modified for use from its original source, www.cityofchicago.org, the official website of the City of Chicago. The City of Chicago makes no claims as to the content, accuracy, timeliness, or completeness of any of the data provided at this site. The data provided at this site is subject to change at any time. It is understood that the data provided at this site is being used at one's own risk.


# ====================
# automate_building_footprint_extraction_using_instance_segmentation.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Automate Building Footprint Extraction using Deep learning

# * 🔬 Data Science
# * 🥠 Deep Learning and Instance Segmentation

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Export training data for deep learning](#Export-training-data-for-deep-learning)
# * [Model training](#Model-training)
#  * [Necessary Imports](#Necessary-Imports)
#  * [Visualize training data](#Visualize-training-data)
#  * [Load model architecture](#Load-model-architecture)
#  * [Train the model](#Train-the-model)
#  * [Visualize detected building footprints](#Visualize-detected-building-footprints)
#  * [Save model](#Save-model)
# * [Model inference](#Model-inference)

# Building footprints are often used for base map preparation, humanitarian aid, disaster management, and transportation planning
# 
# There are several ways of generating building footprints.
# These include manual digitization by using tools to draw outline of each building.
# However, it is a labor intensive and time consuming process. 

# This sample shows how ArcGIS API for Python can be used to train a deep learning model to extract building footprints using satellite images. The trained model can be deployed on ArcGIS Pro or ArcGIS Enterprise to extract building footprints.

# In this workflow, we will basically have three steps.
# 
# - Export Training Data
# - Train a Model
# - Deploy Model and Extract Footprints

# ## Export training data for deep learning

# Training data can be exported using the `Export Training Data For Deep Learning` tool available in ArcGIS Pro as well as ArcGIS Enterprise. For this example we prepared training data in 'RCNN Masks' format using a `chip_size` of 400px and `cell_size` of 30cm in ArcGIS Pro.

# In[1]:


from arcgis.gis import GIS
gis = GIS('home')
portal = GIS('https://pythonapi.playground.esri.com/portal')


# The items below are high resolution satellite imagery and a feature layer of building footprints for Berlin which will be used for exporting training data. 

# In[2]:


berlin_imagery = portal.content.get('c0bd94a10c4649fcb755ee375ae45f2f')
berlin_imagery


# In[3]:


rcnn_labelled_data = gis.content.get('7d3f633a325f4dcf962c82284098ce9d')
rcnn_labelled_data


# You can use the `Export Training Data` for Deep Learning tool to export training samples for training the model. For this sample, choose RCNN Masks as the export format.
# 
# 

# 

# ```python
# arcpy.ia.ExportTrainingDataForDeepLearning("Berlin_Imagery", r"D:\data\maskrcnn_training_data_maskrcnn_400px_30cm", "berlin_building_footprints", "TIFF", 400, 400, 0, 0, "ONLY_TILES_WITH_FEATURES", "RCNN_Masks", 0, "classvalue", 0, None, 0, "MAP_SPACE", "PROCESS_AS_MOSAICKED_IMAGE", "NO_BLACKEN", "FIXED_SIZE")
# ```

# This will create all the necessary files needed for the next step in the specified 'Output Folder'. These files serve as our training data.

# ## Model training

# This step would be done using jupyter notebook and documentation is available [here](https://developers.arcgis.com/python/guide/install-and-set-up/) to install and setup environment.

# ### Necessary Imports

# In[4]:


import os
from pathlib import Path
from arcgis.learn import MaskRCNN, prepare_data
from arcgis.gis import GIS


# `prepare_data` function takes path to training data and creates a fastai databunch with specified transformation, batch size, split percentage, etc.

# In[5]:


training_data = gis.content.get('637825446a3641c2b602ee854776ed47')
training_data


# In[6]:


filepath = training_data.download(file_name=training_data.name)


# In[7]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[8]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# In[10]:


data = prepare_data(data_path, 
                    batch_size=4, 
                    chip_size=400)


# ### Visualize training data

# To get a sense of what the training data looks like, use the `show_batch()` method to randomly pick a few training chips and visualize them. The chips are overlaid with masks representing the building footprints in each image chip.

# In[11]:


data.show_batch(rows=4)


# ### Load model architecture

# `arcgis.learn` provides the MaskRCNN model for instance segmentation tasks, which is based on a pretrained convnet, like ResNet that acts as the 'backbone'. More details about MaskRCNN can be found [here](https://github.com/Esri/arcgis-python-api/blob/master/guide/14-deep-learning/How_MaskRCNN_works.ipynb).

# In[12]:


model = MaskRCNN(data)


# Learning rate is one of the most important hyperparameters in model training. We will use the `lr_find()` method to find an optimum learning rate at which we can train a robust model.

# In[13]:


lr = model.lr_find()
lr


# ### Train the model

# We are using the suggested learning rate above to train the model for 10 epochs.

# In[14]:


model.fit(10,lr=lr)


# ## Visualize detected building footprints

# The `model.show_results()` method can be used to display the detected building footprints. Each detection is visualized as a mask by default.

# In[15]:


model.show_results()


# We can set the mode parameter to  `bbox_mask` for visualizing both mask and bounding boxes.

# In[16]:


model.show_results(mode='bbox_mask')


# ### Save model

# As we can see, with 10 epochs, we are already seeing reasonable results. Further improvments can be achieved through more sophisticated hyperparameter tuning. Let's save the model, so it can be used for inference, or further training subsequently. By default, it will be saved into your data_path that you specified in the very beginning of this notebook, in the `prepare_data` call.

# In[17]:


model.save('Building_footprint_10epochs', publish=True)


# ## Model inference

# The saved model can be used to extract building footprint masks using the 'Detect Objects Using Deep Learning' tool available in ArcGIS Pro, or ArcGIS Enterprise. For this sample we will use high satellite imagery to detect footprints.

# 

# ```python
# arcpy.ia.DetectObjectsUsingDeepLearning("Berlin_Imagery", r"\\ArcGIS\Projects\maskrcnn_inferencing\maskrcnn_inferencing.gdb\maskrcnn_detections", r"\\models\building_footprint_10epochs\building_footprint_10epochs.emd", "padding 100;batch_size 4;threshold 0.9;return_bboxes False", "NO_NMS", "Confidence", "Class", 0, "PROCESS_AS_MOSAICKED_IMAGE")
# ```

# The output of the model is a layer of detected building footprints. However, the detected building footprints need to be post-processed using the [Regularize Building Footprints](https://pro.arcgis.com/en/pro-app/tool-reference/3d-analyst/regularize-building-footprint.htm) tool. This tool normalizes the footprint of building polygons by eliminating undesirable artifacts in their geometry. The post-processed building footprints are shown below:

# 

# <center>A subset of detected building footprints

# To view the above results in a webmap [click here](http://arcg.is/Ca4fP).

# This sample showcases how instance segmentation models like MaskRCNN can be used to automatically extract building footprints in areas where buildings are touching each other. This approach overcomes the limitations of semantic segmentation models, which work well only when buildings are separated from each other by some distance.


# ====================
# automate_road_surface_investigation_using_deep_learning.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Automate Road Surface Investigation Using Deep Learning

# * 🔬 Data Science
# * 🥠 Deep Learning and Object Detection

# ## Table of Contents
# * [Introduction and objective](#Introduction-and-objective)
# * [Necessary imports](#Necessary-imports)
# * [Prepare data that will be used for training](#Prepare-data-that-will-be-used-for-training)
# * [Model training](#Model-training)
#  * [Visualize training data](#Visualize-training-data)
#  * [Load model architecture](#Load-model-architecture)
#  * [Train a model](#Train-a-model)
#  * [Detect and visualize pavement cracks in validation set](#Detect-and-visualize-pavement-cracks-in-validation-set)
#  * [Save the model](#Save-the-model)
# * [Model inference](#Model-inference)
#     * [Detecting pavement cracks on an image](#Detecting-pavement-cracks-on-an-image)
#     * [Detecting pavement cracks from video feed](#Detecting-pavement-cracks-from-video-feed)
# * [Conclusion](#Conclusion)
# * [References](#References)

# ## Introduction and objective

# Deterioration of road surface due to factors including vehicle overloading, poor construction quality, over ageing, natural disasters and other climatic conditions may lead to road pavement failure. This may result in traffic slowness causing jams and vehicle damage due to cracks. This also causes problems for civic authorities who are in need to accurately identify these cracks and do the repair work. If these cracks are not repaired at early stages, cost of repair gradually increases causing unnecessary burden on exchequer. 
# 
# Traditionally, inspection of road surface is done by humans either by visually observing it or by using sophisticated machines which are expensive too. The manual approach to detect damage is not just time consuming but is also ineffective since detection of such damages requires consistent help from subject matter experts who have the ability to identify and differentiate different types of pavement failures. Artificial Intelligence supported by Deep Learning comes to the rescue. Deep learning integrated with ArcGIS plays a crucial role by automating the process. 
# 
# In this notebook, We use a great labeled dataset of asphalt distress images from the [2018 IEEE Bigdata Cup Challenge](http://cci.drexel.edu/bigdata/bigdata2018/BigDataCupChallenges.html) in order to train our model to detect as well as to classify type of road cracks. The training and test data consists of 9,053 photographs, collected from smartphone cameras, hand labeled with the presence or absence of 8 road damage categories [1].
# 
# The table below shows sample images of the dataset corresponding to each of the 8 categories of damage type. 

# <table>
#   <tr>
#     <th>Class Name</th>
#     <th>Class Description</th>    
#     <th>Image</th>
#   </tr>
#   <tr>
#       <td>D00</td>
#       <td>Liner, crack, longitudinal, wheel mark part</td>
#       <td></td>
#   </tr>
#   <tr>
#     <td>D01</td>
#     <td>Liner crack, longitudinal, construction joint part</td>
#     <td></td>
#   </tr>
#   <tr>
#     <td>D10</td>
#     <td>Liner crack, lateral, equal interval</td>
#     <td></td>
#   </tr>
#   <tr>
#     <td>D11</td>
#     <td>Liner crack, lateral, construction, joint part</td>
#     <td></td>
#   </tr>
#   <tr>
#     <td>D20</td>
#     <td>Alligator crack</td>
#     <td></td>
#   </tr>
#   <tr>
#     <td>D40</td>
#     <td>Rutting, bump, pothole, separation</td>
#     <td></td>
#   </tr>    
#   <tr>
#     <td>D43</td>
#     <td>White line blur</td>
#     <td></td>
#   </tr>   
#   <tr>
#     <td>D44</td>
#     <td>Cross walk blur</td>
#     <td></td>
#   </tr>   
# </table>

# Through this sample, we will walk you through step-by-step process to build robust Deep Learning solution to identify road pavement failures and eventually integrate with ArcGIS as a reusable tool.

# ## Necessary imports

# Note: This notebook sample has not been verified to run on ArcGIS Pro. It may be possible to execute the sample within ArcGIS Pro if the libraries it requires can be installed using the [Package Manager](https://pro.arcgis.com/en/pro-app/latest/arcpy/get-started/what-is-conda.htm). If the deep learning frameworks are required, follow [this documentation](https://pro.arcgis.com/en/pro-app/latest/help/analysis/deep-learning/install-deep-learning-frameworks.htm) for more guidance.

# In[1]:


# Restart the kernel after installation is complete
get_ipython().system('pip install opencv-python==4.0.1.24')


# In[2]:


import pandas as pd
import os
import shutil
from pathlib import Path

from arcgis.gis import GIS
from arcgis.features import GeoAccessor
from arcgis.learn import SingleShotDetector, prepare_data


# ## Prepare data that will be used for training

# You can download pavement cracks data from the following link: https://developers.arcgis.com/python/sample-notebooks/automate-road-surface-investigation-using-deep-learning/. Extract the downloaded file and run the code below to prepare data in a format that deep learning models expect.

# In[3]:


# Please uncomment the following code to prepare your training data.

# input_path = Path(input("Enter the path where you extracted data: "))
# output_path = Path(input("Enter the path where you want to create training data: "))
# try:
#     if not os.path.exists(output_path/'images') and os.path.exists(output_path/'labels'):
#         os.mkdir(output_path/'images')
#         os.mkdir(output_path/'labels')
# except: raise
# for fl in os.listdir(input_path):
#     if not(fl.startswith(".")):
#         for f in os.listdir(input_path/fl/'Annotations'):
#             if not(f.startswith(".")):
#                 img_name = f.split('.')[0] + '.jpg'
                
#                 shutil.copyfile(input_path/fl/'JPEGImages'/img_name, output_path/'images'/img_name)
#                 shutil.copyfile(input_path/fl/'Annotations'/f, output_path/'labels'/f)


# ## Model training

# You change the path to your own training data folder that contains "images" and "labels" folder.

# In[4]:


gis = GIS('home')


# In[5]:


training_data = gis.content.get('9c7274bbfac343f3aef33f2dc1ff4baf')
training_data


# In[6]:


filepath = training_data.download(file_name=training_data.name)


# In[7]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[8]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# ``prepare_data`` function takes path to training data and creates a fastai databunch with specified transformation, batch size, split percentage,etc. 

# In[9]:


data = prepare_data(data_path,
                    batch_size=8,
                    chip_size=500,
                    seed=42,
                    dataset_type='PASCAL_VOC_rectangles')


# We can use the ``classes`` attribute of the data object to get information about the number of classes.

# In[10]:


data.classes 


# ### Visualize training data

# To get a sense of what the training data looks like, ``arcgis.learn.show_batch()`` method randomly picks a few training chips and visualize them.

# In[11]:


data.show_batch(rows=2)


# ### Load model architecture

# ``arcgis.learn`` provides the ``SingleShotDetector`` (SSD) model for object detection tasks, which is based on a pretrained convnet, like ResNet that acts as the 'backbone'. More details about SSD can be found [here](https://developers.arcgis.com/python/guide/how-ssd-works/).

# We will use the ``SingleShotDetector`` to train the damage detection model with backbones as ``resnet101``. 

# In[12]:


ssd = SingleShotDetector(data, backbone='resnet101',focal_loss=True)


# Let us have a look at the results of the untrained model.

# In[13]:


ssd.show_results(thresh=0.2)


# We see that the model is randomly detecting the road cracks. In order to give good results our model needs to be trained. 
# 
# Learning rate is one of the most important hyperparameters in model training. We will use the ``lr_find()`` method to find an optimum learning rate at which we can train a robust model fast enough.

# In[14]:


lr = ssd.lr_find()
lr


# ### Train a model

# Based on the suggested learning rate above, we will start training our model with 30 epochs for the sake of time.

# In[15]:


ssd.fit(30, lr=lr)


# The graph below plots training and validation losses. 

# In[16]:


ssd.learn.recorder.plot_losses()


# ``average_precision_score`` method computes average precision on the validation set for each class.

# In[17]:


ssd.average_precision_score() 


# We can see the model accuracy for each class of our validation data. The model is giving varying results. 
# Let's us dig deeper to find the reason for model to preform better on one class in comparison to the other. 
# This will also help us understand why D30 class has zero average precision score. 

# In[18]:


# Calculate the number of images of each classs in training data
all_classes = []
for i, bb in enumerate(data.train_ds.y):
    all_classes += bb.data[1].tolist()
    
df = pd.value_counts(all_classes, sort=False)
df.index = [data.classes[i] for i in df.index] 
df   


# We have only 22 images for training our model to detect class D30 which is very less. Thus, the model is giving poor score for this specific class. 

# ### Detect and visualize pavement cracks in validation set

# In[19]:


ssd.show_results(rows=10, thresh=0.2, nms_overlap=0.5)


# ### Save the model

# As we can see, with 30 epochs, we are already seeing reasonable results. Further improvment can be acheived through more sophisticated hyperparameter tuning. Let's save the model for further training or inference later. The model should be saved into a models folder in your folder. By default, it will be saved into your data_path that you specified in the very beginning of this notebook.

# In[20]:


ssd.save(str(data_path / 'pavement-cracks-model-resnet101'))


# ## Model inference

# We will do model inference using the two methods: `predict` and `predict_video`. Let's get the data required to predict on image and video.

# In[21]:


inference_data = gis.content.get('92a75cec191e4dbbb53067761287b977')
inference_data


# In[22]:


inf_data_path = inference_data.download(file_name=inference_data.name)


# In[23]:


import zipfile
with zipfile.ZipFile(inf_data_path, 'r') as zip_ref:
    zip_ref.extractall(Path(inf_data_path).parent)


# In[24]:


img_file = os.path.join(os.path.splitext(inf_data_path)[0], 'test_img.jpg')
video_file = os.path.join(os.path.splitext(inf_data_path)[0], 'test_video.mp4')
metadata_file = os.path.join(os.path.splitext(inf_data_path)[0], 'metadata.csv')


# ### Detecting pavement cracks on an image

# In[25]:


bbox_data = ssd.predict(img_file, threshold=0.1, visualize=True)


# ### Detecting pavement cracks from video feed

# In[26]:


ssd.predict_video(input_video_path=video_file, 
                  metadata_file=metadata_file, 
                  visualize=True, 
                  resize=True)


# <video width="100%" height="450" loop="loop" controls src="../../static/video/road_crack_prediction.mp4" />

# ## Publish results to your GIS

# The `predict_video` function also updates the metadata file provided in csv format with the detections at each frame. We will now read this csv using pandas and publish it as a layer on our GIS.

# In[27]:


import pandas as pd
df = pd.read_csv(metadata_file)
df


# The code below removes rows from the DataFrame with no detections and also creates a new columns which contains count of the number of detections at each frame.

# In[28]:


# Handle NAN and '\n' values
df.vmtilocaldataset = df.vmtilocaldataset.str.strip()
df.loc[df.vmtilocaldataset == '', 'vmtilocaldataset'] = ''
df['count'] = ( df['vmtilocaldataset'].str.split(';').str.len().fillna(1) - 1)


# In[29]:


fps = 60
a = (pd.Series(df.index.values) / fps)
a = (a - .49).round().abs()
df['group'] = a


# In[30]:


# Get index of row with max detections in each group
max_detection_idxes = df[['group', 'count']].groupby('group').idxmax()['count'].values


# In[31]:


# Extract rows for the indexes
df_flt = df.iloc[max_detection_idxes]


# In[32]:


df_flt.drop(df_flt.loc[df['count']==0].index, inplace=True)


# In[33]:


sdf = GeoAccessor.from_xy(df_flt, 'Sensor Longitude', 'Sensor Latitude')


# In[34]:


cracks_lyr = gis.content.import_data(sdf, title='crack points')


# In[35]:


cracks_lyr


# In[36]:


m1 = gis.map('Haryana, India')
m1


# In[37]:


m1.basemap = "satellite"


# In[38]:


m1.add_layer(cracks_lyr, {"renderer":"ClassedSizeRenderer",
                                      "field_name": "count_"})


# ## Conclusion

# In this notebook, we learnt how civic authorities can automate road surface investigation using deep learning in order to make policy decisions. This will not only help in repairing exisiting cracks but may prevent pavement failures in future.

# ## References

# [1] Hiroya Maeda, et al. "Road Damage Detection Using Deep Neural Networks with Images Captured Through a Smartphone", 1801.09454, arXiv, 2018
# 


# ====================
# automatic_road_extraction_using_deep_learning.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Automatic road extraction using deep learning

# - 🔬 Data Science
# - 🥠 Deep Learning and pixel-based classification

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Introduction" data-toc-modified-id="Introduction-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href="#Area-of-Interest-and-data-pre-processing" data-toc-modified-id="Area-of-Interest-and-data-pre-processing-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Area of Interest and data pre-processing</a></span></li><li><span><a href="#Data-preparation" data-toc-modified-id="Data-preparation-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>Data preparation</a></span></li><li><span><a href="#Visualization-of-prepared-data" data-toc-modified-id="Visualization-of-prepared-data-4"><span class="toc-item-num">4&nbsp;&nbsp;</span>Visualization of prepared data</a></span></li><li><span><a href="#Training-the-model" data-toc-modified-id="Training-the-model-5"><span class="toc-item-num">5&nbsp;&nbsp;</span>Training the model</a></span></li><li><span><a href="#Visualization-of-results" data-toc-modified-id="Visualization-of-results-6"><span class="toc-item-num">6&nbsp;&nbsp;</span>Visualization of results</a></span></li><li><span><a href="#Saving-the-trained-model" data-toc-modified-id="Saving-the-trained-model-7"><span class="toc-item-num">7&nbsp;&nbsp;</span>Saving the trained model</a></span></li><li><span><a href="#Inference-using-the-trained-model,-in-ArcGIS-Pro" data-toc-modified-id="Inference-using-the-trained-model,-in-ArcGIS-Pro-8"><span class="toc-item-num">8&nbsp;&nbsp;</span>Inference using the trained model, in ArcGIS Pro</a></span></li><li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-9"><span class="toc-item-num">9&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href="#References" data-toc-modified-id="References-10"><span class="toc-item-num">10&nbsp;&nbsp;</span>References</a></span></li></ul></div>

# ## Introduction

# Road network is a required layer in a lot of mapping exercises, for example in Basemap preparation <i>(critical for navigation)</i>, humanitarian aid, disaster management, transportation, and for a lot of other applications it is a critical component. 
# 
# This sample shows how ArcGIS API for Python can be used to train a deep learning model <i>(Multi-Task Road Extractor model)</i> to extract the road network from satellite imagery. The models trained can be used with ArcGIS Pro or ArcGIS Enterprise and even support distributed processing for quick results.
# 
# 
# Further details on the Multi-Task Road Extractor implementation in the API (working principle, architecture, best practices, etc.), can be found in the Guide, along with instructions on how to set up the Python environment.
# 
# Before proceeding through this notebook, it is advised to go through the API Reference for Multi-Task Road Extractor (`prepare_data()`, `MultiTaskRoadExtractor()`). It will help in understanding the Multi-Task Road Extractor's workflow in detail.
# 

# _**Objectives:**_

# <ol style="list-style-type:upper-roman">
# <li>Classify roads, utilizing API's Multi-Task Road Extractor model.</li>
# </ol>

# ## Area of Interest and data pre-processing

# For this sample, we will be using a subset of the publically available <a href="https://spacenet.ai/spacenet-roads-dataset/" target="_blank">SpaceNet dataset</a>. Vector labels as _'road centerlines'_ are available for download along with imagery, hosted on _AWS S3_ <a href="#References">[1]</a>.
# 
# The area of interest is Paris, with 425 km of _'road centerline'_ length <i>(As shown in Figure. 1)</i>. Both of these inputs, Imagery, and vector layer <i>(for creating image chips and labels as 'classified tiles')</i> are used to create data that is needed for model training.

# <p align="center">
# </p>
# 
# <center>Figure 1: SpaceNet dataset - AOI 3 - Paris</center>

# Downloaded data has 4 types of imagery: Multispectral, Pan, Pan-sharpened Multispectral, Pan-sharpened RGB. 8-bit RGB imagery support and 16-bit RGB imagery experimental support is available with Multi-Task Road Extractor Model _(Multispectral imagery will be supported in the subsequent release)_. In this sample, Pan-sharpened RGB is used, after converting it to 8-bit imagery.
# 
# _**Pre-processing steps:**_
# 
# 
# - Downloaded vector labels, in _`.geojson`_ format, are converted to _`feature class/shapefile`_. <i>(Refer to ArcGIS Pro's  <a href="https://pro.arcgis.com/en/pro-app/tool-reference/conversion/json-to-features.htm" target="_blank">JSON To Features GP tool.</a>)</i>
# 
# 
# - The converted vector data is checked and repaired if any invalid geometry is found. <i>(Refer to ArcGIS Pro's  <a href="https://pro.arcgis.com/en/pro-app/tool-reference/data-management/repair-geometry.htm" target="_blank">Repair Geometry GP tool.</a>)</i>
# 
# 
# - 'Stretch function' is used to convert 16-bit imagery to 8-bit imagery. <i>(Refer to ArcGIS Pro's  <a href="https://pro.arcgis.com/en/pro-app/help/data/imagery/stretch-function.htm" target="_blank">Stretch raster function.</a>)</i>
# 
# 
# - 'Projected coordinate system' is applied to imagery and road vector data, for ease in the interpretation of results and setting the values of tool parameters.  
# 
# 
# Now, the data is ready for <a href="https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/export-training-data-for-deep-learning.htm" target="_blank">Export Training Data For Deep Learning GP tool</a> <i>(As shown in Figure. 2)</i>. It is used to export data that will be needed for model training. This tool is available in ArcGIS Pro as well as ArcGIS Enterprise. 
# 
# Here, we exported the data in 'Classified Tiles' format using a `Cell Size` of '30 cm'. `Tile Size X` and `Tile Size Y` are set to '512', while `Stride X` and `Stride Y` are set to '128'. If Road centerlines are directly used as an input, then based on the area of interest and types of roads in that region, the appropriate buffer size can be set. Alternatively, ArcGIS Pro's <a href="https://pro.arcgis.com/en/pro-app/tool-reference/feature-analysis/create-buffers.htm" target="_blank">Create Buffers GP tool</a> can be used to convert road centerlines to road polygons and buffer value can be decided iteratively by checking the results of the Create Buffers GP tool.
# 
# 

# <p align="center">
# </p>
# 
# 
# <center>Figure 2: Export Training Data For Deep Learning GP tool</center>

# This tool will create all the necessary files needed in the next step, at the Output Folder's directory.

# ## Data preparation

# _**Imports:**_

# In[ ]:


from arcgis.learn import prepare_data, MultiTaskRoadExtractor


# _**Preparing the exported data:**_

# _Some of the frequently used parameters that can be passed in `prepare_data()` are described below:_
# 
# `path`: the path of the folder containing training data. (Output generated by the "Export Training data for deep learning GP tool")
# 
# `chip_size`: Images are cropped to the specified chip_size.
# 
# `batch_size`: No. of images your model will train on each step inside an epoch, it directly depends on the memory of your graphic card. 
# 
# `val_split_pct`: Percentage of training data to keep as validation.
# 
# `resize_to`: Resize the cropped image to the mentioned size.

# 
# **Note:** _Data meant for 'Try it Live' is a very small subset of the actual data that was used for this sample notebook, so the training time, accuracy, visualization, etc. will change, from what is depicted below._

# In[ ]:


import os, zipfile
from pathlib import Path
from arcgis.gis import GIS
gis = GIS('home')


# In[ ]:


training_data = gis.content.get('b7bbf2f5f4184960890afeabbdb51a32')
training_data


# In[ ]:


filepath = training_data.download(file_name=training_data.name)


# In[ ]:


with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[ ]:


output_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# In[ ]:


data = prepare_data(output_path, chip_size=512, batch_size=4)


# In[ ]:


data.classes


# ## Visualization of prepared data

# `show_batch()` can be used to show the prepared data. Where input imagery is shown with labels overlayed on them. 
# 
# `alpha` is used to control the transparency of labels.

# In[ ]:


data.show_batch(alpha=1)


# ## Training the model

# First, the Multi-Task Road Extractor model object is created, utilizing the prepared data. Some model-specific advance parameters can be set at this stage.
# 
# **All of these parameters are optional, as smart 'default values' are already set, which works best in most cases.**
# 
# _The advance parameters are described below:_
# 
# - `gaussian_thresh`: sets the gaussian threshold which allows setting the required road width.
# 
# 
# - `orient_bin_size`: sets the bin size for orientation angles.
# 
# 
# - `orient_theta`: sets the width of the orientation mask.
# 
# 
# - `mtl_model`: It defines two different architectures used to train the Multi-Task Extractor. Values are "linknet" and "hourglass".
# 
# While, backbones only work with 'linknet' architecture.
# _('resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152' are the supported backbones.)_

# In[ ]:


model = MultiTaskRoadExtractor(data, mtl_model="hourglass")


# Next, `lr_find()` function is used to find the optimal learning rate. It controls the rate at which existing information will be overwritten by newly acquired information throughout the training process. If no value is specified, the optimal learning rate will be extracted from the learning curve during the training process.

# In[ ]:


model.lr_find()


#  

# `fit()` is used to train the model, where a new 'optimum learning rate' is automatically computed or the previously computed optimum learning rate can be passed. <i>(Any other user-defined learning rate can also be passed)</i>
# 
# If `early_stopping` is True, then the model training will stop when the model is no longer improving, regardless of the `epochs` parameter value specified. While an 'epoch' means the dataset will be passed forward and backward through the neural network one time.
# 
# `miou` and `dice` are the performance metrics, shown after completion of each epoch.

# In[ ]:


model.fit(50, 0.0005754399373371565, early_stopping=True)


# ## Visualization of results

# `show_results()` is used to visualize the results of the model, for the same scene with the ground truth. 
# Validation data is used for this.
# 
# - 1<sup>st</sup> column is the _**'ground truth image'**_ overlayed with its corresponding _**'ground truth labels'**_.
# 
# 
# - 2<sup>nd</sup> column is the _**'ground truth image'**_ overlayed with its corresponding _**'predicted labels'**_.

# In[ ]:


model.show_results(rows=4)


# ## Saving the trained model

# The last step, related to training, is saving the model using `save()`. Here apart from model files, performance metrics, a graph of validation and training losses, sample results, etc are also saved.

# In[ ]:


model.save('road_model_for_spacenet_data')


# ## Inference using the trained model, in ArcGIS Pro

# The model saved in the previous step can be used to extract a classified  raster using  <a href="https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/classify-pixels-using-deep-learning.htm" target="_blank">Classify Pixels Using Deep Learning tool</a> <i>(As shown in Figure. 3)</i>.
# 
# Further, the classified raster can be converted into a vector road layer in ArcGIS Pro. The regularisation related GP tools can be used to remove unwanted artifacts in the output. As the model was trained on a `Cell Size` of '30 cm', at this step too, the `Cell Size` is kept equal to '30 cm'. 

# <p align="center">
# </p>
# 
# <center>Figure 3: Classify Pixels Using Deep Learning tool</center>

# ## Conclusion

# This notebook has summarized the end-to-end workflow for the training of a deep learning model for road classification. This type of model can predict the roads occluded by small and medium length shadows, however when roads have larger occlusions from clouds/shadows then it is unable to create connected road networks.

# ## References

# - The SpaceNet dataset used in this sample notebook is licensed under the <a href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.


# ====================
# building_reconstruction_using_mask_rcnn.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Reconstructing 3D buildings from Aerial LiDAR with Deep Learning
# > * 🔬 Data Science
# * 🥠 Deep Learning and Instance Segmentation

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Prerequisites](#Prerequisites)
# * [Part 1 - Data Preparation](#Part-1---Data-Preparation)
#  * [Export training data](#Export-training-data)
# * [Part 2 - Model Training](#Part-2---Model-Training)
#  * [Necessary Imports](#Necessary-Imports)
#  * [Prepare data](#Prepare-data)
#  * [Visualize a few samples from your training data](#Visualize-a-few-samples-from-your-training-data)
#  * [Load model architecture](#Load-model-architecture)
#  * [Find an optimal learning rate](#Find-an-optimal-learning-rate)
#  * [Fit the model](#Fit-the-model)
#  * [Save the model](#Save-the-model)
#  * [Visualize results in validation set](#Visualize-results-in-validation-set)
#  * [Load a saved model to visualize results](#Load-a-saved-model-to-visualize-results)
# * [Part 3 - Deploy Model and Detect Roof Types](#Part-3---Deploy-Model-and-Detect-Roof-Types)
# * [Part 4 - 3D enabling the MaskRCNN results](#Part-4---3D-enabling-the-MaskRCNN-results)
# * [Conclusion](#Conclusion)
# * [References](#References)

# ## Introduction
# 
# The workflow traditionally used to reconstruct 3D building models from aerial LiDAR is relatively straight-forward: the LiDAR point-cloud is transformed into a Digital Surface Model (DSM) raster, then inspected by human editors for buildings present. If a building is found, one or more polygons describing the roof form of the building is manually digitized, e.g. if it is a large hip roof with two gable outlets, there will be three polygons (one hip and two gables on top) drawn by the editor. Once all the roofs are described that way, a set of ArcGIS Procedural rules is applied to extrude the building models using the manually digitized roof segments, with heights and ridge directions computed from the DSM.

# 
# 
# <center>Figure 1. 3D building reconstruction from Lidar example: a building with complex roof shape and its representation in visible spectrum (RGB), Aerial LiDAR, and corresponding roof segments digitized by a human editor. The last one is a 3D reconstruction of the same building using manually digitized masks and ArcGIS Procedural rules. </center>

# The most time-consuming and expensive step in the above workflow is the manual search and digitization of the roof segment polygons from a DSM raster. In this notebook, we are going to focus on this challenging step and demonstrate how to detect instances of roof segments of various types using instance segmentation to make the process more efficient. The workflow consists of four major steps: (1) extract training data, (2) train a deep learning __instance segmentation__ model, (3) model deployment and roof segments detection and (4) 3D enabling the detected segments.

# ## Prerequisites 

# Complete data required to run this sample is packaged together in a project package and can be downloaded from [here](https://pythonapi.playground.esri.com/portal/home/item.html?id=1474b1838f4a4ab68890c6595adaa51d). You are also required to download the rule package used in [Part 4](#Part-4---3D-enabling-the-MaskRCNN-results) of this notebook from [here](https://pythonapi.playground.esri.com/portal/home/item.html?id=7ae7f189665b402c908d475f2daf973e).
# 
# Below are the items present in the project package shared:
#  - <b>D1_D2_D3_Buildings_1</b>: labelled feature data for training data preparation
#  - <b>R7_nDSM_TestVal</b>: raster image for training data preparation
#  - <b>DSM_AOI_Clip</b>: DSM raster for area of interest, required during model inferencing 
#  - <b>DTM_AOI_Clip</b>: DTM raster for area of interest, required during model inferencing
#  - <b>DSM_AOI_Clip_DetectObjects_26032020_t4_220e</b>: sample results obtained from the trained MaskRCNN model inferenced on area of interest obtained after performing [part 3](#Part-3---Deploy-Model-and-Detect-Roof-Types) of the notebook
#  - <b>DSM_AOI_Clip_DetectObjects_26032020_t4_220e_selection_3dEnabling</b>: sample 3D enabled roof segments obtained after performing [part 4](#Part-4---3D-enabling-the-MaskRCNN-results) of the notebook
#  
#  Moreover, there is a toolbox (<b>3d_workflow.tbx</b>) in the 'Toolboxes' section of the project having the script (3dEnabling) to perform [part 4](#Part-4---3D-enabling-the-MaskRCNN-results) of the notebook. 

# ## Part 1 - Data Preparation
# 
# We started with two input data:
# - A single-band raster layer (R7_nDSM_TestVal) with 2.25 square feet per pixel resolution converted from LiDAR point cloud (using the “[LAS Dataset to Raster](https://pro.arcgis.com/en/pro-app/tool-reference/conversion/las-dataset-to-raster.htm)” geoprocessing tool)
# - A feature class (D1_D2_D3_Buildings_1) that defines the location and label (i.e. flat, gable, hip, shed, mansard, vault, dome) of each roof segment.
# 
# We are using single band Lidar data which is essentially elevation to train our deep learning MaskRCNN model.

# 
# 
# <center>Figure 2. Example of different roof types (flat not shown). </center>

# ### Export training data

# Export training data using 'Export Training data for deep learning' tool, detailed documentation [here](https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/export-training-data-for-deep-learning.htm).
# 
# - `Input Raster`: R7_nDSM_TestVal
# - `Output Folder`: Set a location where you want to export the training data, it can be an existing folder or the tool will create that for you.
# - `Input Feature Class Or Classified Raster`: D1_D2_D3_Buldings_1
# - `Image Format`: TIFF format
# - `Tile Size X` & `Tile Size Y` can be set to 256
# - `Stride X` & `Stride Y`: 128
# - `Meta Data Format`: Select 'RCNN Masks' as the data format because we are training a <b>MaskRCNN model</b>.
# - In `Environments` tab set an optimum `Cell Size`. For this example, as we have to perform the analysis on the LiDAR imagery, we used 0.2 cell size.

# 
# 
# 
# <center>Figure 3. Export Training data screenshot from ArcGIS Pro. </center>

# ```Python
# arcpy.ia.ExportTrainingDataForDeepLearning(in_raster="R7_nDSM_TestVal",
# out_folder=r"\Documents\PCNN\Only_nDSM",
# in_class_data="D1_D2_D3_Buildings_1", 
# image_chip_format="TIFF",
# tile_size_x=256,
# tile_size_y=256,
# stride_x=128,
# stride_y=128, 
# output_nofeature_tiles="ONLY_TILES_WITH_FEATURES", 
# metadata_format="RCNN_Masks",
# start_index=0,
# class_value_field="None",
# buffer_radius=0,
# in_mask_polygons=None,
# rotation_angle=0, 
# reference_system="MAP_SPACE",
# processing_mode="PROCESS_AS_MOSAICKED_IMAGE",
# blacken_around_feature="NO_BLACKEN",
# crop_mode="FIXED_SIZE")
# ```

# After filling all details and running the `Export Training Data For Deep Learning tool`, a code like above will be generated and executed. That will create all the necessary files needed for the next step in the 'Output Folder', and we will now call it our training data.

# ## Part 2 - Model Training

# You should already have the training chips with you exported from ArcGIS pro. Please change the path to your own export training data folder that contains "images" and "labels" folder. Note that we set a relatively small `batch_size` here on purpose as instance segmentation is a more computationally intensive task compared with object detection and pixel-based classification. If you run into "insufficient memory" issue during training, you can come back to adjust it to meet your own needs. 

# ### Necessary Imports 

# In[1]:


import os
from pathlib import Path

from arcgis.learn import prepare_data, MaskRCNN


# In[2]:


#connect to GIS
from arcgis.gis import GIS
gis = GIS('home')


# ### Prepare data

# We will now use the `prepare_data()` function to apply various types of transformations and augmentations on the training data. These augmentations enable us to train a better model with limited data and also prevent the model from overfitting. `prepare_data()` takes 3 parameters.
# 
# - `path`: path of the folder containing training data.
# - `batch_size`: No of images your model will train on each step inside an epoch, it directly depends on the memory of your graphic card. 8 worked for us on a 11GB GPU.
# - `imagery_type`: It is a mandatory input to enable a model for multispectral data processing. It can be "landsat8", "sentinel2", "naip", "ms" or "multispectral".

# In[3]:


training_data = gis.content.get('807425fa74f34d7695ec024eb934456c')
training_data


# In[4]:


filepath = training_data.download(file_name=training_data.name)


# In[5]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[6]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# In[7]:


data = prepare_data(data_path, batch_size=8, imagery_type='ms')


# ### Visualize a few samples from your training data
# To get a sense of what the training data looks like, `arcgis.learn.show_batch()` method randomly picks a few training chips and visualizes them. Note that the masks representing different roof segments are overlaid upon the original images with red and pink colors.

# `rows`: No of rows we want to see the results for.

# In[8]:


data.show_batch(rows=2)


# ### Load model architecture

# Here we use Mask R-CNN [1], a well-recognized instance algorithm, to detect roof segments (Figure 3). A Mask R-CNN model architecture and a pretrained model has already been predefined in `arcgis.learn`, so we can just define it with a single line. Please refer to the guide on our [developers' site](https://developers.arcgis.com/python/guide/how-maskrcnn-works/) for more information.
# 
# The idea of Mask R-CNN is to detect objects in an image while simultaneously generating a high-quality segmentation mask for each instance. In other words, it is like a combination of UNet and SSD and does two jobs in one go. This is also why it is relatively computationally more intensive.

# In[9]:


model = MaskRCNN(data)


# 
# <center>Figure 4. Mask R-CNN framework for instance segmentation [1] </center>

# ### Find an optimal learning rate
# [Learning rate](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) is one of the most important hyperparameters in model training. `arcgis.learn` leverages fast.ai’s learning rate finder to find an optimum learning rate for training models. We can use the `lr_find()` method to find the optimum learning rate at which we can train a robust model fast enough.

# In[10]:


# The users can visualize the learning rate of the model with comparative loss. 
lr = model.lr_find()
lr


# ### Fit the model

# Let's train it for a few epochs with the learning rate we have found. For the sake of time, we can start with 10 epochs.

# In[11]:


model.fit(epochs=10, lr=lr)


# Note that the validation loss actually goes higher, but this doesn't necessarily mean the result is getting bad. Because our training data is also missing many buildings, the loss sometimes tells the actual model performance, so let's look at the actual results instead.

# ### Save the model

# We will save the model which we trained as a 'Deep Learning Package' ('.dlpk' format). Deep Learning package is the standard format used to deploy deep learning models on the ArcGIS platform.
# 
# We will use the `save()` method to save the trained model. By default it will be saved to the 'models' sub-folder within our training data folder.

# In[12]:


model.save('10e')


# ### Visualize results in validation set
# Now we have the model, let's look at how the model performs. There are 3 modes when visualizing the results.
# 
# - `bbox` - For visualizing only bounding boxes.
# - `mask` - For visualizing only mask
# - `bbox_mask` - For visualizing both mask and bounding boxes.

# In[13]:


model.show_results(mode='bbox_mask', rows=2)


# As we can see, with only 10 epochs, we are already seeing good results. We have trained it further till 220 epochs and were able to get promising results. Below are the results that we got from our model trained for 220 epochs.

# ### Load a saved model to visualize results 

# To visualize results from a saved model, we can load it using `load()` function.

# In[14]:


# model.load('220e')


# In[15]:


# model.show_results(mode='bbox_mask', rows=5, box_threshold=0.5)


# ## Part 3 - Deploy Model and Detect Roof Segments

# We will use saved model to detect objects using '<b>Detect Objects Using Deep Learning</b>' tool available in both [ArcGIS Pro](https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/detect-objects-using-deep-learning.htm) and [ArcGIS Enterprise](https://developers.arcgis.com/rest/services-reference/detect-objects-using-deep-learning.htm). For this sample, we will use the DSM image for our area of Interest. You will find this image in the ArcGIS project you have downloaded with the name - <b>DSM_AOI_Clip</b>.
# 
#  - `Input Raster`: DSM_AOI_Clip
#  - `Output Detect Objects`: DSM_AOI_Clip_DetectObjects
#  - `Model Definition`: path to the model emd file
#  - `padding`: The 'Input Raster' is tiled and the deep learning model runs on each individual tile separately before producing the final 'detected objects feature class'. This may lead to unwanted artifacts along the edges of each tile as the model has little context to detect objects accurately. Padding as the name suggests allows us to supply some extra information along the tile edges, this helps the model to predict better.
#  - `batch_size`: 4
#  - `threshold`:0.4
#  - `return_bboxes`: False
#  - `Cell Size`: Should be close to at which we trained the model, we specified that at the Export Training Data step .
# 

# 
# <center>Figure 5. Detect objects using Deep Learning screenshot from ArcGIS Pro</center>

# ```Python
# arcpy.ia.DetectObjectsUsingDeepLearning(in_raster="DSM_AOI_Clip",
# out_detected_objects=r"sample.gdb\DSM_AOI_Clip_DetectObjects",
# in_model_definition=r"\models\200e\200e.emd",
# model_arguments="padding 56;batch_size 4;threshold 0.4;return_bboxes False;",
# run_nms="NO_NMS",
# confidence_score_field="Confidence",
# class_value_field="Class",
# max_overlap_ratio=0,
# processing_mode = "PROCESS_AS_MOSAICKED_IMAGE")
# ```

# 
# <center>Figure 6. A subset of detected different types of building roof forms (shown in different color) </center>

# ## Part 4 - 3D enabling the MaskRCNN results
# 
# The next step is to extrude object detection results in 3D using traditional Geoprocessing tools and Procedural rules. First, we used the “[Regularize Building Footprint](https://pro.arcgis.com/en/pro-app/tool-reference/3d-analyst/regularize-building-footprint.htm)” tool from the 3D Analyst toolbox to regularize raw detections. Next, we used surface elevation and DSM rasters to acquire the base elevation for each building segment, and calculate the roof ridge direction. The final step was calling the “[Features from CityEngine Rules](https://pro.arcgis.com/en/pro-app/tool-reference/3d-analyst/features-from-cityengine-rules.htm)” GP tool which applied a procedural rule to these polygons to achieve a final reconstruction of the 3D buildings.

# All the pre-processing steps are recorded in a python script and shared with you in the form of a toolbox within your project. Double click on the script (3dEnabling).

# 
# <center>Figure 7. Screenshot of location of the script to perform 3D enabling task</center>

# The 3dEnabling script takes the following inputs:
# 
#  - `Input Masks`: detected buildings from MaskRCNN model.
#  - `DTM`: 'DTM_AOI_Clip' image from the project.
#  - `DSM`: 'DSM_AOI_Clip' image from the project.
#  - `Rule Package`: 'LGIM_RFE_LOD2.rpk' rule package file from the project.
#  - `Output Buildings`: 3dEnabledPolygons
#  - `Cell Size`: Should be close to at which we trained the model, we specified that at the [Export Training Data](#Export-training-data) step.

# 
# <center>Figure 8. 3dEnabling tool screenshot from ArcGIS Pro</center>

# ```Python
# arcpy.3dworkflow.3dEnabling(Input_Masks="DSM_AOI_Clip_DetectObjects",                                           
# DTM="DTM_AOI_Clip",
# DSM="DSM_AOI_Clip",
# Rule_Package=r"\Reconstruction_part2\LGIM_RFE_LOD2.rpk",
# Output_Buildings=r"\reconstruction_part2.gdb\DSM_AOI_Clip_DetectObjects_3dEnabling")```

# This tool converts the detection obtained from MaskRCNN model in 3D based on the rules defined in the rule package. 

# In[16]:


scene_item = gis.content.get('9ce6cf3170334c61bf3da25dacbdf553')


# In[17]:


from arcgis.widgets import MapView
map2 = MapView(item = scene_item)
map2.extent = {'spatialReference': {'latestWkid': 3857, 'wkid': 102100},
                'xmin': -8919494.611200048,
                'ymin': 2981619.558592063,
               'xmax': -8918883.485987043,
               'ymax': 2981867.4795709597}
map2.mode='3D'
map2


# 
# 

# [Here](https://pythonapi.playground.esri.com/portal/home/webscene/viewer.html?webscene=9ce6cf3170334c61bf3da25dacbdf553&viewpoint=cam:-80.11520339,25.8490231,1650.769;331.049,37.541) is a web map to showcase the results obtained from the notebook.

# ## Conclusion
# In this notebook, we have covered a lot of ground. In part 1, we discussed how to prepare and export training data from aerial Lidar data. In part 2, we demonstrated how to prepare the input data, train an instance segmentation model, visualize the results, as well as apply the model to an unseen image using the deep learning module in ArcGIS API for Python. In part 4, we discussed how the MaskRCNN results obtained can be converted in 3D using the toolbox shared.  
# 
# ## References
# [1] He, K., Gkioxari, G., Dollár, P. and Girshick, R., 2017. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision (pp. 2961-2969).


# ====================
# calculate_impervious_surfaces_from_spectral_imagery.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Calculate Impervious Surfaces from Spectral Imagery

# ## Table of Contents
# * [Introduction and objective](#Introduction-and-objective)
# * [Necessary Imports](#Necessary-Imports)
# * [Get data for analysis](#Get-data-for-analysis)
# * [Classify pixel values into pervious and impervious class](#Classify-pixel-values-into-pervious-and-impervious-class)
# * [Tabulate area of impervious surfaces](#Tabulate-area-of-impervious-surfaces)
# * [Compute stormwater bill](#Compute-stormwater-bill)
# * [Visualize results](#Visualize-results)
# * [Conclusion](#Conclusion)
# * [References](#References)

# ## Introduction and objective

# Ground surfaces that are impenetrable to water can cause serious environmental problems, including flooding and contaminated runoff. Many local government agencies use impervious surface calculations to compute the storm water bill for properties. That depends on how much impervious area is there in each property.

# 

# <center align='middle'>Map highlighting impervious surface areas: driveways, private walks, roof tops, and parking lots [1]. 

# In this notebook, we’ll use a high resolution land cover map obtained from Chesapeake Conservancy to determine which parts of the ground are pervious and impervious.
# 
# Impervious surfaces are generally human-made: buildings, roads, parking lots, brick, or asphalt. Pervious surfaces include vegetation, water bodies, and bare soil. 
# 
# We will reclassify those land-use types into either impervious or pervious surfaces to derive the stormwater bill for each property. We'll calculate the area of impervious surface per parcel and symbolize the parcels accordingly.
# 
# 

# ## Necessary Imports

# In[1]:


import pandas as pd
from datetime import datetime as dt

import arcgis
import arcpy
from arcgis.gis import GIS
from arcgis.raster.functions import RFT           

gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# ## Get data for analysis

# For this analysis we need the following datasets:
# - Land Cover of Kent County, Delaware
# - Kent County parcel feature layer
# - Raster Function Template to extract necessary class labels from the land cover layer
# 
# In the cells below, we access these datasets from the GIS.

# In[2]:


rft_to_reclassify = gis.content.get('eb9fe459cc26486eb4a733b2a2def44f')
rft_to_reclassify


# In[3]:


classified_raster = gis.content.get('7e00802c927a40cbbdc36cefdb2928e4')
classified_raster


# In[4]:


classified_raster.layers[0]


# Twelve land cover classes were mapped:
# 1. Water
# 2. Emergent Wetlands
# 3. Tree Canopy
# 4. Scrub/Shrub
# 5. Low Vegetation
# 6. Barren
# 7. **Structures**
# 8. **Other Impervious Surfaces**
# 9. **Roads**
# 10. Tree Canopy over Structures
# 11. Tree Canopy over Other Impervious Surfaces
# 12. Tree Canopy over Roads
# 
# The complete class definitions and standards can be viewed at the link below.http://goo.gl/THacgg.
# 

# In[5]:


parcels = gis.content.get('1a7e4a24372048048a668c8a176932bf')


# In[6]:


parcels


# In[7]:


parcel_lyr = parcels.layers[0]


# ## Classify pixel values into pervious and impervious class

# Create the RFT object from raster function template portal item. This object serves as a python function corresponding to the Raster Function Template. 

# In[8]:


rft_to_reclassify_ob = RFT(rft_to_reclassify)


# Once an RFT class object has been created, a single question mark before, or after the RFT object will show help relative to it. The help document would display the parameters that were marked as public by the author of the RFT.

# In[9]:


get_ipython().run_line_magic('pinfo', 'rft_to_reclassify_ob')


# We can visualize the function chain and the help document specific to this RFT inorder to understand the input parameters and the functions involved.

# To view the raster function chain visually, we install graphviz Python library.

# In[ ]:


# ! conda install graphviz -y


# In[11]:


# rft_to_reclassify_ob.draw_graph()


# Provide the values for the input parameters in the RFT object to create the Imagery Layer.

# In[12]:


output = rft_to_reclassify_ob(Raster=classified_raster.layers[0])


# In[13]:


persisted_output = output.save('generate_reclassified_raster' + str(dt.now().microsecond))


# In[14]:


impervious_lyr = persisted_output.layers[0]


# In[15]:


impervious_lyr


# ## Tabulate area of impervious surfaces

# We'll determine the area of impervious surfaces within each parcel of land in the neighborhood. We'll first calculate the area and store the results in a stand-alone table. Then, we'll join the table to the Parcels layer.

# In[ ]:


out_path = '/arcgis/home'


# In[16]:


arcpy.CreateFileGDB_management(out_path, "output_table.gdb")


# **Impervious** field shows the area (in feet) of impervious surfaces per parcel, while **Pervious** shows the area of pervious surfaces. We will calculate the impervious area with the **name** field which denotes Parcel_ID. 
# 

# In[17]:


tabulate_op = arcpy.sa.TabulateArea(in_zone_data=parcel_lyr.url, 
                      zone_field="name", 
                      in_class_data=impervious_lyr.url, 
                      class_field="ClassName",
                      out_table=r"/arcgis/home/output_table.gdb/output_table",
                      impervious_lyr.url)


# Now we have the area of impervious surfaces per parcel, but only in a stand-alone table. Next, we'll join the stand-alone table to the Parcels attribute table. A table join updates the input table with the attributes from another table based on a common attribute field. Because we created the impervious area table with the **name** field from the Parcels layer, we'll perform the join based on that field.

# In[18]:


arcpy.management.JoinField(in_data=parcel_lyr.url, 
                           in_field="name", 
                           join_table="/arcgis/home/output_table.gdb/output_table", 
                           join_field="NAME")


# ## Compute stormwater bill

# We will now use add_to_definition method which adds a new field to an existing feature layer. We will then calculate percentage of impervious area to determine the tax value.

# In[19]:


parcel_lyr.manager.add_to_definition(json_dict={"fields":[{"name":"percent_imp",
                                                           "type":"esriFieldTypeDouble",
                                                           "alias":"percent_imp",
                                                           "nullable":True,
                                                           "editable":True}]})
parcel_lyr.manager.add_to_definition(json_dict={"fields":[{"name":"rate",
                                                           "type":"esriFieldTypeDouble",
                                                           "alias":"rate",
                                                           "nullable":True,
                                                           "editable":True}]})


# In[20]:


parcel_lyr.calculate(where="fid > 0", 
                     calc_expression=[{"field":"percent_imp",
                                       "sqlExpression":" ( impervious / ( pervious + impervious ) ) * 100"}])
parcel_lyr.calculate(where="fid > 0", 
                     calc_expression=[{"field":"rate",
                                       "sqlExpression":" impervious * 0.04"}])


# **percent_imp** is the percentage of impervious area.
# 
# **rate** is the tax value calculated for $0.04 per square ft of impervious area.

# In[21]:


parcel_lyr.manager.delete_from_definition({"fields":[{"name":"name_1"}]})


# ## Visualize results

# In[22]:


sdf = pd.DataFrame.spatial.from_layer(parcel_lyr)
sdf[['name', 'mailingadd', 'ownercity', 'propertyus', 'impervious', 'pervious', 'percent_imp', 'rate']]


# ## Symbolize the parcels

# In[23]:


m1 = gis.map('Deleware, kent county')
m1


# The parcels with the highest area of impervious surfaces are rendered with a darker color while the ones with less impervious surface are light in color. While we could symbolize the layer by the percentage of area that is impervious, most storm water fees are based on total area, not percentage of area. The code below applies this symbology using `ClassedColorRenderer`.

# In[24]:


m1.basemap = 'satellite'


# In[25]:


m1.extent = {'spatialReference': {'latestWkid': 3857, 'wkid': 102100},
                         'xmin': -8423844.205248041,
                         'ymin': 4736924.0084905885,
                         'xmax': -8386237.187331785,
                         'ymax': 4752211.414147602}


# In[26]:


m1.add_layer(lyr, { "type": "FeatureLayer",
                    "renderer":"ClassedColorRenderer",
                    "field_name":"impervious",
                    "opacity":0.7})


# ## Conclusion

# In this notebook, we reclassified an aerial image of a neighborhood in Louisville, Kentucky, to show areas that were pervious and impervious to water. We then determined the area of impervious surfaces per land parcel. With the information, the local government would be better equipped to determine storm water bills. 

# ## References

# * https://www.dcwater.com/impervious-area-charge


# ====================
# calculate_post_fire_landslide_risk.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Raster Analytics - Calculate wildfire landslide risk

# <h1>**Table of Contents**<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Raster-Analytics---Calculate-wildfire-landslide-risk" data-toc-modified-id="Raster-Analytics---Calculate-wildfire-landslide-risk-1">Raster Analytics - Calculate wildfire landslide risk</a></span><ul class="toc-item"><li><span><a href="#Table-of-Contents" data-toc-modified-id="Table-of-Contents-1.1">Table of Contents</a></span><ul class="toc-item"><li><span><a href="#Import-required-libraries" data-toc-modified-id="Import-required-libraries-1.1.1">Import required libraries</a></span></li><li><span><a href="#Get-data" data-toc-modified-id="Get-data-1.1.2">Get data</a></span></li></ul></li><li><span><a href="#Create-a-burn-severity-map" data-toc-modified-id="Create-a-burn-severity-map-1.2">Create a burn severity map</a></span><ul class="toc-item"><li><span><a href="#Visual-Assessment" data-toc-modified-id="Visual-Assessment-1.2.1">Visual Assessment</a></span></li><li><span><a href="#Quantitative-Assessment" data-toc-modified-id="Quantitative-Assessment-1.2.2">Quantitative Assessment</a></span></li></ul></li><li><span><a href="#Create-a-slope-index-map" data-toc-modified-id="Create-a-slope-index-map-1.3">Create a slope index map</a></span></li><li><span><a href="#Create-the-landslide-risk-map" data-toc-modified-id="Create-the-landslide-risk-map-1.4">Create the landslide risk map</a></span><ul class="toc-item"><li><span><a href="#Reclassify-landcover-type-to-reflect-vegetation-stabiilty" data-toc-modified-id="Reclassify-landcover-type-to-reflect-vegetation-stabiilty-1.4.1">Reclassify landcover type to reflect vegetation stabiilty</a></span></li><li><span><a href="#Perform-a-weighted-overlay-analysis" data-toc-modified-id="Perform-a-weighted-overlay-analysis-1.4.2">Perform a weighted overlay analysis</a></span></li><li><span><a href="#Generate-a-persistent-analysis-result-via-Raster-Analysis" data-toc-modified-id="Generate-a-persistent-analysis-result-via-Raster-Analysis-1.4.3">Generate a persistent analysis result via Raster Analysis</a></span></li></ul></li><li><span><a href="#Summarize-landslide-risk-by-sub-basin" data-toc-modified-id="Summarize-landslide-risk-by-sub-basin-1.5">Summarize landslide risk by sub-basin</a></span></li></ul></li><li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-2">Conclusion</a></span></li></ul></div>

# In October 2017, wildfires raged through Sonoma and Napa counties, devastating surrounding communities. In the wake of these fires, the burn scars could cause further risk to public safety from a different kind of disaster: landslides. Post-fire landslides are particularly hazardous because there is more erosion and weaker soil in burned areas without vegetation to anchor the topsoil.
# 
# Groups handling rehabilitation, emergency planning and mitigation after a wildfire need to assess the vulnerability of the landscape to landslides. In this notebook, we will provide local emergency management teams a summary of post-wildfire landslide risk, so officials can target mitigation efforts to the most vulnerable watershed basins.
# 
# We will use the imagery layers to assess landslide risk per watershed within the burn area. We will create a landslide risk map and then summarize the landslide risk based on watershed sub-basins. We will use raster function chains to derive a burn severity map, a topographic slope map, and a landcover index map. These individual processing chains will be combined into one processing chain for distributed processing on the Raster Analytics server and then be summarized by watershed sub-basins.

# ## Table of Contents
# * [Import required libraries](#Import-required-libraries)
# * [Get data](#Get-data)
# * [Create-a-burn-severity-map](#Train-the-Model)
#     * [Visual Assessment](#Visual-Assessment)
#     * [Quantitative Assessment](#Quantitative-Assessment)
# * [Create a slope index map](#Create-a-slope-index-map)
# * [Create the landslide risk map](#Create-the-landslide-risk-map)
#     * [Reclassify landcover type to reflect vegetation stabiilty](#Reclassify-landcover-type-to-reflect-vegetation-stabiilty)
#     * [Perform a weighted overlay analysis](#Perform-a-weighted-overlay-analysis)
#     * [Generate a persistent analysis result via Raster Analysis on Portal for ArcGIS](#Generate-a-persistent-analysis-result-via-Raster-Analysis-on-Portal-for-ArcGIS)
# * [Summarize landslide risk by subbasin](#Summarize-landslide-risk-by-subbasin)
# * [Conclusion](#Conclusion)

# ### Import required libraries

# In[1]:


import arcgis
from arcgis.gis import GIS
from arcgis.raster.functions import *
from ipywidgets import *


# In[2]:


gis = GIS(profile="your_enterprise_profile")
arcgis.raster.analytics.is_supported(gis)


# ### Get data
# For this analysis we need the following datasets
#  * a Landsat 8 imagery for before (Before_L8) 
#  * a Landast 8 imagery for after (After_L8) the wildfire
#  * a DEM (digital elevation model) showing the elevation of the terrain
#  * a NLCD (National Landcover Dataset) showing land use and predominant vegetation type
#  * a watershed basin dataset
#  
# In the cells below, we access these datasets from the GIS

# In[3]:


before_l8 = gis.content.search('title:Before_L8 owner:api_data_owner',
                               item_type = "Image Service",
                               outside_org=True)[0].layers[0]
after_l8 = gis.content.search('title:After_L8 owner:api_data_owner',
                              item_type = "Image Service",
                              outside_org=True)[0].layers[0]

before_l8


# In[4]:


dem = gis.content.search('title:Sonoma_DEM owner:api_data_owner',
                         item_type = "Image Service",
                         outside_org=True)[0].layers[0]
nlcd = gis.content.search('title:Sonoma_NLCD2011 owner:api_data_owner',
                          item_type = "Image Service",
                          outside_org=True)[0].layers[0]
basins = gis.content.search('title:Sonoma_Basins owner:api_data_owner',
                            item_type = "Image Service",
                            outside_org=True)[0].layers[0]

# A preview of National Landcover Dataset layer
nlcd


# ## Create a burn severity map

# To compare the burn scars on the before and after Landsat imagery, we’ll choose the multispectral bands `5`,`3`,`2` to be displayed. The [5,3,2] band combination improves visibility of fire and burn scars. Healthy vegetation is shown in *bright red*, while stressed vegetation is displayed as *dull red*. Nonvegetated features such as bare and urban areas are displayed in various shades of *gray and blue*.
# 
# Below, we apply the same bands combination to the before_l8 and after_l8 layers.

# In[5]:


infrared_before = extract_band(before_l8,
    band_names = ['sr_band5','sr_band3','sr_band2'])
infrared_after = extract_band(after_l8,
    band_names = ['sr_band5','sr_band3','sr_band2'])


# ### Visual Assessment
# Below, in order to visually compare the burn effects, we create two maps and load the extracted bands of before and after imagery.

# In[6]:


# Create two maps to compare before and after imageries side by side
map1 = gis.map(location='-122.58, 38.45', zoomlevel=10)
map2 = gis.map(location='-122.58, 38.45', zoomlevel=10)
map1.layout = Layout(flex='1 1', height='500px', padding='10px')
map2.layout = Layout(flex='1 1', height='500px', padding='10px')
map1.add_layer(infrared_before)
map2.add_layer(infrared_after)
box = HBox([map1, map2])
box


# From the maps above, we are able to visually observe the burn scars. Next, let us repeat this process, but this time, we will try to quantify the extent of forest fire.
# 
# ### Quantitative Assessment

# A **Normalized Burn Ratio (NBR)** can be used to delineate the burned areas and identify the severity of the fire. The formula for NBR is very similar to that of NDVI except that it uses near-infrared band 5 and the short-wave infrared band 7:
# 
# $$
# NBR = \frac{B_5 - B_7}{B_5 + B_7}
# $$
# 
# The NBR equation was designed to be calculated from reflectance, but it can be calculated from radiance and digital_number (dn) with changes to the burn severity (discussed in the table below). For a given area, an NBR is calculated from an image just prior to the burn and a second NBR is calculated for an image immediately following the burn. Burn extent and severity is evaluated by taking the difference between these two index layers:
# 
# $$
# \Delta NBR = NBR_{prefire} - NBR_{postfire}
# $$
# 
# 
# The meaning of the ∆NBR values can vary by scene, and interpretation in specific instances should always be based on some field assessment. However, the following table from the USGS FireMon program can be useful as a first approximation for interpreting the NBR difference:
# 
# 
# | \begin{align}{\Delta \mathbf{NBR}}  \end{align}      | Burn Severity |
# | ------------- |:-------------:|
# | -2.0 to 0.1   | Regrowth and Unburned |
# | 0.1 to 0.27   | Low severity burn |
# | 0.27 to 0.44  | Medium severity burn |
# | 0.44 to 0.66 | Moderate severity burn |
# | > 0.66 | High severity burn |
# 
# Source: http://wiki.landscapetoolbox.org/doku.php/remote_sensing_methods:normalized_burn_ratio

# In[7]:


# Calculate before/after NBR indices and their difference
nbr_prefire  = band_arithmetic(before_l8,
                               band_indexes = "(b5 - b7) / (b5 + b7)")
nbr_postfire = band_arithmetic(after_l8,
                               band_indexes = "(b5 - b7) / (b5 + b7)")

nbr_diff = nbr_prefire - nbr_postfire


# In[8]:


# Use Remap function to reclassify the NBR difference score to 1-5
nbr_diff_remap = remap(nbr_diff,
                       input_ranges=[-2.0,  0.1,  # Regrowth and Unburned
                                     0.1, 0.27,   # Low Severity burn
                                     0.27, 0.44,  # Medium Severity burn
                                     0.44, 0.66,  # Moderate Severity
                                     0.66, 2.00], # High Severity
                       output_values=[1, 2, 3, 4, 5], 
                       astype='u8')

# Create a colormap to show reclassified NBR indices with different color
burn_severity = colormap(nbr_diff_remap, 
                        colormap=[[1, 56, 168, 0], [2, 141, 212, 0], 
                                  [3, 255, 255, 0], [4, 255, 128, 0], 
                                  [5, 255, 0, 0]])


# To view the raster function chain visually, we install `graphviz` Python library. Note: *after the installation is done, this notebook kernel has to be restarted and ran from the beginning once again to have graphviz imported and variables declared*.

# In[9]:


get_ipython().system(' conda install graphviz -y')


# In[1]:


# Overview of what raster functions have been applied to 
# create burn_serverity layer
burn_severity.draw_graph()


# In[11]:


# Visualize burnt areas
burn_severity


# So far, we have computed the NBR on images from before and after the burn, and computed the NBR difference to identify places that have suffered the fire. We've also normalized the values to match a burn severity index, and applied a color map that brings out the extent of fire damage, seen in the image above.
# 
# In the next section, we will use the quantitative burn severity image, the DEM, the NLCD, and the watershed basin dataset to predict places that are at a high risk for landslides.

# ## Create a slope index map
# 
# Before we can calculate a landslide risk map, we need to create a slope map. This is a critical layer in determining land stability. Slope steepness is derived from a digital elevation model (DEM). The steeper the slope, the more prone it is to slipping, especially during a rainfall event after stabilizing vegetation has been burned away. Next, we’ll build a raster function chain to calculate percent slope and use it to create a steepness index.

# Let's calculate slope, reclassify slope values, and assign color scheme in one raster function chain.

# In[2]:


# Create slope index layer with a raster function chain
slope_index = colormap(remap(slope(dem),
                             input_ranges=[0,  5,  # Flat
                                           5, 15,  # Low
                                           15, 25,  # Moderate
                                           25, 35,  # Steep
                                           35, 91], # Very Steep
                             output_values=[1, 2, 3, 4, 5], 
                             astype='u8'),
                       colormap=[[1, 56, 168, 0], [2, 141, 212, 0], 
                                 [3, 255, 255, 0], [4, 255, 128, 0], 
                                 [5, 255, 0, 0]])

# Overview of what raster functions have been applied to 
# create slope_index layer
slope_index.draw_graph()


# In[13]:


# Visualize slope index layer
slope_index


# ## Create the landslide risk map

# The landslide risk calculation combines the two variables you just worked with: burn severity and slope. It also includes landcover, which is also important to landslide risk. Vegetation stabilizes slopes via root systems. Wildfire can wipe out much of the stabilizing vegetation. Some types of vegetation have adapted to wildfire, such as species of chaparral, which have root systems that are especially deep underground to survive fires. 
# 
# In this section, we will first reclassify Landcover into five categories depending on its stabilizing effect on slopes. Then, we'll perform a **Weighted Overlay Analysis** based on the following three factors to determine the landslide risk in the study area:
# 
# * Burn severity (less severely burned areas have lower landslide risk)
# * Slope (lower slope areas have lower landslide risk)
# * Stability index (higher vegetation-stablized areas have lower landslide risk)
# 
# The weighted overlay is a standard GIS analysis technique often used for solving multi-criteria problems such as generating surfaces representing site suitability and travel cost. Weighted overlay is used when a number of factors of varying importance should be considered to arrive at a final decision. 

# ### Reclassify landcover type to reflect vegetation stabiilty

# In[14]:


# Remap landcover to derive values for vegetation stability
stability_index = remap(nlcd,
                        input_ranges=[0,  12, 
                                      21, 23, 
                                      23, 23, 
                                      24, 31, 
                                      31, 44,
                                      52, 52,
                                      71, 71,
                                      81, 81,
                                      82, 82,
                                      90, 95], 
                        output_values=[1, 3, 4, 5, 1, 3, 4, 3, 4, 2], 
                        # 1: very stable 2: stable 3: moderate 
                        # 4: unstable 5: very unstable
                        astype='u8')


# ### Perform a weighted overlay analysis
# Now that we have gathered the different layers required for the analysis, we can proceed with the weighted overlay analysis.
# 
# **Step 1: Normalize the input datasets**
# We have already used the `remap` function to normalize all the input datasets (burn_severity, slope_index, and stability_index) to a common scale of 1 - 5. Locations assigned the value of 1 are considered to carry the least landslide risk (e.g. stablized slopes with low burn severity), while locations assigned the value of 5 are considered to carry a high risk of landslide (e.g. the steepest slopes and most burned areas).

# **Step 2: Assign weights to the normalized input datasets based on their relative importance**
# 
# In this step we assign "weights" to the normalized inputs by multiplying each of them by a value between 0.0 and 1.0.  The sum of the weight values must equal 1.0 
# 
# We'll use [map algebra](http://desktop.arcgis.com/en/arcmap/latest/extensions/spatial-analyst/map-algebra/what-is-map-algebra.html) to apply the following weights to the criteria of this study:
# * Burn severity: 30%
# * Slope: 55%
# * Stability index: 15%
# 
# We'll multiply each raster in the step below to produce the final result.

# **Step 3: Calculate the sum of the weighted input datasets**
# 
# In this step, we calculate the final result of the weighted overlay by calculating the sum of the weighted input datasets. Areas that are most likely to have landslides according to our multi-criteria based on burn severity, slope, and stability index are assigned a value of 5 and displayed in red. Areas that are least likely to have landslides are assigned a value of 1 and displayed in green.

# In[15]:


landslide_risk = colormap(# burn severity
                         0.30 * burn_severity
                         + 
                         # Slope index
                         0.55 * slope_index
                         +
                         # Stability index
                         0.15 * stability_index,
                         colormap=[[1, 56, 168, 0], [2, 141, 212, 0], 
                                   [3, 255, 255, 0], [4, 255, 128, 0], 
                                   [5, 255, 0, 0]],
                         astype='u8')


# In[16]:


# Create three maps to compare before and after imageries 
# and the landslide risk map side by side
map1 = gis.map(location='-122.58, 38.45', zoomlevel=15)
map2 = gis.map(location='-122.58, 38.45', zoomlevel=15)
map3 = gis.map(location='-122.58, 38.45', zoomlevel=15)
map1.layout = Layout(flex='1 1', height='500px', padding='5px')
map2.layout = Layout(flex='1 1', height='500px', padding='5px')
map3.layout = Layout(flex='1 1', height='500px', padding='5px')
map1.add_layer(infrared_before)
map2.add_layer(infrared_after)
map3.add_layer(landslide_risk)
box = HBox([map1, map2, map3])
box


# ### Generate a persistent analysis result via Raster Analysis
# Raster functions perform their calculations on-the-fly. Since no intermediate datasets are created, processes can be applied quickly, as opposed to the time it would take to create a processed file on disk.
# 
# ArcGIS Enterprise can perform distributed server-based processing on imagery and raster data. By using Raster Analysis on an ArcGIS Image Server federated with your ArcGIS Enterprise portal, you can generate persistent analysis results from raster functions using distributed processing. 
# 
# This technology enables you to boost the performance of raster processing by processing data across multiple cores, even at full resolution and full extent. The results of this processing are accessed in a web imagery layer hosted in the ArcGIS Enterprise portal.
# 
# For more information, see [Distributed raster analysis](https://pro.arcgis.com/en/pro-app/help/data/imagery/raster-analysis-with-portal.htm)
# 
# To save the landslide risk layer as a persistent Imagery Layer on your GIS, call the `save()` method on the layer. This invokes the distributed raster processing, applies the chain of raster functions at full resolution, and serves the result as a new Imagery Layer.

# In[17]:


# Save the result as an image service
landslide_risk_persistent = landslide_risk.save("Sonoma_Landslide_Risk")


# In[18]:


# Share it to the public
landslide_risk_persistent.share(everyone=True, org=True)


# ## Summarize landslide risk by sub-basin

# Although the landslide risk map is useful, you may want to go further to break down the areas that are most at risk. Because landslide risk is impacted by precipitation patterns and watershed characteristics, we'll summarize risk by watershed basins within the study area. We’ll use the landslide risk map produced in the previous step to summarize risk per sub-basin.

# In[19]:


# Preview of the sub-basin layer
basins


# Now we will use a global raster function called **Zonal Statistics** to calculates statistics on values of a raster within the zones of another dataset. You will need the following parameters:
# 
# * `in_zone_data`: dataset that defines the zones
# * `zone_field`: Field that holds the values that define each zone
# * `in_value_raster`: Raster that contains the values on which to calculate a statistic
# * `ignore_no_data`: Denotes whether `NoData` values in the Value Raster will influence the results of the zone that they fall within.
# * `statistics_type`: The Statistic to be calculated. Can take any of the following values: `MEAN`, `MAJORITY`, `MAXIMUM`, `MEDIAN`, `MINIMUM`, `MINORITY`, `RANGE`, `STD`, `SUM`, `VARIETY`.
# 
# More information about this function can be found here [Zonal Statistics Global Function](http://pro.arcgis.com/en/pro-app/help/data/imagery/zonal-statistics-global-function.htm).

# In[20]:


get_ipython().run_cell_magic('time', '', '# Run global raster function and generate a persistent result in portal\nlandslide_risk_per_basin = gbl.zonal_statistics(in_zone_data = basins, \n                                        zone_field=\'Value\', \n                                        in_value_raster=landslide_risk, \n                                        ignore_nodata=True, \n                                        statistics_type=\'MEAN\')\n\nlandslide_risk_per_basin_saved = landslide_risk_per_basin.save(\n                                        "Landslide_Risk_Per_Basin")\n')


# In[21]:


# Save the final result as a web map
landslide_by_basin_map = gis.map('-122.58, 38.45', 10)

landslide_by_basin_map.add_layer(landslide_risk_per_basin_saved, 
                options={'title':'Landslide risk aggregated per basin'})
landslide_by_basin_map.add_layer(landslide_risk, 
                options={'title':'Landslide risk',
                         'visibility':False})
landslide_by_basin_map.add_layer(infrared_before,
                options={'title':'Pre fire satellite imagery',
                         'visibility':False})
landslide_by_basin_map.add_layer(infrared_after,
                options={'title':'Post fire satellite imagery',
                         'visibility':False})
landslide_by_basin_map.add_layer(dem,
                options={'title':'Elevation layer used to derive slope',
                         'visibility':False})
landslide_by_basin_map.add_layer(nlcd,
                options={'title':'National LandCover Dataset',
                         'visibility':False})
landslide_by_basin_map.add_layer(basins,
                options={'title':'Watershed basins',
                         'visibility':False})

landslide_by_basin_map.add_layer(infrared_after)
landslide_by_basin_map.save({'title':'Landslide risk map',
                            'tags':['landslide','analysis','forest fire'],
                            'snippet':'Landslide risk map per basin.'})


# Thus we have saved a web map containing the landslide risk per basin, overall landslide risk predictions, and all other layers used to derive the output. This map can easily be shared with local emergency management agencies.

# # Conclusion
# 
# In this notebook example, we used raster function chains to create a burn severity map, slope index map, and landcover index map. We then created a landslide risk map, and summarized risk by watershed sub-basin. We used Raster Functions to represent our model as Python functions, chained them together and rendered them on the fly. Finally, to save them as presistent results on the GIS, we used the power of distributed raster processing available on the Image Server and created output landslide risk Imagery Layers. The graphic below shows the entire chain of analysis we have performed so far.

# In[22]:


landslide_risk_per_basin.draw_graph()



# ====================
# calculating_cost_surfaces_using_weighted_overlay_analysis.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Using weighted overlay analysis to identify areas that are natural and accessible

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Using-weighted-overlay-analysis-to-identify-areas-that-are-natural-and-accessible" data-toc-modified-id="Using-weighted-overlay-analysis-to-identify-areas-that-are-natural-and-accessible-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Using weighted overlay analysis to identify areas that are natural and accessible</a></span><ul class="toc-item"><li><span><a href="#Weighted-overlay-analysis" data-toc-modified-id="Weighted-overlay-analysis-1.1"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>Weighted overlay analysis</a></span></li><li><span><a href="#Connect-to-the-GIS" data-toc-modified-id="Connect-to-the-GIS-1.2"><span class="toc-item-num">1.2&nbsp;&nbsp;</span>Connect to the GIS</a></span></li><li><span><a href="#Access-the-data-for-analysis" data-toc-modified-id="Access-the-data-for-analysis-1.3"><span class="toc-item-num">1.3&nbsp;&nbsp;</span>Access the data for analysis</a></span></li><li><span><a href="#Get-the-study-area-geometry-using-data-from-the-Living-Atlas" data-toc-modified-id="Get-the-study-area-geometry-using-data-from-the-Living-Atlas-1.4"><span class="toc-item-num">1.4&nbsp;&nbsp;</span>Get the study area geometry using data from the Living Atlas</a></span></li><li><span><a href="#Get-the-coordinates-of-the-study-area-extent-using-geocoding" data-toc-modified-id="Get-the-coordinates-of-the-study-area-extent-using-geocoding-1.5"><span class="toc-item-num">1.5&nbsp;&nbsp;</span>Get the coordinates of the study area extent using geocoding</a></span></li></ul></li><li><span><a href="#Preview-the-analysis-data" data-toc-modified-id="Preview-the-analysis-data-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Preview the analysis data</a></span><ul class="toc-item"><li><ul class="toc-item"><li><span><a href="#Human-Modification-for-North-America" data-toc-modified-id="Human-Modification-for-North-America-2.0.1"><span class="toc-item-num">2.0.1&nbsp;&nbsp;</span>Human Modification for North America</a></span></li><li><span><a href="#Elevation" data-toc-modified-id="Elevation-2.0.2"><span class="toc-item-num">2.0.2&nbsp;&nbsp;</span>Elevation</a></span></li><li><span><a href="#Slope-(derived-from-elevation-via-the-Slope-raster-function)" data-toc-modified-id="Slope-(derived-from-elevation-via-the-Slope-raster-function)-2.0.3"><span class="toc-item-num">2.0.3&nbsp;&nbsp;</span>Slope (derived from elevation via the Slope raster function)</a></span></li></ul></li></ul></li><li><span><a href="#Extract-the-analysis-data-within-the-study-area" data-toc-modified-id="Extract-the-analysis-data-within-the-study-area-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>Extract the analysis data within the study area</a></span></li><li><span><a href="#Perform-a-weighted-overlay-analysis" data-toc-modified-id="Perform-a-weighted-overlay-analysis-4"><span class="toc-item-num">4&nbsp;&nbsp;</span>Perform a weighted overlay analysis</a></span><ul class="toc-item"><li><ul class="toc-item"><li><span><a href="#Step-1:-Normalize-the-input-datasets" data-toc-modified-id="Step-1:-Normalize-the-input-datasets-4.0.1"><span class="toc-item-num">4.0.1&nbsp;&nbsp;</span>Step 1: Normalize the input datasets</a></span></li><li><span><a href="#Step-2:-Assign-weights-to-the-normalized-input-datasets-based-on-their-relative-importance." data-toc-modified-id="Step-2:-Assign-weights-to-the-normalized-input-datasets-based-on-their-relative-importance.-4.0.2"><span class="toc-item-num">4.0.2&nbsp;&nbsp;</span>Step 2: Assign weights to the normalized input datasets based on their relative importance.</a></span></li><li><span><a href="#Step-3:-Calculate-the-sum-of-the-weighted-input-datasets." data-toc-modified-id="Step-3:-Calculate-the-sum-of-the-weighted-input-datasets.-4.0.3"><span class="toc-item-num">4.0.3&nbsp;&nbsp;</span>Step 3: Calculate the sum of the weighted input datasets.</a></span></li><li><span><a href="#Perform-the-analysis-in-a-single-operation." data-toc-modified-id="Perform-the-analysis-in-a-single-operation.-4.0.4"><span class="toc-item-num">4.0.4&nbsp;&nbsp;</span>Perform the analysis in a single operation.</a></span></li></ul></li><li><span><a href="#Generate-a-persistent-analysis-result-via-Raster-Analysis-on-Portal-for-ArcGIS." data-toc-modified-id="Generate-a-persistent-analysis-result-via-Raster-Analysis-on-Portal-for-ArcGIS.-4.1"><span class="toc-item-num">4.1&nbsp;&nbsp;</span>Generate a persistent analysis result via Raster Analysis on Portal for ArcGIS.</a></span><ul class="toc-item"><li><span><a href="#Invoking-Raster-Analysis-on-ArcGIS-Enterprise" data-toc-modified-id="Invoking-Raster-Analysis-on-ArcGIS-Enterprise-4.1.1"><span class="toc-item-num">4.1.1&nbsp;&nbsp;</span>Invoking Raster Analysis on ArcGIS Enterprise</a></span></li></ul></li></ul></li><li><span><a href="#Conclusions" data-toc-modified-id="Conclusions-5"><span class="toc-item-num">5&nbsp;&nbsp;</span>Conclusions</a></span></li></ul></div>

# Many people vacation to scenic areas free from everyday noise and congestion. However, many scenic areas can be difficult to reach and challenging to navigate once there. Many places also vary in their degree of remoteness from everyday human activity that travelers like to escape. This sample identifies areas in the State of Washington that are more "natural" and easy to get to and visit based on the following criteria:
# 
# * elevation (lower elevations are easier to travel)
# * steepness of the terrain (lower slopes are easier to travel)
# * degree of human alteration of the landscape (less altered landscapes are more natural)
# 
# The input data for this analysis includes a DEM (Digital Elevation Model), and a dataset showing the degree of human modification to the landscape.
# ## Weighted overlay analysis
# The weighted overlay is a standard GIS analysis technique often used for solving multicriteria problems such as generating surfaces representing site-suitability and travel-cost. Weighted overlay is used when a number of factors of variying importance should be considered to arrive at a final decision. This sample shows how raster anlaysis and raster arithmetic can be used to perform such analysis to solve spatial problems. The graphic below explains the logic behind weighted overlay, refer to this help for [a detailed review of weighted overlay analysis](http://desktop.arcgis.com/en/arcmap/10.3/tools/spatial-analyst-toolbox/how-weighted-overlay-works.htm)
# 
# ![](http://desktop.arcgis.com/en/arcmap/10.3/tools/spatial-analyst-toolbox/GUID-D6D1FED1-18BA-4CFC-A056-498736B56C89-web.gif)
# 
# In the illustration, the two input rasters have been reclassified to a common measurement scale of 1 to 3. Each raster is assigned a percentage influence. The cell values are multiplied by their percentage influence, and the results are added together to create the output raster. For example, consider the upper left cell. The values for the two inputs become (2 \* 0.75) = 1.5 and (3 \* 0.25) = 0.75. The sum of 1.5 and 0.75 is 2.25. Because the output raster from Weighted Overlay is integer, the final value is rounded to 2.

# ## Connect to the GIS

# In[1]:


# import GIS from the arcgis.gis module
from arcgis.gis import GIS
from arcgis.features import FeatureLayer


# In[2]:


# Connect to the GIS
gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')
print("Successfully connected to {0}".format(gis.properties.name))


# ## Access the data for analysis

# In[3]:


# Search for the Human Modified Index imagery layer item by title
hmna_item = gis.content.search('title:Human Modification for the United States', 'Imagery Layer')[0]
hmna_item


# In[4]:


# Search for the DEM imagery layer item by title
elev_item = gis.content.search('title:National Elevation Dataset (NED) owner:api_data_owner', 'Imagery Layer')[0]
elev_item


# ## Get the study area geometry using data from the Living Atlas

# In[5]:


# Access the USA States item from the Living Atlas using the item id value
url = 'https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services/USA_States_Generalized/FeatureServer/0'
states_lyr = FeatureLayer(url)
states_lyr


# We choose the **State of Washington** as our study area for this example. We will query for the geometry and then set the spatial reference.

# In[6]:


# Access the feature for the State of Washington
study_area_query = states_lyr.query("STATE_NAME='Washington'", return_geometry=True)


# In[7]:


# Get the geometry of the State of Washington feature.
# We will use this geometry to extract the input data for the study area.
study_area_geom= study_area_query.features[0].geometry
study_area_geom['spatialReference'] = study_area_query.spatial_reference


# ## Get the coordinates of the study area extent using geocoding

# In[8]:


# Import the geocode function
from arcgis.geocoding import geocode
# Use geocoding to get the location of the study area in the spatial reference of the input data for the analysis.
study_area_gcd = geocode(address='State of Washington, USA', out_sr=hmna_item.layers[0].properties["spatialReference"])
# Get the geographic extent of the study area.
# This extent will be used for displaying the input data and output results.
study_area_extent = study_area_gcd[1]['extent']
study_area_extent


# # Preview the analysis data

# ### Human Modification for North America

# In[9]:


# Get a reference to the imagery layer from the portal item
hmna_lyr = hmna_item.layers[0]
# Set the layer extent to geographic extent of study area and display the data.
hmna_lyr.extent = study_area_extent
hmna_lyr


# ### Elevation

# In[10]:


# Get a reference to the imagery layer from the portal item
elev_lyr = elev_item.layers[0]
# Set the layer extent to the geographic extent of study area and display the data.
elev_lyr.extent = study_area_extent
elev_lyr


# ### Slope (derived from elevation via the Slope raster function)

# In[11]:


# Import the raster functions from the API
from arcgis.raster.functions import *


# In[27]:


# Derive a slope layer from the DEM layer using the slope function
slope_lyr = slope(dem=elev_lyr, slope_type='DEGREE', z_factor=1)
slope_lyr.extent = study_area_extent
# Use the stretch function to enhance the display of the slope layer.
stretch(raster=slope_lyr, stretch_type='StdDev', dra='true')


# # Extract the analysis data within the study area

# In[28]:


# Human Modification for North America
hmna_study_area = clip(raster=hmna_lyr, geometry=study_area_geom)
hmna_study_area


# In[29]:


# National Elevation Dataset
elev_study_area = clip(raster=elev_lyr, geometry=study_area_geom)
elev_study_area


# In[30]:


# Slope (derived from the NED via the Slope function)
slope_study_area = clip(raster=slope_lyr, geometry=study_area_geom)
# Apply the Stretch function to enhance the display of the slope_clipped layer.
stretch(raster=slope_study_area, stretch_type='StdDev', dra='true')


# # Perform a weighted overlay analysis
# Now that we have gathered the different layers required for the analysis and clipped them to the extent of the study area, we can proceed with the weighted overlay analysis

# ### Step 1: Normalize the input datasets 
# 
# In this step we use the Remap function to normalize the input datasets to a common scale of 1 - 9. Locations assigned the value of 1 are considered the least suitable according to our criteria (e.g. the steepest slopes are the least accessible for travel), while locations assigned the value of 9 are considered the most suitable according to our criteria (e.g. areas that are least modified by human activity are more "natural").

# In[ ]:


# Create a colormap to display the analysis results with 9 colors ranging from red to green to yellow.
clrmap=  [[1, 230, 0, 0], [2, 242, 85, 0], [3, 250, 142, 0], [4, 255, 195, 0], [5, 255, 255, 0], [6, 197, 219, 0],  
          [7, 139, 181, 0], [8, 86, 148, 0], [9, 38, 115, 0]]


# In[32]:


# Reclassify (normalize) the elevation data
elev_normalized = remap(elev_study_area,
                       input_ranges=[0,490, 490,980, 980,1470, 1470,1960, 1960,2450, 
                                     2450,2940, 2940,3430, 3430,3700, 3920,4100],
                       output_values=[9,8,7,6,5,4,3,2,1], astype='U8')

# Display color-mapped image of the reclassified elevation data
colormap(elev_normalized, colormap=clrmap)


# In[33]:


# Reclassify the slope data
slope_normalized = remap(raster=slope_study_area, 
                        input_ranges=[0,1, 1,2, 2,3, 3,5, 5,7, 7,9, 9,12, 12,15, 15,100],
                        output_values=[9,8,7,6,5,4,3,2,1],  astype='U8') 

# Display a color-mapped image of the reclassified slope data
colormap(slope_normalized, colormap=clrmap)


# In[34]:


# Reclassify the Human Modified Index data
hmna_normalized = remap(raster=hmna_study_area,
                  input_ranges=[0.0,0.1, 0.1,0.2, 0.2,0.3, 0.3,0.4, 0.4,0.5,
                                0.5,0.6, 0.6,0.7, 0.7,0.8, 0.8,1.1],
                  output_values=[9,8,7,6,5,4,3,2,1],  astype='U8')

# Display a color-mapped image of the reclassified HMI data
colormap(hmna_normalized, colormap=clrmap)


# ### Step 2: Assign weights to the normalized input datasets based on their relative importance.
# 
# In this step we assign "weights" to the normalized inputs by multiplying each of them by a value between 0.0 and 1.0.  The sum of the weight values must equal 1.0 

# We'll use [map algebra](https://desktop.arcgis.com/en/arcmap/latest/extensions/spatial-analyst/map-algebra/what-is-map-algebra.htm) to apply the following weights to the criteria of this study:
# * Human Modified Index: 60%
# * Slope: 25%
# * Elevation: 15%
# We'll multiply each raster in the step below to produce the final result.

# ### Step 3: Calculate the sum of the weighted input datasets.
# 
# In this step we calculate the final result of the weighted overlay by calculating the sum of the weighted input datasets.  Areas that are most suitable according to our multi-criteria based on slope, elevation, and degree of human modification are assigned a value of 9 and displayed in green.  Areas that are least suitable are assigned a value of 1 and displayed in red.

# In[35]:


result_weighted_overlay = colormap(hmna_normalized * 0.6 + 
                                    slope_normalized * 0.25 + 
                                    elev_normalized * 0.15,
                                    colormap=clrmap)
result_weighted_overlay


# ###  Perform the analysis in a single operation.
# To illustrate the mechanics of a weighted overlay analysis, we just performed each step of the analysis as separate operations.  However, for conciseness, we can also perform a complete analysis in a single operation, for example.

# In[36]:


result_weighted_overlay_one_op = colormap(
        # Human modified index layer
        0.60 * remap(clip(raster=hmna_lyr, geometry=study_area_geom),
                     input_ranges=[0.0,0.1, 0.1,0.2, 0.2,0.3, 0.3,0.4, 0.4,0.5,
                                   0.5,0.6, 0.6,0.7, 0.7,0.8, 0.8,1.1],
                     output_values=[9,8,7,6,5,4,3,2,1])
        + 
        # Slope layer
        0.25 * remap(clip(raster=slope_lyr, geometry=study_area_geom), 
                     input_ranges=[0,1, 1,2, 2,3, 3,5, 5,7, 7,9, 9,12, 12,15, 
                                   15,100],
                     output_values=[9,8,7,6,5,4,3,2,1]) 
        +
        # Elevation layer
        0.15 * remap(clip(raster=elev_lyr, geometry=study_area_geom), 
                    input_ranges=[-90,250, 250,500, 500,750, 750,1000, 1000,1500, 
                                  1500,2000, 2000,2500, 2500,3000, 3000,5000],
                    output_values=[9,8,7,6,5,4,3,2,1]),
    colormap=clrmap,  astype='U8'
)
result_weighted_overlay_one_op


# ## Generate a persistent analysis result via Raster Analysis on Portal for ArcGIS.
# Raster functions perform their calculations on-the-fly. Since no intermediate datasets are created, processes can be applied quickly, as opposed to the time it would take to create a processed file on disk.  On the other hand, Portal for Arcgis as a component of ArcGIS Enterprise has been enhanced with the ability to perform distributed server based processing on imagery and raster data.  By using Raster Analysis on Portal for ArcGIS you can generate persistent analysis results from raster functions using distributed server-based processing on imagery and raster data. This technology enables you to boost the performance of raster processing by processing data in a distributed fashion, even at full resolution and full extent. The results of this processing can be accessed in the form of a web imagery layer that is hosted in the ArcGIS Organization.
# 
# For more information, see [Raster analysis on Portal for ArcGIS](https://pro.arcgis.com/en/pro-app/help/data/imagery/raster-analysis-with-portal.htm)

# In[ ]:


# Does the GIS support raster analytics?
import arcgis
arcgis.raster.analytics.is_supported(gis)


# ### Invoking Raster Analysis on ArcGIS Enterprise
# 
# The .save() function invokes generate_raster from the arcgis.raster.analytics
# module to run the analysis at the source resolution of the input datasets and stores the result as a 
# persistent web imagery layer in the GIS.  For example...
# 
# `result_persistent = result_weighted_overlay.save("NaturalAndAccessible_WashingtonState")`
# 
# A persistent analysis result has already been generated for this sample.  We conclude by comparing the persistent result obtained via Raster Analysis on Portal for ArcGIS with the temporary result obtained via on-the-fly processing.

# In[40]:


# Search for the persistent analysis result that was generated via Raster Analytics on the GIS.
search_persistent_result = gis.content.search(query="title:NaturalAndAccessible_WashingtonState")[0]
search_persistent_result


# In[41]:


# Create a map to display the persistent analysis result in a map
map1 = gis.map('State of Washington, USA')
map1.add_layer(search_persistent_result)
# Create a map to display the dynamic analysis result in another map.
map2 = gis.map('State of Washington, USA')
map2.add_layer(result_weighted_overlay)


# In[2]:


map1


# In[3]:


map2


# In[4]:


# Display the two maps side-by-side
from ipywidgets import *
map1.layout = Layout(flex='1 1', padding = '10px')
map2.layout = Layout(flex='1 1', padding = '10px')
box = HBox([map1, map2])
box


# # Conclusions
# 
# Raster functions apply processing directly to the pixels of imagery and raster datasets.  There are many out-of-the-box functions that can be used for radiometric correction, geometric correction, data management, visualization, and for analysis, as shown in this sample.  You may have noticed that the analysis results from on-the-fly processing are not identical to those from Raster Analysis on Portal for ArcGIS.  With on-the-fly processing, each time the map is re-drawn, the analysis is executed at the resolution of the visible screen pixels, which are resampled from the pixels in the source data to lower resolutions as you zoom out to smaller map scales.  This dynamic resampling behavior is a feature of on-the-fly processing that enables raster functions to maintain a consistent level of high performance over a broad range of map scales.  In contrast, with Raster Analysis on Portal for ArcGIS, the analysis is run by default at the full resolution of the source data.  Therefore the differences in the two results become less pronounced as you zoom in to larger map scales.  


# ====================
# calculating_nXn_od_cost_matrix.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Calculating Origin Destinations nXn Matrix given set of origins and destinations

# <h2>Table of Contents<span class="tocSkip"></span></h2>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Origin-Destinations-nXn-Matrix-given-set-of-origins-and-destinations" data-toc-modified-id="Origin-Destinations-nXn-Matrix-given-set-of-origins-and-destinations-1">Origin Destinations nXn Matrix given set of origins and destinations</a></span><ul class="toc-item"><li><span><a href="#Create-origins-layer:" data-toc-modified-id="Create-origins-layer:-1.1">Create origins layer:</a></span></li><li><span><a href="#Get-destinations-layer:" data-toc-modified-id="Get-destinations-layer:-1.2">Get destinations layer:</a></span></li><li><span><a href="#Convert-to-matrix-format" data-toc-modified-id="Convert-to-matrix-format-1.3">Convert to matrix format</a></span></li><li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-1.4">Conclusion</a></span></li></ul></li></ul></div>

# 
# The [Origin Destination(OD) Cost Matrix service](http://desktop.arcgis.com/en/arcmap/latest/extensions/network-analyst/od-cost-matrix.htm) helps you to create an OD cost matrix for multiple `origins` to multiple `destinations`. An OD cost matrix is a table that contains cost, such as travel time or travel distance, from each origin to each destination. Additionally, it ranks the destinations in ascending order based on the minimum cost required to travel. When generating an OD cost matrix, you can optionally specify the maximum number of destinations to find for each origin and the maximum time or distance to travel when searching for destinations.
# 
# By default, the matrix is generated with columns - origin id, destination id, destination rank, total time and total distance. 
# In this sample notebook , we will use this tool to get OD matrix if given a set of origin and destination points, either as a csv with latitude and longitude or csv file with list of addresses. In later part of this sample, we will format the table to get n by n matrix.
# 
# This is useful when you want to solve other transportation problems with open source tools or heuristics. When it comes to real world TSP(Travelling Salesman Problem) or VRP(Vehicle Routing Problem) or other tranportation problems, data about travel time from every point to every other point can give you more realistic results than with euclidean distance.

# **Note** :If you run the tutorial using ArcGIS Online, 0.003 [credit](https://www.esri.com/en-us/arcgis/products/arcgis-online/pricing/credits) will be consumed as there are 6 origin-destination pairs.
# 
# As a first step, let's import required libraries and establish a connection to your organization which could be an ArcGIS Online organization or an ArcGIS Enterprise. If you dont have an ArcGIS account, [get ArcGIS Trial](https://www.esri.com/en-us/arcgis/trial).

# In[3]:


import arcgis
from arcgis.gis import GIS
import pandas as pd
import datetime
import getpass
from IPython.display import HTML

from arcgis import geocoding
from arcgis.features import Feature, FeatureSet
from arcgis.features import GeoAccessor, GeoSeriesAccessor


# In[4]:


my_gis = GIS('home')


# We will see how to create layer for origins and destinations when we have latitude and longitude and when we have addresses to geocode for converting to layer respectively.

# ## Create origins layer:

# We have latitude and longitude information for origins, with the following code snippet, we can create a layer from the information. We will reverse geocode the latitude longitude information to find the locations.
# 
# **Note**: Geocoding the addresses will consume [credits](https://www.esri.com/en-us/arcgis/products/arcgis-online/pricing/credits).

# In[6]:


origin_coords = ['-117.187807, 33.939479', '-117.117401, 34.029346']
origin_features = []

for origin in origin_coords:
    reverse_geocode = geocoding.reverse_geocode({"x": origin.split(',')[0], 
                                              "y": origin.split(',')[1]})    

    origin_feature = Feature(geometry=reverse_geocode['location'], 
                           attributes=reverse_geocode['address'])
    origin_features.append(origin_feature)

origin_fset = FeatureSet(origin_features, geometry_type='esriGeometryPoint',
                          spatial_reference={'latestWkid': 4326})
origin_fset


# ## Get destinations layer:

# In[8]:


addresses_item = my_gis.content.search('destinations_address', 'feature layer')[0]
addresses_item


# In[11]:


destinations_sdf = addresses_item.layers[0].query(as_df=True)
destinations_sdf


# In[12]:


destinations_fset = destinations_sdf.spatial.to_featureset()
destinations_fset


# With these inputs, solve the problem with Origin Destintion matrix solver. Look up [the doc](https://developers.arcgis.com/rest/network/api-reference/origin-destination-cost-matrix-service.htm) to understand how this tool works and its parameters. Remember, `0.0005` credits per input origin and destination pair will be charged. For example, if there are `100` origins and `200` destinations, the cost will be `10` credits. If you specify a cutoff or limit the number of destinations, for instance, to find only `5` closest destinations within `10` minutes of every origin, the cost will still be `10` credits, as the credits depend on the number of input origin destination pairs. 
# 
# `TargetDestinationCount`- The maximum number of destinations that must be found for the origin. If a value is not specified, the value from the Number of Destinations to Find parameter is used. 
# 
# `Cutoff`- Specify the travel time or travel distance value at which to stop searching for destinations from the origin. Any destination beyond the cutoff value will not be considered. The value needs to be in the units specified by the Time Units parameter if the impedance attribute in your travel mode is time based or in the units specified by the Distance Units parameter if the impedance attribute in your travel mode is distance based. If a value is not specified, the tool will not enforce any travel time or travel distance limit when searching for destinations.
# 
# Specify `origin_destination_line_shape` to see the output in map. Even though the lines are straight for performance reasons, they always store the travel time and travel distance along the street network, not straight-line distance.

# In[12]:


get_ipython().run_cell_magic('time', '', "# solve OD cost matrix tool for the origns and destinations\nfrom arcgis.network.analysis import generate_origin_destination_cost_matrix\nresults = generate_origin_destination_cost_matrix(origins= origin_fset, #origins_fc_latlong, \n                                                destinations= destinations_fset, #destinations_fs_address,\n                                                cutoff=200,\n                                                origin_destination_line_shape='Straight Line')\nprint('Analysis succeeded? {}'.format(results.solve_succeeded))\n")


# Let's see the output lines table. 

# In[15]:


od_df = results.output_origin_destination_lines.sdf
od_df


# ## Convert to matrix format
# We need to change the format to get a matrix with rows as origins and columns as destinations, with impedance value as travel time or travel distance. We will use the `pivot_table` feature of Pandas to accomplish that.

# In[29]:


# filter only the required columns
od_df2 = od_df[['DestinationOID','OriginOID','Total_Distance','Total_Time']]

# user pivot_table
od_pivot = od_df2.pivot_table(index='OriginOID', columns='DestinationOID')
od_pivot


# Write the pivot table to disk

# In[30]:


od_pivot.to_csv('data/OD_Matrix.csv')


# This is how we can get OD cost matrix when we have csv files with origin and destinations location information. We could read this matrix and provide this as input to a heuristics or an open-source algorithm. 

# In[25]:


od_map = my_gis.map('Loma Linda, CA')
od_map


# In[23]:


od_map.draw(results.output_origin_destination_lines)
od_map.draw(destinations_fset, symbol={"type": "esriSMS","style": "esriSMSSquare","color": [255,115,0,255], "size": 10})
od_map.draw(origin_fset, symbol={"type": "esriSMS","style": "esriSMSCircle","color": [76,115,0,255],"size": 8})


# ## Conclusion
# This sample demonstrated how you can constuct an OD cost matrix using the Python API. We stared by defining `2` origin and `3` destination points. We used the `generate_origin_destination_cost_matrix()` method under the `network` module to compute the OD cost matrix.
# 
# ### How can you use this?
# The OD cost matrix becomes an important analytical output for downstream routing and other network analysis problems. Imagine you run a pizza shop and receive orders for delivery in a central location. Based on the distance to these demand points, you need to decide which supply point (pizza shop) should service which demand point (customer address). You can solve problems such as these by computing the OD cost matrix.


# ====================
# california_wildfires_2017_thomas_fire_analysis.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # California wildfires 2017 - Thomas Fire analysis

# The Thomas Fire was a massive wildfire that started in early December 2017 in Ventura and Santa Barbara counties and grew into California's largest fire ever.
# 
# ![](../../static/img/thomasfire_cropped.jpg)

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#California-wildfires-2017---Thomas-Fire-analysis" data-toc-modified-id="California-wildfires-2017---Thomas-Fire-analysis-1">California wildfires 2017 - Thomas Fire analysis</a></span><ul class="toc-item"><li><span><a href="#Visualize-the-extent-of-damage" data-toc-modified-id="Visualize-the-extent-of-damage-1.1">Visualize the extent of damage</a></span><ul class="toc-item"><li><span><a href="#Nob-Hill,-Ventura,-CA" data-toc-modified-id="Nob-Hill,-Ventura,-CA-1.1.1">Nob Hill, Ventura, CA</a></span></li><li><span><a href="#Vista-Del-Mar-Hospital,-Ventura,-CA" data-toc-modified-id="Vista-Del-Mar-Hospital,-Ventura,-CA-1.1.2">Vista Del Mar Hospital, Ventura, CA</a></span></li></ul></li><li><span><a href="#Remote-Sensing-and-Image-Processing" data-toc-modified-id="Remote-Sensing-and-Image-Processing-1.2">Remote Sensing and Image Processing</a></span><ul class="toc-item"><li><span><a href="#Select-before-and-after-rasters" data-toc-modified-id="Select-before-and-after-rasters-1.2.1">Select before and after rasters</a></span></li></ul></li><li><span><a href="#Visual-Assessment" data-toc-modified-id="Visual-Assessment-1.3">Visual Assessment</a></span><ul class="toc-item"><li><span><a href="#Visualize-Burn-Scars" data-toc-modified-id="Visualize-Burn-Scars-1.3.1">Visualize Burn Scars</a></span></li></ul></li><li><span><a href="#Quantitative-Assessment" data-toc-modified-id="Quantitative-Assessment-1.4">Quantitative Assessment</a></span><ul class="toc-item"><li><span><a href="#Use-Band-Arithmetic-and-Map-Algebra" data-toc-modified-id="Use-Band-Arithmetic-and-Map-Algebra-1.4.1">Use Band Arithmetic and Map Algebra</a></span></li><li><span><a href="#Area-calculation" data-toc-modified-id="Area-calculation-1.4.2">Area calculation</a></span></li><li><span><a href="#Report-burnt-area" data-toc-modified-id="Report-burnt-area-1.4.3">Report burnt area</a></span></li><li><span><a href="#Visualize-burnt-areas" data-toc-modified-id="Visualize-burnt-areas-1.4.4">Visualize burnt areas</a></span></li></ul></li><li><span><a href="#Raster-to-Feature-layer-conversion" data-toc-modified-id="Raster-to-Feature-layer-conversion-1.5">Raster to Feature layer conversion</a></span></li><li><span><a href="#Impact-Assessment" data-toc-modified-id="Impact-Assessment-1.6">Impact Assessment</a></span><ul class="toc-item"><li><span><a href="#Compute-infrastructure-and-human-impact" data-toc-modified-id="Compute-infrastructure-and-human-impact-1.6.1">Compute infrastructure and human impact</a></span></li><li><span><a href="#Visualize-affected-roads-on-map" data-toc-modified-id="Visualize-affected-roads-on-map-1.6.2">Visualize affected roads on map</a></span></li><li><span><a href="#Age-Pyramid-of-affected-population" data-toc-modified-id="Age-Pyramid-of-affected-population-1.6.3">Age Pyramid of affected population</a></span></li></ul></li></ul></li></ul></div>

# In[1]:


import arcgis
from arcgis import *
from arcgis.mapping import MapImageLayer


# In[47]:


gis = GIS(profile="your_online_profile")


# ## Visualize the extent of damage

# In[3]:


from ipywidgets import *

postfire = MapImageLayer('https://tiles.arcgis.com/tiles/DO4gTjwJVIJ7O9Ca/arcgis/rest/services/Digital_Globe_Imagery_Dec_11th/MapServer')

def side_by_side(address):
    location = geocode(address)[0]

    satmap1 = gis.map(location)
    satmap1.basemap = 'satellite'

    satmap2 = gis.map(location)
    satmap2.add_layer(postfire)

    satmap1.layout=Layout(flex='1 1', padding='6px', height='450px')
    satmap2.layout=Layout(flex='1 1', padding='6px', height='450px')

    box = HBox([satmap1, satmap2])
    return box


# ### Nob Hill, Ventura, CA

# In[ ]:


side_by_side('Montclair Dr, Ventura, CA')


# 

# ### Vista Del Mar Hospital, Ventura, CA

# In[ ]:


side_by_side('801 Seneca St, Ventura, CA 93001')


# 

# ## Remote Sensing and Image Processing

# In[48]:


landsat_item = gis.content.get('d9b466d6a9e647ce8d1dd5fe12eb434b')
landsat = landsat_item.layers[0]
landsat_item


# ### Select before and after rasters

# In[12]:


aoi = {'spatialReference': {'latestWkid': 3857, 'wkid': 102100}, 'type': 'extent', 
       'xmax': -13305000, 'xmin': -13315000, 'ymax': 4106000, 'ymin': 4052000}

arcgis.env.analysis_extent = {"xmin":-13337766,"ymin":4061097,"xmax":-13224868,"ymax":4111469,
                              "spatialReference":{"wkid":102100,"latestWkid":3857}}

landsat.extent = aoi


# In[13]:


import pandas as pd
from datetime import datetime

selected = landsat.filter_by(where="(Category = 1)",
                             time=[datetime(2017, 11, 15), datetime(2018, 1, 1)],
                             geometry=arcgis.geometry.filters.intersects(aoi))

df = selected.query(out_fields="AcquisitionDate, GroupName, CloudCover, DayOfYear", 
                    order_by_fields="AcquisitionDate").sdf
df['AcquisitionDate'] = pd.to_datetime(df['AcquisitionDate'], unit='ms')
df.tail(5)


# In[21]:


prefire = landsat.filter_by('OBJECTID=' + str(df['OBJECTID'][0])) # 2017-11-23 
midfire = landsat.filter_by('OBJECTID=' + str(df['OBJECTID'][1])) # 2017-12-09 


# ## Visual Assessment

# In[33]:


from arcgis.raster.functions import *

apply(midfire, 'Natural Color with DRA')


# ### Visualize Burn Scars

# Extract the [6, 4, 1] bands to improve visibility of fire and burn scars. This band combination pushes further into the SWIR range of the electromagnetic spectrum, where there is less susceptibility to smoke and haze generated by a burning fire.

# In[34]:


extract_band(midfire, [6,4,1])


# In[35]:


extract_band(prefire, [6,4,1])


# For comparison, the same area before the fire started shows no burn scar.

# ## Quantitative Assessment

# The **Normalized Burn Ratio (NBR)** can be used to delineate the burnt areas and identify the severity of the fire. 
# 
# The formula for the NBR is very similar to that of NDVI except that it uses near-infrared band 5 and the short-wave infrared band 7:
# \begin{align}
# {\mathbf{NBR}} = \frac{\mathbf{B_5} - \mathbf{B_7}}{\mathbf{B_5} + \mathbf{B_7}} \\   
# \end{align}
# 
# The NBR equation was designed to be calcualted from reflectance, but it can be calculated from radiance and digital_number_(dn) with changes to the burn severity table below. 
# 
# For a given area, NBR is calculated from an image just prior to the burn and a second NBR is calculated for an image immediately following the burn. Burn extent and severity is judged by taking the difference between these two index layers:
# 
# \begin{align}
# {\Delta \mathbf{NBR}} = \mathbf{NBR_{prefire}} - \mathbf{NBR_{postfire}} \\   
# \end{align}
# 
# The meaning of the ∆NBR values can vary by scene, and interpretation in specific instances should always be based on some field assessment. However, the following table from the USGS FireMon program can be useful as a first approximation for interpreting the NBR difference:
# 
# 
# | \begin{align}{\Delta \mathbf{NBR}}  \end{align}      | Burn Severity |
# | ------------- |:-------------:|
# | 0.1 to 0.27   | Low severity burn |
# | 0.27 to 0.44  | Medium severity burn |
# | 0.44 to 0.66 | Moderate severity burn |
# | > 0.66 | High severity burn |
# 
# [Source: http://wiki.landscapetoolbox.org/doku.php/remote_sensing_methods:normalized_burn_ratio]

# ### Use Band Arithmetic and Map Algebra 

# In[24]:


nbr_prefire  = band_arithmetic(prefire, "(b5 - b7) / (b5 + b7+1000)")
nbr_postfire = band_arithmetic(midfire, "(b5 - b7) / (b5 + b7+1000)")

nbr_diff = nbr_prefire - nbr_postfire


# In[26]:


burnt_areas = colormap(remap(nbr_diff,
                             input_ranges=[0.1,  0.27,  # low severity 
                                           0.27, 0.44,  # medium severity
                                           0.44, 0.66,  # moderate severity
                                           0.66, 1.00], # high severity burn
                             output_values=[1, 2, 3, 4],                    
                             no_data_ranges=[-1, 0.1], astype='u8'), 
                             colormap=[[4, 0xFF, 0xC3, 0], [3, 0xFA, 0x8E, 0], [2, 0xF2, 0x55, 0], [1, 0xE6, 0,    0]])


# In[28]:


burnt_areas.draw_graph()


# ### Area calculation

# In[39]:


ext = {"xmax": -13246079.10806628, "ymin": 4035733.9433013694, "xmin": -13438700.419344831, "ymax": 4158033.188557592,
       "spatialReference": {"wkid": 102100, "latestWkid": 3857}, "type": "extent"}
pixx = (ext['xmax'] - ext['xmin']) / 1200.0
pixy = (ext['ymax'] - ext['ymin']) / 450.0

res = burnt_areas.compute_histograms(ext, pixel_size={'x':pixx, 'y':pixy})

numpix = 0
histogram = res['histograms'][0]['counts'][1:]
for i in histogram:
    numpix += i


# ### Report burnt area

# In[40]:


from IPython.display import HTML
sqmarea = numpix * pixx * pixy # in sq. m
acres = 0.00024711 * sqmarea   # in acres

HTML('<h3>Thomas fire has consumed <i>{:,} acres</i>  till {}</h3>.'.format(int(acres), df.iloc[-1]['AcquisitionDate'].date()))


# In[41]:


import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

plt.title('Distribution by severity', y=-0.1)
plt.pie(histogram, labels=['Low Severity', 'Medium Severity', 'Moderate Severity', 'High Severity']);
plt.axis('equal');


# ### Visualize burnt areas

# In[88]:


m = gis.map('Carpinteria, CA')
m


# In[43]:


m.add_layer([midfire, burnt_areas])


# ## Raster to Feature layer conversion

# Use Raster Analytics and Geoanalytics to convert the burnt area raster to a feature layer. The `to_features()` method converts the raster to a feature layer and `create_buffers()` fills holes in the features and dissolves them to output one feature that covers the extent of the Thomas Fire.

# In[47]:


persisted_fire_output = burnt_areas.save()
persisted_fire_output_layer = persisted_fire_output.layers[0]
fire_item = persisted_fire_output_layer.to_features(output_name='ThomasFire_Boundary')

fire_layer = fire_item.layers[0]
fire_layer.filter = 'st_area_sh > 3000000'


# In[33]:


fire = gis.content.search('ThomasFire_Boundary', 'Feature Layer')[0]
fire


# ## Impact Assessment

# ### Compute infrastructure and human impact

# In[77]:


from arcgis.geoenrichment import enrich
from arcgis.features import SpatialDataFrame, FeatureLayer

sdf = pd.DataFrame.spatial.from_layer(fire.layers[0])

fire_geometry = sdf.iloc[0].SHAPE
sa_filter = geometry.filters.intersects(geometry=fire_geometry, sr=4326)

secondary_roads_layer = FeatureLayer("https://tigerweb.geo.census.gov/arcgis/rest/services/TIGERweb/Transportation_LargeScale/MapServer/1")
secondary_roads = secondary_roads_layer.query(geometry_filter=sa_filter, out_sr=4326)

local_roads_layer = FeatureLayer("https://tigerweb.geo.census.gov/arcgis/rest/services/TIGERweb/Transportation_LargeScale/MapServer/2")
local_roads = local_roads_layer.query(geometry_filter=sa_filter, out_sr=4326)

def age_pyramid(df):
    import warnings
    import seaborn as sns
    import matplotlib.pyplot as plt

    get_ipython().run_line_magic('matplotlib', 'inline')
    warnings.simplefilter(action='ignore', category=FutureWarning)
    pd.options.mode.chained_assignment = None 
    plt.style.use('ggplot')

    df = df[[x for x in impacted_people.columns if 'MALE' in x or 'FEM' in x]]
    sf = pd.DataFrame(df.sum())
    sf['age'] = sf.index.str.extract('(\d+)').astype('int64')

    f = sf[sf.index.str.startswith('FEM')]
    m = sf[sf.index.str.startswith('MALE')]
    f = f.sort_values(by='age', ascending=False).set_index('age')
    m = m.sort_values(by='age', ascending=False).set_index('age')

    popdf = pd.concat([f, m], axis=1)
    popdf.columns = ['F', 'M']
    popdf['agelabel'] = popdf.index.map(str) + ' - ' + (popdf.index+4).map(str)
    popdf.M = -popdf.M
    
    sns.barplot(x="F", y="agelabel", color="#CC6699", label="Female", data=popdf, edgecolor='none')
    sns.barplot(x="M",  y="agelabel", color="#008AB8", label="Male",   data=popdf,  edgecolor='none')
    plt.ylabel('Age group')
    plt.xlabel('Number of people');
    return plt;


# ### Visualize affected roads on map

# In[89]:


impactmap = gis.map('Carpinteria, CA')
impactmap.basemap = 'streets'

impactmap


# In[79]:


impactmap.draw([local_roads, secondary_roads])


# ### Age Pyramid of affected population

# In[ ]:


impacted_people = enrich(sdf, 'Age')
age_pyramid(impacted_people);


# 


# ====================
# change_detection_of_buildings_from_satellite_imagery.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Change Detection of Buildings from Satellite Imagery

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Export training data for deep learning](#Export-training-data-for-deep-learning)
# * [Model training](#Model-training)
#  * [Necessary imports](#Necessary-imports)
#  * [Get training data](#Get-training-data)
#  * [Visualize training data](#Visualize-training-data)
#  * [Load model architecture](#Load-model-architecture)
#  * [Train the model](#Train-the-model)
#  * [Visualize detected changes](#Visualize-detected-changes)
#  * [Evaluate model performance](#Evaluate-model-performance)
#  * [Save model](#Save-model)
# * [Model inference](#Model-inference)
# * [Conclusion](#Conclusion)
# * [References](#References)

# ## Introduction

# The World is changing every day and monitoring that change on ground can be a tedious and labor intensive task. so, is there a way to automate it? 

# 

# This notebook will walk you through how deep learning can be used to perform change detection using satellite images.
# 
# One of the popular models available in the `arcgis.learn` module of ArcGIS API for Python, `ChangeDetector` is used to identify areas of persistent change between two different time periods using remotely sensed images. It can help you identify where new buildings have come up for instance. This model is based upon the latest research in deep learning and works well with objects of various sizes.
# The `ChangeDetector` model workflow consists of three parts: 
# - Preparing the training data, 
# - training a model 
# - using the trained model for inferencing. 
# 
# Let’s first prepare the training data. 

# ## Export training data for deep learning

# In the cells below, we have provided the input rasters and input mask polygons needed to export training data.

# In[1]:


from arcgis.gis import GIS
gis = GIS('home')


# In[2]:


input_data = gis.content.get('3ebf8ca5f6c245d69e2e0f4358986ed3')
input_data


# In[3]:


mask_polygons = gis.content.get('6ee0b48611c44b31b499f6cbe202686f')
mask_polygons


# `ChangeDetector` model requires data in this folder format : images_after, images_before and labels folder. The label indicates where there are changes in the before and after images.
# These images are too large to process in the GPU memory for training the model, so we need to create small image chips or tiles. Training data can be exported by using the `Export Training Data For Deep Learning tool` available in [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview) as well as [ArcGIS Image Server](https://www.esri.com/en-us/arcgis/products/arcgis-image-server).
# 
# - Input Raster: 2014rasters
# - Tile Size X & Tile Size Y: 256
# - Stride X & Stride Y: 64
# - Meta Data Format: 'Export Tiles'
# - Environments: 0.3

# <div>
# </div>

# As shown below, the above tool needs to be run thrice each for image before, image after and the change labels in order to create training data.

# 

# ## Model training

# This step would be done using jupyter notebook and documentation is available [here](https://developers.arcgis.com/python/guide/install-and-set-up) to install and setup environment.

# ### Necessary imports

# In[3]:


import os
from pathlib import Path
from arcgis.learn import prepare_data, ChangeDetector


# ### Get training data 

# We have already exported the data which can be directly downloaded using the steps below:

# In[4]:


training_data = gis.content.get('d284e2083b254f6b8508f9cf41f53713')
training_data


# In[5]:


filepath = training_data.download(file_name=training_data.name)


# In[6]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[7]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# `prepare_data` function takes path to training data and creates a fast.ai databunch with specified transformation, batch size, split percentage, etc.

# In[8]:


data = prepare_data(data_path,
                    chip_size=256,
                    dataset_type='ChangeDetection', 
                    batch_size=4
                   )


# ### Visualize training data

# To get a sense of what the training data looks like, use the `show_batch()` method to randomly pick a few training chips and visualize them. The chips are overlaid with masks representing the building footprints in each image chip.

# In[9]:


data.show_batch()


# ### Load model architecture

# `arcgis.learn` provides the `ChangeDetector` model for identifying areas of persistent change tasks, which is based on a pretrained convnet, like ResNet that acts as the 'backbone'. More details about `ChangeDetector` can be found [here](https://developers.arcgis.com/python/api-reference/arcgis.learn.toc.html#changedetector).

# In[10]:


cd = ChangeDetector(data, backbone='resnet50')


# ### Train the model

# Learning rate is one of the most important hyperparameters in model training. We will use the `lr_find()` method to find an optimum learning rate at which we can train a robust model.

# In[11]:


lr = cd.lr_find()


# We are using the suggested learning rate above to train the model for 100 epochs.

# In[12]:


cd.fit(epochs=100, lr=lr)


# We have further trained the model for 100 more epochs to improve model performance. For the sake of time, the cell below is commented out.  

# In[ ]:


# cd.fit(100)


# ### Visualize detected changes

# It's a good practice to see results of the model viz-a-viz ground truth. The code below picks random samples and shows us ground truth and model predictions, side by side. This enables us to preview the results of the model within the notebook.

# In[17]:


cd.show_results(rows=8) 


# ### Evaluate model performance

# As we have 2 classes for this change detection task, we need to do accuracy assessment for each of those. For that ArcGIS API for Python provides `precision_recall_score` function that will calculate precision and recall for each class.

# In[14]:


cd.precision_recall_score()


# ### Save model

# We will save the model which we trained as a 'Deep Learning Package' ('.dlpk' format). Deep Learning package is the standard format used to deploy deep learning models on the ArcGIS platform.
# 
# We will use the `save()` method to save the trained model. By default, it will be saved to the 'models' sub-folder within our training data folder.

# In[ ]:


cd.save('change_detection_model_e200')


# ## Model inference

# Using `predict` function, we can apply the trained model on a test image/area to detect changes that occurred in the satellite images during two different time periods. 

# In[19]:


inference_data = gis.content.get('6b32a534228b44b284c14e75b3d3f5f5')
inference_data


# In[20]:


test_path = inference_data.download(file_name=inference_data.name)


# In[21]:


import zipfile
with zipfile.ZipFile(test_path, 'r') as zip_ref:
    zip_ref.extractall(Path(test_path).parent)


# In[22]:


test_images = Path(os.path.join(os.path.splitext(test_path)[0]))


# In[23]:


before_img = os.path.join(test_images, 'before.tif')
after_img = os.path.join(test_images, 'after.tif')


# The predict function takes in before and after image path as required variables. You can optionally pass `visualize=True` if you want to see the results in the notebook. Additionally, you can pass `save=True` function in order to save the image to the local disk.

# In[24]:


cd.predict(before_image=before_img,
           after_image=after_img,
           visualize=True)


# ## Conclusion

# In this notebook, we learned how to solve various problems like identifying new construction. The same workflow can also be used to find out which new roads have come up in the past five years for instance. With just some labeled data and with little to no human involvement by using deep learning, we can now perform change detection using satellite images.

# ## References

# - https://developers.arcgis.com/python/guide/how-change-detection-works/


# ====================
# chennai_floods_analysis_rn.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Chennai Floods 2015 – A Geographic Analysis
# 
# On December 1–2, 2015, the Indian city of Chennai received more rainfall in 24 hours than it had seen on any day since 1901. The deluge followed a month of persistent monsoon rains that were already well above normal for the Indian state of Tamil Nadu. At least 250 people had died, several hundred had been critically injured, and thousands had been affected or displaced by the flooding that has ensued.

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Chennai-Floods-2015-–-A-Geographic-Analysis" data-toc-modified-id="Chennai-Floods-2015-–-A-Geographic-Analysis-1">Chennai Floods 2015 – A Geographic Analysis</a></span><ul class="toc-item"><li><span><a href="#Summary-of-this-sample" data-toc-modified-id="Summary-of-this-sample-1.1">Summary of this sample</a></span></li><li><span><a href="#Chennai-Floods-Explained" data-toc-modified-id="Chennai-Floods-Explained-1.2">Chennai Floods Explained</a></span></li><li><span><a href="#How-much-rain-and-where?" data-toc-modified-id="How-much-rain-and-where?-1.3">How much rain and where?</a></span></li><li><span><a href="#Spatial-Analysis" data-toc-modified-id="Spatial-Analysis-1.4">Spatial Analysis</a></span></li><li><span><a href="#What-caused-the-flooding-in-Chennai?" data-toc-modified-id="What-caused-the-flooding-in-Chennai?-1.5">What caused the flooding in Chennai?</a></span><ul class="toc-item"><li><span><a href="#A-wrong-call-that-sank-Chennai" data-toc-modified-id="A-wrong-call-that-sank-Chennai-1.5.1">A wrong call that sank Chennai</a></span></li></ul></li><li><span><a href="#Nature's-fury-or-human-made-disaster?" data-toc-modified-id="Nature's-fury-or-human-made-disaster?-1.6">Nature's fury or human made disaster?</a></span></li><li><span><a href="#Flood-Relief-Camps" data-toc-modified-id="Flood-Relief-Camps-1.7">Flood Relief Camps</a></span><ul class="toc-item"><li><span><a href="#Routing-Emergency-Supplies-to-Relief-Camps" data-toc-modified-id="Routing-Emergency-Supplies-to-Relief-Camps-1.7.1">Routing Emergency Supplies to Relief Camps</a></span></li></ul></li></ul></li></ul></div>

# 

# The image above provides satellite-based estimates of rainfall over southeastern India on December 1–2, accumulating in 30–minute intervals. The rainfall data is acquired from the Integrated Multi-Satellite Retrievals for GPM (IMERG), a product of the [Global Precipitation Measurement](http://www.nasa.gov/mission_pages/GPM/main/index.html) mission. The brightest shades on the maps represent rainfall totals approaching 400 millimeters (16 inches) during the 48-hour period. These regional, remotely-sensed estimates may differ from the totals measured by ground-based weather stations. According to Hal Pierce, a scientist on the GPM team at NASA’s Goddard Space Flight Center, the highest rainfall totals exceeded 500 mm (20 inches) in an area just off the southeastern coast.
# 
# [Source: NASA http://earthobservatory.nasa.gov/IOTD/view.php?id=87131]

# ## Summary of this sample
# This sample showcases not just the analysis and visualization capabilities of your GIS, but also the ability to store illustrative text, graphics and live code in a Jupyter notebook.
# 
# The sample starts off reporting the devastating effects of the flood. We plot the locations of rainfall guages and **interpolate** the data to create a continuous surface representing the amount of rainfall throughout the state.
# 
# Next we plot the locations of major lakes and **trace downstream** the path floods waters would take. We create a **buffer** around this path to demark at risk areas.
# 
# In the second part of the sample, we take a look at **time series** satellite imagery and observe the human impacts on natural reservoirs over a period of two decades.
# 
# We then vizualize the locations of relief camps and analyze their capacity using **pandas** and **matplotlib**. We **aggregate** the camps district wise to understand which ones have the largest number of refugees.
# 
# In the last part, we perform a **routing** analysis to figure out the best path to route emergency supplies from storage to the relief camps
# 
# First, let's import all the necessary libraries and connect to our GIS via an existing profile or creating a new connection by e.g. `gis = GIS("https://www.arcgis.com", "username", "Password")`.

# In[2]:


import datetime

get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib.pyplot as pd
from IPython.display import display, YouTubeVideo

import arcgis
from arcgis.gis import GIS
from arcgis.features.analyze_patterns import interpolate_points
from arcgis.geocoding import geocode
from arcgis.features.find_locations import trace_downstream
from arcgis.features.use_proximity import create_buffers

gis = GIS('home')


# 
# ## Chennai Floods Explained

# In[2]:


YouTubeVideo('x4dNIfx6HVs')


# The catastrophic flooding in Chennai is the result of the heaviest rain in several decades, which forced authorities to release a massive 30,000 cusecs from the Chembarambakkam reservoir into the Adyar river over two days, causing it to flood its banks and submerge neighbourhoods on both sides. It did not help that the Adyar’s stream is not very deep or wide, and its banks have been heavily encroached upon over the years.
# Similar flooding triggers were in action at Poondi and Puzhal reservoirs, and the Cooum river that winds its way through the city.
# While Chief Minister J Jayalalithaa said, during the earlier phase of heavy rain last month, that damage during the monsoon was “inevitable”, the fact remains that the mindless development of Chennai over the last two decades — <b>the filling up of lowlands and choking of stormwater drains and other exits for water — has played a major part in the escalation of the crisis.</b>
# 
# [Source: Indian Express http://indianexpress.com/article/explained/why-is-chennai-under-water/#sthash.LlhnqM4B.dpuf]

# ## How much rain and where?

# To get started with our analysis, we bring in a map of the affected region. The map is a live widget that is internally using the ArcGIS JavaScript API.

# In[16]:


chennai_pop_map = gis.map("Chennai")
chennai_pop_map


# We can search for content in our GIS and add layers to our map that can be used for visualization or analysis:

# In[13]:


chennaipop = gis.content.search("India Subdistrict boundaries owner: api_data_owner", 
                                item_type="Feature Layer")[0]
chennaipop


# Assign an optional JSON paramter to specify its opacity, e.g. `map.add_layer(chennaipop, {"opacity":0.7})` or else just add the layer with no transparency.

# In[14]:


chennai_pop_map.add_layer(chennaipop, {"renderer":"ClassedColorRenderer", 
                           "field_name": "TOTPOP_CY", 
                           "opacity":0.5})


# To get a sense of how much it rained and where, let's use rainfall data for December 2nd 2015, obtained from the Regional Meteorological Center in Chennai. Tabular data is hard to visualize, so let's bring in a map from our GIS to visualize the data:

# In[19]:


rainfall = gis.content.search("Chennai_precipitation owner: api_data_owner", 
                              item_type="Feature Layer")[0]


# In[68]:


rainfall_map = gis.map("Tamil Nadu, India")
rainfall_map


# We then add this layer to our map to see the locations of the weather stations from which the rainfall data was collected:

# In[8]:


rainfall_map.add_layer(rainfall, {"renderer":"ClassedSizeRenderer", 
                          "field_name":"RAINFALL" })


# Here we used the **smart mapping** capability of the GIS to automatically render the data with proportional symbols.

# ## Spatial Analysis
# Rainfall is a continuous phenonmenon that affects the whole region, not just the locations of the weather stations. Based on the observed rainfall at the monitoring stations and their locations, we can interpolate and deduce the approximate rainfall across the whole region. We use the **Interpolate Points** tool from the GIS's spatial analysis service for this.
# 
# The Interpolate Points tool uses <a href="http://desktop.arcgis.com/en/desktop/latest/guide-books/extensions/geostatistical-analyst/what-is-empirical-bayesian-kriging-.htm">empirical Bayesian kriging</a> to perform the interpolation.

# In[9]:


interpolated_rf = interpolate_points(rainfall, field='RAINFALL')


# Let us create another map of Tamil Nadu state and render the output from Interpolate Points tool

# In[69]:


intmap = gis.map("Tamil Nadu")
intmap


# In[11]:


intmap.add_layer(interpolated_rf['result_layer'])


# We see that rainfall was most severe in and around Chennai as well some parts of central Tamil Nadu.

# ## What caused the flooding in Chennai?

# ### A wrong call that sank Chennai
# Much of the flooding and subsequent waterlogging was a consequence of the outflows from major reservoirs into swollen rivers and into the city following heavy rains. The <b>release of waters from the Chembarambakkam reservoir</b> in particular has received much attention. [Source: The Hindu, http://www.thehindu.com/news/cities/chennai/chennai-floods-a-wrong-call-that-sank-the-city/article7967371.ece]

# In[70]:


lakemap = gis.map("Chennai")
lakemap.height='450px'
lakemap


# Let's have look at the major lakes and water reservoirs that were filled to the brim in Chennai due the rains. We plot the locations of some of the reservoirs that had a large outflow during the rains:
# 
# To plot the locations, we use geocoding tools from the `tools` module. Your GIS can have more than 1 geocoding service, for simplicity, the sample below chooses the first available geocoder to perform an address search

# In[14]:


lakemap.draw(geocode("Chembarambakkam, Tamil Nadu")[0], 
             {"title": "Chembarambakkam", "content": "Water reservoir"})
lakemap.draw(geocode("Puzhal Lake, Tamil Nadu")[0], 
             {"title": "Puzhal", "content": "Water reservoir"})
lakemap.draw(geocode("Kannampettai, Tamil Nadu")[0], 
             {"title": "Poondi Lake ", "content": "Water reservoir"})


# To identify the flood prone areas, let's trace the path that the water would take when released from the lakes. To do this, we first bring in a layer of lakes in Chennai:

# In[24]:


chennai_lakes = gis.content.search("ChennaiLakes owner: api_data_owner", 
                                   item_type="Feature Layer")[0]
chennai_lakes


# Now, let's call the **`Trace Downstream`** analysis tool from the GIS:

# In[17]:


downstream = trace_downstream(chennai_lakes)
downstream.query()


# The areas surrounding the trace paths are most prone to flooding and waterlogging. To identify the areas that were at risk, we buffer the traced flow paths by one mile in each direction and visualize it on the map. We see that large areas of the city of Chennai were susceptible to flooding and waterlogging.

# In[18]:


floodprone_buffer = create_buffers(downstream, [ 1 ], units='Miles')


# In[19]:


lakemap.add_layer(floodprone_buffer)


# ## Nature's fury or human made disaster?
# 
# "It is easy to attribute the devastation from unexpected flooding to the results of nature and climate change when in fact it is a result of poor planning and infrastructure. In Chennai, as in several cities across the country, we are experiencing the wanton destruction of our natural buffer zones—rivers, creeks, estuaries, marshlands, lakes—in the name of urban renewal and environmental conservation.
# 
# The recent floods in Chennai are a fallout of real estate riding roughshod over the city’s waterbodies. Facilitated by an administration that tweaked and modified building rules and urban plans, the real estate boom has consumed the city’s lakes, ponds, tanks and large marshlands.
# 
# The Ennore creek that used to be home to sprawling mangroves is fast disappearing with soil dredged from the sea being dumped there. The Kodungaiyur dump site in the Madhavaram–Manali wetlands is one of two municipal landfills that service the city. Velachery and Pallikaranai marshlands are a part of the Kovalam basin that was the southern-most of the four river basins for the city. Today, the slightest rains cause flooding and water stagnation in Velachery, home to the city’s largest mall, several other commercial and residential buildings, and also the site where low income communities were allocated land.
# The <b>Pallikaranai marshlands</b>, once a site for beautiful migratory birds, are now home to the second of the two landfills in the city where the garbage is rapidly leeching into the water and killing the delicate ecosystem."
# 
# [Source: Chennai's Rain Check https://www.epw.in/journal/2015/49/commentary/chennais-rain-check.html]
# 
# There are several marshlands and mangroves in the Chennai region that act as natural buffer zones to collect rain water. Let's see the human impact on Pallikaranai marshland over the last decade by comparing satellite images.

# In[20]:


def exact_search(my_gis, title, owner_value, item_type_value, max_items_value=20):
    final_match = None
    search_result = my_gis.content.search(query= title + ' AND owner:' + owner_value, 
                                          item_type=item_type_value, max_items=max_items_value, outside_org=True)
    
    if "Imagery Layer" in item_type_value:
        item_type_value = item_type_value.replace("Imagery Layer", "Image Service")
    elif "Layer" in item_type_value:
        item_type_value = item_type_value.replace("Layer", "Service")
    
    for result in search_result:
        if result.title == title:
            final_match = result
            break
    return final_match

ls_water = exact_search(gis, 'Landsat GLS Multispectral', 'esri', 'Imagery Layer')
ls_water


# Lets us see how the Pallikaranai marshland has changed over the past few decades, and how this has also contributed to the flooding. We create two maps and load the Land / Water Boundary layer to visualize this. This image layer is time enabled, and the map widget gives you the ability to navigate this dataset via time as well.

# In[ ]:


ls_water_lyr = ls_water.layers[0]


# In[22]:


from arcgis.geocoding import geocode
area = geocode("Tamil Nadu, India", out_sr=ls_water_lyr.properties.extent.spatialReference)[0]
ls_water_lyr.extent = area['extent']


# In the cell below, we will use a band combination [5,4,3] (a.k.a. mid-IR (Band 5), near-IR (Band 4) and red (Band 3)) of Landsat to provide definition of land-water boundaries and highlights subtle details not readily apparent in the visible bands alone. The reason that we use more infrared bands is to locate inland lakes and streams with greater precision. Generally, the wetter the soil, the darker it appears, because of the infrared absorption capabilities of water.

# In[23]:


# data source option 
from arcgis.raster.functions import stretch, extract_band
target_img_layer = stretch(extract_band(ls_water_lyr, [5,4,3]),
                           stretch_type="percentclip", gamma=[1,1,1], dra=True)


# Use the cell below to filter imageries based on the temporal conditions, and export the filtered results as local images, then show comparatively with other time range. You can either use the where clause e.g. `where="(Year = " + str(start_year) + ")",` or use the temporal filter as shown below.

# In[24]:


import pandas as pd
from arcgis import geometry
import datetime as dt

def filter_images(my_map, start_year, end_year):
    selected = target_img_layer.filter_by(where="(Category = 1) AND (CloudCover <=0.2)",
                                          time=[dt.datetime(start_year, 1, 1), dt.datetime(end_year, 1, 1)],
                                          geometry=arcgis.geometry.filters.intersects(ls_water_lyr.extent))
    my_map.add_layer(selected)
    
    fs = selected.query(out_fields="AcquisitionDate, GroupName, Month, DayOfYear, WRS_Row, WRS_Path")
    tdf = fs.sdf  
    return tdf


# First, search for qualified satellite imageries (tiles) intersecting with the area of interest at year 1991.

# In[1]:


satmap1 = gis.map("Pallikaranai, Tamil Nadu, India", 13)
df = filter_images(satmap1, 1991, 1992) 
df.head()


# Then search for satellite imageries intersecting with the area of interest at 2009.

# In[2]:


satmap2 = gis.map("Pallikaranai, Tamil Nadu, India", 13)
df = filter_images(satmap2, 2009, 2010)
df.head()


# In[3]:


from ipywidgets import *

satmap1.layout=Layout(flex='1 1', padding='10px', height='300px')
satmap2.layout=Layout(flex='1 1', padding='10px', height='300px')

box = HBox([satmap1, satmap2])
box


# The human impact on the marshland is all too apparent in the satellite images. The marshland has shrunk to less than a third of its size in just two decades.
# 
# "Not long ago, it was a 50-square-kilometre water sprawl in the southern suburbs of Chennai. Now, it is 4.3 square kilometres – less than a tenth of its original. The growing finger of a garbage dump sticks out like a cancerous tumour in the northern part of the marshland.  Two major roads cut through the waterbody with few pitifully small culverts that are not up to the job of transferring the rain water flows from such a large catchment. The edges have been eaten into by institutes like the National Institute of Ocean Technology. Ironically, NIOT is an accredited consultant to prepare Environmental Impact Assessments on various subjects, including on the implications of constructing on waterbodies.
# 
# Other portions of this wetland have been sacrificed to accommodate the IT corridor. But water offers no exemption to elite industry. Unmindful of the lofty intellectuals at work in the glass and steel buildings of the software parks, rainwater goes by habit to occupy its old haunts, bringing the back-office work of American banks to a grinding halt."
# 
# [Source: http://scroll.in/article/769928/chennai-floods-are-not-a-natural-disaster-theyve-been-created-by-unrestrained-construction]

# ## Flood Relief Camps
# 
# To provide emergency assistance, the Tamil Nadu government has set up several flood relief camps in the flood affected areas. They provide food, shelter and the basic necessities to thousands of people displaced by the floods. The locations of the flood relief camps was obtained from http://cleanchennai.com/floodrelief/2015/12/09/relief-centers-as-on-8-dec-2015/ with https://ciifoundation.in/Tamil-Nadu-Flood-Relief-2015.php and published to the GIS as a layer, that is visualized below:

# In[ ]:


relief_centers = gis.content.search("Chennai Relief Centers owner: api_data_owner", item_type="Feature Layer")[0]


# In[26]:


relief_centers


# In[28]:


reliefmap = gis.map("Chennai")
reliefmap


# Assign an optional JSON paramter to specify its opacity, e.g. `reliefmap.add_layer(chennaipop, {"opacity":0.5})` or else just add the layer with no transparency.

# In[73]:


reliefmap.add_layer(chennaipop, {"opacity":0.5})


# In[74]:


reliefmap.add_layer(relief_centers)


# Let us read the relief center layer as a pandas dataframe to analyze the data further

# In[32]:


relief_data = relief_centers.layers[0].query().sdf
relief_data.head()


# In[33]:


relief_data['No_of_pers'].sum()


# In[34]:


relief_data['No_of_pers'].describe()


# In[35]:


relief_data['No_of_pers'].hist()


# In our dataset, each row represents a relief camp location. To quickly get the dimensions (rows & columns) of our data frame, we use the `shape` property

# In[36]:


relief_data.shape


# As of 8th December, 2015, there were 31,478 people in the 136 relief camps. Let's aggregate them by the district the camp is located in. To accomplish this, we use the `aggregate_points` tool.

# In[37]:


chennai_pop_featurelayer = chennaipop.layers[3]


# In[38]:


res = arcgis.features.summarize_data.aggregate_points(  relief_centers, 
                                                        chennai_pop_featurelayer, 
                                                        False, 
                                                        ["No_of_pers Sum"])


# In[39]:


aggr_lyr = res['aggregated_layer']


# In[75]:


reliefmap.add_layer(aggr_lyr, { "renderer": "ClassedSizeRenderer", 
                               "field_name":"sum_no_of_pers"})


# In[155]:


df = aggr_lyr.query().sdf
df


# Let us represent the aggreate result as a table:

# In[156]:


df = aggr_lyr.query().sdf

df2 = df[['NAME', 'sum_no_of_pers']]
df2.set_index('NAME', inplace=True)
df2


# In[157]:


df2.plot(kind='bar')


# ### Routing Emergency Supplies to Relief Camps

# A centralized location has been established at Nehru Stadium to organise the relief materials collected from various organizations and volunteers.  From there, the relief material is distributed to the needy flood affected people.
# 
# The GIS provided routing tools that can help plan routes of the relief trucks from the center to relief camps:

# In[1]:


routemap = gis.map("Chennai")
routemap


# In[47]:


nehru_stadium = geocode('Jawaharlal Nehru Stadium, Chennai')[0]
routemap.draw(nehru_stadium, {"title": "Nehru Stadium", 
                              "content": "Chennai Flood Relief Center"})


# In[48]:


start_time = datetime.datetime(2015, 12, 13, 9, 0)


# In[49]:


routes = arcgis.features.use_proximity.plan_routes(
    relief_centers, 
    15, 
    15, 
    start_time, 
    nehru_stadium, 
    stop_service_time=30)
routemap.add_layer(routes['routes_layer'])


# In[50]:


routemap.add_layer(routes['assigned_stops_layer'])
routemap.add_layer(routes['routes_layer'])


# Once the routes have been generated, they can be given to drivers, and used to ensure that relief material is promptly delivered to those in need and help alleviate the suffering they are going through.


# ====================
# classification_of_sfm_derived_point_clouds_using_deep_learning.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Classification of SfM-derived point clouds using deep learning

# - 🥠 Deep Learning
# - ☁️ SfM-derived Point Cloud
# - 🛰️ Remote Sensing

# <p align="center"></p>

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Introduction" data-toc-modified-id="Introduction-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href="#Area-of-interest-and-pre-processing" data-toc-modified-id="Area-of-interest-and-pre-processing-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Area of interest and pre-processing</a></span></li><li><span><a href="#Data-preparation" data-toc-modified-id="Data-preparation-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>Data preparation</a></span></li><li><span><a href="#Visualization-of-prepared-data" data-toc-modified-id="Visualization-of-prepared-data-4"><span class="toc-item-num">4&nbsp;&nbsp;</span>Visualization of prepared data</a></span></li><li><span><a href="#Training-the-model" data-toc-modified-id="Training-the-model-5"><span class="toc-item-num">5&nbsp;&nbsp;</span>Training the model</a></span></li><li><span><a href="#Visualization-of-results-in-notebook" data-toc-modified-id="Visualization-of-results-in-notebook-6"><span class="toc-item-num">6&nbsp;&nbsp;</span>Visualization of results in notebook</a></span></li><li><span><a href="#Saving-the-trained-model" data-toc-modified-id="Saving-the-trained-model-7"><span class="toc-item-num">7&nbsp;&nbsp;</span>Saving the trained model</a></span></li><li><span><a href="#Classification-using-the-trained-model" data-toc-modified-id="Classification-using-the-trained-model-8"><span class="toc-item-num">8&nbsp;&nbsp;</span>Classification using the trained model</a></span></li><li><span><a href="#Visualization-of-results-in-ArcGIS-Pro" data-toc-modified-id="Visualization-of-results-in-ArcGIS-Pro-9"><span class="toc-item-num">9&nbsp;&nbsp;</span>Visualization of results in ArcGIS Pro</a></span></li><li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-10"><span class="toc-item-num">10&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href="#References" data-toc-modified-id="References-11"><span class="toc-item-num">11&nbsp;&nbsp;</span>References</a></span></li></ul></div>

# ## Introduction

# Historically, point clouds have been created by active remote sensing scanners, such as radar and laser scanners (LiDAR), that can be used on aerial, mobile, and terrestrial platforms. With the advancement in computer vision algorithms, point clouds can now also be created using Semi-Global Matching (SGM) and Structure from Motion (SfM) methods. These techniques are also based on overlapping imagery, just like 'manual stereo compilation', which is a photogrammetric method for creating point clouds. However, SfM and SGM methods both give far better and more detailed results and are less time consuming than the photogrammetric method <a href="#References">[1]</a>.
# 
# There are many technical terms that are loosely interchanged for such point clouds, such as SfM-derived point clouds, photogrammetric point clouds, synthetic point clouds, UAV point clouds, etc. In this sample notebook, we will be using the term: 'SfM-derived point clouds' with context to describe point clouds generated from ESRI's <a href="https://www.esri.com/en-us/arcgis/products/site-scan-for-arcgis/overview" target="_blank">Site Scan for ArcGIS</a> and <a href="https://www.esri.com/en-us/arcgis/products/arcgis-drone2map/overview" target="_blank">ArcGIS Drone2Map</a>.  
# 

# SfM derived point clouds and LiDAR point clouds cannot be treated as the same. Some of the key differences between them are:
# 
# - Only the visible surface is modeled ('first returns') for SfM, SGM, or traditional photogrammetry methods <a href="#References">[1]</a>.
# 
# 
# - Photo quality (exposure, shadows, etc.) and photo processing both have a significant impact on the quality of the generated point clouds in non-LiDAR approaches <a href="#References">[1]</a>.
# 
# 
# - The SfM-derived point clouds can be enriched with the information for other imagery bands, like NIR. Even an NDVI band can be easily added as an additional attribute to the dataset <a href="#References">[1]</a>. 
# 
# 
# - The geometry of the objects in SfM point clouds can have distorted/soft edges. This usually depends upon the quality of GCPs, variation in flying height of the drone, redundancies in overlapping imagery for a particular region, etc.
# 
# 
# - In many cases, LiDAR-based data acquisition is costlier and requires more planning than drone imagery acquisition that generates point clouds using the SfM technique.
# 
# 

# For these reasons, many of the traditional non-'deep learning' methods developed for LiDAR point clouds fall flat when it comes to SfM-derived point clouds. Fortunately, deep learning can fill this processing gap and help in the point cloud classification of these datasets.
# 
# 
# Point clouds generated by <a href="https://www.esri.com/en-us/arcgis/products/site-scan-for-arcgis/overview" target="_blank">Site Scan for ArcGIS</a> are first labeled with the 'objects of interest' to create the training data. Next, deep learning capabilities in 'ArcGIS API for Python' are utilized for the classification of these SfM-derived point clouds.
# 
# Further details on the PointCNN implementation in the API <i>(working principle, architecture, best practices, etc.)</i>, can be found in the <a href="https://developers.arcgis.com/python/guide/point-cloud-segmentation-using-pointcnn" target="_blank">PointCNN guide</a>, along with instructions on how to set up the Python environment. Additional sample notebooks related to PointCNN can be found in the <a href="https://developers.arcgis.com/python/sample-notebooks/" target="_blank">sample notebook section</a> on the website.
# 
# Before proceeding through this notebook, it is advised that you go through the <a href="https://developers.arcgis.com/python/api-reference/arcgis.learn.toc.html#pointcnn" target="_blank">API reference</a> for PointCNN (`prepare_data()`, `Transform3d()`, and `PointCNN()`), along with the resources and tool references for point cloud classification using deep learning in ArcGIS Pro, found <a href="https://pro.arcgis.com/en/pro-app/latest/help/data/las-dataset/introduction-to-deep-learning-and-point-clouds.htm" target="_blank">here</a>.

# _**Objective:**_
# 
# <ol style="list-style-type:upper-roman">
# <li>Classify points representing cars in SfM-derived point clouds using the API's PointCNN model. A model is trained for two classes, 'cars' and 'background'. Apart from the geometric information in the data, additional attributes (Red, Blue, and Green), are also used for model training.</li>
# <br>
# 

# ## Area of interest and pre-processing

# Any SfM-derived point clouds and areas of interest can be used, but for this sample, we are using point clouds generated by <a href="https://www.esri.com/en-us/arcgis/products/site-scan-for-arcgis/overview" target="_blank">Site Scan for ArcGIS</a>, using drone imagery of some parking lots in the United States. The dataset already has points labeled for 'cars', represented by classcode '18', and 'background', represented by classcode '0'.

# _**Pre-processing steps:**_
# 
# - Split the `.las` files into three unique sets, one for training, one for validation, and one for testing. Create LAS datasets for all three sets using the <a href="https://pro.arcgis.com/en/pro-app/latest/tool-reference/data-management/create-las-dataset.htm" target="_blank">'Create LAS Dataset'</a>  tool. There is no fixed rule, but generally, the validation data for point clouds in `.las` format should be at least 5-10 % <i>(by size)</i> of the total data available, with appropriate diversity within the validation dataset. <i>(For ease in splitting the big `.las` files into the appropriate ratios, ArcGIS Pro's <a href="https://pro.arcgis.com/en/pro-app/tool-reference/3d-analyst/tile-las.htm" target="_blank">'Tile LAS'</a> tool can be used.)</i>
# 
# 
# - Alternatively, polygons can also be used to define regions of interest that should be considered as training or validation datasets. These polygons can be used later in the export tool. If the dataset is very large, then the <a href="https://pro.arcgis.com/en/pro-app/latest/tool-reference/data-management/build-las-dataset-pyramid.htm" target="_blank">'Build LAS Dataset Pyramid'</a> tool can be leveraged for faster rendering/visualization of the data, which will also help in exploring and splitting the dataset.
# 
# 
# - If there is a pre-trained imagery model for the object of interest, then that model can help in creating the training dataset. The pre-trained imagery model can be used on the data from which the SfM-derived point cloud was created. Later, the <a href="https://pro.arcgis.com/en/pro-app/latest/tool-reference/3d-analyst/set-las-class-codes-using-raster.htm" target="_blank">'Set LAS Class Codes Using Raster'</a> tool or the <a href="https://pro.arcgis.com/en/pro-app/latest/tool-reference/3d-analyst/set-las-class-codes-using-features.htm" target="_blank">'Set LAS Class Codes Using Features'</a>  tool can be used to classify point cloud datasets using vector or raster labels obtained from the imagery pre-trained model. For example, in this sample notebook, the objects of interest are 'car'(s), and overlapping drone imagery is used to generate the point clouds. As such, ESRI's <a href="https://arcg.is/1OabnW" target="_blank">Car Detection - USA model</a> can be used on the orthophoto to generate car footprint polygons that can later be used to classify cars in the point cloud. Also, as the point cloud is derived from the imagery, moving objects, like a 'car', will not change positions, which can happen when using LiDAR data.

# ## Data preparation

# _**Imports:**_

# In[ ]:


from arcgis.learn import prepare_data, Transform3d, PointCNN
from arcgis.gis import GIS
gis = GIS()


# **Note:** The data used in this sample notebook can be downloaded as a zip file, from <a href="https://arcg.is/4rO8n"  target="_blank">here</a>. It contains both 'training data' and 'test data', where the 'test data' is used for inferencing. It can also be accessed via its `itemId`, as shown below.

# In[ ]:


training_data = gis.content.get('cae020a8e7f24fedb1359d114ca3abf1')
training_data


# _**Exporting the data:**_

# In this step, `.las` files are converted to a 'HDF5 binary data format'. For this step of exporting the data into an intermediate format, use the <a href="https://pro.arcgis.com/en/pro-app/latest/tool-reference/3d-analyst/prepare-point-cloud-training-data.htm" target="_blank">Prepare Point Cloud Training Data</a> tool in the <a href="https://pro.arcgis.com/en/pro-app/latest/tool-reference/3d-analyst/an-overview-of-the-3d-analyst-toolbox.htm" target="_blank">3D Analyst extension</a>, available from ArcGIS Pro 2.8 onwards <i>(as shown in figure 1)</i>.
# 
# The tool needs two LAS datasets, one for the training data and one for the validation data or regions of interest defined by polygons. Next, the `block size` is set to '50 meters', as our objects of interest will mostly be smaller than that, and the default value of '8192' is used for `block point limit`.
# 
# 
# <p align="center"></p>
# 
# <center>Figure 1. <a href="https://pro.arcgis.com/en/pro-app/latest/tool-reference/3d-analyst/prepare-point-cloud-training-data.htm" target="_blank">Prepare Point Cloud Training Data</a> tool.</center>
# 

# Here, all the additional attributes are included in the exported data. Later, a subset of additional attributes like intensity, number of returns, etc. can be selected that will be considered for training.
# 
# After the export is completed at the provided output path, the folder structure of the exported data will have two folders, each with converted HDF files in them <i>(as shown in figure 2)</i>. The exported training and validation folders will also contain histograms of the distributions of data that provide additional understanding and can help in tweaking the parameters that are being used in the workflow.
# 
# <p align="center"></p>
# 
# <center>Figure 2. Exported data.</center>

# _**Preparing the data:**_

# For `prepare_data()`, deciding the value of `batch_size` will depend on either the available RAM or VRAM, depending upon whether CPU or GPU is being used. `transforms` can also be used for introducing rotation, jitter, etc. to make the dataset more robust. `data.classes` can be used to verify what classes the model will be learning about.
# 
# The `classes_of_interest` and `min_points` parameters can be used to filter out less relevant blocks. These parameters are useful when training a model for SfM-derived or mobile/terrestrial point clouds. In specific scenarios when the 'training data' is not small, usage of these features can result in speeding up the 'training time', improving the convergence during training, and addressing the class imbalance up to some extent.
# 
# In this sample notebook X, Y, Z, Red, Blue, and Green are considered for training the model, so all three attributes, 'Red', 'Blue', and 'Green', are selected as `extra_features`. The names of the classes are also defined using `class_mapping` and will be saved inside the model for future reference.

# In[ ]:


output_path = r'C:\project\exported_data.pctd'


# In[ ]:


colormap = {'0':[255,69,0], '18':[253,247,83]}


# In[ ]:


data = prepare_data(output_path, 
                    dataset_type='PointCloud',
                    batch_size=2,
                    min_points=800,
                    transforms=None,
                    color_mapping=colormap,
                    extra_features=['red', 'blue', 'green'],
                    class_mapping={'18':'car','0':"background"})


# In[ ]:


data.classes


# ## Visualization of prepared data

# `show_batch()` helps in visualizing the exported data. Navigation tools available in the graph can be used to zoom and pan to the area of interest.

# In[ ]:


data.show_batch(rows=1)


# <p align="center"></p>
# 
# <center>Figure 3. Visualization of batch.</center>

# ## Training the model

# First, the PointCNN model object is created, utilizing the prepared data.

# In[ ]:


pc = PointCNN(data)


# Next, the `lr_find()` function is used to find the optimal learning rate that controls the rate at which existing information will be overwritten by newly acquired information throughout the training process. If no value is specified, the optimal learning rate will be extracted from the learning curve during the training process.

# In[ ]:


pc.lr_find()


# The `fit()` method is used to train the model, either applying a new 'optimum learning rate' or the previously computed 'optimum learning rate' <i>(any other user-defined learning rate can also be used.)</i>.
# 
# If `early_stopping` is set to 'True', then the model training will stop when the model is no longer improving, regardless of the `epochs` parameter value specified. The best model is selected based on the metric selected in the `monitor` parameter. A list of `monitor`'s available metrics can be generated using the `available_metrics` property.
# 
# An 'epoch' means the dataset will be passed forward and backward through the neural network one time, and if `Iters_per_epoch` is used, a subset of data is passed per epoch. To track information like gradients, losses, metrics, etc. while the model training is in progress, `tensorboard` can be set to 'True'.
# 

# In[ ]:


pc.available_metrics


# In[ ]:


pc.fit(20, 0.001, monitor='f1', tensorboard=True, early_stopping=True)


# ## Visualization of results in notebook

# `show_results()` will visualize the results of the model for the same scene as the ground truth. Navigation tools available in the graph can be used to zoom and pan to the area of interest.
# 
# The `compute_precision_recall()` method can be used to compute per-class performance metrics, which are calculated against the validation dataset.

# In[ ]:


pc.show_results(rows=1)


# <p align="center"></p>
# 
# <center>Figure 4. Visualization of results.</center>

# ## Saving the trained model

# The last step related to training is to save the model using the `save()` method. Along with the model files, this method also saves performance metrics, a graph of training loss vs validation loss, sample results, etc. <i>(as shown in figure 5)</i>.

# <p align="center"></p>
# 
# <center>Figure 5. Saved model.</center>
# 

# In[ ]:


pc.save('cars_and_background')


# ## Classification using the trained model

# For inferencing, <a href="https://pro.arcgis.com/en/pro-app/latest/tool-reference/3d-analyst/classify-point-cloud-using-trained-model.htm" target="_blank">Classify Points Using Trained Model</a> tool in the <a href="https://pro.arcgis.com/en/pro-app/latest/tool-reference/3d-analyst/an-overview-of-the-3d-analyst-toolbox.htm" target="_blank">3D Analyst extension</a>, available from ArcGIS Pro 2.8 onwards, can be used <i>(as shown in figure 6)</i>.
# 

# <p align="center"></p>
# 
# <center>Figure 6. <a href="https://pro.arcgis.com/en/pro-app/latest/tool-reference/3d-analyst/classify-point-cloud-using-trained-model.htm" target="_blank">Classify Points Using Trained Model</a> tool.</center>
# 

# Additional features, like target classification and class preservation in input data, are also available. After the prediction, LAS files will have 'car points', with the class code '18', and the rest of the points will have the class code '0' <i>(referred to as 'Background' in this sample)</i>. To visualize the results after the process is completed, the 'Symbology' can be changed to 'class' from the 'Appearance' tab, if not done initially.

# ## Visualization of results in ArcGIS Pro

#   

# <p align="center"></p>
# 
# <center>Figure 7. Visualization of results in ArcGIS Pro.</center>

# <i> This <a href="https://geosaurus.maps.arcgis.com/home/webscene/viewer.html?webscene=6a50627e143c47fe9f040a4b5eb5a86e" target="_blank">web scene</a> has the final outputs related to the illustrated test data in this notebook. It can also be accessed via its `itemId`, as shown below.</i>

# In[ ]:


results = gis.content.get('6a50627e143c47fe9f040a4b5eb5a86e')
results


# ## Conclusion

# This notebook has demonstrated the workflow for training a deep learning model for the classification of points representing 'cars' in SfM-derived point clouds, using additional attributes, like Red, Blue, and Green bands. A similar approach can be applied to classify other objects of interest, like trees, buildings, ground, etc.

# ## References

# - Dharmapuri, S., & Tully, M. (2018). Evolution of Point Cloud. LIDAR Magazine. https://lidarmag.com/2018/07/16/evolution-of-point-cloud/
# 
# 
# 
# 


# ====================
# cloud_detector_part1_cloudless_sentinel_and_unsupervised.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Generating cloud masks from satellite imagery - Part I 

# ## Table of Contents <a class="anchor" id="0"></a>
# * [Introduction](#1) 
# * [Imports](#2)
# * [Connecting to ArcGIS](#3)
# * [Accessing & Visualizing the datasets](#4)
# * [Method 1: Cloud mask using Sentinel Cloudless](#4a)    
#     * [Data Preparation](#5)
#        * [Define the Area of Interest in NYC](#6)
#        * [Visualize Area of Interest in NYC](#7) 
#        * [Convert scene to array](#8)    
#     * [Initiate cloud detector](#9)  
#     * [Generate cloud mask](#10) 
#     * [Visualize the cloud mask](#11)
# * [Method 2: Cloud mask using unsupervised learning](#12)
#     * [Accessing & Visualizing datasets](#13)
#     * [Data Pre-processing](#14)
#     * [Model Initialization](#15) 
#     * [Model Training](#16)
#     * [Cloud Prediction](#17) 
#     * [Result Visualization](#18) 
# * [Conclusion](#19)
# * [Data resources](#20)

# ## Introduction <a class="anchor" id="1"></a>

# This notebook describes the different ways cloud masks can be generated from satellite imagery, in this instance using sentinel imagery.
# 
# Cloud presence causes problems for remote sensing analysis of surface properties, for instance in analyses like land use and land cover classification, image compositing, or change detection. In cases of single scene image processing, it is relatively easy to manually filter out clouds; however, for studies that use a larger number of images, an automated approach for removing or masking out clouds is necessary.
# 
# In parts 1 and 2 of this notebook series, we will demonstrate three methods of cloud mask extraction:
# 
# - First, we use the sentinel2 cloudless python package, which is Sentinel's hub cloud detector that works only on sentinel imagery.
# 
# - Second, an unsupervised model using the mlmodel framework is applied to generate a cloud mask. This can run on any kind of imagery.
# 
# - Third, we train a pixel classifier model based on Unet.
# 
# The first two methods are described in this notebook, while the third method is described in the second part of the notebook series, along with a comparison of the results of the three different methods.

# ## Imports <a class="anchor" id="2"></a>

# Here, we import the modules we will be using in this notebook, including the pixel cloud detector from the Sentinel Cloudless package.

# In[1]:


get_ipython().run_line_magic('matplotlib', 'inline')

import pandas as pd
from datetime import datetime
from IPython.display import Image
from IPython.display import HTML
import matplotlib.pyplot as plt
import numpy as np

from sklearn.preprocessing import MinMaxScaler
from datetime import datetime as dt

import arcgis
from arcgis.gis import GIS
from arcgis.learn import MLModel, prepare_tabulardata
from arcgis.raster import Raster

from fastai.vision import *


# The Sentinel cloudless package can be installed using the following commands:

# In[ ]:


pip install sentinelhub --upgrade

pip install s2cloudless


# In[2]:


from s2cloudless import S2PixelCloudDetector, CloudMaskRequest, get_s2_evalscript


# ## Connecting to ArcGIS <a class="anchor" id="3"></a>

# In[3]:


gis = GIS("home")


# ## Accessing & Visualizing datasets  <a class="anchor" id="4"></a>
# 
# Next, we will access the Sentinel-2 imagery, which has a high resolution of 10m and has 13 bands. This imagery can be accessed from the ArcGIS Enterprise portal, where it is sourced from the AWS collection.

# In[4]:


# get image
s2 = gis.content.get('fd61b9e0c69c4e14bebd50a9a968348c')
sentinel = s2.layers[0]
s2


# ## Method 1: Cloud mask using Sentinel Cloudless <a class="anchor" id="4a"></a>

# ### Data Preparation <a class="anchor" id="5"></a>

# #### Define the Area of Interest in NYC <a class="anchor" id="6"></a>
# The area of interest is defined using the four latitude and longitude values from a certain part of NYC with cloud presence, as can be seen from the images.

# In[6]:


#  extent in 3857 for NYU rain clouds
NYU_cloud_extent = {
    "xmin": -8231176.77,
    "ymin": 4967559.25,
    "xmax": -8242898.16,
    "ymax": 4973524.61,
    "spatialReference": {"wkid": 3857}
}


# In[7]:


# NYU - The respective scene having the above area is selected
selected = sentinel.filter_by(where="(Category = 1) AND (cloudcover >=0.05)",                              
                              geometry=arcgis.geometry.filters.intersects(NYU_cloud_extent))
df = selected.query(out_fields="AcquisitionDate, GroupName, CloudCover, DayOfYear",
                   order_by_fields="AcquisitionDate").sdf

df['AcquisitionDate'] = pd.to_datetime(df['acquisitiondate'], unit='ms')
df


# #### Visualize Area of Interest in NYC <a class="anchor" id="7"></a>

# In[8]:


# The scene is selected with the least cloud cover and extracted using the selected NYC extent
NYU_clouds_scene4 = sentinel.filter_by('OBJECTID=352211')
NYU_clouds_scene4.extent = NYU_cloud_extent
NYU_clouds_scene4


# In[9]:


from arcgis.raster.functions import apply


# In[10]:


s2_ms = apply(NYU_clouds_scene4, None)


# In[11]:


pd.DataFrame(s2_ms.key_properties()['BandProperties'])


# #### Convert scene to arrray <a class="anchor" id="8"></a>

# In[12]:


# convert the scene to a numpy array
test_array = s2_ms.export_image(f='numpy_array')


# In[13]:


test_array.shape


# In[14]:


# Change array to float data type
test_array_sr = test_array/10000
test_array_sr = test_array_sr.astype('float32')


# ### Initiate cloud detector <a class="anchor" id="9"></a>
# Finally, the cloud detector can be initiated using specified parameters. The values can be adjusted for different amounts of cloud cover.

# In[15]:


# Initiate sentinel cloud detector 
cloud_detector = S2PixelCloudDetector(
    threshold=0.7,
    average_over=4,
    dilation_size=2,
    all_bands=True
)


# ### Generate cloud mask <a class="anchor" id="10"></a>
# 
# Once initiated, the cloud detector can be used to extract the cloud mask from the chosen scene.

# In[16]:


get_ipython().run_cell_magic('time', '', 'test_cloud_mask = cloud_detector.get_cloud_masks(test_array_sr)\n')


# In[17]:


test_cloud_mask.shape


# In[18]:


test_cloud_mask


# ### Visualize the cloud mask<a class="anchor" id="11"></a>

# In[20]:


plt.figure(figsize=(25,8))
plt.imshow(test_cloud_mask)
plt.title('Clouds mask by Sentinel Cloudless')
plt.axis('off')
plt.show()


# Upon successfully visualizing the results, we can see that the clouds have been detected properly, separating the actual clouds from other objects with similar appearance.

# Next, a different scene with clouds over forested areas will be tested with the same workflow used above. First, an area of interest is  defined, then that scene is converted to an array to which the the cloud detector is appllied, resulting in a final cloud mask.

# In[21]:


#  extent in 3857 for amazon rain clouds
cloud_extent = {
    "xmin": -6501664.396,
    "ymin": -610686.285,
    "xmax": -6490039.703,
    "ymax": -600222.721,
    "spatialReference": {"wkid": 3857}
}


# In[22]:


# The respective scene having the above area is selected
selected = sentinel.filter_by(where="(Category = 1) AND (cloudcover >=0.05)",                              
                              geometry=arcgis.geometry.filters.intersects(cloud_extent))
df = selected.query(out_fields="AcquisitionDate, GroupName, CloudCover, DayOfYear",
                   order_by_fields="AcquisitionDate").sdf

df['AcquisitionDate'] = pd.to_datetime(df['acquisitiondate'], unit='ms')
df


# In[23]:


# The scene is selected with the least cloud cover and extracted using the amzon extent
amazon_clouds_scene3 = sentinel.filter_by('OBJECTID=6738997')
amazon_clouds_scene3.extent = cloud_extent
amazon_clouds_scene3


# In[24]:


s2_amazon_ms = apply(amazon_clouds_scene3, None)


# In[25]:


pd.DataFrame(s2_amazon_ms.key_properties()['BandProperties'])


# In[26]:


test_array = s2_amazon_ms.export_image(f='numpy_array')
test_array_sr = test_array/10000
test_array_sr = test_array_sr.astype('float32')


# In[27]:


# Initiate sentinel cloud detector 
cloud_detector = S2PixelCloudDetector(
    threshold=0.3,
    average_over=4,
    dilation_size=2,
    all_bands=True
)


# In[28]:


get_ipython().run_cell_magic('time', '', 'test_cloud_mask = cloud_detector.get_cloud_masks(test_array_sr)\n')


# In[29]:


plt.figure(figsize=(25,8))
plt.imshow(test_cloud_mask)
plt.title('Clouds mask by Sentinel Cloudless')
plt.axis('off')
plt.show()


# The results show the clouds detected as yellow pixels, with some light clouds not originally visible in the scene being detected in the left portion of the image. The parameters of the cloud detector can be changed to detect different intensities of clouds.

# ## Method 2: Cloud mask using unsupervised learning <a class="anchor" id="12"></a>

# In this section, the unsupervised learning method of k-means clustering is used for cloud detection. First, a scene from a preselected area of interest over Iowa is accessed.

# ### Accessing & Visualizing datasets  <a class="anchor" id="13"></a>

# In[30]:


iowa_clouds = Raster(r"https://iservices6.arcgis.com/SMX5BErCXLM7eDtY/arcgis/rest/services/small_rgb_iowa3/ImageServer", gis=gis)


# In[31]:


iowa_clouds.extent


# In[32]:


ncols = iowa_clouds.columns
nrows = iowa_clouds.rows 
iowa_clouds.read(ncols=ncols,nrows=nrows).shape


# In[33]:


iowa_clouds.export_image(size=[2749,1816])


# In the scene above, only clouds are present in the area of interest.

# ### Data Pre-processing <a class="anchor" id="14"></a>

# In[34]:


iowa_clouds.name


# Here, the imagery name is rgbnir_iowa.tif, and the name of the 4 bands of blue, green, red, and near infrared are small_rgb_iowa3, small_rgb_iowa3_1, small_rgb_iowa3_2, small_rgb_iowa3_3 respectively. These bands will be used for defining the preprocessors.

# In[35]:


iowa_clouds.band_names


# This is a four band imagery, so the preprocessors are first defined for scaling.

# In[36]:


preprocessors = [('small_rgb_iowa3', 'small_rgb_iowa3_1', 'small_rgb_iowa3_2', 'small_rgb_iowa3_3', MinMaxScaler())]


# In[37]:


data = prepare_tabulardata(explanatory_rasters=[iowa_clouds], preprocessors=preprocessors)


# In[38]:


data.show_batch()


# ### Model Initialization <a class="anchor" id="15"></a>

# Once the data is prepared, an unsupervised model of k-means clustering from scikit-learn can be used for clustering the pixels into areas of clouds and no clouds. The clustering model is passed inside an MLModel, with the number of clusters set as three for the classes of no clouds, medium clouds, and dense clouds.

# In[39]:


model = MLModel(data, 'sklearn.cluster.KMeans', n_clusters=3, init='k-means++', random_state=43)


# ### Model Training <a class="anchor" id="16"></a>
# Finally, the model is ready to be trained, labeling the pixels into the three different classes.

# In[40]:


model.fit()


# In[41]:


model.show_results()


# ### Cloud Prediction<a class="anchor" id="17"></a>

# In[42]:


# creating the local output raster path
import tempfile
local_path=tempfile.gettempdir()
output_raster_path=local_path+r"/result"+str(dt.now().microsecond)+".tif"
output_raster_path


# In[43]:


# predicting the cloud masks using the fitted model
pred_new = model.predict(explanatory_rasters=[iowa_clouds], 
                         prediction_type='raster',
                         output_raster_path=output_raster_path)


# ## Result Visualization<a class="anchor" id="18"></a>

# In[44]:


iowa_predicted_cloud_mask = Raster(output_raster_path)
iowa_predicted_cloud_mask.export_image(size=[2749,1816])


# From the results, it can be seen that the cloud mask has been created and is represented by the white and grey pixels, while the black pixels represent areas with no cloud coverage. These results could be further reclassified to consist of two distinct classes of clouds and no-clouds in Arcgis Pro, for generating the final mask. This mask can be further processed in Arcgis Pro to create a polygon mask if needed.

# ## Conclusion<a class="anchor" id="19"></a>
# 
# In this sample notebook, two methods were described to create cloud masks from satellite images. A third method will be described in the second part of this notebook series.
# 
# In the first method the cloudless sentinel package performed well in detecting clouds and provides flexibility to the user to detect different intensities of clouds by changing the model's initialization parameters. However, the core caveat of this model is that it can only be used on Sentinel imagery.
# 
# The second method performed well with scenes where there were only clouds, and has the benefit of being able to run an analysis on any kind of imagery like Sentinel, Landsat, Surface reflectance etc. The model can also be used for automatically labelling training data for a pixel classification cloud detection model, which will be covered in the next part.

# ### Data resources <a class="anchor" id="20"></a>

# | Dataset | Source | Link |
# | -| - |-|
# | sat imagery| sentinel2  |https://registry.opendata.aws/sentinel-2/|


# ====================
# cloud_extraction_using_sentinel_imagery.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Generating cloud masks from satellite imagery - Part I 

# ## Table of Contents <a class="anchor" id="0"></a>
# * [Introduction](#1) 
# * [Imports](#2)
# * [Connecting to ArcGIS](#3)
# * [Accessing & Visualizing the datasets](#4)
# * [Method 1: Cloud mask using Sentinel Cloudless](#4a)    
#     * [Data Preparation](#5)
#        * [Define the Area of Interest in NYC](#6)
#        * [Visualize Area of Interest in NYC](#7) 
#        * [Convert scene to array](#8)    
#     * [Initiate cloud detector](#9)  
#     * [Generate cloud mask](#10) 
#     * [Visualize the cloud mask](#11)
# * [Method 2: Cloud mask using unsupervised learning](#12)
#     * [Accessing & Visualizing datasets](#13)
#     * [Data Pre-processing](#14)
#     * [Model Initialization](#15) 
#     * [Model Training](#16)
#     * [Cloud Prediction](#17) 
#     * [Result Visualization](#18) 
# * [Conclusion](#19)
# * [Data resources](#20)

# ## Introduction <a class="anchor" id="1"></a>

# This notebook describes the different ways cloud masks can be generated from satellite imagery, in this instance using sentinel imagery.
# 
# Cloud presence causes problems for remote sensing analysis of surface properties, for instance in analyses like land use and land cover classification, image compositing, or change detection. In cases of single scene image processing, it is relatively easy to manually filter out clouds; however, for studies that use a larger number of images, an automated approach for removing or masking out clouds is necessary.
# 
# In parts 1 and 2 of this notebook series, we will demonstrate three methods of cloud mask extraction:
# 
# - First, we use the sentinel2 cloudless python package, which is Sentinel's hub cloud detector that works only on sentinel imagery.
# 
# - Second, an unsupervised model using the mlmodel framework is applied to generate a cloud mask. This can run on any kind of imagery.
# 
# - Third, we train a pixel classifier model based on Unet.
# 
# The first two methods are described in this notebook, while the third method is described in the second part of the notebook series, along with a comparison of the results of the three different methods.

# ## Imports <a class="anchor" id="2"></a>

# Here, we import the modules we will be using in this notebook, including the pixel cloud detector from the Sentinel Cloudless package.

# In[1]:


get_ipython().run_line_magic('matplotlib', 'inline')

import pandas as pd
from datetime import datetime
from IPython.display import Image
from IPython.display import HTML
import matplotlib.pyplot as plt
import numpy as np

from sklearn.preprocessing import MinMaxScaler
from datetime import datetime as dt

import arcgis
from arcgis.gis import GIS
from arcgis.learn import MLModel, prepare_tabulardata
from arcgis.raster import Raster

from fastai.vision import *


# The Sentinel cloudless package can be installed using the following commands:

# In[ ]:


pip install sentinelhub --upgrade

pip install s2cloudless


# In[2]:


from s2cloudless import S2PixelCloudDetector, CloudMaskRequest, get_s2_evalscript


# ## Connecting to ArcGIS <a class="anchor" id="3"></a>

# In[3]:


gis = GIS("home")


# ## Accessing & Visualizing datasets  <a class="anchor" id="4"></a>
# 
# Next, we will access the Sentinel-2 imagery, which has a high resolution of 10m and has 13 bands. This imagery can be accessed from the ArcGIS Enterprise portal, where it is sourced from the AWS collection.

# In[4]:


# get image
s2 = gis.content.get('fd61b9e0c69c4e14bebd50a9a968348c')
sentinel = s2.layers[0]
s2


# ## Method 1: Cloud mask using Sentinel Cloudless <a class="anchor" id="4a"></a>

# ### Data Preparation <a class="anchor" id="5"></a>

# #### Define the Area of Interest in NYC <a class="anchor" id="6"></a>
# The area of interest is defined using the four latitude and longitude values from a certain part of NYC with cloud presence, as can be seen from the images.

# In[6]:


#  extent in 3857 for NYU rain clouds
NYU_cloud_extent = {
    "xmin": -8231176.77,
    "ymin": 4967559.25,
    "xmax": -8242898.16,
    "ymax": 4973524.61,
    "spatialReference": {"wkid": 3857}
}


# In[7]:


# NYU - The respective scene having the above area is selected
selected = sentinel.filter_by(where="(Category = 1) AND (cloudcover >=0.05)",                              
                              geometry=arcgis.geometry.filters.intersects(NYU_cloud_extent))
df = selected.query(out_fields="AcquisitionDate, GroupName, CloudCover, DayOfYear",
                   order_by_fields="AcquisitionDate").sdf

df['AcquisitionDate'] = pd.to_datetime(df['acquisitiondate'], unit='ms')
df


# #### Visualize Area of Interest in NYC <a class="anchor" id="7"></a>

# In[8]:


# The scene is selected with the least cloud cover and extracted using the selected NYC extent
NYU_clouds_scene4 = sentinel.filter_by('OBJECTID=352211')
NYU_clouds_scene4.extent = NYU_cloud_extent
NYU_clouds_scene4


# In[9]:


from arcgis.raster.functions import apply


# In[10]:


s2_ms = apply(NYU_clouds_scene4, None)


# In[11]:


pd.DataFrame(s2_ms.key_properties()['BandProperties'])


# #### Convert scene to arrray <a class="anchor" id="8"></a>

# In[12]:


# convert the scene to a numpy array
test_array = s2_ms.export_image(f='numpy_array')


# In[13]:


test_array.shape


# In[14]:


# Change array to float data type
test_array_sr = test_array/10000
test_array_sr = test_array_sr.astype('float32')


# ### Initiate cloud detector <a class="anchor" id="9"></a>
# Finally, the cloud detector can be initiated using specified parameters. The values can be adjusted for different amounts of cloud cover.

# In[15]:


# Initiate sentinel cloud detector 
cloud_detector = S2PixelCloudDetector(
    threshold=0.7,
    average_over=4,
    dilation_size=2,
    all_bands=True
)


# ### Generate cloud mask <a class="anchor" id="10"></a>
# 
# Once initiated, the cloud detector can be used to extract the cloud mask from the chosen scene.

# In[16]:


get_ipython().run_cell_magic('time', '', 'test_cloud_mask = cloud_detector.get_cloud_masks(test_array_sr)\n')


# In[17]:


test_cloud_mask.shape


# In[18]:


test_cloud_mask


# ### Visualize the cloud mask<a class="anchor" id="11"></a>

# In[20]:


plt.figure(figsize=(25,8))
plt.imshow(test_cloud_mask)
plt.title('Clouds mask by Sentinel Cloudless')
plt.axis('off')
plt.show()


# Upon successfully visualizing the results, we can see that the clouds have been detected properly, separating the actual clouds from other objects with similar appearance.

# Next, a different scene with clouds over forested areas will be tested with the same workflow used above. First, an area of interest is  defined, then that scene is converted to an array to which the the cloud detector is appllied, resulting in a final cloud mask.

# In[21]:


#  extent in 3857 for amazon rain clouds
cloud_extent = {
    "xmin": -6501664.396,
    "ymin": -610686.285,
    "xmax": -6490039.703,
    "ymax": -600222.721,
    "spatialReference": {"wkid": 3857}
}


# In[22]:


# The respective scene having the above area is selected
selected = sentinel.filter_by(where="(Category = 1) AND (cloudcover >=0.05)",                              
                              geometry=arcgis.geometry.filters.intersects(cloud_extent))
df = selected.query(out_fields="AcquisitionDate, GroupName, CloudCover, DayOfYear",
                   order_by_fields="AcquisitionDate").sdf

df['AcquisitionDate'] = pd.to_datetime(df['acquisitiondate'], unit='ms')
df


# In[23]:


# The scene is selected with the least cloud cover and extracted using the amzon extent
amazon_clouds_scene3 = sentinel.filter_by('OBJECTID=6738997')
amazon_clouds_scene3.extent = cloud_extent
amazon_clouds_scene3


# In[24]:


s2_amazon_ms = apply(amazon_clouds_scene3, None)


# In[25]:


pd.DataFrame(s2_amazon_ms.key_properties()['BandProperties'])


# In[26]:


test_array = s2_amazon_ms.export_image(f='numpy_array')
test_array_sr = test_array/10000
test_array_sr = test_array_sr.astype('float32')


# In[27]:


# Initiate sentinel cloud detector 
cloud_detector = S2PixelCloudDetector(
    threshold=0.3,
    average_over=4,
    dilation_size=2,
    all_bands=True
)


# In[28]:


get_ipython().run_cell_magic('time', '', 'test_cloud_mask = cloud_detector.get_cloud_masks(test_array_sr)\n')


# In[29]:


plt.figure(figsize=(25,8))
plt.imshow(test_cloud_mask)
plt.title('Clouds mask by Sentinel Cloudless')
plt.axis('off')
plt.show()


# The results show the clouds detected as yellow pixels, with some light clouds not originally visible in the scene being detected in the left portion of the image. The parameters of the cloud detector can be changed to detect different intensities of clouds.

# ## Method 2: Cloud mask using unsupervised learning <a class="anchor" id="12"></a>

# In this section, the unsupervised learning method of k-means clustering is used for cloud detection. First, a scene from a preselected area of interest over Iowa is accessed.

# ### Accessing & Visualizing datasets  <a class="anchor" id="13"></a>

# In[30]:


iowa_clouds = Raster(r"https://iservices6.arcgis.com/SMX5BErCXLM7eDtY/arcgis/rest/services/small_rgb_iowa3/ImageServer", gis=gis)


# In[31]:


iowa_clouds.extent


# In[32]:


ncols = iowa_clouds.columns
nrows = iowa_clouds.rows 
iowa_clouds.read(ncols=ncols,nrows=nrows).shape


# In[33]:


iowa_clouds.export_image(size=[2749,1816])


# In the scene above, only clouds are present in the area of interest.

# ### Data Pre-processing <a class="anchor" id="14"></a>

# In[34]:


iowa_clouds.name


# Here, the imagery name is rgbnir_iowa.tif, and the name of the 4 bands of blue, green, red, and near infrared are small_rgb_iowa3, small_rgb_iowa3_1, small_rgb_iowa3_2, small_rgb_iowa3_3 respectively. These bands will be used for defining the preprocessors.

# In[35]:


iowa_clouds.band_names


# This is a four band imagery, so the preprocessors are first defined for scaling.

# In[36]:


preprocessors = [('small_rgb_iowa3', 'small_rgb_iowa3_1', 'small_rgb_iowa3_2', 'small_rgb_iowa3_3', MinMaxScaler())]


# In[37]:


data = prepare_tabulardata(explanatory_rasters=[iowa_clouds], preprocessors=preprocessors)


# In[38]:


data.show_batch()


# ### Model Initialization <a class="anchor" id="15"></a>

# Once the data is prepared, an unsupervised model of k-means clustering from scikit-learn can be used for clustering the pixels into areas of clouds and no clouds. The clustering model is passed inside an MLModel, with the number of clusters set as three for the classes of no clouds, medium clouds, and dense clouds.

# In[39]:


model = MLModel(data, 'sklearn.cluster.KMeans', n_clusters=3, init='k-means++', random_state=43)


# ### Model Training <a class="anchor" id="16"></a>
# Finally, the model is ready to be trained, labeling the pixels into the three different classes.

# In[40]:


model.fit()


# In[41]:


model.show_results()


# ### Cloud Prediction<a class="anchor" id="17"></a>

# In[42]:


# creating the local output raster path
import tempfile
local_path=tempfile.gettempdir()
output_raster_path=local_path+r"/result"+str(dt.now().microsecond)+".tif"
output_raster_path


# In[43]:


# predicting the cloud masks using the fitted model
pred_new = model.predict(explanatory_rasters=[iowa_clouds], 
                         prediction_type='raster',
                         output_raster_path=output_raster_path)


# ## Result Visualization<a class="anchor" id="18"></a>

# In[44]:


iowa_predicted_cloud_mask = Raster(output_raster_path)
iowa_predicted_cloud_mask.export_image(size=[2749,1816])


# From the results, it can be seen that the cloud mask has been created and is represented by the white and grey pixels, while the black pixels represent areas with no cloud coverage. These results could be further reclassified to consist of two distinct classes of clouds and no-clouds in Arcgis Pro, for generating the final mask. This mask can be further processed in Arcgis Pro to create a polygon mask if needed.

# ## Conclusion<a class="anchor" id="19"></a>
# 
# In this sample notebook, two methods were described to create cloud masks from satellite images. A third method will be described in the second part of this notebook series.
# 
# In the first method the cloudless sentinel package performed well in detecting clouds and provides flexibility to the user to detect different intensities of clouds by changing the model's initialization parameters. However, the core caveat of this model is that it can only be used on Sentinel imagery.
# 
# The second method performed well with scenes where there were only clouds, and has the benefit of being able to run an analysis on any kind of imagery like Sentinel, Landsat, Surface reflectance etc. The model can also be used for automatically labelling training data for a pixel classification cloud detection model, which will be covered in the next part.

# ### Data resources <a class="anchor" id="20"></a>

# | Dataset | Source | Link |
# | -| - |-|
# | sat imagery| sentinel2  |https://registry.opendata.aws/sentinel-2/|


# ====================
# coastline_classification_using_feature_classifier.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Coastline classification using Feature Classifier
# 
# > * 🔬 Data Science
# > * 🥠 Deep Learning and Object classification

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Necessary imports](#Necessary-imports)
# * [Connect to your GIS](#Connect-to-your-GIS)
# * [Export training data](#Export-training-data)
# * [Train the model](#Train-the-model)
#  * [Prepare data](#Prepare-data)
#  * [Visualize training data](#Visualize-training-data)
#  * [Load model architecture](#Load-model-architecture)
#  * [Find an optimal learning rate](#Find-an-optimal-learning-rate)
#  * [Fit the model](#Fit-the-model)
#  * [Visualize results in validation set](#Visualize-results-in-validation-set)
#  * [Accuracy assessment](#Accuracy-assessment)
#  * [Save the model](#Save-the-model)
# * [Model inference](#Model-inference)
# * [Results](#Results)
# * [Conclusion](#Conclusion)

# ## Introduction

# We have already seen how we can [extract coastlines using Landsat-8 multispectral imagery and band ratio technique](https://developers.arcgis.com/python/sample-notebooks/coastline-extraction-usa-landsat8-multispectral-imagery/), and next, we will classify these coastlines into multiple categories. To achieve this, we can train a model that can classify a coastline as one of the different categories shown in the screenshot below:
# 
# 
# <br>
# <center>Figure 1. Coastline categories</center>
# 
# In this sample notebook, we will see how we can classify these coastlines in the categories mentioned in figure 1, by training a `Feature Classifier` model.

# ## Necessary imports

# In[1]:


import os
import glob
import zipfile
from pathlib import Path

from arcgis.gis import GIS
from arcgis.learn import prepare_data, FeatureClassifier


# ## Connect to your GIS

# In[2]:


# Connect to GIS
gis = GIS("home")


# ## Export training data 

# Using [ArcGIS Maritime](https://desktop.arcgis.com/en/arcmap/latest/extensions/maritime-charting/what-is-the-arcgis-for-maritime-charting-.htm ), we imported [NOAA’s Electronic Navigational Charts](https://www.charts.noaa.gov/ENCs/ENCs.shtml). The maritime data in these charts contain the Coastline Feature class with the Category of Coastline details. The Sentinel 2 imagery was downloaded from the [Copernicus Open Access Hub](https://scihub.copernicus.eu/dhus/).
# 
# Before exporting the data, we will first create grids along the coastlines that can act as a feature class while exporting the data. For this, we will use the `Generate Rectangles Along Lines` tool. The parameters required to run the function are:
# 
# - `Input Line Features`: Coastlines (belonging to each category)
# - `Output Feature Class`: Output Feature class name
# - `Length Along the Line`: 1000
# - `Length Perpendicular to the Line`: 1000
# - `Spatial Sort Method`: Upper left

# 
# 

# Figure 3 shows the output from the tool when run on a feature class belonging to category 3 (sandy shore).
# 
# Next, we add a category column to the feature class that can act as the class value field while exporting the data. We created a category column of the long data type named <b>CATCOA</b>. In this column, we set the value to be the same as the feature's coastline category using the `Add` and `Calculate` options respectively, as shown in figure 4.

# 
# 
# <br>
# <center>Figure 4. Feature class after added category column</center>

# We then performed a similar process for each type of coastline category. Next, we exported this data in the "Labeled Tiles" metadata format for a small extent with multiple categories, using the [`Export Training Data For Deep Learning`](https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/export-training-data-for-deep-learning.htm) tool. This `Export Training Data For Deep Learning` tool is available in [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview), as well as [ArcGIS Image Server](https://www.esri.com/en-us/arcgis/products/arcgis-image-server).
# 
# - `Input Raster`: Sentinel2 imagery
# - `Input Feature class Or Classified Raster`: Feature class as shown in figure 4.
# - `Tile Size X & Tile Size Y`: 256
# - `Stride X & Stride Y`: 128
# - `Meta Data Format`: 'Labeled Tiles' as we are training a `Feature Classifier` model.
# - `Environments`: Set optimum `Cell Size`, `Processing Extent`.

# 
# 
# <br>
# <center>Figure 5. Export Training Data for Deep Learning tool</center>

# ```Python
# with arcpy.EnvManager(extent="MINOF", cellSize=10):
#     arcpy.ia.ExportTrainingDataForDeepLearning("Multispectral_MTD_MSIL1C", r"D:\Coastline category\Data\Category_1", "CoastlineL_Clip_category1_Rectangles", "TIFF", 256, 256, 128, 128, "ONLY_TILES_WITH_FEATURES", "Labeled_Tiles", 0, "CATCOA", 0, None, 0, "MAP_SPACE", "PROCESS_AS_MOSAICKED_IMAGE", "NO_BLACKEN", "FIXED_SIZE")
# ```   

# We also created separate folders for each category to demonstrate the recently added multi-folder training support. Alternatively, you can choose the same folder each time you export the data, resulting in the newly exported images being amended in the existing folder.

# We have also provided a subset of training data exported from each category. You can use this data directly to run these experiments.

# In[3]:


training_data = gis.content.get('9251417cb9ab4a059eb538282f82883c')
training_data


# In[4]:


filepath = training_data.download(file_name=training_data.name)


# In[5]:


with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[6]:


output_path = os.path.join(os.path.splitext(filepath)[0], "*")


# In[7]:


output_path = glob.glob(output_path)


# ## Train the model

# `arcgis.learn` provides the ability to determine the class of each feature in the form of a FeatureClassifier model. To learn more about it's workings and use cases, see this guide - ["How feature classifier works?"](https://developers.arcgis.com/python/guide/how-feature-categorization-works/).

# ### Prepare data

# Here, we will specify the path to our training data and a few hyperparameters.
# 
# - `path`: path of the folder/list of folders containing training data.
# - `batch_size`: Number of images your model will train on each step inside an epoch, it directly depends on the memory of your graphic card. 128 worked for us on a 32GB GPU.

# In[ ]:


# output_path = [ r'D:\Coastline_category\Data_generate_rectangles\Category_1',
#            r'D:\Coastline_category\Data_generate_rectangles\Category_2',
#            r'D:\Coastline_category\Data_generate_rectangles\Category_3',
#            r'D:\Coastline_category\Data_generate_rectangles\Category_4',
#            r'D:\Coastline_category\Data_generate_rectangles\Category_6',
#            r'D:\Coastline_category\Data_generate_rectangles\Category_7',
#            r'D:\Coastline_category\Data_generate_rectangles\Category_8',
#            r'D:\Coastline_category\Data_generate_rectangles\Category_10']


# In[7]:


data = prepare_data(
    path=output_path,
    batch_size=128,
    val_split_pct=0.2
)


# ### Visualize training data

# To get a sense of what the training data looks like, the `arcgis.learn.show_batch()` method randomly picks a few training chips and visualizes them.
# - `rows`: Number of rows to visualize

# In[8]:


data.show_batch(rows=5)


# ### Load model architecture

# In[9]:


model = FeatureClassifier(data, oversample=True)


# ### Find an optimal learning rate

# [Learning rate](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) is one of the most important hyperparameters in model training. `ArcGIS API for Python` provides a learning rate finder that automatically chooses the optimal learning rate for you.

# In[10]:


lr = model.lr_find()


# ### Fit the model 

# We will train the model for a few epochs with the learning rate we have found. For the sake of time, we can start with 20 epochs.

# In[11]:


model.fit(20, lr=lr)


# Here, with only 20 epochs, we can see reasonable results — both training and validation losses have gone down considerably, indicating that the model is learning to classify coastlines.

# ### Visualize results in validation set

# It is a good practice to see the results of the model viz-a-viz ground truth. The code below picks random samples and shows us ground truth and model predictions, side by side. This enables us to preview the results of the model within the notebook.

# In[12]:


model.show_results(rows=4)


# ### Accuracy assessment

# `arcgis.learn` provides `plot_confusion_matrix()` that plots a confusion matrix of the model predictions to evaluate its accuracy.

# In[13]:


model.plot_confusion_matrix()


# The confusion matrix validates that the trained model is learning to classify coastlines. The diagonal numbers show the number of chips correctly classified to the respective categories. The results are good for all but category 2. By looking at the row for category 2, we can see that there are very few chips in the validation set of our data (5 in total). As such, we can increase the number of chips either by increasing the value of the `val_split_pct` parameter in `prepare_data()` or by exporting more data for that particular category. We may need to re-train the model if we add more data to it.

# ### Save the model

# Now, we will save the model that we trained as a 'Deep Learning Package' ('.dlpk' format). A Deep Learning package is the standard format used to deploy deep learning models on the ArcGIS platform.
# 
# We will use the `save()` method to save the trained model. By default, it will be saved to the 'models' sub-folder within our training data folder.

# In[14]:


model.save('model-20e_8classes')


# ## Model inference

# In order for us to perform inferencing in ArcGIS Pro, we need to create a feature class along the coastlines using the `Generate Rectangles Along Lines` tool, as shown in figure 2, for an area that is not already seen by the model.
# 
# Now, we will use the `Classify Objects Using Deep Learning tool` for inferencing the results. The parameters required to run the function are:
# 
# - `Input Raster`: Sentinel2 imagery
# - `Input Features`: Output from the `Generate Rectangles Along coastlines` tool.
# - `Output CLassified Objects Feature Class`: Output feature class.
# - `Model Definition`: The model that we trained.
# - `Class Label Field`: Feild name that will contain the detected class number.
# - `Environments`: Set optimum `Cell Size`, `Processing Extent` and `Processor Type`.

# 
# 
# <br>
# <center>Figure 6. Classify Objects Using Deep Learning tool</center>

# ## Results 

# We selected an unseen (by the model) sandy shoreline (category 3) and generated the required rectangles along it using the `Generate Rectangles Along Lines` tool. We then used our model for classification. Below are the results that we got.

# 
# 

# You can observe in figure 9 that two rectangles got misclassified into category 7 (mangrove), and that the rest were classified correctly as belonging to category 3. Further training of the model could produce even more accurate results.

# ## Conclusion 

# In this notebook, we demonstrated how to use the `FeatureClassifier` model from the `ArcGIS API for Python` to classify coastlines into multiple categories.


# ====================
# coastline_extraction-usa-landsat8_multispectral_imagery.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# ## Coastline extraction using Landsat-8 multispectral imagery and band ratio technique

# ## Table of Contents
# * [Prerequisites](#20)
# * [Introduction](#1)
# * [Necessary imports](#2)
# * [Connect to your GIS](#3)
# * [Get the data for analysis](#4) 
# * [Prepare data for analysis](#6)
#     * [Create geometry of aoi](#7)
#     * [Filter out the Landsat-8 tiles](#8)
#     * [Create mosaic raster from tiles](#9)
# * [Preprocessing of the data](#10)
#     * [Extract bands from Landsat-8 tiles](#11)
# * [Band ratio technique](#12)
# * [Postprocessing of results](#13)
#     * [Clip out extra area](#13)
#     * [Get the data in feature layer](#14)
#     * [Coastline extraction](#15)
#     * [Get the coastline](#16)
# * [Results](#17)
# * [Conclusion](#18)
# * [Data resources](#19)

# ## Prerequisites<a class="anchor" id="20"></a>
# 
# - This sample demonstrates how the coastlines can be extracted using [ArcGIS Image Server](https://www.esri.com/en-us/arcgis/products/arcgis-image-server). Alternatively, this can be done using [ArcGIS Image for ArcGIS Online](https://www.esri.com/en-us/arcgis/products/arcgis-image/options/arcgis-online).

# ## Introduction<a class="anchor" id="1"></a>

# Due to anthropogenic activities and natural processes i.e. changes in sea level, sedimentation and wave energy coastlines are changing throughout the world. Coastline is contact line between sea and land, it is an important linear feature on earth's surface with dynamic nature. 
# 
# Traditionally, the coastlines were manually digitized which was time-consuming and labour intensive. Remote sensing is a good alternative to extract coastlines using satellite imagery, this way both temporal and spatial aspects can be covered.
# 
# Satellite imagery of visible range can be used for interpretation and can be easily obtained. But the imageries covering infrared wavelength is best to extract boundary between land and water. So, the satellites which covers both visible and infrared spectrum are widely accepted for coastline extraction and mapping.
# 
# Landsat-8 multispectral imagery is used in the current study as it covers a wavelength ranging from 0.43 to 12.51 micrometers, and hence suitable for coastal and aerosol studies. 

# ![image.png](attachment:image.png)

# ## Neccessary Imports <a class="anchor" id="2"></a>

# In[1]:


import os
import glob
from zipfile import *

import arcgis
import arcpy
from arcpy.management import PolygonToLine
from datetime import datetime
import pandas as pd
from arcgis.features import GeoAccessor, GeoSeriesAccessor
from arcgis.raster.analytics import convert_raster_to_feature
from arcgis.features.manage_data import overlay_layers
from arcgis.raster.functions import equal_to, greater_than, greater_than_equal, clip, apply, extract_band, stretch


# ## Connect to your GIS <a class="anchor" id="3"></a>

# In[2]:


from arcgis import GIS
gis =  GIS('home')
gis2 = GIS(profile="your_enterprise_portal")


# ## Get the data for analysis<a class="anchor" id="4"></a>

# [Multispectral Landsat](https://www.arcgis.com/home/item.html?id=d9b466d6a9e647ce8d1dd5fe12eb434b) includes Landsat GLS and Landsat 8 imagery for use in visualization and analysis. This layer is time enabled and includes a number band combinations and indices rendered on demand. The Landsat 8 imagery includes eight multispectral bands from the Operational Land Imager (OLI) with 30m spatial resolution and two bands from the Thermal Infrared Sensor (TIRS) of 100m spatial resolution.  It is updated daily with new imagery directly sourced from the Landsat on AWS collection.

# In[3]:


landsat_item = gis.content.get('d9b466d6a9e647ce8d1dd5fe12eb434b')
landsat = landsat_item.layers[0]
landsat_item


# The buffer feature of 20 km was created from USA boundaries. This feature layer geometry will be used to get the Landsat-8 tiles of coastal areas.

# In[4]:


aoi = gis.content.get('6ee0006608704d0389881e0bcf936778')
aoi


# ## Prepare data for analysis<a class="anchor" id="6"></a>

# A map widget was created, which will define the extent of area of interest for the analysis.

# In[5]:


m = gis.map('USA', 4)
m.add_layer(aoi)
m


# 

# In[6]:


m.extent = {'spatialReference': {'latestWkid': 3857, 'wkid': 102100},
 'xmin': -10124000.92647267,
 'ymin': 3341135.188124092,
 'xmax': -9823144.78314236,
 'ymax': 3463434.4333803146}


# Multispectral imagery layers consist of data for whole world. First step, is to filter out the cloud free data for the study area which will be used in the analysis.

# ### Create geometry of aoi<a class="anchor" id="7"></a>

# The aoi has four polygons representing: Islands, East coast, West coast and Alaska.

# In[7]:


dfm = aoi.layers[0].query(out_fields="OBJECTID, type, code").sdf
dfm


# In the current analysis  the area of interest is situated at East coast. The geometry of aoi was created for filtering out the landsat-8 tiles for the study area. `objectid=2` was used as it represents the east coast.

# In[8]:


aoi_layer = aoi.layers[0]

## use the correct objectid by refering to the above dataframe for your area of interest. 
aoi_feature = aoi_layer.query(where="objectid=2")
aoi_geom = aoi_feature.features[0].geometry
aoi_geom['spatialReference'] = {'wkid':3857}


# ### Filter out the Landsat-8 tiles<a class="anchor" id="8"></a>
# 
# The landsat-8 tiles were filtered out on the basis of date of acquisition and cloud cover. The order of tiles is on the basis of cloud cover from low to high.

# In[9]:


import pandas as pd
from datetime import datetime
selected = landsat.filter_by(where="(Category = 1) and (CloudCover<0.03)",
                             time=[datetime(2016, 1, 1), datetime(2019, 12, 31)],
                             geometry=arcgis.geometry.filters.intersects(aoi_geom))

df = selected.query(out_fields="AcquisitionDate, GroupName, CloudCover, DayOfYear", 
                    order_by_fields="CloudCover").sdf
df['AcquisitionDate'] = pd.to_datetime(df['AcquisitionDate'], unit='ms')
df


# In[10]:


## Create a list of tiles 
oid = df["OBJECTID"].tolist()


# ### Create mosaic raster from tiles<a class="anchor" id="9"></a>
# 
# After creating a list of landsat-8 tiles, next step was to create a mosaic raster for the study area. `mosaic_by` was used to lock all the tiles of landsat-8 list.

# In[11]:


landsat.mosaic_by(method='lock_rasters', lock_rasters=oid)
landsat.extent = m.extent
landsat


# ## Preprocessing of the data<a class="anchor" id="10"></a>

# ### Extract bands from Landsat-8 tiles<a class="anchor" id="11"></a>

# From the mosaic raster, 3 single band raster were created for NIR, Red and Blue band using [extract_band](https://desktop.arcgis.com/en/arcmap/latest/manage-data/raster-and-images/extract-bands-function.htm) function.

#  
# 
# #### <Center>Landsat-8 band spectrum table<Center> ####

# Landsat-8 fifth band covers 0.85 - 0.88 micrometers of EMR spectrum. NIR band is best for delineating land and water interface. Due to high absorption of water in NIR spectrum the reflectance of water is around zero whereas vegetation and land has high reflectance value. In NIR imagery the water appears black and land & vegetation appears bright grey to white which makes it easier to delineate land and water boundary. 

# In[12]:


nir1 = extract_band(landsat, [5])
nir1


# Second band of landsat-8 represents blue spectrum and covers 0.45 - 0.51 micrometer. Water has high reflectance in blue spectrum.

# In[13]:


blue1 = extract_band(landsat, [2])
blue1


# Fourth band represents red spectrum and covers 0.64 - 0.67 micrometer wavelength of EMR spectrum. Red spectrum is highly absorbed by vegetation and this is useful to delineate between soil and vegetation.

# In[14]:


red1 = extract_band(landsat, [4])
red1


# ### Band ratio technique<a class="anchor" id="12"></a>

# A recent remote sensing technique to extract coastline is `Band Ratio`. In this technique the DN values of bands are divided to create a binary raster. NIR, Red and Blue bands were used for creating the binary raster. NIR band is selected as it is able to delineate water-land boundary, Red band is important for vegetation and water content and Blue band has high reflectance in water bodies. In the current study the following band ratio formula was used:
# 
#                                      Blue>NIR & Blue>Red
# The output of this formula is a binary raster which has 0 and 1 pixel value. 1 represents water (white pixels) and 0 represents land (black pixels) i.e. bare surface, vegetation etc.

# In[15]:


binary1 = arcgis.raster.functions.raster_calculator([nir1, blue1, red1],  
                                                    ['nir', 'blue', 'red'],
                                                    "(blue>nir)&(blue>red)", 
                                                    extent_type='FirstOf', 
                                                    cellsize_type='FirstOf')
binary1


#  For vizualisation, [stretch](https://desktop.arcgis.com/en/arcmap/latest/manage-data/raster-and-images/stretch-function.htm#:~:text=Stretching%20improves%20the%20appearance%20of,results%20in%20the%20raster%20display.) raster function was used to stretch function the histogram. After applying stretch the pixel values were changed to 0 and 255 where 0 represents land and 255 represents water.

# In[16]:


dra1 = arcgis.raster.functions.stretch(binary1, 
                                      stretch_type='MinMax', 
                                      dra=True)
dra1


# ## Postprocessing of results<a class="anchor" id="13"></a>
# ### Clip out extra area<a class="anchor" id="13"></a>
# 
# The stretched binary raster was clipped out using aoi geometry to clip out the extra area and to minimize the processing time. [Clip](https://developers.arcgis.com/python/api-reference/arcgis.raster.functions.html#clip) raster function was used.

# In[17]:


clip_diff1 = clip(dra1, aoi_geom)


# In[18]:


water_ras = greater_than([clip_diff1, 0], extent_type='FirstOf', cellsize_type='FirstOf')
water_raster = water_ras.save('water_ras'+str(datetime.now().microsecond), gis=gis2)
water_ras 


# In[19]:


dra2 = arcgis.raster.functions.stretch(water_ras, 
                                      stretch_type='MinMax', 
                                      dra=True)
dra2_ras = dra2.save("dra_raster"+str(datetime.now().microsecond), gis=gis2)
dra2


# ### Get the data in feature layer<a class="anchor" id="14"></a>
# 
# The clipped binary raster was converted to polygon for coastline extraction. [convert_raster_to_feature](https://developers.arcgis.com/python/api-reference/arcgis.raster.analytics.html?highlight=convert_raster_to_feature#arcgis.raster.analytics.convert_raster_to_feature) function was used for conversion.

# In[20]:


b2_poly = convert_raster_to_feature(dra2_ras.layers[0], 
                                    field='Value', 
                                    output_type='Polygon', 
                                    simplify=True, 
                                    output_name='coastline_poly'+str(datetime.now().microsecond), 
                                    gis=gis2)


# ### Coastline extraction<a class="anchor" id="15"></a>

# #### Filter out the water polygons
# 
# The feature layer was converted to dataframe using `query` and the polygons were ordered on the basis on `Shape_Area` column.

# In[21]:


## Create dataframe from feature layer and get water polygons
dfm1 = b2_poly.layers[0].query('gridcode=255').sdf 

## Convert dataframe to feature layer
water_poly = gis2.content.import_data(dfm1, title='coast_poly_full'+str(datetime.now().microsecond))
dfm1


# #### Extract the coastline polygon
# 
# The polygon with largest area represents the coastline.The polygon with highest value in `Shape_Area`column was extracted by referring the `df`. The new dataframe was converted to feature layer using `import_data` function.

# In[22]:


## Get the polygon with largest area as it will represent the coastline
df = water_poly.layers[0].query().sdf
df['MYAREA'] = df.SHAPE.geom.area
dfm5 = df[df['MYAREA']==df['MYAREA'].max()]
coast_poly = gis2.content.import_data(dfm5, title='coast_poly'+str(datetime.now().microsecond))


# #### Convert coastline polygons to line

# The coastline polygon was converted to line using [PolygonToLine](https://desktop.arcgis.com/en/arcmap/10.3/tools/data-management-toolbox/polygon-to-line.htm) arcpy function.

# In[23]:


dc_df = pd.DataFrame.spatial.from_layer(coast_poly.layers[0])
display(dc_df.head())


# In[24]:


# Create empty dataframe
df = pd.DataFrame()

# Create polyline
df['SHAPE'] = dc_df.SHAPE.geom.boundary()
coast_polyline = df.spatial.to_featurelayer('coastline_polyline'+str(datetime.now().microsecond))


# #### Remove noise
# 
# A mask was created using the clipped raster, it represents the extent of raster. The mask will be used to remove the extra lines.

# In[25]:


#Create a mask
mask = greater_than_equal([clip_diff1, -1], extent_type='FirstOf', cellsize_type='FirstOf')
mask_ras = mask.save('mask'+str(datetime.now().microsecond), gis=gis2)

# Create a mask to remove noise which covers whole extent of clipped raster
aoi2 = convert_raster_to_feature(mask_ras.layers[0], 
                                field='Value', 
                                output_type='Polygon', 
                                simplify=True, 
                                output_name='aoi2'+str(datetime.now().microsecond), 
                                gis=gis2)


# The coastline layer which we got in the previous step has some extra lines representing the boundary of aoi layer. To remove the extra lines a negative buffer of 10 meter was created using `create_buffer` function.

# In[26]:


### Create neg_buffer to remove the noise
neg_buffer = arcgis.create_buffers(aoi2, 
                                   distances=[-10],
                                   units='Meters', 
                                   output_name='aoi_neg_buf'+str(datetime.now().microsecond), 
                                   gis=gis2)


# Extra lines were masked out using the negative buffer boundary.

# In[27]:


coastline_final = overlay_layers(coast_polyline.layers[0], neg_buffer.layers[0], output_name='extracted_coastline'+str(datetime.now().microsecond))


# ## Extracted Coastlines<a class="anchor" id="17"></a>

# In[28]:


m9= gis2.map('USA', 4)
m9


# 

# In[29]:


m9.add_layer(landsat)
m9.zoom_to_layer(coastline_final)


# In[30]:


## Create dataframe for visualization
dfm_1= coastline_final.layers[0].query(out_fields="*").sdf

## add the dataframe to map widget
dfm_1.spatial.plot(map_widget=m9,
                renderer_type='s', 
                pallete='spring',
                alpha=1
                )


# Coastline for whole USA were extracted with the above workflow using Landsat-8 imagery. To see the results, click [here](https://arcg.is/SGS5X)

# ## Conclusion<a class="anchor" id="18"></a>

# Band Ratio technique is a efficient method, which gives highly accurate results with less processing time and it is able to cover both temporal and spatial aspects of coastline changes. Band Ratio technique is a easy to calculate method which gives highly accurate results with less processing time. The workflow can be applied on any area using multispectral imagery i.e. Landsat-8, Sentinel-2, etc.

# ## Literature resources<a class="anchor" id="19">

# |Literature | Source | Author |
# | -| - |-| 
# |Research Paper|                       Shoreline Change Mapping Using Remote Sensing and GIS     | Ali Kourosh Niya, Ali Asghar Alesheikh, Mohsen Soltanpor, Mir Masoud Kheirkhahzarkesh
# | Research Paper|                   Shoreline change assessment using remote sensing and GIS techniques: a case study of the Medjerda delta coast, Tunisia           | Mourad Louati & Hanen Saidi & Fouad Zargouni


# ====================
# constructing_drive_time_based_service_areas.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Constructing drive time based service areas
# This sample shows how the `network` module of the ArcGIS API for Python can be used to construct service areas. In this sample, we generate service areas for two of the fire stations in central Tokyo, Japan. We later observe how the service area varies by time of day for a fire station in the city of Los Angeles.

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Constructing-drive-time-based-service-areas" data-toc-modified-id="Constructing-drive-time-based-service-areas-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Constructing drive time based service areas</a></span><ul class="toc-item"><li><ul class="toc-item"><li><span><a href="#Service-areas" data-toc-modified-id="Service-areas-1.0.1"><span class="toc-item-num">1.0.1&nbsp;&nbsp;</span>Service areas</a></span></li></ul></li><li><span><a href="#Connect-to-the-GIS" data-toc-modified-id="Connect-to-the-GIS-1.1"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>Connect to the GIS</a></span><ul class="toc-item"><li><span><a href="#Create-a-Network-Layer" data-toc-modified-id="Create-a-Network-Layer-1.1.1"><span class="toc-item-num">1.1.1&nbsp;&nbsp;</span>Create a Network Layer</a></span></li></ul></li><li><span><a href="#Create-fire-station-facility-layer" data-toc-modified-id="Create-fire-station-facility-layer-1.2"><span class="toc-item-num">1.2&nbsp;&nbsp;</span>Create fire station facility layer</a></span></li><li><span><a href="#Compute-the-service-area" data-toc-modified-id="Compute-the-service-area-1.3"><span class="toc-item-num">1.3&nbsp;&nbsp;</span>Compute the service area</a></span><ul class="toc-item"><li><ul class="toc-item"><li><span><a href="#Read-the-result-back-as-a-FeatureSet" data-toc-modified-id="Read-the-result-back-as-a-FeatureSet-1.3.0.1"><span class="toc-item-num">1.3.0.1&nbsp;&nbsp;</span>Read the result back as a <code>FeatureSet</code></a></span></li></ul></li><li><span><a href="#Visualize-the-service-area-on-the-map" data-toc-modified-id="Visualize-the-service-area-on-the-map-1.3.1"><span class="toc-item-num">1.3.1&nbsp;&nbsp;</span>Visualize the service area on the map</a></span></li></ul></li><li><span><a href="#Constructing-service-areas-for-different-times-of-the-day" data-toc-modified-id="Constructing-service-areas-for-different-times-of-the-day-1.4"><span class="toc-item-num">1.4&nbsp;&nbsp;</span>Constructing service areas for different times of the day</a></span></li></ul></li></ul></div>

# ### Service areas
# A network service area is a region that encompasses all accessible streets (that is, streets that are within a specified impedance). For instance, the 5-minute service area for a point on a network includes all the streets that can be reached within five minutes from that point.
# 
# Service areas also help evaluate accessibility. Concentric service areas show how accessibility varies. Once service areas are created, you can use them to identify how much land, how many people, or how much of anything else is within the neighborhood or region.
# 
# ![](http://desktop.arcgis.com/en/arcmap/latest/extensions/network-analyst/GUID-0166E6A5-CDEC-4A03-A022-F4A6EB7F9EAD-web.png)
# 
# Service area solver provides functionality for finding out how far a vehicle could go within a specified time or distance limit. 
# 
# ## Connect to the GIS
# Establish a connection to your organization which could be an ArcGIS Online organization or an ArcGIS Enterprise. To be able to run the code in this sample notebook, you would need to provide credentials of a user within an ArcGIS Online organization.

# In[1]:


from datetime import datetime
from IPython.display import HTML
import pandas as pd
from arcgis.gis import GIS

my_gis = GIS('home')


# ### Create a Network Layer
# To perform any network analysis (such as finding the closest facility, the best route between multiple stops, or service area around a facility), you would need to create a `NetworkLayer` object. In this sample, since we are creating service areas, we need to create a `ServiceAreaLayer` which is a type of `NetworkLayer`.
# 
# To create any `NetworkLayer` object, you would need to provide the URL to the appropriate network analysis service. Hence, in this sample, we provide a `serviceArea` URL to create a `ServiceAreaLayer` object. 
# 
# Since all ArcGIS Online organizations already have access to those routing services, you can access this URL through the `GIS` object's `helperServices` property. If you have your own ArcGIS Server based map service with network analysis capability enabled, you would need to provide the URL for this service.
# 
# Let us start by importing the `network` module

# In[3]:


import arcgis.network as network


# In[4]:


service_area_url = my_gis.properties.helperServices.serviceArea.url
service_area_url


# In[9]:


sa_layer = network.ServiceAreaLayer(service_area_url, gis=my_gis)


# ## Create fire station facility layer

# We obtained the coordinates of two fire stations in Tokyo. We construct `Feature` and `FeatureSet` objects to represent them.

# In[6]:


fire_station_1_coord = '139.546910,35.695729'
fire_station_2_coord = '139.673726,35.697988'

from arcgis.features import Feature, FeatureSet

f1 = Feature(geometry={'x':float(fire_station_1_coord.split(',')[0]),
                      'y':float(fire_station_1_coord.split(',')[1])})

f2 = Feature(geometry={'x':float(fire_station_2_coord.split(',')[0]),
                      'y':float(fire_station_2_coord.split(',')[1])})

fire_station_fset = FeatureSet([f1,f2], geometry_type='esriGeometryPoint', 
                            spatial_reference={'latestWkid': 4326})


# Let us display the fire stations on a map

# In[2]:


map1 = my_gis.map('Tokyo', zoomlevel=12)
map1


# In[31]:


fire_truck_symbol = {"type":"esriPMS",
                     "url":"http://static.arcgis.com/images/Symbols/SafetyHealth/FireTruck.png",
                     "contentType": "image/png", "width":20, "height":20}

map1.draw(fire_station_fset, symbol=fire_truck_symbol)


# ## Compute the service area
# To compute the service area (area accessible to each facility based on drive times), we use the `solve_service_area()` method of a `ServiceAreaLayer` object. As the fire trucks will be travelling away from the stations, we need to specify the direction of travel in the `travel_direction` parameter. Also, since for the type of vehicles is fire trucks, we could specify the travel mode to make it easier to supply all other related parameters.

# In[10]:


travel_modes = sa_layer.retrieve_travel_modes()
truck_mode = [t for t in travel_modes['supportedTravelModes'] if t['name'] == 'Trucking Time'][0]

result = sa_layer.solve_service_area(fire_station_fset, default_breaks=[5,10,15], 
                                     travel_direction='esriNATravelDirectionFromFacility',
                                     travel_mode=truck_mode)


# #### Read the result back as a `FeatureSet`
# The `result` variable contains the service area as a dictionary. We inspect its keys and construct `Feature` and `FeatureSet` objects out of it to display in the map

# In[11]:


result.keys()


# In[12]:


result['saPolygons'].keys()


# In[17]:


poly_feat_list = []
for polygon_dict in result['saPolygons']['features']:
    f1 = Feature(polygon_dict['geometry'], polygon_dict['attributes'])
    poly_feat_list.append(f1)


# In[18]:


service_area_fset = FeatureSet(poly_feat_list, 
                         geometry_type=result['saPolygons']['geometryType'],
                         spatial_reference= result['saPolygons']['spatialReference'])


# Let us inspect the service area as a Pandas `DataFrame` to understand the attribute information

# In[26]:


service_area_fset.sdf


# ### Visualize the service area on the map
# From the DataFrame above, we know, there are 3 service area polygons for each fire station. The drive times are given as a range between `FromBreak` and `ToBreak` columns. Let us use this information to visualize the polygons with different colors and appropriate popup messags on the map

# In[25]:


colors = {5: [0, 128, 0, 90], 
          10: [255, 255, 0, 90], 
          15: [255, 0, 0, 90]}

fill_symbol = {"type": "esriSFS","style": "esriSFSSolid",
               "color": [115,76,0,255]}


# In[30]:


for service_area in service_area_fset.features:
    
    #set color based on drive time
    fill_symbol['color'] = colors[service_area.attributes['ToBreak']]
    
    #set popup
    popup={"title": "Service area", 
            "content": "{} minutes".format(service_area.attributes['ToBreak'])}
    
    map1.draw(service_area.geometry, symbol=fill_symbol, popup=popup)


# Click the drive time areas to explore their attributes. Because the content of the pop-ups may include HTML source code, it is also possible to have the pop-up windows include other resources such as tables and images.

# ## Constructing service areas for different times of the day
# The service areas for the facilities may look different depending on what time of day a vehicle would start driving. Therefore, we will run the solver using multiple day times for the `time_of_day` parameter to be able to compare visually the difference between the service areas. We will generate service areas for the following times: 6am, 10am, 2pm, 6pm, and 10pm. 
# 
# In the following example, we assume that the facility is in the downtown of Los Angeles and we want to generate drive time areas at different times during the same day.

# In[44]:


times = [datetime(2017, 6, 10, h).timestamp() * 1000 
         for h in [6, 10, 14, 18, 22]]

# fire station location
fire_station = '-118.245847, 34.038608'

#loop through each time of the day and compute the service area
sa_results = []
for daytime in times:
    result = sa_layer.solve_service_area(facilities=fire_station, default_breaks=[5,10,15], 
                                         travel_direction='esriNATravelDirectionFromFacility',
                                         time_of_day=daytime, time_of_day_is_utc=False)
    sa_results.append(result)


# The service area has been computed, we process it to generate a list of `FeatureSet` objects to animate on the map

# In[72]:


LA_fset_list=[]
for result in sa_results:
    poly_feat_list = []
    for polygon_dict in result['saPolygons']['features']:
        f1 = Feature(polygon_dict['geometry'], polygon_dict['attributes'])
        poly_feat_list.append(f1)
        
    service_area_fset = FeatureSet(poly_feat_list, 
                         geometry_type=result['saPolygons']['geometryType'],
                         spatial_reference= result['saPolygons']['spatialReference'])
    
    LA_fset_list.append(service_area_fset)


# Draw and animate the results on a map

# In[1]:


map2= my_gis.map("Los Angeles, CA")
map2


# In[75]:


import time
map2.clear_graphics()

times = ['6 am', '10 am', '2 pm', '6 pm', '10 pm']
j=0
time.sleep(2)

for fset in LA_fset_list:
    print(times[j])
    map2.draw(fset)
    j+=1
    time.sleep(1)


# Thus from the animation above, we notice the service area is smallest at 6 AM and increases progressively later during the day.


# ====================
# count_cars_in_aerial_imagery_using_deep_learning.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Count cars in aerial imagery using deep learning

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Necessary imports](#Necessary-imports)
# * [Connect to your GIS](#Connect-to-your-GIS)
# * [Get data and model for analysis](#Get-data-and-model-for-analysis)
# * [Detect and count cars](#Detect-and-count-cars)
# * [Visualize detections on map](#Visualize-detections-on-map)
# * [Conclusion](#Conclusion)
# * [References](#References)

# ## Introduction 

# ArcGIS pretrained models automate the task of digitizing and extracting geographical features from imagery and point cloud datasets. Manually extracting features from raw data, such as digitizing cars, is time consuming. Deep learning automates the process and minimizes the manual interaction necessary to complete these tasks. However, training a deep learning model can be complicated as it needs large amounts of data, computing resources, and knowledge of how deep learning works.
# 
# With ArcGIS pretrained models, we do not need to invest time and energy into training a deep learning model. The ArcGIS models have been trained on data from a variety of geographies and work well across them. These pretrained models are available on [ArcGIS Living Atlas of the World](https://livingatlas.arcgis.com/en/home/) to anyone with an ArcGIS account.
# 
# [Car Detection-USA](https://doc.arcgis.com/en/pretrained-models/latest/imagery/introduction-to-car-detection-usa.htm) is used to detect cars in high-resolution drone or aerial imagery. Car detection can be used for applications such as traffic management and analysis, parking lot utilization, urban planning, and more. It can also be used as a proxy for deriving economic indicators and estimating retail sales. High-resolution aerial and drone imagery can be used for car detection due to its high spatiotemporal coverage.

# ## Necessary imports

# In[1]:


import arcgis
from arcgis.gis import GIS
from arcgis.learn import detect_objects
from arcgis.raster.functions import equal_to

import pandas as pd
from datetime import datetime as dt
from ipywidgets import HBox, VBox, Label, Layout


# ## Connect to your GIS

# In[2]:


gis = GIS("home")


# First, search for your imagery layer in ArcGIS Online. We can search for content shared by users outside our organization by setting `outside_org` to True.

# ## Get data and model for analysis

# We have downloaded aerial imagery from [OpenAerialMap](https://openaerialmap.org/) and published it as an imagery layer, allowing us to search for it below.

# In[3]:


imagery = gis.content.search("aerial*")
imagery


# We will use four items for our analysis. Since the item is an Imagery Layer, accessing the layers property will give us a list of Imagery objects.

# In[16]:


area1 = imagery[0]
area2 = imagery[1]
area3 = imagery[2]
area4 = imagery[3]


# Search for the pretrained model item in `ArcGIS Living Atlas of the World`. 

# In[6]:


model = gis.content.search(
    "Car Detection-USA owner: esri_analytics", "Deep Learning Package", outside_org=True
)[0]
model


# ## Detect and count cars

# The following code runs `detect_objects` function over each imagery and calcultates the total count of cars detected. It then stores the item id and and its cars count in a dictionary named `num_cars`.

# In[ ]:


num_cars = {}
for i in range(len(imagery)):
    raster = imagery[i].layers[0]
    detected_cars = detect_objects(
        input_raster=raster,
        model=model,
        model_arguments={
            "padding": "100",
            "batch_size": "16",  # change batch size as per GPU specifications
            "threshold": "0.5",  # get detections greater than 50% confidence
            "return_bboxes": "False",
            "tile_size": "224",
        },
        output_name="detected_cars" + str(dt.now().microsecond),
        context={"processorType": "GPU", "cellSize": 1},
    )  # cell size is based on imagery resolution
    num_cars[area1.name] = len(detected_cars.layers[0].query(as_df=True))


# ## Visualize detections on map

# Let's visualize the detected cars in one of the aerial images. 

# In[74]:


map1 = gis.map()
map1


# In[70]:


map1.zoom_to_layer(detected_cars.layers[0])


# In[59]:


map1.extent = {
    "spatialReference": {"latestWkid": 3857, "wkid": 102100},
    "xmin": -7827819.073182879,
    "ymin": -3167815.9803417902,
    "xmax": -7827745.621976033,
    "ymax": -3167786.122127625,
}


# In[23]:


num_cars  # shows number of cars detected in each imagery layer.


# ## Conclusion

# This sample demonstrated how Car Detection-USA pretrained model can be used to detect cars in an aerial imagery and get the cars count in an area. 

# ## References

# - Imagery from [OpenAerialMap](https://openaerialmap.org/) is licensed under [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/)


# ====================
# counting_features_in_satellite_images_using_scikit_image.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Counting features in satellite images using scikit-image
# 
# The example below uses scikit-image library to detect circular features in farms using center pivot irrigation in Saudi Arabia. It then counts and reports the number of farms. This is one of the ways in which libraries from the scientific Python ecosystem can be integrated with the ArcGIS platform.
# 
# It uses the Multispectral Landsat imagery available at ArcGIS Online.
# 
# **Note**: to run this sample, you need a few extra libraries in your conda environment. If you don't have the libraries, install them by running the following commands from cmd.exe or your shell
# 
# ```
# conda install scipy
# conda install matplotlib
# conda install scikit-image
# ```

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Counting-features-in-satellite-images-using-scikit-image" data-toc-modified-id="Counting-features-in-satellite-images-using-scikit-image-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Counting features in satellite images using scikit-image</a></span><ul class="toc-item"><li><span><a href="#Blob-detection-using-scikit-image" data-toc-modified-id="Blob-detection-using-scikit-image-1.1"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>Blob detection using scikit-image</a></span></li></ul></li></ul></div>

# In[1]:


from arcgis.gis import GIS
gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# In[2]:


l8 = gis.content.get('d9b466d6a9e647ce8d1dd5fe12eb434b')
l8lyr = l8.layers[0]
l8


# The code below sets the extent of the layer, to a known extent of farms in Saudi Arabia and then visualizes the landsat layer: 

# In[4]:


l8lyr.extent = {'spatialReference': {'latestWkid': 3857, 'wkid': 102100},
                                         'type': 'extent',
                                         'xmax': 4296559.143733407,
                                         'xmin': 4219969.241391764,
                                         'ymax': 3522726.823081019,
                                         'ymin': 3492152.0117669892}
l8lyr


# We can preprocess the imagery using raster functions. The code below uses the ndvi raster function to identify areas that have healthy vegetation. This preprocessing step makes the scikit-image blob detection algorithm work better.

# In[5]:


from arcgis.raster.functions import ndvi, stretch


# In[6]:


stretch(ndvi(l8lyr), stretch_type='PercentClip', min_percent=30, max_percent=70, dra=True)


# The code below exports the imagery to a file from which we read it using matplotlib's image API and plot it:

# In[7]:


img = stretch(ndvi(l8lyr), stretch_type='PercentClip', min_percent=30, max_percent=70, dra=True).export_image(bbox=l8lyr.extent, bbox_sr=102100, size=[1200, 450],
                       export_format='jpeg', save_folder='.', save_file='centerpivotfarms.jpg', f='image')


# In[9]:


import matplotlib.pyplot as plt
import matplotlib.image as mpimg

img = mpimg.imread('centerpivotfarms.jpg')

# what does it look like?
plt.imshow(img)
plt.show()


# ## Blob detection using scikit-image
# 
# The code below uses scikit-image library to find blobs in the given grayscale image, and reports the number of farms thus detected. It also plots them for visualization using matplotlib.
# 
# Blobs are found using the [Difference of Gaussian (DoG)](http://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.blob_dog) method.

# In[10]:


from skimage import feature, color
import matplotlib.pyplot as plt
bw = img.mean(axis=2)

fig = plt.figure(figsize = (15,15))
ax = fig.add_subplot(1,1,1)

blobs_dog = [(x[0],x[1],x[2]) for x in feature.blob_dog(-bw, 
                                                        min_sigma=4, 
                                                        max_sigma=8,
                                                        threshold=0.1,
                                                        overlap=0.6)]

#remove duplicates
blobs_dog = set(blobs_dog)

img_blobs = color.gray2rgb(img)

for blob in blobs_dog:
    y, x, r = blob
    c = plt.Circle((x, y), r+1, color='red', linewidth=2, fill=False)
    ax.add_patch(c)

plt.imshow(img_blobs)
plt.title('Center Pivot Farms')

plt.show()
print('Number of center pivot farms detected: ' + str(len(blobs_dog)))



# ====================
# covid19_part1_mapping_the_pandemic.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Mapping the 2019 Novel Coronavirus Pandemic

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#1.-Import-Data" data-toc-modified-id="1.-Import-Data-1">1. Import Data</a></span><ul class="toc-item"><li><span><a href="#Necessary-Imports" data-toc-modified-id="Necessary-Imports-1.1">Necessary Imports</a></span></li><li><span><a href="#Get-data-for-the-analysis" data-toc-modified-id="Get-data-for-the-analysis-1.2">Get data for the analysis</a></span><ul class="toc-item"><li><span><a href="#Query-the-source-feature-layer" data-toc-modified-id="Query-the-source-feature-layer-1.2.1">Query the source feature layer</a></span></li><li><span><a href="#Query-the-reference-feature-layers" data-toc-modified-id="Query-the-reference-feature-layers-1.2.2">Query the reference feature layers</a></span></li></ul></li></ul></li><li><span><a href="#2.-Map-the-COVID-19-cases-in-China" data-toc-modified-id="2.-Map-the-COVID-19-cases-in-China-2">2. Map the COVID-19 cases in China</a></span><ul class="toc-item"><li><span><a href="#Map-the-confirmed-COVID-19-cases-in-China" data-toc-modified-id="Map-the-confirmed-COVID-19-cases-in-China-2.1">Map the confirmed COVID-19 cases in China</a></span><ul class="toc-item"><li><span><a href="#Display-confirmed-cases-in-China-as-points" data-toc-modified-id="Display-confirmed-cases-in-China-as-points-2.1.1">Display confirmed cases in China as points</a></span></li><li><span><a href="#Display-confirmed-cases-in-China-as-polygons" data-toc-modified-id="Display-confirmed-cases-in-China-as-polygons-2.1.2">Display confirmed cases in China as polygons</a></span></li></ul></li><li><span><a href="#Map-the-deaths-caused-by-COVID-19-in-China" data-toc-modified-id="Map-the-deaths-caused-by-COVID-19-in-China-2.2">Map the deaths caused by COVID-19 in China</a></span><ul class="toc-item"><li><span><a href="#Display-death-cases-in-China-as-points" data-toc-modified-id="Display-death-cases-in-China-as-points-2.2.1">Display death cases in China as points</a></span></li><li><span><a href="#Display-death-cases-in-China-as-polygons" data-toc-modified-id="Display-death-cases-in-China-as-polygons-2.2.2">Display death cases in China as polygons</a></span></li></ul></li><li><span><a href="#Map-the-recovered-COVID-19-cases-in-China" data-toc-modified-id="Map-the-recovered-COVID-19-cases-in-China-2.3">Map the recovered COVID-19 cases in China</a></span><ul class="toc-item"><li><span><a href="#Display-the-recovered-cases-in-China-as-points" data-toc-modified-id="Display-the-recovered-cases-in-China-as-points-2.3.1">Display the recovered cases in China as points</a></span></li><li><span><a href="#Display-the-recovered-cases-in-China-as-polygons" data-toc-modified-id="Display-the-recovered-cases-in-China-as-polygons-2.3.2">Display the recovered cases in China as polygons</a></span></li></ul></li></ul></li><li><span><a href="#3.-Map-the-COVID-19-cases-in-the-U.S." data-toc-modified-id="3.-Map-the-COVID-19-cases-in-the-U.S.-3">3. Map the COVID-19 cases in the U.S.</a></span><ul class="toc-item"><li><span><a href="#Map-the-confirmed-COVID-19-cases-in-the-U.-S." data-toc-modified-id="Map-the-confirmed-COVID-19-cases-in-the-U.-S.-3.1">Map the confirmed COVID-19 cases in the U. S.</a></span><ul class="toc-item"><li><span><a href="#Map-the-confirmed-cases-in-the-U.S.-as-points" data-toc-modified-id="Map-the-confirmed-cases-in-the-U.S.-as-points-3.1.1">Map the confirmed cases in the U.S. as points</a></span></li><li><span><a href="#Map--the-confirmed-cases-in-the-U.S.-as-polygons" data-toc-modified-id="Map--the-confirmed-cases-in-the-U.S.-as-polygons-3.1.2">Map  the confirmed cases in the U.S. as polygons</a></span></li></ul></li><li><span><a href="#Map-the-COVID-19-deaths-in-the-U.-S." data-toc-modified-id="Map-the-COVID-19-deaths-in-the-U.-S.-3.2">Map the COVID-19 deaths in the U. S.</a></span><ul class="toc-item"><li><span><a href="#Map-the-death-cases-in-the-U.S.-as-points" data-toc-modified-id="Map-the-death-cases-in-the-U.S.-as-points-3.2.1">Map the death cases in the U.S. as points</a></span></li><li><span><a href="#Map-the-death-cases-in-the-U.S.-as-polygons" data-toc-modified-id="Map-the-death-cases-in-the-U.S.-as-polygons-3.2.2">Map the death cases in the U.S. as polygons</a></span></li></ul></li></ul></li><li><span><a href="#4.-Map-the-COVID-19-cases-globally" data-toc-modified-id="4.-Map-the-COVID-19-cases-globally-4">4. Map the COVID-19 cases globally</a></span><ul class="toc-item"><li><span><a href="#Map-the-confirmed-COVID-19-cases-globally" data-toc-modified-id="Map-the-confirmed-COVID-19-cases-globally-4.1">Map the confirmed COVID-19 cases globally</a></span><ul class="toc-item"><li><span><a href="#Map-the-global-confirmed-cases-as-points" data-toc-modified-id="Map-the-global-confirmed-cases-as-points-4.1.1">Map the global confirmed cases as points</a></span></li><li><span><a href="#Map-the-global-confirmed-cases-as-polygons" data-toc-modified-id="Map-the-global-confirmed-cases-as-polygons-4.1.2">Map the global confirmed cases as polygons</a></span></li></ul></li></ul></li><li><span><a href="#5.-What's-next?" data-toc-modified-id="5.-What's-next?-5">5. What's next?</a></span></li><li><span><a href="#References" data-toc-modified-id="References-6">References</a></span></li></ul></div>

# According to <a href="https://www.who.int/emergencies/diseases/novel-coronavirus-2019">WHO</a>, 2019 Novel Corona Virus (**COVID-19**) is a virus (more specifically, a coronavirus) identified as the cause of an outbreak of respiratory illness, which was unknown before the outbreak began in Wuhan, China, in December 2019 <a href="#References">[1]</a>. Early on, the disease demonstrated an animal-to-person spread, then a person-to-person spread. Infections with COVID-19, were reported in a growing number of international locations, including the United States". The United States reported the first confirmed instance of person-to-person spread with this virus on January 30, 2020 <a href="#References">[2]</a>.
# 
# This notebook shows how to use the `ArcGIS API for Python` to monitor the spread of COVID-19 as it became a pandemic (updated as of June 28th, 2021).
# 
# ## 1. Import Data
# 
# Esri provides an open-to-public and free-to-share <a href="https://coronavirus-disasterresponse.hub.arcgis.com/datasets/bbb2e4f589ba40d692fab712ae37b9ac">feature layer</a> that contains the most up-to-date COVID-19 cases covering China, the United States, Canada, Australia (at province/state level), and the rest of the world (at country level, represented by either the country centroids or their capitals). Data sources are <a href="https://www.who.int/">WHO</a>, <a href="https://www.cdc.gov/">US CDC</a>, <a href="https://en.wikipedia.org/wiki/National_Health_Commission">China NHC</a>, <a href="https://www.ecdc.europa.eu/en">ECDC</a>, and <a href="http://www.dxy.cn/">DXY</a>. The China data is updated automatically at least once per hour, and non-China data is updating manually. The data source repo that this layer referenced from, is created and maintained by <a href="https://systems.jhu.edu/">the Center for Systems Science and Engineering (CSSE) at the Johns Hopkins University</a>, and can be viewed <a href="https://gisanddata.maps.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6">here</a>. In this notebook, we will use the feature layer supported by Esri Living Atlas team and JHU Data Services, and provide a different perspective in viewing the global maps of COVID-19 via the use of ArcGIS API for Python.

# **NOTE: "Since COVID-19 is continuously evolving, the sample reflects data as of June 28th, 2021 (or if you are looking at a previously published version of this notebook, the data and maps were fetched/made as of July 4th, 2020). Running this notebook at a later date might reflect a different result, but the overall steps should hold good."**

# __**DISCLAIMER: "This notebook is for the purpose of illustrating an analytical process using Jupyter Notebooks and ArcGIS API for Python, and should not be used as medical or epidemiological advice."**__

# ### Necessary Imports

# In[1]:


from io import BytesIO
import requests
import pandas as pd
from arcgis.features import FeatureLayer
from arcgis.gis import GIS
from arcgis.mapping import WebMap


# In[2]:


"""
# if you are using arcgis api for python with version 1.8.0 or above,
# make sure that the pandas version>=1.0,
# if not, use `pip install --upgrade pandas>=1` to upgrade.
"""
pd.__version__


# ### Get data for the analysis
# 
# #### Query the source feature layer
# 
# The dashboard item contributed to the public by `Esri` and `JHU CSSE`, is accessible through here:

# In[3]:


gis = GIS(profile="your_online_profile")


# In[8]:


item = gis.content.search("Coronavirus_2019_nCoV_Cases owner:CSSE_covid19", outside_org=True)[0]
item


# Through the `API Explorer` provided along with the dashboard product, we can easily fetch the source URL for the Feature Service containing daily updated COVID-19 statistics, which can then be used to create a `FeatureLayer` object good for querying and visualizing.

# In[9]:


src_url = "https://services1.arcgis.com/0MSEUqKaxRlEPj5g/arcgis/rest/services/Coronavirus_2019_nCoV_Cases/FeatureServer/1"
fl = FeatureLayer(url=src_url)


# In[10]:


df_global = fl.query(where="1=1",
                     return_geometry=True,
                     as_df=True)


# As stated in the dashboard, the source data can be grouped into:
#   - A. Countries or regions of which data are collected at province/state level, e.g. China, the United States, Canada, Australia; 
#   - B. Countries or regions for the rest of the world of which data collected at country level, and shape represented by either the country centroids or their capitals;
#   - C. Cruise Ships with confirmed COVID-19 cases.
#  
# ##### Group A
# 
# Let us first take a look at how many countries are within **group A**, that `Country_Region` and `Province_State` are not null or NAN.

# In[11]:


df_global[~pd.isnull(df_global['Province_State'])].groupby('Country_Region').sum(numeric_only=True)[['Confirmed', 'Recovered', 'Deaths']]


# Each country/region in Group A, has more than 1 feature, as what we have seen below from the query() results.

# In[12]:


fset_usa = fl.query(where="Country_Region='US'")
fset_usa


# In[13]:


fset_china = fl.query(where="Country_Region='China'")
fset_china


# In[14]:


fl.query(where="Country_Region='Denmark'")


# ##### Group C
# 
# Group C contains cruise ships across the globe with reported cases:

# In[11]:


df_cruise_ships = fl.query(where="Province_State='Diamond Princess' or \
                                  Province_State='Grand Princess' or \
                                  Country_Region='MS Zaandam' or \
                                  Country_Region='Diamond Princess'",
                           as_df=True)


# In[12]:


df_cruise_ships[["Province_State", "Country_Region", "Last_Update", "Confirmed", "Recovered", "Deaths"]]


# ##### Group B
# 
# In the `df_global`, other than the 22 countries (Australia, Canada, China, etc.) in Group A, and those cruise ships in Group C, all other countries/regions fall into Group B, e.g. Thailand. The major difference between Group A and Group B is that the latter contains one and only feature per country.

# In[17]:


fl.query(where="Country_Region='Thailand'")


# #### Query the reference feature layers
# 
# Because the geo-information provided by the dashboard contains only the coordinates representing the centroid of each country/region, the feature layer can only be rendered as points on Map. If this is what you want, you can now skip the rest of section 1, and jump right onto <a href="#2.-Map-the-COVID-19-cases-in-China">section 2</a>.
# 
# On the other hand, if you want to visualize the confirmed/death/recovered cases per country/region as polygons, in other words as a choropleth map, please read along:
# 
# First, we need to access the feature service that contains geometry/shape info for all provinces in Mainland China, and merge with the COVID-19 DataFrame.
# 
# ##### Access the reference feature layer of China

# In[18]:


provinces_item = gis.content.get("0f57da7f853c4a1aa5b2e048ff8655d2")
provinces_item


# In[19]:


provinces_flayer = provinces_item.layers[0]
provinces_df = provinces_flayer.query(as_df=True)
provinces_df.columns


# In[20]:


tmp = provinces_df.sort_values('NAME', ascending=True)
provinces_df = tmp.drop_duplicates(subset='NAME', keep='last')
provinces_df.shape


# ##### DataFrame Merging for China Dataset
# 
# The subsets of dataframe being created in the previous section now needs to be merged with feature services which have geographic information (e.g. geometries, shape, or longitude/latitude) in order to provide location and geometries required for geographic mapping. First, let's acquire the geometries from feature services existing on living atlas or arcgis online organization to represent the geographic information needed of `overlap_rows_china`.

# In[21]:


df_china = fset_china.sdf[['Province_State', 'Confirmed', 'Recovered', 'Deaths']]
df_china = df_china.assign(NAME = df_china["Province_State"])
df_china.head()


# Because the names are inconsistent between the two data sources (e.g. provinces represented differently in `df_china['Province_State']` and `provinces_df['NAME"]`), the `replace_value_in_column` method is declared below to  edit the records and unify the column names.

# In[22]:


def replace_value_in_column(data_frame, l_value, r_value, column_name = 'NAME'):
    data_frame.loc[data_frame[column_name] == l_value, column_name] = r_value

replace_value_in_column(df_china, 'Guangxi', 'Guangxi Zhuang Autonomous Region')
replace_value_in_column(df_china, 'Inner Mongolia', 'Inner Mongolia Autonomous Region')
replace_value_in_column(df_china, 'Ningxia', 'Ningxia Hui Autonomous Region')
replace_value_in_column(df_china, 'Tibet', 'Tibet Autonomous Region')


# Now the two DataFrame objects have got unified column names, we can go ahead to use a single function in `Pandas` called `merge` as an entry point to perform in-memory standard database join operations (similar to that of relational databases such as SQL), and its syntax is shown here -
# 
# ```
# pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None,
#          left_index=False, right_index=False, sort=True)
# ```
# 
# Note that, the `how` argument specifies how to merge (a.k.a. how to determine which keys are to be included in the resulting table). If a key combination does not appear in either the left or the right tables, the values in the joined table will be NA. The table below then shows a summary of the how options and their SQL equivalent names −
# 
# | Merge Method | SQL Equivalent | Description |
# |------------- |----------------|-------------|
# | left         | LEFT OUTER JOIN| Use keys from left object|
# | right        |RIGHT OUTER JOIN| Use keys from right object|
# | outer        | FULL OUTER JOIN| Use union of keys|
# | inner        | INNER JOIN     | Use intersection of keys|
# 
# In this case, we will be calling `merge()` with `how='inner'` to perform the inner join of the two DataFrame objects on the index field `"NAME"` and only to keep the intersection of keys.

# In[23]:


cols_2 = ['NAME', 'AREA', 'TOTPOP_CY','SHAPE']
overlap_rows_china = pd.merge(left = provinces_df[cols_2], right = df_china, 
                              how='inner', on = 'NAME')
overlap_rows_china.head()


# In[24]:


cols_2 = ['NAME', 'AREA', 'TOTPOP_CY','SHAPE','Shape__Area', 'Shape__Length']
overlap_rows_china = pd.merge(left = provinces_df[cols_2], right = df_china, how='inner',
                        on = 'NAME')
overlap_rows_china.head()


# As shown in `overlap_rows_china`, each province/state in China is now merged while retaining the `SHAPE` column, and is ready to be rendered as polygons.

# ##### Access the reference feature layer of the United States
# 
# Next, we need to access the feature service that contains geometry/shape info for all states in the U. S., and merge with the DataFrame depicting COVID-19 statistics.

# In[25]:


us_states_item = gis.content.get('99fd67933e754a1181cc755146be21ca')
us_states_item


# In[26]:


us_states_flayer = us_states_item.layers[0]
us_states_df = us_states_flayer.query(as_df=True)
us_states_df.columns


# ##### DataFrame Merging for U.S. Dataset

# In[27]:


df_usa = fset_usa.sdf[['Province_State', 'Confirmed', 'Recovered', 'Deaths']]
df_usa = df_usa.assign(STATE_NAME = df_usa["Province_State"])
df_usa.head()


# In[28]:


cols_4 = ['STATE_NAME','SHAPE']
overlap_rows_usa = pd.merge(left = us_states_df[cols_4], right = df_usa, 
                            how='inner', on = 'STATE_NAME')
overlap_rows_usa.head()


# ##### Access the reference feature layer of world countries

# In[29]:


countries_item = gis.content.get('2b93b06dc0dc4e809d3c8db5cb96ba69')
countries_item


# In[30]:


countries_flayer = countries_item.layers[0]
countries_df = countries_flayer.query(as_df=True)
countries_df.columns


# ##### DataFrame Merging for global Dataset

# The `df_global` has listed its `Country_Region` column with their current best-known names in English, while the `countries_df` uses their currently best-known equivalents, and this difference in naming countries has created a problem for the `merge()` operation to understand if the two countries listed in two DataFrame objects are the same. We need to hence run the following cell in order to make country names consistent between the two DataFrames to be merged.

# In[31]:


df_global.loc[df_global['Country_Region']=='US', 'Country_Region'] = 'United States'
df_global.loc[df_global['Country_Region']=='Korea, South', 'Country_Region'] = 'South Korea'
df_global.loc[df_global['Country_Region']=='Korea, North', 'Country_Region'] = 'North Korea'
df_global.loc[df_global['Country_Region']=='Russia', 'Country_Region'] = 'Russian Federation'
df_global.loc[df_global['Country_Region']=='Czechia', 'Country_Region'] = 'Czech Republic'


# ###### List the top 10 countries with largest numbers
# 
# With `df_global` ready, we can now sort countries or regions by their numbers of confirmed/recovered/death cases, through usage of `groupby()`, and `sort_values()`.

# In[32]:


# sorted by # of confirmed cases
df_global_sum = df_global.groupby('Country_Region').sum(numeric_only=True)[['Confirmed', 'Recovered', 'Deaths']]
df_global_sum_c = df_global_sum.sort_values(by = ['Confirmed'], ascending = False)
df_global_sum_c.head(10)


# In[33]:


# sorted by death tolls
df_global_sum_d = df_global_sum.sort_values(by = ['Deaths'], ascending = False)
df_global_sum_d.head(10)


# ###### Joining the COVID-19 stats and world countries DataFrames

# In[34]:


world_merged1 = pd.merge(df_global_sum_c, countries_df[['COUNTRY', 'SHAPE']], 
                         left_index=True, right_on='COUNTRY',
                         how="left")
world_merged1[['COUNTRY', 'Confirmed','Deaths', 'Recovered']].head(10)


# Now, each country/region in `world_merged1` is now merged with the `SHAPE` column, and is ready to be rendered as polygons.

# ## 2. Map the COVID-19 cases in China
# 
# Next, let us start visualizing the following scenarios targeting at China:
#   - Confirmed cases rendered as points, and polygons
#   - Death cases rendered as points, and polygons
#   - Recovered cases rendered as points, and polygons

# ### Map the confirmed COVID-19 cases in China
# 
# We can either call the `add_layer()` function to add the specified layer or item (i.e. the FeatureLayer object created as `fl`) to the map widget, and set the visualization options to be using `ClassedSizeRenderer`, or plot the derived SeDF on the map view directly with further descriptions such as <a href="https://developers.arcgis.com/python/guide/visualizing-data-with-the-spatially-enabled-dataframe/
# ">how to renderer spatial data using symbol and color palette</a> (Here, the SeDF is the derivative of the merged DataFrame, which now contains a SHAPE column that we can use to plot in the Map widget as polygons).
# 
# #### Display confirmed cases in China as points

# In[30]:


map1 = gis.map('China', zoomlevel=4)
map1


# In[28]:


map1.add_layer(fl,   { "type": "FeatureLayer",
                       "renderer":"ClassedSizeRenderer",
                       "field_name":"Confirmed"})


# In[29]:


map1.zoom = 3
map1.legend = True


# #### Display confirmed cases in China as polygons

# In[26]:


map1b = gis.map('China')
map1b


# In[44]:


map1b.clear_graphics()
overlap_rows_china.spatial.plot(  kind='map', map_widget=map1b,
                                  renderer_type='c',  # for class breaks renderer
                                  method='esriClassifyNaturalBreaks',  # classification algorithm
                                  class_count=4,  # choose the number of classes
                                  col='Confirmed',  # numeric column to classify
                                  cmap='inferno',  # color map to pick colors from for each class
                                  alpha=0.7  # specify opacity
                                 )


# In[45]:


map1b.zoom = 4
map1b.legend=True


# The Map view above (`map1b`) displays the number of confirmed cases per province in Mainland China. Orange polygons refer to provinces with number of confirmed cases in the range of [45423, 68134], and black polygons represent those in the range of [1, 22712].
# 
# Also, we can save the MapView object into a Web Map item for the purpose of future references and modifications.

# In[46]:


map1b.save({'title':'Confirmed COVID-19 Cases in China',
            'snippet':'Map created using Python API showing confirmed COVID-19 cases in China',
            'tags':['automation', 'COVID19', 'world health', 'python']})


# For example, we can browse the web map in the browser, change its symbology to different color maps in the configuration pane, then visualize it again with different looks here.

# In[51]:


map1b_item = gis.content.search('Confirmed COVID-19 Cases in China')[0]
WebMap(map1b_item)


# ### Map the deaths caused by COVID-19 in China
# 
# #### Display death cases in China as points

# In[34]:


map2 = gis.map('China', zoomlevel=4)
map2


# In[32]:


map2.add_layer(fl,   { "type": "FeatureLayer",
                       "renderer":"ClassedSizeRenderer",
                       "field_name":"Deaths"})


# In[33]:


map2.legend = True


# #### Display death cases in China as polygons

# In[52]:


map2b = gis.map('China')
map2b


# In[40]:


map2b = gis.map('China')
map2b


# In[53]:


map2b.clear_graphics()
overlap_rows_china.spatial.plot(  kind='map', map_widget=map2b,
                                  renderer_type='c',  # for class breaks renderer
                                  method='esriClassifyNaturalBreaks',  # classification algorithm
                                  class_count=4,  # choose the number of classes
                                  col='Deaths',  # numeric column to classify
                                  cmap='inferno',  # color map to pick colors from for each class
                                  alpha=0.7  # specify opacity
                                 )


# In[54]:


map2b.zoom = 4
map2b.legend = True


# Using the same approach, we can then map the number of death cases per province in Mainland China. With legend displayed, `map2b` shows us orange polygons refer to provinces with number of death cases in the range of [3008, 4512], and black polygons represent those in the range of [0, 1504].
# 
# Similarly, we can create an additional deliverable - the Web Map Item created on the active `GIS` - and then browse the web map in the browser, change its symbology to different color maps in the configuration pane, and/or visualize it again with different looks here.

# In[55]:


map2b_item = map2b.save({'title':'COVID-19 Death Cases in China',
                         'snippet':'Map created using Python API showing COVID-19 death cases in China',
                         'tags':['automation', 'COVID19', 'world health', 'python']})


# In[58]:


WebMap(map2b_item)


# ### Map the recovered COVID-19 cases in China
# 
# #### Display the recovered cases in China as points

# In[44]:


map3 = gis.map('China', zoomlevel=4)
map3


# In[42]:


map3.add_layer(fl,   { "type": "FeatureLayer",
                       "renderer":"ClassedSizeRenderer",
                       "field_name":"Recovered"})


# In[43]:


map3.legend = True


# #### Display the recovered cases in China as polygons

# In[48]:


map3b = gis.map('China')
map3b


# In[60]:


map3b.clear_graphics()
overlap_rows_china.spatial.plot(  kind='map', map_widget=map3b,
                                  renderer_type='c',  # for class breaks renderer
                                  method='esriClassifyNaturalBreaks',  # classification algorithm
                                  class_count=4,  # choose the number of classes
                                  col='Recovered',  # numeric column to classify
                                  cmap='inferno',  # color map to pick colors from for each class
                                  alpha=0.7  # specify opacity
                                 )


# In[61]:


map3b.zoom = 4
map3b.legend = True


# Same here in `map3b`, with legend displayed, we can tell orange polygons refer to provinces with number of recovered cases in the range of [42411, 63616], and black polygons represent those in the range of [1, 21206].
# 
# Based on the sets of maps plotted above, the Hubei province has topped all provinces in China no matter when we are looking at the confirmed cases, or the recovered/death tolls.
# 
# Again as how we create an additional deliverable - the Web Map Item - on the active GIS in previous two sections, we can then browse the web map in the browser, change its symbology to different color maps in the configuration pane, and/or visualize it again with different looks.

# In[62]:


map3b_item = map3b.save({'title':'COVID-19 Recovered Cases in China',
                         'snippet':'Map created using Python API showing COVID-19 recovered cases in China',
                         'tags':['automation', 'COVID19', 'world health', 'python']})


# In[64]:


WebMap(map3b_item)


# ## 3. Map the COVID-19 cases in the U.S.
# 
# Similar to how we map the scenarios in China, visualizing these scenarios targeting at the United States can be divided into these sub-sections:
#   - Confirmed cases rendered as points, and polygons
#   - Death cases rendered as points, and polygons

# ### Map the confirmed COVID-19 cases in the U. S.
# 
# There are two ways to map the COVID-19 cases:
#   - call the `add_layer()` function to add the specified layer or item (i.e. the FeatureLayer object created as `fl`) to the map widget, and set the visualization options to be using `ClassedSizeRenderer`, 
#   - plot the derived SeDF on the map view directly with further descriptions such as how to renderer spatial data using symbol and color palette (Here, the SeDF is the derivative of the merged DataFrame, which now contains a SHAPE column that we can use to plot in the Map widget as polygons).
#   
# We are going to show how the confirmed cases in the U.S. look like in both ways:

# #### Map the confirmed cases in the U.S. as points

# In[41]:


map4 = gis.map("US", zoomlevel=4)
map4


# In[36]:


map4.add_layer(fl, { "type": "FeatureLayer",
                     "renderer":"ClassedSizeRenderer",
                     "field_name":"Confirmed"})


# In[37]:


map4.legend = True


# #### Map  the confirmed cases in the U.S. as polygons

# In[46]:


map4a = gis.map("US")
map4a


# In[43]:


map4a.clear_graphics()
overlap_rows_usa.spatial.plot( kind='map', map_widget=map4a,
                               renderer_type='c',  # for class breaks renderer
                               class_count=4,  # choose the number of classes
                               method='esriClassifyEqualInterval',  # classification algorithm
                               col='Confirmed',  # numeric column to classify
                               cmap='inferno',  # color map to pick colors from for each class
                               alpha=0.7  # specify opacity
                              )


# In[44]:


map4a.zoom = 4
map4a.legend = True


# ### Map the COVID-19 deaths in the U. S.

# #### Map the death cases in the U.S. as points

# In[50]:


map4b = gis.map("US")
map4b


# In[48]:


map4b.add_layer(fl, { "type": "FeatureLayer",
                     "renderer":"ClassedSizeRenderer",
                     "field_name":"Deaths"})


# In[49]:


map4b.zoom=4
map4b.legend=True


# #### Map the death cases in the U.S. as polygons

# In[54]:


map4c = gis.map("US")
map4c


# In[52]:


map4c.clear_graphics()
overlap_rows_usa.spatial.plot( kind='map', map_widget=map4c,
                               renderer_type='c',  # for class breaks renderer
                               class_count=4,  # choose the number of classes
                               method='esriClassifyEqualInterval',  # classification algorithm
                               col='Deaths',  # numeric column to classify
                               cmap='inferno',  # color map to pick colors from for each class
                               alpha=0.7  # specify opacity
                              )


# In[53]:


map4c.zoom = 4
map4c.legend = True


# ## 4. Map the COVID-19 cases globally

# Now we have learned how to map COVID-19 cases in China, and the United States, let's take a look at how to map across the globe. For illustration purpose, the rest of this section will only talk about mapping the confirmed cases globally, yet the workflow is almost the same to map deaths and recovered cases globally.
# 
# ### Map the confirmed COVID-19 cases globally
# 
# We are going to walk through how the confirmed cases across the globe can be mapped in the Map widget: 
#   - call the `add_layer()` function to add the specified layer or item (i.e. the FeatureLayer object created as `fl`) to the map widget, and set the visualization options to be using `ClassedSizeRenderer`, 
#   - plot the derived SeDF on the map view directly with further descriptions such as how to renderer spatial data using symbol and color palette (Here, the SeDF is the derivative of the merged DataFrame, which now contains a SHAPE column that we can use to plot in the Map widget as polygons).

# #### Map the global confirmed cases as points

# In[61]:


map5a = gis.map("Italy")
map5a


# In[59]:


map5a.add_layer(fl, { "type": "FeatureLayer",
                     "renderer":"ClassedSizeRenderer",
                     "field_name":"Confirmed"})


# In[60]:


map5a.legend=True
map5a.zoom = 4


# #### Map the global confirmed cases as polygons

# In[65]:


map5b = gis.map("Italy")
map5b


# In[63]:


world_merged1.spatial.plot(  kind='map', map_widget=map5b,
                             renderer_type='c',  # for class breaks renderer
                             method='esriClassifyStandardDeviation',  # classification algorithm
                             class_count=7,  # choose the number of classes
                             col='Confirmed',  # numeric column to classify
                             cmap='inferno',  # color map to pick colors from for each class
                             alpha=0.7  # specify opacity
                            )


# In[64]:


map5b.zoom = 4
map5b.legend = True


# When `map5b` is created centering Brazil, the legend added along shows that yellow polygons (the 2nd class in the colormap) indicate that the countries are of number of confirmed cases ranging within (28026277, 33631532], and purple polygons represent those ranging between (5605255, 11210511].

# ## 5. What's next?
# 
# This notebook demonstrates accessing the corona viruses statistics, tabularizing them in DataFrames, and finally mapping the outbreak in different regions or counties. Next step, you can refer to <a href="./covid19_part2_timeseries_analysis.ipynb">Part 2 notebook</a> for analyzing and charting time-series of the 2019 Novel Coronavirus.

# ## References
# 
# [1] https://www.cdc.gov/coronavirus/2019-ncov/about/index.html
# 
# [2] https://www.cdc.gov/coronavirus/2019-nCoV/


# ====================
# covid19_part2_timeseries_analysis.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Time Series Analysis of the 2019 Novel Coronavirus Pandemic

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#1.-Import-Data" data-toc-modified-id="1.-Import-Data-1">1. Import Data</a></span><ul class="toc-item"><li><span><a href="#Necessary-Imports" data-toc-modified-id="Necessary-Imports-1.1">Necessary Imports</a></span></li><li><span><a href="#Read-Google-Sheet-into-DataFrame" data-toc-modified-id="Read-Google-Sheet-into-DataFrame-1.2">Read Google Sheet into DataFrame</a></span><ul class="toc-item"><li><span><a href="#Global-Dataset" data-toc-modified-id="Global-Dataset-1.2.1">Global Dataset</a></span></li><li><span><a href="#U.S.-Dataset" data-toc-modified-id="U.S.-Dataset-1.2.2">U.S. Dataset</a></span></li></ul></li><li><span><a href="#Analysis" data-toc-modified-id="Analysis-1.3">Analysis</a></span><ul class="toc-item"><li><span><a href="#Confirmed-Cases" data-toc-modified-id="Confirmed-Cases-1.3.1">Confirmed Cases</a></span></li><li><span><a href="#Recovered-Cases" data-toc-modified-id="Recovered-Cases-1.3.2">Recovered Cases</a></span></li><li><span><a href="#Death-Cases" data-toc-modified-id="Death-Cases-1.3.3">Death Cases</a></span></li></ul></li></ul></li><li><span><a href="#2.-Parse-the-Time-Series-Data-(Mainland-China)" data-toc-modified-id="2.-Parse-the-Time-Series-Data-(Mainland-China)-2">2. Parse the Time-Series Data (Mainland China)</a></span><ul class="toc-item"><li><span><a href="#Time-Series-of-Confirmed-Cases-in-Mainland-China" data-toc-modified-id="Time-Series-of-Confirmed-Cases-in-Mainland-China-2.1">Time-Series of Confirmed Cases in Mainland China</a></span></li><li><span><a href="#The-Recovered,-and-Death-cases-in-Mainland-China" data-toc-modified-id="The-Recovered,-and-Death-cases-in-Mainland-China-2.2">The Recovered, and Death cases in Mainland China</a></span></li><li><span><a href="#Cross-Comparisons-within-different-categories" data-toc-modified-id="Cross-Comparisons-within-different-categories-2.3">Cross-Comparisons within different categories</a></span></li><li><span><a href="#Cross-comparisons-within-provinces" data-toc-modified-id="Cross-comparisons-within-provinces-2.4">Cross-comparisons within provinces</a></span></li><li><span><a href="#Identify-periodicities-&amp;-similarities" data-toc-modified-id="Identify-periodicities-&amp;-similarities-2.5">Identify periodicities &amp; similarities</a></span></li></ul></li><li><span><a href="#3.-Parse-the-time-series-data-(in-the-U.-S.,-Australia-&amp;-Canada)" data-toc-modified-id="3.-Parse-the-time-series-data-(in-the-U.-S.,-Australia-&amp;-Canada)-3">3. Parse the time-series data (in the U. S., Australia &amp; Canada)</a></span><ul class="toc-item"><li><span><a href="#Confirmed-Cases-in-Australia-&amp;-Canada" data-toc-modified-id="Confirmed-Cases-in-Australia-&amp;-Canada-3.1">Confirmed Cases in Australia &amp; Canada</a></span></li><li><span><a href="#Confirmed-Cases-in-the-U.S." data-toc-modified-id="Confirmed-Cases-in-the-U.S.-3.2">Confirmed Cases in the U.S.</a></span></li><li><span><a href="#Confirmed-Cases-on-Cruise-Ships" data-toc-modified-id="Confirmed-Cases-on-Cruise-Ships-3.3">Confirmed Cases on Cruise Ships</a></span></li></ul></li><li><span><a href="#4.-Time-Series-Lag-Scatter-Plots" data-toc-modified-id="4.-Time-Series-Lag-Scatter-Plots-4">4. Time Series Lag Scatter Plots</a></span></li><li><span><a href="#5.-Time-Series-Autocorrelation-Plots" data-toc-modified-id="5.-Time-Series-Autocorrelation-Plots-5">5. Time Series Autocorrelation Plots</a></span></li><li><span><a href="#Conclusions" data-toc-modified-id="Conclusions-6">Conclusions</a></span></li><li><span><a href="#References" data-toc-modified-id="References-7">References</a></span></li></ul></div>

# This notebook is to perform analysis and time series charting of 2019 novel coronavirus disease (COVID-19) globally (updated as of June 28th, 2021):
# 
# ## 1. Import Data
# 
# The data source repo to be used, is created and maintained by the <a href="https://systems.jhu.edu/">the Center for Systems Science and Engineering (CSSE) at the Johns Hopkins University</a>, and the official maps can be viewed <a href="https://gisanddata.maps.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6">here</a>. If you are having an issue accessing the Google Sheet, please try downloading the data source provided on their <a href="https://github.com/CSSEGISandData/2019-nCoV">GitHub repo</a>.
# 
# The csv file has three types of cases - `Confirmed`, `Recovered` and `Deaths` - spotted inside and outside Mainland China, across the time span from 1/21/2020 to Current.

# ### Necessary Imports

# In[1]:


from io import BytesIO
import requests
import pandas as pd
import datetime as dt
import matplotlib.pyplot as plt


# ### Read Google Sheet into DataFrame
# 
# #### Global Dataset
# 
# First, read the three tabs on Google Sheet - namely `'confirmed'`, `'death'`, `'recovered'` - into three individual DataFrames, and append them one after another into an empty list.

# In[2]:


cases = ['confirmed', 'deaths', 'recovered']
sheet = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_'
suffix = '_global.csv'
df_list = []

for i in range(len(cases)):
    ts_url = sheet + cases[i] + suffix
    df = pd.read_csv(ts_url, header=0, escapechar='\\')
    display(df.head(3))
    df_list.append(df)
    exec("{0}=df".format(cases[i]))


# In[3]:


# shape of matrices for confirmed, death, and recovered
df_list[0].shape, df_list[1].shape, df_list[2].shape


# #### U.S. Dataset

# In[4]:


cases = ['confirmed', 'deaths']
sheet = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_'
suffix = '_US.csv'
us_df_list = []

for i in range(len(cases)):
    us_ts_url = sheet + cases[i] + suffix
    df = pd.read_csv(us_ts_url, header=0, escapechar='\\')
    display(df.head(3))
    us_df_list.append(df)
    exec("{0}=df".format(cases[i]))


# In[5]:


# shape of matrices for confirmed, death, and recovered
us_df_list[0].shape, us_df_list[1].shape


# ##### Repair and summarize the U.S. Data
# 
# In the U.S. Dataset, there could be one or more administrative regions per state, and in order to summarize and simplify the dataset, the following function `sum_all_admins_in_state` is to be declared and used to sum all `Admin` inside one state into a single record.

# In[5]:


def sum_all_admins_in_state(df, state):
    
    # query all sub-records of the selected country
    tmp_df = df[df["Province_State"]==state]
    
    # create a new row which is to sum all statistics of this country, and 
    # assign the summed value of all sub-records to the date_time column of the new row
    sum_row = tmp_df.sum(axis=0)
    
    # assign the constants to the ['Province/State', 'Country/Region', 'Lat', 'Long'] columns; 
    # note that the Country/Region column will be renamed from solely the country name to country name + ", Sum".
    sum_row.loc['UID'] = "NaN"
    sum_row.loc['Admin2'] = "NaN"
    sum_row.loc['FIPS'] = "NaN"
    sum_row.loc['iso2'] = "US"
    sum_row.loc['iso3'] = "USA"
    sum_row.loc['code3'] = 840
    sum_row.loc['Country_Region'] = "US"
    sum_row.loc['Province_State'] = state + ", Sum"
    sum_row.loc['Lat'] = tmp_df['Lat'].values[0]
    sum_row.loc['Long_'] = tmp_df['Long_'].values[0]
    
    # append the new row to the original DataFrame, and 
    # remove the sub-records of the selected country.
    df = pd.concat([df, sum_row.to_frame().T], ignore_index=True)
    #display(df[df["Province_State"].str.contains(state + ", Sum")])
    df=df[df['Province_State'] != state]
    df.loc[df.Province_State == state+", Sum", 'Province_State'] = state
    
    return df


# In[6]:


for i in range(2):
    usa_ts_df=us_df_list[i]
    for state in usa_ts_df.Province_State.unique():
        usa_ts_df = sum_all_admins_in_state(usa_ts_df, state)
    us_df_list[i]=usa_ts_df


# ### Analysis
# 
# Now, let's proceed to these three categories (`'confirmed'`, `'death'`, `'recovered'`) individually.
# 
# #### Confirmed Cases

# As shown below, of the 279 rows of records in the first DataFrame being converted and parsed from Google Sheet, 34 rows are for cases reported for each province in Mainland China.

# In[7]:


df_confirmed = df_list[0]
print(df_confirmed[df_confirmed['Country/Region'] == 'China'].shape)
df_confirmed[df_confirmed['Country/Region'] == 'China'].head()


# While some countries/regions like China have been displayed with multiple rows each representing confirmed cases per province with reported cases, others are listed in the DataFrame as a single row that sums confirmed cases across states/provinces, such as the United States shown below:

# In[8]:


df_confirmed_usa = df_confirmed[df_confirmed['Country/Region'] == 'US']
print(df_confirmed_usa.shape)
df_confirmed_usa.head()


# #### Recovered Cases
# 
# Same here, for the purpose of comparison, we would need to get the figures of the other two categories - `recovered` and `deaths` besides `confirmed` cases.

# In[9]:


df_recovered = df_list[2]
df_recovered.tail(3)


# #### Death Cases

# In[10]:


df_death = df_list[1]
df_death.tail(3)


# As we can tell from the snapshots of these dataframes, cases are reported in three geographic administrative units: 
#   1. for countries/regions with significant numbers of confirmed/deaths/recovered cases (e.g. Mainland China), number of cases are reported per province/state; 
#   2. for other regions/countries, number of cases are summarized per region/country (e.g. Australia, or Canada); 
#   3. Also, the global DataFrame lists `Cruise Ship` since the Diamond Princess and several other cruise ships themselves contain a considerable amount of confirmed/deaths cases.

# ## 2. Parse the Time-Series Data (Mainland China)
# 
# Now we have obtained DataFrame for each type of coronavirus cases across the globe. We will apply a filter on each of the DataFrames, and analyze the Confirmed, Recovered, and Death cases separately, to see how the time-series evolved inside Mainland China.
# 
# ### Time-Series of Confirmed Cases in Mainland China
# 
# First, a list called `provinces_list` needs to be extracted from the selected rows, and then be concatenated with the category (e.g. `_Confirmed`), in order to differentiate from the other two categories (e.g. `_Recovered` and `_Deaths`).

# In[11]:


provinces_list = df_confirmed[df_confirmed['Country/Region'] == 'China'].iloc[:,0:1].T.values.tolist()[0]


# In[12]:


map_output = map(lambda x: x + '_Confirmed', provinces_list)
list_map_output = list(map_output)


# Next, let's remove the first five rows from the DataFrame `df` (which are the row#, Province/State, Country/Region, Unnamed:2, and Unnamed:3 columns, and are not needed for time-series charting), specify the index to the matrix, and perform a `Transpose` to have the `date_time` index shown as row indices.

# In[19]:


df0 = df_confirmed[df_confirmed['Country/Region'] == 'China'].iloc[:,5:].fillna(0)
df0.index = pd.Index(list_map_output, name='date_time')
df0 = df0.T
df0.tail(3)


# Also, we would need to standardize the date_time string (esp. that the year should be represented as XXXX instead of XX), and then to convert it from a string type to a datetime type:

# In[20]:


df0.index


# In[21]:


df0.index = pd.to_datetime(df0.index, format='%m/%d/%y', exact = False)


# If the datetime conversion is successful, use the following cell to validate and check how many rows of datetime records are in the dataframe.

# In[22]:


print("Dataframe shape: ", df0.shape)
time_diff = (df0.index[-1] - df0.index[0])
print("Number of hours between start and end dates: ", time_diff.total_seconds()/3600 + 1)


# The following will achieve three different plots:
# 1. Plotting all the time series on one axis (line-plot)
# 2. Plotting them all on separate subplots to see them more clearly (sharing the x axis)
# 3. Plotting all the time series on one axis (scatterplot)

# In[24]:


df0.plot(figsize=(15,10.5), title='Plotting all the time series on one axis (line-plot)').legend(loc='upper left')
plt.xlabel('Date Time'); text = plt.ylabel('Num of Cases')


# In[34]:


ax_array = df0.plot(subplots=True, figsize=(15,18))
for ax in ax_array:
    ax.legend(loc='upper left')
plt.xlabel('Date Time'); plt.ylabel('Num of Cases')
text = plt.title('Plotting all time-series on separate subplots (sharing the x axis)', pad="-120", y=2.0, loc="center")


# In[48]:


df0.plot(y=list_map_output, linestyle=':', linewidth=4, 
         figsize=(15,10.5), grid=True,
         title="Plotting all time series on one axis (scatterplot)").legend(loc='upper left')
plt.xlabel('Date Time'); text = plt.ylabel('Num of Cases')


# From the three plots shown above, we can tell that within Mainland China, Hubei province has the largest number of confirmed COVID-19 cases, preceded by Guangdong province, and the city of Shanghai.

# ### The Recovered, and Death cases in Mainland China
# 
# Now the confirmed COVID-19 cases for each province in mainland China are shown as above, we are to define a function `plot_per_country` that is to help plot other countries/regions, not only for the `confirmed` cases, but also other cases (`Recovered` or `Deaths`).

# In[59]:


def plot_per_country(df, country_name, category = "Confirmed", ref_df = df0):
    """to help us plot other countries/regions, not only for the confirmed cases, 
    but also other cases (Recovered or Deaths).
    """
    if 'Country/Region' in df.columns:
        provinces_list = df[df['Country/Region'] == country_name].iloc[:,0:1].T.values.tolist()[0]
    else:
        provinces_list = df[df['Country_Region'] == country_name].iloc[:,6:7].T.values.tolist()[0]
        
    map_output = map(lambda x: x + '_' + category, provinces_list)
    list_map_output = list(map_output)

    if 'Country/Region' in df.columns:
        df0 = df[df['Country/Region'] == country_name].iloc[:,5:].fillna(0)
    else:
        df0 = df[df['Country_Region'] == country_name].iloc[:,11:].fillna(0)
    
    df0.index = pd.Index(list_map_output, name='date_time')
    df0 = df0.loc[:, ~df0.columns.str.contains('^Unnamed')]
    df0 = df0.T
    df0.index = pd.to_datetime(df0.index, format='%m/%d/%y', exact = False)
    
    width_multiplier = df0.shape[1]/5

    df0.plot(figsize=(15,2*width_multiplier), 
             title='Plotting all the time series on one axis (line-plot)').legend(loc='upper left')
    plt.xlabel('Date Time'); plt.ylabel('Num of Cases')
    
    ax_array = df0.plot(subplots=True, figsize=(15,3*width_multiplier))
    for ax in ax_array:
        ax.legend(loc='upper left')
    plt.xlabel('Date Time'); plt.ylabel('Num of Cases')
    text = plt.title('Plotting all time-series on separate subplots (sharing the x axis)', pad="-120",
                     y=2.0, loc="center")
    
    df0.plot(y=list_map_output, linestyle=':', linewidth=4, 
             grid=True, figsize=(15,2*width_multiplier),
              title="Plotting all time series on one axis (scatterplot)").legend(loc='upper left')
    plt.xlabel('Date Time'); plt.ylabel('Num of Cases')
    
    return df0


# In[60]:


df_recovered_china = plot_per_country(df_recovered, "China", "Recovered")


# The three plots shown above indicate that Hubei province also has the largest number of recovered cases from COVID-19, preceded by Guangdong province, and the city of Shanghai.

# In[61]:


df_death_china = plot_per_country(df_death, "China", "Death")


# Unfortunately, among all provinces of Mainland China, Hubei again has the largest number of deaths caused by COVID-19.

# ### Cross-Comparisons within different categories
# 
# Now we have seen the confirmed, recovered, and death cases caused by COVID-19 for each province in mainland China, let's perform a cross comparison within these three categories by viewing the three time-series in a single plot per single province.
# 
# First, make sure these three DataFrames are of the same length (in time-series).

# In[62]:


df0.shape, df_recovered_china.shape, df_death_china.shape


# In[67]:


def cross_compare_per_province(province="Hubei"):
    """ Used to plot the time-series of the confirmed, recovered and death 
    cases per province;
        Input: string for province name
        Output: provides three plots of the cross comparison per province
    """
    key0 = province + '_Confirmed'
    key1 = province + '_Recovered'
    key2 = province + '_Death'
    
    df_l = df0.loc[~df0.index.duplicated(keep='first')]
    df_m = df_recovered_china.loc[~df_recovered_china.index.duplicated(keep='first')]
    df_r = df_death_china.loc[~df_death_china.index.duplicated(keep='first')]
    
    df_all_china = pd.concat([df_l[key0], df_m[key1], df_r[key2]], axis=1)
    df_all_china.plot(figsize=(15,2))
    plt.xlabel('Date Time'); plt.ylabel('Num of Cases')
    
    df_all_china.plot(subplots=True, figsize=(15,3))
    plt.xlabel('Date Time'); plt.ylabel('Num of Cases')
    
    df_all_china.plot(y=[key0, key1, key2], style='.', figsize=(15,2), linestyle=":")
    plt.xlabel('Date Time'); plt.ylabel('Num of Cases')
    
    return df_all_china


# In[68]:


df_hubei = cross_compare_per_province()


# In order to help understand the development trend of COVID-19, we need to obtain the `Recovered_Rate` and `Death_Rate` per state/province/region/country, and decide when is the turning point for Recovered_Rate to have surpassed Death_Rate.
# ```
# Recovered_Rate = (# of Recovered Cases)/(# of Confirmed Cases)
# Death_Rate = (# of Death Cases)/(# of Confirmed Cases)
# ```
# Function `calc_rate_per_province` is defined to plot the trends of these two rates:

# In[69]:


def calc_rate_per_province(province="Hubei"):
    """ Used to plot the time-series of the recovered and death 
    rates per province;
        Input: string for province name
        Output: provides three plots of the cross comparison per province
    """
    key0 = province + '_Confirmed'
    key1 = province + '_Recovered'
    key2 = province + '_Death'
    
    df_l = df0.loc[~df0.index.duplicated(keep='first')]
    df_m = df_recovered_china.loc[~df_recovered_china.index.duplicated(keep='first')]
    df_r = df_death_china.loc[~df_death_china.index.duplicated(keep='first')]

    df_all_china = pd.concat([(df_m[key1]*1.0).div((df_l[key0]*1.0), axis='index'),
                              (df_r[key2]*1.0).div((df_l[key0]*1.0), axis='index')],
                             keys=[key1 + '_Rate', key2 + '_Rate'],
                             axis=1)
    
    df_all_china.plot(figsize=(15,2))
    plt.xlabel('Date Time'); plt.ylabel('% of Confirmed Cases')
    
    df_all_china.plot(subplots=True, figsize=(15,3))
    plt.xlabel('Date Time'); plt.ylabel('% of Confirmed Cases')
    
    df_all_china.plot(style='.', figsize=(15,2), linestyle=":")   
    plt.xlabel('Date Time'); plt.ylabel('% of Confirmed Cases')


# In[70]:


calc_rate_per_province()


# We can tell from the plots above, the `Recovered Rate` has since surpassed the `Death Rate` for Hubei Province after 02/05/2020.

# ### Cross-comparisons within provinces
# 
# After looking at the different categories per province, let us now view the various provinces in one chart to see the variance from a different dimension. To start with, we pick the three provinces in China that are with largest numbers of confirmed COVID-19 cases.

# In[71]:


df1 = df_confirmed[df_confirmed['Province/State'].isin(['Hubei', 'Guangdong', 'Shanghai'])].iloc[:,5:].fillna(0)
df1


# In[72]:


df1.index = pd.Index(['Guangdong_Confirmed', 'Hubei_Confirmed', 'Shanghai_Confirmed'],name='date_time')
df1 = df1.T
df1.index = pd.to_datetime(df1.index, format='%m/%d/%y', exact = False)
df1


# In[73]:


print("Dataframe shape: ", df1.shape)
time_diff = (df1.index[-1] - df1.index[0])
print("Number of hours between start and end dates: ", time_diff.total_seconds()/3600 + 1)


# In[74]:


df1.plot(figsize=(15,2))
plt.xlabel('Date Time'); text=plt.ylabel('Num of Cases')
    
df1.plot(subplots=True, figsize=(15,3))
plt.xlabel('Date Time'); text=plt.ylabel('Num of Cases')

df1.plot(y=['Guangdong_Confirmed', 'Hubei_Confirmed', 'Shanghai_Confirmed'], linestyle=':', figsize=(15,2))
plt.xlabel('Date Time'); text=plt.ylabel('Num of Cases')


# It is obvious that the curves of confirmed cases for Guangdong province and the city of Shanghai look quite similar, while that of Hubei province displays different pattern.

# ###  Identify periodicities & similarities
# 
# Next, let us explore how `compositing` can be done to identify the periodicities and similarities from the many observations along the time-series. This is especially useful when one is looking for responses to a specific time-series event that are combined with noise from a lot of other influences. Examples of compositing include the climatic response to a volcanic eruption, the global weather response to el Niño, calculating the mean diurnal cycle of surface temperature in Dallas at Texas, or finding if precipitation responds to the phase of the moon. The last two natural phenomena relate to sorting out the true amplitude of cyclic responses <a href="#References">[1]</a>.
# 
# Often compositing will reveal periodic phenomena with fixed phase that cannot be extracted from spectral analysis if the signal is small compared to the noise. Compositing makes no assumption of linearity, and it is good at separating small signals from noise, with sample large enough <a href="#References">[1]</a>.
# 
# What we need to do here, is to average the data in some clever way in relation to the event, the event signal will remain and all other influences will tend to average out. The overall workflow <a href="#References">[1]</a> includes:
#   - Select the basis for compositing and define the categories;
#   - Compute the means and statistics for each category;
#   - Organize and display the results;
#   - Validate the results.
# 
# Let's now split the time series up into its constituent cycles and stack them together via `pandas` and `matplotlib`. At this point we will also downsample to a 8-day rate, which makes the plot a bit clearer and quicker to generate.

# In[91]:


minima = ["2020-01-23", "2020-01-31", 
          "2020-02-07", "2020-02-15", "2020-02-23", 
          "2020-03-03", "2020-03-11", "2020-03-19", 
          "2020-03-27", "2020-04-03", "2020-04-11",
          "2020-04-19", "2020-04-27", "2020-05-05", 
          "2020-05-13", "2020-05-21", 
          "2020-05-29", "2020-06-05", "2020-06-13",
          "2020-06-21", "2020-06-29", "2020-07-03"]

def split_into_cycles(province = 'Hubei', df2 = df_hubei):
    """Returns a list of dataframes, one for each cycle"""
    if df2 is None:
        key0 = province + '_Confirmed'
        key1 = province + '_Recovered'
        key2 = province + '_Death'
        df0.reset_index(drop= True)
        df_recovered_china.reset_index(drop= True)
        df_death_china.reset_index(drop= True)
        df2 = pd.concat([df0[key0],
                         df_recovered_china[key1],
                         df_death_china[key2]],
                        axis=1, ignore_index=True)
    else:
        print("Use existing DataFrame")
    df2.index = pd.to_datetime(df2.index, format='%m/%d/%Y %H:%M', exact = False)
    df_daily = df2.resample("8D").mean()
    
    cycles = []
    # Split by cycle
    for start, end in zip(minima[0:-1], minima[1:]):
        cycle = df2[start:end]
        # Convert from dates to days from minimum
        cycle.index = (cycle.index - cycle.index[0]).days
        # Extend so that each cycle lasts a full 100 days (filled with nan)
        cycle = cycle[~cycle.index.duplicated()]
        
        import numpy as np
        ix = pd.Int64Index(np.arange(0,100))
        cycle.reindex(ix)
        cycles.append(cycle)
    return cycles

cycles = split_into_cycles('Hubei', df_hubei)


# By manually composing the list `minima`, we are expecting the output of `split_into_cycles` to be a list, `cycles` in this case, which contains 20 DataFrames, and each containing a different cycle. On each DataFrame, we have changed the index into the number of days from the minimum, and used `.reindex()` to fix them all to the same length so that we can perform arithmetic operations on them together. The following will create a plot of each parameter, with the cycles superposed over each other. 
# 
# In this example, we first create the figure and its axes using `matplotlib` directly (using `sharex=True` to link the x-axes on each plot), then direct the `pandas` plotting commands to point them to the axis we want each thing to plot onto using the ax kwarg. We also calculate the mean of the stacked time series.

# In[92]:


def plot_with_cycles(province = "Hubei"):
    fig, axes = plt.subplots(3, 1, figsize=(15,25), sharex=True)
    plt.subplots_adjust(wspace=0.25, 
                        hspace=0.25)
    key0 = province + '_Confirmed'
    key1 = province + '_Recovered'
    key2 = province + '_Death'
    for i, cycle in enumerate(cycles):
        cycle[key0].plot(ax=axes[0], label=f"Cycle {i}")
        cycle[key1].plot(ax=axes[1])
        cycle[key2].plot(ax=axes[2])
    N_cycles = len(cycles)
    (sum(cycles)[key0]/N_cycles).plot(ax=axes[0], color="black", label='Mean_'+key0)
    (sum(cycles)[key1]/N_cycles).plot(ax=axes[1], color="orange", label='Mean_'+key1)
    (sum(cycles)[key2]/N_cycles).plot(ax=axes[2], color="pink", label='Mean_'+key2)
    
    axes[0].set_ylabel(key0)
    axes[1].set_ylabel(key1)
    axes[2].set_ylabel(key2)
    axes[2].set_xlabel("Days since minimum")
    for ax in axes:
        ax.grid()
    axes[0].legend(loc="lower left")

plot_with_cycles()


# This helps us to see how the cycles differ from each other. For example, the two most recent cycles (cycles 19 & 20 in this case) are consistently higher than the mean, both in the Recovered and Confirmed cases, while other cycles are always lower than the mean. Overall, there is no similar pattern between cycles detected via the plots.
# 
# By constructing the mean of the cycles, we are actually reinforcing the similar pattern over each cycle and reducing the effect of the random noise. This is the basis of a technique called <a href="https://www.sciencedirect.com/science/article/abs/pii/S1364682606000125?via%3Dihub">`superposed epoch analysis`</a>, which is useful for identifying periodicities and similarities between noisy time series.

# ## 3. Parse the time-series data (in the U. S., Australia & Canada)
# 
# Let's summarize the data we have seen so far:
#   1. for countries/regions with significant numbers of confirmed/deaths/recovered cases (e.g. Mainland China), number of cases are reported per province/state; 
#   2. for other regions/countries, number of cases are summarized per region/country (e.g. Australia, or Canada); 
#   3. Also, the global DataFrame lists Cruise Ship since the Diamond Princess and several other cruise ships themselves contain a considerable amount of confirmed/deaths cases.
# 
# So far, we have covered the first case for Mainland China. Next, using the `plot_per_function` function defined previously, we can also look into the statistics for regions other than Mainland China that fall under case 1.

# ### Confirmed Cases in Australia & Canada

# In[78]:


df_au_confirmed = plot_per_country(df_confirmed, "Australia")


# In[79]:


df_ca_confirmed = plot_per_country(df_confirmed, "Canada")


# As shown in the plots above, `Australian Capitol Terrotory` tops in Australia with nearly 20,000, and `Alberta` tops in Canada with almost 500,000 cases confirmed.
# 
# Next, we will look into the statistics collected for the United States.

# ### Confirmed Cases in the U.S.
# 
# Note here, the global DataFrame (a.k.a. `df_confirmed`) created previously only contains one row for the overall statistics of the entire United States, so we will instead use the other DataFrame object (i.e. `us_df_list[0]`) that was imported and parsed from the other GitHub link representing the per-state cases in the United States.

# In[80]:


df_usa_confirmed = plot_per_country(us_df_list[0], "US")


# The top 3 states with the highest numbers of confirmed cases in the U.S. so far are the States of California, Florida and New York.

# ### Confirmed Cases on Cruise Ships
# 
# In case 3 of this dataset, Cruise ships themselves are listed under the `Country\Region` column. We then plot the time-series of its data, appearing in two DataFrames - `df_confirmed` and `us_df_list[0]`, for cruise ships owned by other countries and the United States, respectively.
# 
# A customized function `time_series_per_location` is declared and used below to draw the time-series of confirmed cases reported on cruise ships.

# In[84]:


def time_series_per_location(df=us_df_list[0], location="Diamond Princess", op=1):
    """ Used to plot the time-series of confirmed cases on cruise ships
    """
    if op==0:
        if 'Country/Region' in df.columns:
            df0 = df[df['Country/Region'] == location].iloc[:,5:].fillna(0)
        else:
            df0 = df[df['Country_Region'] == location].iloc[:,11:].fillna(0)
    elif op==1:
        if 'Province/State' in df.columns:
            df0 = df[df['Province/State'] == location].iloc[:,5:].fillna(0)
        else:
            df0 = df[df['Province_State'] == location].iloc[:,11:].fillna(0)
    
    df0 = df0.loc[:, ~df0.columns.str.contains('^Unnamed')]
    df0 = df0.T
    df0.index = pd.to_datetime(df0.index, format='%m/%d/%y', exact = False)
    
    width_multiplier = df0.shape[1]/5

    df0.plot(figsize=(15,2*width_multiplier), 
             title='Plotting the time-series of confirmed cases (line-plot)').legend(loc='upper left')
    plt.xlabel('Date Time'); plt.ylabel('Num of Cases')


# In[82]:


df_confirmed[df_confirmed['Country/Region'].str.contains('Princess')]


# In[85]:


time_series_per_location(df=df_confirmed, location="Diamond Princess", op=0)


# In[86]:


us_df_list[0][us_df_list[0]['Province_State']=='Diamond Princess']


# In[87]:


time_series_per_location()


# In[88]:


us_df_list[0][us_df_list[0]['Province_State']=='Grand Princess']


# In[89]:


time_series_per_location(location="Grand Princess")


# Comparing these three time-series plots, we can see that for the cruise ship #1, the number of cases experienced a rapid rise in mid-Feb of 2020, while for #2, the rapid climb of confirmed cases happened in mid-March and end of March (in year 2020), respectively.

# ## 4. Time Series Lag Scatter Plots
# 
# Time series modeling assumes a relationship between an observation and the previous observation. Previous observations in a time series are called `lags`, with the observation at the previous time step called `lag1`, the observation at two time steps ago `lag2`, and so on.
# 
# A useful type of plot to explore the relationship between each observation and a lag of that observation is the <a href="https://machinelearningmastery.com/time-series-data-visualization-with-python/">scatter plot</a>. `Pandas` has a built-in function for exactly this called `the lag plot`. It plots the observation at time `t` on the x-axis and the lag1 observation `(t-1)` on the y-axis.
#   - If the points cluster along a diagonal line from the bottom-left to the top-right of the plot, it suggests a positive correlation relationship.
#   - If the points cluster along a diagonal line from the top-left to the bottom-right, it suggests a negative correlation relationship.
#   - Either relationship is good as they can be modeled.
# 
# More points tighter in to the diagonal line suggests a stronger relationship and more spread from the line suggests a weaker relationship. A ball in the middle or a spread across the plot suggests a weak or no relationship.
# 
# Below is an example of a lag plot for the `Hubei_Confirmed` Series.

# In[89]:


from matplotlib import pyplot
from pandas.plotting import lag_plot
lag_plot(df1['Hubei_Confirmed'])
pyplot.show()


# Because the points cluster along the diagonal line from bottom-left to the top-right, and more points are tighter to the diagonal line compared to points loosely spread from the line, so we can say the plot suggests a strong positive correlation relationship between the observation and its lag1.
# 
# We can repeat this process for an observation and any lag values. Perhaps with the observation at the same time last week, last month, or last year, or any other domain-specific knowledge we may wish to explore. For example, we can create a scatter plot for the observation with each value in the previous seven days. Below is an example of this for the `Hubei_Confirmed` Series.
# 
# In the cell below, first, a new DataFrame is created with the lag values as new columns. The columns are named appropriately. Then a new subplot is created that plots each observation with a different lag value.

# In[90]:


values = df1['Hubei_Confirmed']
lags = 7
columns = [values]
for i in range(1,(lags + 1)):
    columns.append(values.shift(i))
dataframe = pd.concat(columns, axis=1)
columns = ['t+1']
for i in range(1,(lags + 1)):
    columns.append('t-' + str(i))
dataframe.columns = columns
pyplot.figure(1, figsize=(12,9))
for i in range(1,(lags + 1)):
    ax = pyplot.subplot(240 + i)
    pyplot.subplots_adjust(bottom=0.4, wspace = 0.4, hspace = 0.4)
    ax.set_title('t+1 vs t-' + str(i))
    pyplot.scatter(x=dataframe['t+1'].values, y=dataframe['t-'+str(i)].values)
pyplot.show()


# Running the code block above suggests the strongest relationship between an observation with its lag1 value, but generally a good positive correlation with each value in the last week.

# ## 5. Time Series Autocorrelation Plots
# 
# We can quantify the strength and type of relationship between observations and their lags. In statistics, this is called correlation, and when calculated against lag values in time series, it is called `autocorrelation` (self-correlation).
# 
# A correlation value calculated between two groups of numbers, such as observations and their lag1 values, results in a number between -1 and 1. The sign of this number indicates a negative or positive correlation respectively. A value close to zero suggests a weak correlation, whereas a value closer to -1 or 1 indicates a strong correlation.
# 
# Correlation values, called `correlation coefficients`, can be calculated for each observation and different lag values. Once calculated, a plot can be created to help better understand how this relationship changes over the lag. This type of plot is called `an autocorrelation plot` and `Pandas` provides this capability built in, called the `autocorrelation_plot()` function.
# 
# The cell below creates an <a href="https://machinelearningmastery.com/time-series-data-visualization-with-python/">autocorrelation plot</a> for the `Hubei_Confirmed` Series:

# In[65]:


from pandas.plotting import autocorrelation_plot
ax = autocorrelation_plot(df1['Hubei_Confirmed'], label="Value of Self-Correlation")
ax.set_xlabel("Lag (in days)")
ax.set_ylabel("Value of Autocorrelation")
pyplot.show()


# The resulting plot shows lag (in days) along the x-axis and the value of correlation on the y-axis. Dotted lines are provided that indicate any correlation values beyond those lines are `statistically significant`. We can see that for the `Hubei_Confirmed` Series, cycles of strong positive correlation before day 6. This captures the relationship of an observation with past observations, and there is no seasonal or recurrent trends.

# ## Conclusions
# 
# While many countries or regions have struggled to cope with the rapid spread of the virus, it is important for everyone to closely monitor the situations and take precautionary measures. Serving to the purpose, this notebook has discussed how to perform data analysis and time series charting of COVID-19 cases across the globe at various levels, which includes all provinces in Mainland China, the United States, Australia, and Canada, and Cruise Ships with reported cases. Also, approaches to perform cross-comparisons within different categories, or provinces/states, are demonstrated here, to help figure out which provinces/states in the selected countries are impacted the most, or at critical phases of virus development. Then, Time-Series Lag Scatter plots and Autocorrelation Plots are drawn in the process of studying the time-lag-correlation, and the autocorrelation of the COVID-19 time-series. 
# 
# More info can be found at <a href="https://storymaps.arcgis.com/stories/4fdc0d03d3a34aa485de1fb0d2650ee0">storymaps</a>. You can also check out <a href="./covid19_part3_predictive_analysis.ipynb">Part 3 notebook</a> for predictive analysis of COVID-19 trends.

# ## References
# 
# [1] "Compositing or Superposed Epoch Analysis", https://atmos.washington.edu/~dennis/552_Notes_2.pdf, accessed on 07/04/2020


# ====================
# covid_case_forecasting_for_alabama_state_using_timeseriesmodel_from_arcgis_learn.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Covid case forecasting Using TimeSeriesModel from arcgis.learn

# ## Table of Contents <a class="anchor" id="0"></a>
# * [Introduction](#1) 
# * [Importing libraries](#2)
# * [Connecting to your GIS](#3)
# * [Accessing the dataset](#4) 
# * [Raw data cleaning](#5) 
#     * [Calculate Moving Average for Confirmed cases](#6)
#     * [Cut off first 6 days date](#7) 
# * [Time series data preprocessing](#8) 
#     * [Collecting the counties of the Alabama State](#9) 
# * [Time series modeling and forecasting](#10) 
# * [Result visualization](#11) 
# * [Conclusion](#12)

# ## Introduction<a class="anchor" id="1"></a> 

# COVID-19 forecasting has been vital for efficiently planning health care policy during the pandemic. There are many forecasting models, a few of which require explanatory variables like population, social distancing, etc. This notebook uses the deep learning `TimeSeriesModel` from `arcgis.learn` for data modeling and is helpful in the prediction of future trends.
# 
# To demonstrate the utility of this method, this notebook will analyze confirmed cases for all counties in Alabama. The dataset contains the unique county FIPS ID, county Name, State ID, and cumulative confirmed cases datewise for each county. The dataset ranges from January 2020 to February 2022, with the data from January 2022 to February 2022 being used to validate the quality of the forecast. The approach utilized in this analysis for forecasting future COVID-19 cases involves: (a) Data Processing (calculating the seven day moving average for removing the noise and vertically stacking the county data), (b) creating functions for test-train splitting, tabular data preparation, model fitting using Inception Time for a sequence length of 60, and forecasting, and (c) validation and visualization of the predicted data and results.
# 
# 

# ## Importing Libraries<a class="anchor" id="2"></a> 

# In[1]:


get_ipython().run_line_magic('matplotlib', 'inline')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
import sklearn.metrics as metrics

from arcgis.gis import GIS
from arcgis.learn import TimeSeriesModel, prepare_tabulardata


# ## Connecting to your GIS<a class="anchor" id="3"></a> 

# In[2]:


gis = GIS("home")


# ## Accessing the dataset <a class="anchor" id="4"></a>
# The latest dataset can be downloaded from USAFacts: 
# https://usafacts.org/visualizations/coronavirus-covid-19-spread-map/

# In[3]:


# Access the data table
data_table = gis.content.get("b222748b885e4741839f3787f207b2b1")
data_table


# In[4]:


# Download the csv and saving it in local folder
data_path = data_table.get_data()


# In[5]:


# # Read the csv file
confirmed = pd.read_csv(data_path)
confirmed.head()


# ## Raw data cleaning<a class="anchor" id="5"></a>   

# In[6]:


# Extract the data of Alabama State
confirmed_AL = confirmed.loc[
    (confirmed["countyFIPS"] >= 1000) & (confirmed["countyFIPS"] <= 1133)]


# In[7]:


# Stack the table for cumulative confirmed cases
confirmed_AL = confirmed_AL.set_index(["countyFIPS"])
confirmed_AL = confirmed_AL.drop(columns=["State", "County Name", "StateFIPS"])
confirmed_stacked_df = (
    confirmed_AL.stack()
    .reset_index()
    .rename(columns={"level_1": "OriginalDate", 0: "ConfirmedCases"})
)
confirmed_stacked_df


# In[8]:


# Converting into date time field format
confirmed_stacked_df["DateTime"] = pd.to_datetime(
    confirmed_stacked_df["OriginalDate"], infer_datetime_format=True
)
confirmed_stacked_df = confirmed_stacked_df.drop(columns=["OriginalDate"])
confirmed_stacked_df.info()


# ### Calculate Moving Average for Confirmed cases<a class="anchor" id="6"></a> 
# Here, we calculate a 7-day simple moving average to smooth out the data and remove noise caused by spikes in testing results.

# In[9]:


# Set moving average window = 7 days
SMA_Window = 7
# Copy the dataframe and set columns need to be calculated
df = confirmed_stacked_df
cols = {1: "ConfirmedCases"}


# In[10]:


SMA_Window = 7
for fips in df.countyFIPS.unique():
    for col in cols:
        field = f"{cols[col]}_SMA{SMA_Window}"
        df.loc[df["countyFIPS"] == fips, field] = (
            df.loc[df["countyFIPS"] == fips]
            .iloc[:, col]
            .rolling(window=SMA_Window)
            .mean()
        )


# ### Cut off first 6 day's date<a class="anchor" id="7"></a> 
# As the first moving average value starts from the seventh day, we will disregard the first 6 days.

# In[11]:


firstMADay = df["DateTime"].iloc[0] + pd.DateOffset(days=SMA_Window - 1)
firstMADay


# In[12]:


df_FirstMADay = df.loc[df["DateTime"] >= firstMADay]
df_FirstMADay.reset_index(drop=True, inplace=True)
df_FirstMADay


# ## Time series data preprocessing<a class="anchor" id="8"></a>   
# The preprocessing of the data for multivariate time series modeling includes the selection of required columns, converting time into the date-time format, and collecting all the counties of the state.

# In[13]:


# Selecting the required columns for modeling
df = df_FirstMADay[["DateTime", "ConfirmedCases_SMA7", "countyFIPS"]].copy()
df.columns = ["date", "cases", "countyFIPS"]
df.date = pd.to_datetime(df.date, format="%Y-%m-%d")


# In[14]:


df.tail()


# ### Collecting the counties of Alabama<a class="anchor" id="9"></a>   

# In[15]:


# This cell collects all counties by their Unique FIPS IDs.
counties = df.countyFIPS.unique()
counties = [county for county in counties if county != 0]
len(counties)


# The next cell can be used to forecast for a specific county. You can declare the county to forecast by using its FIPS ID.

# In[16]:


# counties = df.countyFIPS.unique()
# counties = [county for county in counties if county == 1001]


# ## Time series modeling and forecasting<a class="anchor" id="10"></a>  
# Here, we will create the different functions for preparing tabular data, modeling, and forecasting that will later be called for each county.

# In[17]:


# This function selects the specified county data and splits the train and test data
def CountyData(county, test_size):
    data_file = df[df["countyFIPS"] == county]
    data_file.reset_index(inplace=True, drop=True)
    train, test = train_test_split(data_file, test_size=test_size, shuffle=False)
    return train, test


# The next function prepares the tabular data and initializes the model from the available set of backbones (InceptionTime, ResCNN, Resnet, and FCN). The sequence length here is provided as 15, which was found by performing a grid search. To train the model, the `model.fit` method is used and is provided with the number of training epochs and the learning rate.

# In[18]:


def Model(train, seq_len, test_size):
    data = prepare_tabulardata(
        train, variable_predict="cases", index_field="date", seed=42
    )  # Preparing the tabular data
    tsmodel = TimeSeriesModel(
        data, seq_len=seq_len, model_arch="InceptionTime"
    )  # Model initialization
    lr_rate = tsmodel.lr_find()  # Finding the Learning rate
    tsmodel.fit(100, lr=lr_rate, checkpoint=False)  # Model training
    sdf_forecasted = tsmodel.predict(
        train, prediction_type="dataframe", number_of_predictions=test_size
    )  # Forecasting using the trained TimeSeriesModel
    return sdf_forecasted


# In[19]:


# This function evalutes the model metrics and returns the dictionary
def evaluate(test, sdf_forecasted):
    r2_test = r2_score(test["cases"], sdf_forecasted["cases_results"][-14:])
    mse = metrics.mean_squared_error(
        test["cases"], sdf_forecasted["cases_results"][-14:]
    )
    mae = metrics.mean_absolute_error(
        test["cases"], sdf_forecasted["cases_results"][-14:]
    )
    return {
        "DATE": test["date"],
        "cases_actual": test["cases"],
        "cases_predicted": sdf_forecasted["cases_results"][-14:],
        "R-square": round(r2_test, 2),
        "V_RMSE": round(np.sqrt(mse), 4),
        "MAE": round(mae, 4),
    }


# In[20]:


# This class calls all the defined functions
class CovidModel(object):
    seq_len = 15
    test_size = 14

    def __init__(self, county):
        self.county = county

    def CountyData(self):
        self.train, self.test = CountyData(self.county, self.test_size)

    def Model(self):
        self.sdf_forecasted = Model(self.train, self.seq_len, self.test_size)

    def evaluate(self):
        return evaluate(self.test, self.sdf_forecasted)


# Training the model for all counties and saving the metrics in the dictionary.

# In[21]:


dct = {}

for i, county in enumerate(counties):
    covidmodel = CovidModel(county)
    covidmodel.CountyData()
    covidmodel.Model()
    dct[county] = covidmodel.evaluate()


# ## Result Visualization<a class="anchor" id="11"></a>  
# Finally, the actual and forecasted values are plotted to visualize their distribution over the validation period, with the orange line representing the forecasted values and the blue line representing the actual values.

# In[22]:


# Specifying few counties for visualizing the results
viz_counties = [1007,1113]

for i, county in enumerate(viz_counties):
    result_df = pd.DataFrame(dct[county])
    plt.figure(figsize=(20, 5))
    plt.plot(result_df["DATE"], result_df[["cases_actual", "cases_predicted"]])
    plt.xlabel("Date")
    plt.ylabel("Covid Cases")
    plt.legend(["Cases_Actual", "Cases_Predicted"], loc="upper left")
    plt.title(str(county) + ": Covid Forecast Result")
    plt.show()


# In[23]:


# Here the Alabama counties feature layer is accessed and converted to spatial dataframe
item = gis.content.get("41e8eb46285d4e1f85ee6e826b05e077")
flayer = item.layers[0]
f_sdf = flayer.query().sdf


# In[24]:


# Adding the RMSE and MAE from the output dictionary to the spatial dataframe
RMSE = []
MAE = []
for i, county in enumerate(counties):
    MAE.append(dct[county]["MAE"])
    RMSE.append(dct[county]["V_RMSE"])

f_sdf = f_sdf.assign(RMSE=RMSE, MAE=MAE)


# Next, we will publish this spatial dataframe as a feature layer.

# In[25]:


published_sdf = gis.content.import_data(f_sdf, title='Alabama Covid Time Series Model Metrics')
published_sdf


# Next, we will open the published web layer and input the item id of the published output layer.

# In[26]:


item = gis.content.get("9d197a4870a1479c81ddfd6b739816da")
map1 = gis.map("Alabama")
map1.add_layer(item)
map1.legend = True
map1


# 

# From the map, it can be seen that most of the counties have RMSE ranging from 18-400 cases, represented by the blue polygons. The fewer green and cream colored counties have higher RMSE, and the one red county has the maximum RMSE. This indicates that `InceptionTime` is performing well for this state, and that other backbones can be introduced to further reduce the RMSE in the counties that have higher RMSE.

# ## Conclusion<a class="anchor" id="12"></a>

# This study conducted a univariate time series analysis using the Deep learning `TimeSeriesModel` from the `arcgis.learn` library and forecasted the COVID-19 confirmed cases for the counties in Alabama. The initial raw data was averaged over 7 days using the seven-day moving average method to avoid sudden spikes. The methodology also included preparing a time series dataset using the `prepare_tabulardata()` method, followed by modeling, predicting, and validating the test dataset. The TimeSeriesModel from `arcgis.learn` includes backbones, such as `InceptionTime`, `ResCNN`, `ResNet`, and `FCN`, that do not need fine-tuning of multiple hyperparameters before fitting the model. Our method produced reasonably accurate results, and users can change the sequence length or backbone for forecasting in other areas.


# ====================
# creating_building_models_using_point_cloud_classification.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Creating building models using point cloud classification

# - 🔬 Data Science
# - 🥠 Deep Learning
# - 🌍 GIS
# - ☁️ Point Cloud Classification

# <p align="center"></p>

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Introduction" data-toc-modified-id="Introduction-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href="#Area-of-interest-and-pre-processing" data-toc-modified-id="Area-of-interest-and-pre-processing-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Area of interest and pre-processing</a></span></li><li><span><a href="#Data-preparation" data-toc-modified-id="Data-preparation-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>Data preparation</a></span></li><li><span><a href="#Visualization-of-prepared-data" data-toc-modified-id="Visualization-of-prepared-data-4"><span class="toc-item-num">4&nbsp;&nbsp;</span>Visualization of prepared data</a></span></li><li><span><a href="#Training-the-model" data-toc-modified-id="Training-the-model-5"><span class="toc-item-num">5&nbsp;&nbsp;</span>Training the model</a></span></li><li><span><a href="#Visualization-of-results-in-notebook" data-toc-modified-id="Visualization-of-results-in-notebook-6"><span class="toc-item-num">6&nbsp;&nbsp;</span>Visualization of results in notebook</a></span></li><li><span><a href="#Saving-the-trained-model" data-toc-modified-id="Saving-the-trained-model-7"><span class="toc-item-num">7&nbsp;&nbsp;</span>Saving the trained model</a></span></li><li><span><a href="#Classification-using-the-trained-model" data-toc-modified-id="Classification-using-the-trained-model-8"><span class="toc-item-num">8&nbsp;&nbsp;</span>Classification using the trained model</a></span></li><li><span><a href="#Post-processing-in-ArcGIS-Pro-and-City-Engine" data-toc-modified-id="Post-processing-in-ArcGIS-Pro-and-City-Engine-9"><span class="toc-item-num">9&nbsp;&nbsp;</span>Post-processing in ArcGIS Pro and City Engine</a></span></li><li><span><a href="#Visualization-of-results-in-ArcGIS-Pro" data-toc-modified-id="Visualization-of-results-in-ArcGIS-Pro-10"><span class="toc-item-num">10&nbsp;&nbsp;</span>Visualization of results in ArcGIS Pro</a></span></li><li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-11"><span class="toc-item-num">11&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href="#References" data-toc-modified-id="References-12"><span class="toc-item-num">12&nbsp;&nbsp;</span>References</a></span></li></ul></div>

# ## Introduction

# Classification of point clouds has always been a challenging task, due to its naturally unordered data structure. The workflow described in this sample is about going from raw unclassified point clouds to digital twins: near-perfect representation of real-world entities. Within the scope of this sample, we are only interested in 'digital twins of buildings' <i>(3D building multipatches/models)</i>. This work can also be used for guidance, in other relevant use-cases for various objects of interest.
# 
# First, deep learning capabilities in 'ArcGIS API for Python' are utilized for point cloud classification, then 'ArcGIS Pro' and 'City Engine' are used for the GIS-related post-processing.
# 
# Further details on the PointCNN implementation in the API <i>(working principle, architecture, best practices, etc.)</i>, can be found in the <a href="https://developers.arcgis.com/python/guide/point-cloud-segmentation-using-pointcnn" target="_blank">PointCNN guide</a>, along with instructions on how to set up the Python environment. Additional sample notebooks related to PointCNN can be found in the <a href="https://developers.arcgis.com/python/sample-notebooks/" target="_blank">sample notebook section</a> on the website.
# 
# Before proceeding through this notebook, it is advised that you go through the <a href="https://developers.arcgis.com/python/api-reference/arcgis.learn.toc.html#pointcnn" target="_blank">API reference</a> for PointCNN (`prepare_data()`, `Transform3d()`, and `PointCNN()`), along with the resources and tool references for point cloud classification using deep learning in ArcGIS Pro, found <a href="https://pro.arcgis.com/en/pro-app/latest/help/data/las-dataset/introduction-to-deep-learning-and-point-clouds.htm" target="_blank">here</a>.

# _**Objectives:**_
# 
# <ol style="list-style-type:upper-roman">
# <li>Classify building points using API's PointCNN model, where we train it for two classes: viz. 'Buildings' and 'Background'.</li>
# <br>
# <li>Generate 3D building multipatches, from classified building points using 'ArcGIS Pro' and 'City Engine'.</li>
# </ol>

# ## Area of interest and pre-processing

# Any airborne point cloud dataset and area of interest can be used. But for this sample, <a href="https://downloads.pdok.nl/ahn3-downloadpage/" target="_blank"> AHN3 dataset</a>, provided by the Government of The Netherlands is considered <a href="#References">[1]</a>, which is one of the highest quality open datasets available currently, in terms of accurate labels and point density. While the area of interest for this work is 'Amsterdam' and its nearby regions. Its unique terrain with canals and from modern to 17<sup>th</sup> - century architecture style makes it a good candidate for a sample.

# _**Pre-processing steps:**_
# 
# - Uncompress, the downloaded AHN3 dataset's `.laz` files, into `.las` format. <i>(Refer, ArcGIS Pro's <a href="https://pro.arcgis.com/en/pro-app/tool-reference/conversion/convert-las.htm" target="_blank">Convert LAS</a> tool.)</i>
# 
# 
# - Reassign class codes, so that only two classes remain in the data: ‘Buildings’ and ‘Background’ <i>(Refer, ArcGIS Pro's <a href="https://pro.arcgis.com/en/pro-app/tool-reference/3d-analyst/change-las-class-codes.htm" target="_blank">Change LAS Class Codes</a> tool)</i>. In this sample, ASPRS-defined class codes are followed <i>(optional, for the PointCNN workflow)</i>. Hence, '6' is used to represent 'Buildings'. And reserved-class code '19' is used to represent 'Background' <i>(rest of the points)</i>, as it is also a class which the model will learn about, along with 'Buildings' class. So, the class code for 'points in undefined state': '1' and ' points in never-classified state': '0', are not used to represent 'Background' class.
#  
# 
# - Split the `.las` files into three unique sets, one for training, one for validation, and one for testing. Create LAS datasets for all three sets using the <a href="https://pro.arcgis.com/en/pro-app/latest/tool-reference/data-management/create-las-dataset.htm" target="_blank">'Create LAS Dataset'</a>  tool. There is no fixed rule, but generally, the validation data for point clouds in `.las` format should be at least 5-10 % <i>(by size)</i> of the total data available, with appropriate diversity within the validation dataset. <i>(For ease in splitting the big `.las` files into the appropriate ratios, ArcGIS Pro's <a href="https://pro.arcgis.com/en/pro-app/tool-reference/3d-analyst/tile-las.htm" target="_blank">'Tile LAS'</a> tool can be used.)</i>
# 
# 
# - Alternatively, polygons can also be used to define regions of interest that should be considered as training or validation datasets. These polygons can be used later in the export tool. If the dataset is very large, then the <a href="https://pro.arcgis.com/en/pro-app/latest/tool-reference/data-management/build-las-dataset-pyramid.htm" target="_blank">'Build LAS Dataset Pyramid'</a> tool can be leveraged for faster rendering/visualization of the data, which will also help in exploring and splitting the dataset.  

# ## Data preparation

# _**Imports:**_

# In[ ]:


from arcgis.learn import prepare_data, Transform3d, PointCNN
from arcgis.gis import GIS
gis = GIS()


# **Note:** The data used in this sample notebook can be downloaded as a zip file, from <a href="https://arcg.is/0jzODe"  target="_blank">here</a>. It contains both 'training data' and 'test data', where the 'test data' is used for inferencing. It can also be accessed via its `itemId`, as shown below.

# In[ ]:


training_data = gis.content.get('50390f56a0e740ac88c72ae1fb1eda7a')
training_data


# _**Exporting the data:**_

# In this step, `.las` files are converted to a 'HDF5 binary data format'. For this step of exporting the data into an intermediate format, use the <a href="https://pro.arcgis.com/en/pro-app/latest/tool-reference/3d-analyst/prepare-point-cloud-training-data.htm" target="_blank">Prepare Point Cloud Training Data</a> tool in the <a href="https://pro.arcgis.com/en/pro-app/latest/tool-reference/3d-analyst/an-overview-of-the-3d-analyst-toolbox.htm" target="_blank">3D Analyst extension</a>, available from ArcGIS Pro 2.8 onwards <i>(as shown in figure 1)</i>.
# 
# The tool needs two LAS datasets, one for the training data and one for the validation data or regions of interest defined by polygons. Next, the `block size` is set to '50 meters', as our objects of interest will mostly be smaller than that, and the default value of '8192' is used for `block point limit`.
# 
# 
# 
# <p align="center"></p>
# 
# <center>Figure 1. <a href="https://pro.arcgis.com/en/pro-app/latest/tool-reference/3d-analyst/prepare-point-cloud-training-data.htm" target="_blank">Prepare Point Cloud Training Data</a> tool.</center>
# 
# 
# 
# Here, all the additional attributes are included in the exported data. Later, a subset of additional attributes like intensity, number of returns, etc. can be selected that will be considered for training.
# 
# After the export is completed at the provided output path, the folder structure of the exported data will have two folders, each with converted HDF files in them <i>(as shown in figure 2)</i>. The exported training and validation folders will also contain histograms of the distributions of data that provide additional understanding and can help in tweaking the parameters that are being used in the workflow.
# 
# <p align="center"></p>
# 
# <center>Figure 2. Exported data.</center>

# _**Preparing the data:**_

# For `prepare_data()`, deciding the value of `batch_size` will depend on either the available RAM or VRAM, depending upon whether CPU or GPU is being used. `transforms` can also be used for introducing rotation, jitter, etc. to make the dataset more robust. `data.classes` can be used to verify what classes the model will be learning about.
# 
# The `classes_of_interest` and `min_points` parameters can be used to filter out less relevant blocks. These parameters are useful when training a model for SfM-derived or mobile/terrestrial point clouds. In specific scenarios when the 'training data' is not small, usage of these features can result in speeding up the 'training time', improving the convergence during training, and addressing the class imbalance up to some extent.
# 
# In this sample notebook X, Y, Z, and intensity are considered for training the model. So, 'intensity' is selected as `extra_features`. The names of the classes are also defined using `class_mapping` and will be saved inside the model for future reference.

# In[ ]:


output_path = r'C:\project\exported_data.pctd'


# In[ ]:


colormap = {'6':[255,69,0], '19':[253,247,83]}


# In[ ]:


data = prepare_data(output_path, 
                    dataset_type='PointCloud',
                    batch_size=2,
                    transforms=None,
                    color_mapping=colormap,
                    extra_features=['intensity'],
                    class_mapping={'6':'building','19':"background"})


# In[ ]:


data.classes


#   

# ## Visualization of prepared data

# `show_batch()` helps in visualizing the exported data. Navigation tools available in the graph can be used to zoom and pan to the area of interest.

# In[ ]:


data.show_batch(rows=1)


# <p align="center"></p>
# 
# <center>Figure 3. Visualization of batch.</center>

# ## Training the model

# First, the PointCNN model object is created, utilizing the prepared data.

# In[ ]:


pc = PointCNN(data)


# Next, the `lr_find()` function is used to find the optimal learning rate that controls the rate at which existing information will be overwritten by newly acquired information throughout the training process. If no value is specified, the optimal learning rate will be extracted from the learning curve during the training process.

# In[ ]:


pc.lr_find()


#   

# The `fit()` method is used to train the model, either applying a new 'optimum learning rate' or the previously computed 'optimum learning rate' <i>(any other user-defined learning rate can also be used.)</i>.
# 
# If `early_stopping` is set to 'True', then the model training will stop when the model is no longer improving, regardless of the `epochs` parameter value specified. The best model is selected based on the metric selected in the `monitor` parameter. A list of `monitor`'s available metrics can be generated using the `available_metrics` property.
# 
# An 'epoch' means the dataset will be passed forward and backward through the neural network one time, and if `Iters_per_epoch` is used, a subset of data is passed per epoch. To track information like gradients, losses, metrics, etc. while the model training is in progress, `tensorboard` can be set to 'True'.
# 

# In[ ]:


pc.available_metrics


# In[ ]:


pc.fit(30, 0.0003311311214825911, monitor='f1', tensorboard=True, early_stopping=True)


# ## Visualization of results in notebook

# `show_results()` will visualize the results of the model for the same scene as the ground truth. Navigation tools available in the graph can be used to zoom and pan to the area of interest.
# 
# The `compute_precision_recall()` method can be used to compute per-class performance metrics, which are calculated against the validation dataset.

# In[ ]:


pc.show_results(rows=1)


# <p align="center"></p>
# 
# <center>Figure 4. Visualization of results.</center>

# ## Saving the trained model

# The last step related to training is to save the model using the `save()` method. Along with the model files, this method also saves performance metrics, a graph of training loss vs validation loss, sample results, etc. <i>(as shown in figure 5)</i>.
# 
# <p align="center"></p>
# 
# <center>Figure 5. Saved model.</center>

# In[ ]:


pc.save('building_and_background')


# ## Classification using the trained model

# For inferencing, <a href="https://pro.arcgis.com/en/pro-app/latest/tool-reference/3d-analyst/classify-point-cloud-using-trained-model.htm" target="_blank">Classify Points Using Trained Model</a> tool in the <a href="https://pro.arcgis.com/en/pro-app/latest/tool-reference/3d-analyst/an-overview-of-the-3d-analyst-toolbox.htm" target="_blank">3D Analyst extension</a>, available from ArcGIS Pro 2.8 onwards, can be used <i>(as shown in figure 6)</i>.
# 

# <p align="center"></p>
# 
# <center>Figure 6. <a href="https://pro.arcgis.com/en/pro-app/latest/tool-reference/3d-analyst/classify-point-cloud-using-trained-model.htm" target="_blank">Classify Points Using Trained Model</a> tool.</center>
# 
# 

# Additional features, like target classification and class preservation in input data, are also available. After the prediction, LAS files will have 'building points', with the class code '6', and the rest of the points will have the class code '19' <i>(referred to as 'Background' in this sample)</i>. To visualize the results after the process is completed, the 'Symbology' can be changed to 'class' from the 'Appearance' tab, if not done initially.

#  

# ## Post-processing in ArcGIS Pro and City Engine

# There can be multiple unsupervised/semi-supervised workflows to clean the noise and generate building footprints from classified building points. The method used for this work is described below: 
# 
# We start with PointCNN's classified building points<i> (as shown in figure 7)</i>. 
# 
# <p align="center"></p>
# 
# <center>Figure 7. Visualization of results in ArcGIS Pro.</center>
# 

# _**Model Builder:**_
# 
# Then using multiple geoprocessing tools in ArcGIS Pro within a model builder, noises are cleaned and building footprints are generated. These footprints are later used to generate multipatches. 
# 
# The model builder used in this sample can be downloaded from <a href="https://arcg.is/0Kvi0r" target="_blank">here</a> . The model builder can be used via its tool UI <i>(as shown in figure 8.1)</i>, or it can also be used via the model builder wizard in ArcGIS Pro <i>(as shown in figure 8.2)</i>. If needed the workflow can be also be edited/customized.
# 

# <p align="center"></p>
# 
# 
# <center>Figure 8.1. Model builder tool UI.</center>
# 

# <p align="center"></p>
# 
# 
# <center>Figure 8.2. Model builder.</center>
# 

# The model builder takes filtered 'building points' as 'input' <i>('LAS filter' can be used for this, as shown in figure 9)</i>. In this post-processing pipeline, important prior information is that "no building will have a very small area". This information is used to apply 'area-based thresholding' using <a href="https://pro.arcgis.com/en/pro-app/help/mapping/navigation/select-features-using-attributes.htm" target="_blank">Select by Attribute</a> and reduce the noise polygons generated from noise points. If needed, this 'area-based threshhold value' can be changed by editing the model builder.
# 
# 
# <p align="center"></p>
# 
# <center>Figure 9. Filtering of points. </center>
# 

# After smoothening and regularizing the polygons <i>(as shown in figure 10)</i>,
# <a href="https://pro.arcgis.com/en/pro-app/tool-reference/spatial-analyst/zonal-statistics.htm"  target="_blank">Zonal Statistics</a> is used to populate the footprint polygon's attribute table with 'avg. building height'. Later, other information like building type, no. of floors, etc. are also added, which are later used by City Engine's rule package to generate better digital twins of buildings.
# 
# <p align="center"></p>
# 
# <center>Figure 10. Building footprint.</center>
# 
# 
#     
#     
#     

# Lastly, these footprints are used to generate realistic 3D models/multipatches using City Engine's rule packages. Where <a href="https://pro.arcgis.com/en/pro-app/tool-reference/3d-analyst/features-from-cityengine-rules.htm" target="_blank">Features From CityEngine Rules</a> is used. The rule package used in this sample can be downloaded from <a href="https://arcg.is/05qDXj" target="_blank">here</a>. It is created using City Engine's  CGA rules, and the 'connection attributes' can be noted down from City Engine<i> (as shown in figure 11)</i>. 
# 
# <p align="center"></p>
# 
# <center>Figure 11. City Engine's CGA rules.</center>
# 
# 

# ## Visualization of results in ArcGIS Pro

# Building Multipatches are the final output of this sample <i>(as shown in figure 12)</i>.
# The output is very close to real-world buildings from the 'area of interest', in terms of 'accurate depiction' and 'aesthetics'.
# 
# 
# <p align="center"></p>
# 
# <center>Figure 12. Building Multipatches.</center>

#   

# <i> This <a href="https://www.arcgis.com/home/webscene/viewer.html?webscene=908161d7ebfe49e3b318aa7139968ba5" target="_blank">web scene</a> has the final and intermediate outputs related to the illustrated test data in this notebook. It can also be accessed via its `itemId`, as shown below.</i>

# In[ ]:


results = gis.content.get('908161d7ebfe49e3b318aa7139968ba5')
results


# ## Conclusion

# This notebook has walked us through an end-to-end workflow for training a deep learning model for point cloud classification and generating digital twins of real-world objects from the classified points. A similar approach can be applied to classify other objects of interest like trees, traffic lights, wires, etc.

# ## References

# - AHN3 data used in this sample is licensed under the <a href="https://creativecommons.org/publicdomain/zero/1.0/deed.nl" target="_blank">Creative Commons Zero license</a>.


# ====================
# creating_hurricane_tracks_using_geoanalytics.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Creating hurricane tracks using Geoanalytics

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Creating-hurricane-tracks-using-Geoanalytics" data-toc-modified-id="Creating-hurricane-tracks-using-Geoanalytics-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Creating hurricane tracks using Geoanalytics</a></span><ul class="toc-item"><li><span><a href="#Reconstruct-tracks" data-toc-modified-id="Reconstruct-tracks-1.1"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>Reconstruct tracks</a></span></li><li><span><a href="#Data-used" data-toc-modified-id="Data-used-1.2"><span class="toc-item-num">1.2&nbsp;&nbsp;</span>Data used</a></span></li><li><span><a href="#Inspect-the-data-attributes" data-toc-modified-id="Inspect-the-data-attributes-1.3"><span class="toc-item-num">1.3&nbsp;&nbsp;</span>Inspect the data attributes</a></span></li><li><span><a href="#Create-a-data-store" data-toc-modified-id="Create-a-data-store-1.4"><span class="toc-item-num">1.4&nbsp;&nbsp;</span>Create a data store</a></span></li><li><span><a href="#Perform-data-aggregation-using-reconstruct-tracks-tool" data-toc-modified-id="Perform-data-aggregation-using-reconstruct-tracks-tool-1.5"><span class="toc-item-num">1.5&nbsp;&nbsp;</span>Perform data aggregation using reconstruct tracks tool</a></span><ul class="toc-item"><li><span><a href="#Reconstruct-tracks-tool" data-toc-modified-id="Reconstruct-tracks-tool-1.5.1"><span class="toc-item-num">1.5.1&nbsp;&nbsp;</span>Reconstruct tracks tool</a></span></li></ul></li><li><span><a href="#Inspect-the-results" data-toc-modified-id="Inspect-the-results-1.6"><span class="toc-item-num">1.6&nbsp;&nbsp;</span>Inspect the results</a></span></li><li><span><a href="#What-can-geoanalytics-do-for-you?" data-toc-modified-id="What-can-geoanalytics-do-for-you?-1.7"><span class="toc-item-num">1.7&nbsp;&nbsp;</span>What can geoanalytics do for you?</a></span></li></ul></li></ul></div>

# The sample code below uses big data analytics (GeoAnalytics) to reconstruct hurricane tracks using data registered on a big data file share in the GIS. Note that this functionality is currently available on ArcGIS Enterprise 10.5 and not yet with ArcGIS Online.
# 
# ## Reconstruct tracks
# Reconstruct tracks is a type of data aggregation tool available in the `arcgis.geoanalytics` module. This tool works with a layer of point features or polygon features that are time enabled. It first determines which points belong to a track using an identification number or identification string. Using the time at each location, the tracks are ordered sequentially and transformed into a line representing the path of movement.
# 
# ## Data used
# For this sample, hurricane data from over a period of 50 years, totalling about 150,000 points split into 5 shape files was used. The [National Hurricane Center](http://www.nhc.noaa.gov/gis/) provides similar datasets that can be used for exploratory purposes.
# 
# To illustrate the nature of the data a subset was published as a feature service and can be visualized as below:

# In[10]:


from arcgis.gis import GIS

#Let us connect to an ArcGIS Enterprise
gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')
hurricane_pts = gis.content.get('ebdb876ca1a74cc89a81c3f8ee481e94')
hurricane_pts


# In[1]:


subset_map = gis.map("USA")
subset_map


# In[3]:


subset_map.add_layer(hurricane_pts)


# ## Inspect the data attributes
# Let us query the first layer in hurricane_pts and view its attribute table as a Pandas dataframe.

# In[14]:


hurricane_pts.layers[0].query(as_df=True).head()


# ## Create a data store
# For the GeoAnalytics server to process your big data, it needs the data to be registered as a data store. In our case, the data is in multiple shape files and we will register the folder containing the files as a data store of type `bigDataFileShare`.
# 
# Let us connect to an ArcGIS Enterprise

# In[4]:


gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# Get the geoanalytics datastores and search it for the registered datasets:

# In[5]:


# Query the data stores available
import arcgis
datastores = arcgis.geoanalytics.get_datastores()

bigdata_fileshares = datastores.search(id='a215eebc-1bab-42d5-9aa0-45fe2549ba55')
bigdata_fileshares


# The dataset `hurricanes_all` data is registered as a big data file share with the Geoanalytics datastore, so we can reference it:

# In[6]:


data_item = bigdata_fileshares[0]


# If there is no big data file share for hurricane track data registered on the server, we can register one that points to the shared folder containing the shape files.

# In[17]:


# data_item = datastores.add_bigdata("Hurricane_tracks", r"\\path_to_hurricane_data")


# Once a big data file share is registered, the GeoAnalytics server processes all the valid file types to discern the schema of the data, including information about the geometry in a dataset. If the dataset is time-enabled, as is required to use some GeoAnalytics Tools, the manifest reports the necessary metadata about how time information is stored as well.
# 
# This process can take a few minutes depending on the size of your data. Once processed, querying the manifest property returns the schema. As you can see from below, the schema is similar to the subset we observed earlier in this sample.

# In[23]:


data_item.manifest['datasets'][0] #for brevity only a portion is printed


# ## Perform data aggregation using reconstruct tracks tool
# 
# When you add a big data file share, a corresponding item gets created in your GIS. You can search for it like a regular item and query its layers.

# In[7]:


search_result = gis.content.search("bigDataFileShares_hurricanes_all", item_type = "big data file share")
search_result


# In[8]:


data_item = search_result[0]
data_item


# In[9]:


years_50 = data_item.layers[0]
years_50


# ### Reconstruct tracks tool
# 
# The `reconstruct_tracks()` function is available in the `arcgis.geoanalytics.summarize_data` module. In this example, we are using this tool to aggregate the numerous points into line segments showing the tracks followed by the hurricanes. The tool creates a feature layer item as an output which can be accessed once the processing is complete.

# In[12]:


from arcgis.geoanalytics.summarize_data import reconstruct_tracks
from datetime import datetime as dt


# In[20]:


agg_result = reconstruct_tracks(years_50, 
                                track_fields='Serial_Num',
                                output_name='construct tracks test' + str(dt.now().microsecond))


# ## Inspect the results
# Let us create a map and load the processed result which is a feature service

# In[2]:


processed_map = gis.map("USA")
processed_map


# In[15]:


processed_map.add_layer(agg_result)


# Thus we transformed a bunch of ponints into tracks that represents paths taken by the hurricanes over a period of 50 years. We can pull up another map and inspect the results a bit more closely

# Our input data and the map widget is time enabled. Thus we can filter the data to represent the tracks from only the years 1860 to 1870

# In[16]:


processed_map.set_time_extent('1860', '1870')


# ## What can geoanalytics do for you?
# 
# With this sample we just scratched the surface of what big data analysis can do for you. ArcGIS Enterprise at 10.5 packs a powerful set of tools that let you derive a lot of value from your data. You can do so by asking the right questions, for instance, a weather dataset such as this could be used to answer a few interesting questions such as
#  
#  - did the number of hurricanes per season increase over the years?
#  - give me the hurricanes that travelled longest distance
#  - give me the ones that stayed for longest time. Do we see a trend?
#  - how are wind speed and distance travelled correlated?
#  - my assets are located in a tornado corridor. How many times in the past century, was there a hurricane within 50 miles from my assets?
#  - my industry is dependent on tourism, which is heavily impacted by the vagaries of weather. From historical weather data, can I correlate my profits with major weather events? How well is my business insulated from freak weather events?
#  - over the years do we see any shifts in major weather events - do we notice a shift in when the hurricane season starts?
#  
# The ArcGIS API for Python gives you a gateway to easily access the big data tools from your ArcGIS Enterprise. By combining it with other powerful libraries from the pandas and scipy stack and the rich visualization capabilities of the Jupyter notebook, you can extract a lot of value from your data, big or small.


# ====================
# creating_raster_information_product_using_raster_analytics.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Creating Raster Information Product using Raster Analytics

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Creating-Raster-Information-Product-using-Raster-Analytics" data-toc-modified-id="Creating-Raster-Information-Product-using-Raster-Analytics-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Creating Raster Information Product using Raster Analytics</a></span><ul class="toc-item"><li><span><a href="#Raster-Analytics" data-toc-modified-id="Raster-Analytics-1.1"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>Raster Analytics</a></span></li><li><span><a href="#Imagery-layers" data-toc-modified-id="Imagery-layers-1.2"><span class="toc-item-num">1.2&nbsp;&nbsp;</span>Imagery layers</a></span></li><li><span><a href="#Raster-functions" data-toc-modified-id="Raster-functions-1.3"><span class="toc-item-num">1.3&nbsp;&nbsp;</span>Raster functions</a></span></li></ul></li><li><span><a href="#Creating-a-Raster-Information-Product-using-Landsat-8-imagery" data-toc-modified-id="Creating-a-Raster-Information-Product-using-Landsat-8-imagery-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Creating a Raster Information Product using Landsat 8 imagery</a></span></li></ul></div>

# ## Raster Analytics
# 
# ArcGIS Enterprise at 10.5 provides you with the ability to perform large raster analysis using distributed computing. This capability is provided in the `arcgis.raster.analytics` module and includes functionality to summarize data, analyze patterns, images, terrain and manage data. This sample show the capabilities of imagery layers and raster analytics.

# ## Imagery layers

# In[3]:


import arcgis
from arcgis.gis import GIS
from IPython.display import display

gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# Here we're getting the multispectral landsat imagery item:

# In[2]:


items = gis.content.get('4ca13f0e4e29403fa68c46d188c4be73')
imglyr = items.layers[0]
items


# The code below cycles through and lists the Raster Functions published with the `imglyr`:

# In[6]:


for fn in imglyr.properties['rasterFunctionInfos']:
    print (fn['name'])


# Let us create a map widget and load this layer

# In[ ]:


marthasbasin = arcgis.geocoding.geocode("Marthas Basin, Montana")[0]


# In[ ]:


map1 = gis.map(marthasbasin, zoomlevel=12)


# In[1]:


map1


# In[ ]:


map1.add_layer(imglyr)


# The utility of raster functions is better seen when we interactively cycle through these raster functions and apply them to the map. The code below cycles through the first 6 raster functions stored with the Imagery Layer and a small time delay to illustrate. The image processing occurs on-the-fly at display resolution to show how the layer can be visualized using these different raster functions published with the layer.

# In[11]:


import time
from arcgis.raster.functions import apply

for fn in imglyr.properties['rasterFunctionInfos'][:6]:
    print(fn['name'])
    map1.remove_layers()
    map1.add_layer(apply(imglyr, fn['name']))
    time.sleep(4)  


# ## Raster functions

# Developers can create their own **raster functions**, by chaining different raster functions. For instance, the code below is doing an Extract Band and extracting out the [4,5,3] band combination, and applying a Stretch to get the land-water boundary visualization that makes it easy to see where land is and where water is. Its worth noting that the raster function is applied at display resolution and only for the visible extent using on the fly image processing.

# In[ ]:


from arcgis.raster.functions import stretch, extract_band

def process_bands(layer, bands):
    return stretch(extract_band(layer, bands), 
                    stretch_type='percentclip', min_percent=0.1, max_percent=0.1, gamma=[1, 1, 1], dra=True)


# Let us apply this raster function to the image layer to visualize the results.

# In[3]:


map2 = gis.map(marthasbasin, zoomlevel=12)
map2


# In[ ]:


map2.add_layer(process_bands(imglyr, [4, 5, 3]))


# # Creating a Raster Information Product using Landsat 8 imagery

# This part of the notebook shows how **Raster Analytics** (in ArcGIS Enterprise 10.5) can be used to generate a raster information product, by applying the same raster function across the extent of an image service on the portal. The raster function is applied at source resolution and creates an Information Product, that can be used for further analysis and visualization.

# In[5]:


montana_landsat = gis.content.get('09a4edaefae74ec6a2cb114f155091e3')
montana_landsat


# In the code below, we use extract and stretch the [7, 5, 2] band combination. This improves visibility of fire and burn scars by pushing further into the SWIR range of the electromagnetic spectrum, as there is less susceptibility to smoke and haze generated by a burning fire.

# In[ ]:


montana_lyr = montana_landsat.layers[0]


# In[ ]:


fire_viz = process_bands(montana_lyr, [7, 5, 2])
fire_viz


# ![fire_visualization_MT](../../static/img/sample_04_rasterinfo4.png)

# We can use the `save` method to apply the raster function across the entire extext of the input image layer at source resolution, and presist the result in an output image layer. This creates a raster product similar that can be used for further analysis and visualization.

# In[ ]:


arcgis.env.verbose = True


# In[24]:


from datetime import datetime as dt
montana_fires_lyr = fire_viz.save('Montana_Burn_scars'+str(dt.now().microsecond))


# In[28]:


montana_fires_lyr


# In[ ]:


base_map = gis.map(marthasbasin, 12)

natural_color_map = gis.map(marthasbasin, 12)
natural_color_map.add_layer(montana_landsat)

false_color_map = gis.map(marthasbasin, 12)
false_color_map.add_layer(montana_fires_lyr)


# We can compare the natural color and false color images uaing a tabbed widget. 
# 
# In the false color image the red and brownish pixels correspond to burn scars of the fire:

# In[4]:


import ipywidgets as widgets

tab = widgets.Tab([base_map, natural_color_map, false_color_map])
tab.set_title(0, 'Basemap')
tab.set_title(1, 'Natural Color')
tab.set_title(2, 'False Color')
tab


# Thus using the same raster function, we were able to both visualize on the fly and derive a persisted image service using distributed raster analysis.


# ====================
# crime_analysis_and_clustering_using_geoanalytics_and_pyspark.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Crime analysis and clustering using geoanalytics and pyspark.ml

# * [Introduction](#Introduction)
# * [Necessary Imports](#Necessary-Imports)
# * [Connect to your ArcGIS Enterprise Organization](#Connect-to-your-ArcGIS-Enterprise-Organization)
# * [Ensure your GIS supports GeoAnalytics](#Ensure-your-GIS-supports-GeoAnalytics)
# * [Prepare the data](#Prepare-the-data)
#     * [Register a big data file share](#Register-a-big-data-file-share)
# * [Get data for analysis](#Get-data-for-analysis)
# * [Describe data](#Describe-data)
# * [Analyze patterns](#Analyze-patterns)
#     * [Aggregate points](#Aggregate-points)
#     * [Calculate density](#Calculate-density)
#     * [Find hot spots](#Find-hot-spots)
# * [Use Spark Dataframe and Run Python Script](#Use-Spark-Dataframe-and-Run-Python-Script)
#     * [Location of crime](#Location-of-crime)
#     * [Type of crime](#Type-of-crime)
#     * [Location of theft](#Location-of-theft)
#     * [Count of crime incidents by block group](#Count-of-crime-incidents-by-block-group)
#     * [Get crime types for a particular block group](#Get-crime-types-for-a-particular-block-group)
#     * [Crime distribution by the hour](#Crime-distribution-by-the-hour)
#     * [Big data machine learning using pyspark.ml](#Big-data-machine-learning-using-pyspark.ml)
#         * [Find the optimal number of clusters](#Find-the-optimal-number-of-clusters)
#         * [K-Means clustering](#K-Means-Clustering)
# * [Conclusion](#Conclusion)

# ## Introduction

# Many of the poorest neighborhoods in the City of Chicago face violent crimes. With rapid increase in crime, amount of crime data is also increasing. Thus, there is a strong need to identify crime patterns in order to reduce its occurrence. Data mining using some of the most powerful tools available in ArcGIS API for Python is an effective way to analyze and detect patterns in data. Through this sample, we will demonstrate the utility of a number of geoanalytics tools including ``find_hot_spots``, ``aggregate_points`` and ``calculate_density`` to visually understand geographical patterns. 
# 

# 

# The `pyspark module` available through ``run_python_script`` tool provides a collection of distributed analysis tools for data management, clustering, regression, and more. The ``run_python_script`` task automatically imports the `pyspark module` so you can directly interact with it. By calling this implementation of k-means in the ``run_python_script`` tool, we will cluster crime data into a predefined number of clusters. Such clusters are also useful in identifying crime patterns. 
# 
# Further, based on the results of the analysis, the segmented crime map can be used to help efficiently dispatch officers throughout a city.

# ## Necessary Imports

# In[1]:


get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime as dt

import arcgis
import arcgis.geoanalytics
from arcgis.gis import GIS
from arcgis.geoanalytics.summarize_data import describe_dataset, aggregate_points
from arcgis.geoanalytics.analyze_patterns import calculate_density, find_hot_spots
from arcgis.geoanalytics.manage_data import clip_layer, run_python_script


# ## Connect to your ArcGIS Enterprise Organization

# In[2]:


agol_gis = GIS('home')
gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# ## Ensure your GIS supports GeoAnalytics

# Before executing a tool, we need to ensure an ArcGIS Enterprise GIS is set up with a licensed GeoAnalytics server. To do so, call the [is_supported()](https://developers.arcgis.com/python/api-reference/arcgis.geoanalytics.toc.html#is-supported) method after connecting to your Enterprise portal. See the [Components of ArcGIS URLs](http://enterprise.arcgis.com/en/portal/latest/administer/linux/components-of-arcgis-urls.htm) documentation for details on the urls to enter in the [GIS](https://developers.arcgis.com/python/api-reference/arcgis.gis.toc.html#gis) parameters based on your particular Enterprise configuration.

# In[3]:


arcgis.geoanalytics.is_supported()


# ## Prepare the data

# To register a file share or an HDFS, we need to format datasets as subfolders within a single parent folder and register the parent folder. This parent folder becomes a datastore, and each subfolder becomes a dataset. Our folder hierarchy would look like below:

# 

# Learn more about preparing your big data file share datasets [here](https://enterprise.arcgis.com/en/server/latest/get-started/windows/what-is-a-big-data-file-share.htm).

# 
# ### Register a big data file share

# The `get_datastores()` method of the geoanalytics module returns a `DatastoreManager` object that lets you search for and manage the big data file share items as Python API `Datastore` objects on your GeoAnalytics server.

# In[4]:


bigdata_datastore_manager = arcgis.geoanalytics.get_datastores()
bigdata_datastore_manager


# We will register chicago crime data as a big data file share using the `add_bigdata()` function on a `DatastoreManager` object. 
# 
# When we register a directory, all subdirectories under the specified folder are also registered with the server. Always register the parent folder (for example, \\machinename\mydatashare) that contains one or more individual dataset folders as the big data file share item. To learn more, see [register a big data file share](https://enterprise.arcgis.com/en/server/latest/manage-data/windows/registering-your-data-with-arcgis-server-using-manager.htm#ESRI_SECTION1_0D55682C9D6E48E7857852A9E2D5D189).
# 
# Note: 
# You cannot browse directories in ArcGIS Server Manager. You must provide the full path to the folder you want to register, for example, \\myserver\share\bigdata. Avoid using local paths, such as C:\bigdata, unless the same data folder is available on all nodes of the server site.

# In[5]:


# data_item = bigdata_datastore_manager.add_bigdata("Chicago_Crime_2001_2020", r"\\machine_name\data\chicago")


# In[6]:


bigdata_fileshares = bigdata_datastore_manager.search(id='0e7a861d-c1c5-4acc-869d-05d2cebbdbee')
bigdata_fileshares


# In[7]:


file_share_folder = bigdata_fileshares[0]


# Once a big data file share is created, the GeoAnalytics server samples the datasets to generate a [manifest](https://enterprise.arcgis.com/en/server/latest/get-started/windows/understanding-the-big-data-file-share-manifest.htm), which outlines the data schema and specifies any time and geometry fields. A query of the resulting manifest returns each dataset's schema. This process can take a few minutes depending on the size of your data. Once processed, querying the manifest property returns the schema of the datasets in your big data file share.

# In[8]:


manifest = file_share_folder.manifest['datasets'][1]
manifest


# ## Get data for analysis

# Adding a big data file share to the Geoanalytics server adds a corresponding big data file share item on the portal. We can search for these types of items using the ``item_type`` parameter.

# In[9]:


search_result = gis.content.search("bigDataFileShares_GA_Data", item_type = "big data file share")
search_result


# In[12]:


ga_item = search_result[0]


# In[13]:


ga_item


# Querying the layers property of the [item](https://developers.arcgis.com/python/api-reference/arcgis.gis.toc.html#item) returns a featureLayer representing the data. The object is actually an API Layer object.

# In[14]:


ga_item.layers


# In[15]:


crime_lyr = ga_item.layers[1]


# In[21]:


illinois_blk_grps = agol_gis.content.get('a11d886be35149cb9dab0f7aac75a2af')


# In[22]:


illinois_blk_grps


# In[23]:


blk_lyr = illinois_blk_grps.layers[0]


# We will filter the blockgroups by 031 code which is county code for Chicago.

# In[24]:


blk_lyr.filter = "COUNTYFP = '031'"


# In[17]:


m2 = gis.map('chicago')
m2


# In[18]:


m2.add_layer(blk_lyr)


# ## Describe data

# The `describe_dataset` method provides an overview of big data. By default, the [tool](https://developers.arcgis.com/python/api-reference/arcgis.features.toc.html#table) outputs a table layer containing calculated field statistics and a dict outlining geometry and time settings for the input layer.
# 
# Optionally, the tool can output a feature layer representing a sample set of features using the `sample_size` parameter, or a single polygon feature layer representing the input feature layers' extent by setting the `extent_output parameter` to True.

# In[19]:


description = describe_dataset(input_layer=crime_lyr,
                               extent_output=True,
                               sample_size=1000,
                               output_name="Description of crime data" + str(dt.now().microsecond),
                               return_tuple=True)


# In[20]:


description.output_json


# In[21]:


sdf_desc_output = description.output.query(as_df=True)
sdf_desc_output.head()


# In[22]:


description.sample_layer


# In[23]:


sdf_slyr = description.sample_layer.query(as_df=True)
sdf_slyr.head()


# In[24]:


m1 = gis.map('chicago')
m1


# In[25]:


m1.add_layer(description.sample_layer)


# In[26]:


m1.legend = True


# ## Analyze patterns

# The GeoAnalytics Tools use a process spatial reference during execution. Analyses with square or hexagon bins require a projected coordinate system. We'll use 26771 as seen from http://epsg.io/?q=illinois%20kind%3APROJCRS.

# In[27]:


arcgis.env.process_spatial_reference = 26771 


# ### Aggregate points

# We can use the `aggregate_points` method in the `arcgis.geoanalytics.summarize_data` submodule to group call features into individual block group features. The output polygon feature layer summarizes attribute information for all calls that fall within each block group. If no calls fall within a block group, that block group will not appear in the output.
# 
# The GeoAnalytics Tools use a [process spatial reference](https://developers.arcgis.com/rest/services-reference/process-spatial-reference.htm) during execution. Analyses with square or hexagon bins require a projected coordinate system. We'll use the World Cylindrical Equal Area projection (WKID 54034) below. All results are stored in the spatiotemporal datastore of the Enterprise in the WGS 84 Spatial Reference.
# 
# See the GeoAnalytics Documentation for a full explanation of [analysis environment settings](https://enterprise.arcgis.com/en/portal/latest/use/geoanalyticstool-useenvironmentsettings.htm).

# In[28]:


agg_result = aggregate_points(crime_lyr, 
                              polygon_layer=blk_lyr,
                              output_name="aggregate results of crime" + str(dt.now().microsecond))


# In[29]:


agg_result


# In[30]:


m3 = gis.map('chicago')
m3


# In[31]:


m3.add_layer(agg_result)


# In[32]:


m3.legend = True


# ### Calculate density

# The [`calculate_density`](https://developers.arcgis.com/python/api-reference/arcgis.geoanalytics.analyze_patterns.html#calculate-density) method creates a density map from point features by spreading known quantities of some phenomenon (represented as attributes of the points) across the map. The result is a layer of areas classified from least dense to most dense. In this example, we will create density map by aggregating points within a bin of 1 kilometer. To learn more. please see [here](https://developers.arcgis.com/rest/services-reference/calculate-density-geoanalytics.htm).

# In[33]:


cal_density = calculate_density(crime_lyr,
                                weight='Uniform',
                                bin_type='Square',
                                bin_size=1,
                                bin_size_unit="Kilometers",
                                time_step_interval=1,
                                time_step_interval_unit="Years",
                                time_step_repeat_interval=1,
                                time_step_repeat_interval_unit="Months",
                                time_step_reference=dt(2001, 1, 1),
                                radius=1000,
                                radius_unit="Meters",
                                area_units='SquareKilometers',
                                output_name="calculate density of crime" + str(dt.now().microsecond))


# In[34]:


m4 = gis.map('chicago')
m4


# In[35]:


m4.add_layer(cal_density)


# In[36]:


m4.legend = True


# The `find_hot_spots` tool analyzes point data and finds statistically significant spatial clustering of high (hot spots) and low (cold spots) numbers of incidents relative to the overall distribution of the data.

# ### Find hot spots

# The `find_hot_spots` tool analyzes point data and finds statistically significant spatial clustering of high (hot spots) and low (cold spots) numbers of incidents relative to the overall distribution of the data.

# In[37]:


hot_spots = find_hot_spots(crime_lyr, 
                           bin_size=100,
                           bin_size_unit='Meters',
                           neighborhood_distance=250,
                           neighborhood_distance_unit='Meters',
                           output_name="get hot spot areas of crime" + str(dt.now().microsecond))


# In[38]:


m5 = gis.map('chicago')
m5


# In[39]:


m5.add_layer(hot_spots)


# In[40]:


m5.legend = True


# The darkest red features indicate areas where you can state with 99 percent confidence that the clustering of crime features is not the result of random chance but rather of some other variable that might be worth investigating. Similarly, the darkest blue features indicate that the lack of crime incidents is most likely not just random, but with 90% certainty you can state it is because of some variable in those locations. Features that are beige do not represent statistically significant clustering; the number of crimes could very likely be the result of random processes and random chance in those areas.

# ## Use Spark Dataframe and Run Python Script

# The `run_python_script` method executes a Python script directly in an ArcGIS GeoAnalytics server site . The script can create an analysis pipeline by chaining together multiple GeoAnalytics tools without writing intermediate results to a data store. The tool can also distribute Python functionality across the GeoAnalytics server site.
# 
# Geoanalytics Server installs a Python 3.6 environment that this tool uses. The environment includes `Spark 2.2.0`, the compute platform that distributes analysis across multiple cores of one or more machines in your GeoAnalytics Server site. The environment includes the `pyspark module` which provides a collection of distributed analysis tools for data management, clustering, regression, and more. The `run_python_script` task automatically imports the `pyspark module` so you can directly interact with it.
# 
# When using the `geoanalytics` and pyspark packages, most functions return analysis results as Spark DataFrame memory structures. You can write these data frames to a data store or process them in a script. This lets you chain multiple geoanalytics and pyspark tools while only writing out the final result, eliminating the need to create any bulky intermediate result layers. For more details, click [here](https://developers.arcgis.com/rest/services-reference/using-webgis-layers-in-pyspark.htm).

# The **Location Description** field represents areas with the most common crime locations. We will write a function to group our data by location description. This will help us count the number of crimes occurring at each location type.

# In[41]:


def groupby_description():
    from datetime import datetime as dt
    # crime data is stored in a feature service and accessed as a DataFrame via the layers object
    df = layers[0]
    # group the dataframe by Location Description field and count the number of crimes for each Location Description. 
    out = df.groupBy('Location Description').count()
    # Write the final result to our datastore.
    out.write.format("webgis").save("groupby_location_description" + str(dt.now().microsecond))


# In[42]:


run_python_script(code=groupby_description, layers=[crime_lyr])


# The result is saved as a feature layer. We can Search for the saved item using the `search()` method. Providing the search keyword same as the name we used for writing the result will retrieve the layer.

# In[43]:


groupby_description = gis.content.search('groupby_location_description')[0]
groupby_description_lyr = groupby_description.tables[0] #retrieve table from the item
groupby_description_df = groupby_description_lyr.query(as_df=True) #read layer as dataframe
groupby_description_df.sort_values(by='count', ascending=False, inplace=True) #sort count field in decreasing order


# ### Location of crime

# In[44]:


groupby_description_df[:10].plot(x='Location_Description', 
                                 y='count', kind='barh')
plt.xticks(
    rotation=45,
    horizontalalignment='center',
    fontweight='light',
    fontsize='medium',
);


# Street is the most frequent location for crime occurrance. 

# The **Primary Type** field contains the type for the crime. Let's investigate the most frequent type of crime in the Chicago by writing our own function:

# In[45]:


def groupby_texttype():
    from datetime import datetime as dt
    # crime data is stored in a feature service and accessed as a DataFrame via the layers object
    df = layers[0]
    # group the dataframe by TextType field and count the crime incidents for each crime type. 
    out = df.groupBy('Primary Type').count()
    # Write the final result to our datastore.
    out.write.format("webgis").save("groupby_type_of_crime" + str(dt.now().microsecond))


# In[46]:


run_python_script(code=groupby_texttype, layers=[crime_lyr])


# In[47]:


groupby_texttype = gis.content.search('groupby_type_of_crime')[0]


# In[48]:


groupby_texttype


# In[49]:


groupby_texttype_df = groupby_texttype.tables[0].query(as_df=True)


# In[50]:


groupby_texttype_df.head()


# In[51]:


groupby_texttype_df.sort_values(by='count', ascending=False, inplace=True)


# ### Type of crime

# In[52]:


groupby_texttype_df.head(10).plot(x='Primary_Type', y='count', kind='barh')
plt.xticks(
    rotation=45,
    horizontalalignment='center',
    fontweight='light',
    fontsize='medium',
);


# Theft is the most common type of crime in the city of Chicago.

# In[53]:


theft = groupby_texttype_df[groupby_texttype_df['Primary_Type'] == 'THEFT']


# In[54]:


theft


# In[55]:


def theft_description():
    from datetime import datetime as dt
    # crime data is stored in a feature service and accessed as a DataFrame via the layers object
    df = layers[0]
    df[df['Primary Type'] == 'THEFT']
    out = df.groupBy('Location Description').count()
    # Write the final result to our datastore.
    out.write.format("webgis").save("theft_description" + str(dt.now().microsecond))


# In[56]:


run_python_script(code=theft_description, layers=[crime_lyr])


# In[57]:


theft_description = gis.content.search('theft_description')[0]


# In[58]:


theft_description_df = theft_description.tables[0].query(as_df=True)


# In[59]:


theft_description_df.sort_values(by='count', ascending=False, inplace=True)


# ### Location of theft

# In[60]:


theft_description_df[:10].plot(x='Location_Description', y='count', kind='barh')
plt.xticks(
    rotation=45,
    horizontalalignment='center',
    fontweight='light',
    fontsize='medium',
);


# This plot shows the relation between crime type and crime location. It indicates that most of the theft activities occur on streets.

# In[61]:


def grpby_type_blkgrp():
    from datetime import datetime as dt
    # Load the big data file share layer into a DataFrame
    df = layers[0]
    out = df.groupBy('Primary Type', 'Block').count()
    out.write.format("webgis").save("grpby_type_blkgrp" + str(dt.now().microsecond))


# In[62]:


run_python_script(code=grpby_type_blkgrp, layers=[crime_lyr])


# In[63]:


grpby_cat_blk = gis.content.search('grpby_type_blkgrp')[0]


# In[64]:


grpby_cat_blk


# In[65]:


grpby_cat_blk_df = grpby_cat_blk.tables[0].query(as_df=True)


# In[66]:


grpby_cat_blk_df.head()


# ### Count of crime incidents by block group

# In[67]:


grpby_cat_blk_df.sort_values(by='count', ascending=False, inplace=True)


# In[68]:


grpby_cat_blk_df.head(10).plot(x='Block', y='count', kind='barh')
plt.xticks(
    rotation=45,
    horizontalalignment='center',
    fontweight='light',
    fontsize='medium',
);


# ### Get crime types for a particular block group

# In[69]:


blk_addr_high = grpby_cat_blk_df[grpby_cat_blk_df['Block'] == '001XX N STATE ST']


# In[70]:


blk_addr_high.Primary_Type.sort_values(ascending=False).head()


# In[71]:


def crime_by_datetime():
    from datetime import datetime as dt
    # Load the big data file share layer into a DataFrame
    from pyspark.sql import functions as F
    df = layers[0]
    out = df.withColumn('datetime', F.unix_timestamp('Date', 'dd/MM/yyyy hh:mm:ss a').cast('timestamp'))
    out.write.format("webgis").save("crime_by_datetime" + str(dt.now().microsecond))


# In[72]:


run_python_script(code=crime_by_datetime, layers=[crime_lyr])


# In[73]:


calls_with_datetime = gis.content.search('crime_by_datetime')[0]


# In[74]:


calls_with_datetime_lyr = calls_with_datetime.layers[0]


# In[75]:


def crime_with_added_date_time_cols():
    from datetime import datetime as dt
    # Load the big data file share layer into a DataFrame
    from pyspark.sql.functions import year, month, hour
    df = layers[0]
    df = df.withColumn('month', month(df['datetime']))
    out = df.withColumn('hour', hour(df['datetime']))
    out.write.format("webgis").save("crime_with_added_date_time_cols" + str(dt.now().microsecond))


# In[76]:


run_python_script(code=crime_with_added_date_time_cols, layers=[calls_with_datetime_lyr])


# In[77]:


date_time_added_item = gis.content.search('crime_with_added_date_time_cols')


# In[78]:


date_time_added_lyr = date_time_added_item[0].layers[0]


# In[79]:


def grp_crime_by_hour():
    from datetime import datetime as dt
    # Load the big data file share layer into a DataFrame
    df = layers[0]
    out = df.groupBy('hour').count()
    out.write.format("webgis").save("grp_crime_by_hour" + str(dt.now().microsecond))


# In[80]:


run_python_script(code=grp_crime_by_hour, layers=[date_time_added_lyr])


# In[81]:


hour = gis.content.search('grp_crime_by_hour')[0]


# In[82]:


grp_hour = hour.tables[0]


# In[83]:


df_hour = grp_hour.query(as_df=True)


# ### Crime distribution by the hour

# In[84]:


(df_hour
 .dropna()
 .sort_values(by='hour')
 .astype({'hour' : int})
 .plot(x='hour', y='count', kind='bar'))
plt.xticks(
    rotation=45,
    horizontalalignment='center',
    fontweight='light',
    fontsize='medium',
);


# This graph shows that the crime activities are more common at the peak hours 12 A.M. and 12 P.M.

# ### Big data machine learning using pyspark.ml

# #### Find the optimal number of clusters

# The average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering. To learn more about silhouette analysis, click [here](https://runawayhorse001.github.io/LearningApacheSpark/clustering.html). 

# In[85]:


def optimal_k():
    import time
    import numpy as np
    import pandas as pd
    from pyspark.ml.feature import VectorAssembler
    from pyspark.ml.clustering import KMeans
    from datetime import datetime as dt
    from pyspark.ml.evaluation import ClusteringEvaluator
    from pyspark.sql.context import SQLContext
    from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, FloatType

    silh_lst = []
    k_lst = np.arange(3, 70)

    crime_locations = layers[0]
    assembler = VectorAssembler(inputCols=["X Coordinate", "Y Coordinate"], outputCol="features")
    crime_locations = assembler.setHandleInvalid("skip").transform(crime_locations)
    
    for k in k_lst:
        silh_val = []
        for run in np.arange(1, 3):
            # Trains a k-means model.
            kmeans = KMeans().setK(int(k)).setSeed(int(np.random.randint(100, size=1)))
            model = kmeans.fit(crime_locations.select("features"))

            # Make predictions
            predictions = model.transform(crime_locations)

            # Evaluate clustering by computing Silhouette score
            evaluator = ClusteringEvaluator()
            silhouette = evaluator.evaluate(predictions)
            silh_val.append(silhouette)

        silh_array=np.asanyarray(silh_val)
        silh_lst.append(silh_array.mean())        

    silhouette = pd.DataFrame(list(zip(k_lst,silh_lst)),columns = ['k', 'silhouette'])
    schema = StructType([StructField('k',IntegerType(),True), StructField('silhouette',FloatType(),True)])
    out = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark).createDataFrame(silhouette, schema)
    # Write the result DataFrame to the relational data store
    out.write.format("webgis").option("dataStore","relational").save("optimalKmeans" + str(dt.now().microsecond))


# In[86]:


run_python_script(code=optimal_k, layers=[crime_lyr])


# In[87]:


optimal_k = gis.content.search('optimalKmeans')[0]


# In[88]:


optimal_k_tbl = optimal_k.tables[0]


# In[89]:


k_df = optimal_k_tbl.query().sdf


# In[90]:


k_df.sort_values(by='silhouette', ascending=False)


# In[91]:


num_clusters = k_df.sort_values(by='silhouette', ascending=False).loc[0]['k']
num_clusters


# #### K-Means Clustering

# In[92]:


def cluster_crimes():
    
    from pyspark.ml.feature import VectorAssembler
    from pyspark.ml.clustering import KMeans
    from datetime import datetime as dt
    # Crime data is stored in a feature service and accessed as a DataFrame via the layers object
    crime_locations = layers[0]
    
    # Combine the x and y columns in the DataFrame into a single column called "features"
    assembler = VectorAssembler(inputCols=["X Coordinate", "Y Coordinate"], outputCol="features")
    crime_locations = assembler.setHandleInvalid("skip").transform(crime_locations)

    # Fit a k-means model with 15 clusters using the "features" column of the crime locations
    kmeans = KMeans(k=15)
    model = kmeans.fit(crime_locations.select("features"))
    
    cost = model.computeCost(crime_locations)
    # Add the cluster labels from the k-means model to the original DataFrame
    crime_locations_clusters = model.transform(crime_locations)
    # Write the result DataFrame to the relational data store
    crime_locations_clusters.write.format("webgis").save("Crime_Clusters_KMeans" + str(dt.now().microsecond))


# In[93]:


run_python_script(code=cluster_crimes, layers=[crime_lyr])


# In[94]:


clusters = gis.content.search('Crime_Clusters_KMeans')[0]


# In[95]:


clusters


# 

# By symbolizing on the predictions made by the k-means model, we can visualize the clustered crime events as shown in the screen shot above.

# ### Conclusion

# In this sample, we have covered how to chain together geoanalytics and pyspark tools in order to analyze big data, while only writing out the final result to a data store, eliminating the need to create any intermediate result layers. We have really gained a lot of knowledge about the use of data mining and clustering to help manage huge amount of data and deduce useful information from criminal data. 


# ====================
# detecting-airplanes-on-satellite-imagery-using-deep-learning.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Detecting airplanes in satellite imagery using deep learning

# ## Table of Contents

# * [Prerequisites](#9)
# * [Introduction](#1)
# * [Necessary Imports](#2)
# * [Visualize training data](#3)
# * [Train a model](#4)
# * [Detect and visualize airplanes in validation set](#5)
# * [Model inference in ArcGIS Pro](#6)
# * [Conclusion](#7)
# * [References](#8)

# ## Prerequisites <a class="anchor" id="9"></a>

# The dataset we used in this notebook is downloaded from [RarePlanes](https://www.cosmiqworks.org/rareplanes/). **RarePlanes** is an open-source machine learning dataset from **CosmiQ Works** and **[AI.Reverie](https://aireverie.com/)** that incorporates both real and synthetically generated satellite imagery. This data is licensed under the [CC-BY-SA 4.0 license](https://creativecommons.org/licenses/by-sa/4.0/). We are going to use real training data that contains 5,815 training images. The images have dimensions of 512x512 and a resolution of 30cm. Tiled images have also been put into an images folder, and the annotations have been converted to the "PASCAL_VOC_rectangles" format and have been put into a labels folder. The dataset can be downloaded in the necessary format [here](https://geosaurus.maps.arcgis.com/home/item.html?id=9982c9a749f94f7797e82358a90527c1).

# ## Introduction <a class="anchor" id="1"></a>

# The goal of this notebook is to demonstrate how we can use the `FasterRCNN` model to detect airplanes in satellite imagery. Airplane detection is a valuable process across multiple industries, as it allows for the identification and monitoring of airports at a global scale.

# ## Necessary Imports <a class="anchor" id="2"></a>

# In[ ]:


from arcgis import learn
from arcgis.gis import GIS


# We will now use the `prepare_data()` function to apply various types of transformations and augmentations to the training data. These augmentations enable us to train a better model with limited data, as well as better prevent the overfitting of the model.
# 
# This dataset contains multiple classes of planes, however, we are only interested in detecting planes and not classifying their categories. So, we will map the planes to one class using `class_mapping`.
# 
# This function returns a data object that can be fed into a model for training.

# In[ ]:


data = learn.prepare_data(path="airplane_chips", dataset_type="PASCAL_VOC_rectangles",
                          class_mapping={**dict.fromkeys(['1', '2', '3', '4', '5', '6', '7'],'airplane')})


# ## Visualize training data <a class="anchor" id="3"></a>

# In[ ]:


data.show_batch()


# ## Train a model <a class="anchor" id="4"></a>

# arcgis.learn provides the following deep learning algorithms that we can use to detect airplanes:
# 
# - [SSD](https://developers.arcgis.com/python/guide/how-ssd-works/)
# - [YOLOv3](https://developers.arcgis.com/python/guide/yolov3-object-detector/)
# - [Faster R-CNN](https://developers.arcgis.com/python/guide/faster-rcnn-object-detector/)
# - [RetinaNet](https://developers.arcgis.com/python/guide/how-retinanet-works/)
# 
# Here, we are going to use `FasterRCNN`

# In[ ]:


model = learn.FasterRCNN(data)


# Next, we can simply call the `model.fit()` method and pass a number of epochs to it. The model will then begin to train after finding the optimal learning rate itself.

# In[ ]:


model.fit(15)


# ## Detect and visualize airplanes on the validation set <a class="anchor" id="5"></a>

# Now that we have the trained model, we will examine how the model performs on data it has not yet seen.

# In[ ]:


model.show_results()


# As we can see, with only 15 epochs, we are already seeing reasonable results. Further improvement can be achieved through more training and sophisticated hyperparameter tuning.

# We will now save the model for further training or later inferences. By default, the model should save into a models folder in your data folder.

# In[ ]:


model.save("airplane_15ep")


# ## Model inference in ArcGIS Pro <a class="anchor" id="6"></a>

# Next, we will use the saved model to detect airplanes using the Detect Objects Using Deep Learning tool available in both ArcGIS Pro and ArcGIS Image Server. For this sample, we will deploy the model on the Amsterdam airport region.
# 
# - Input Raster: Imagery
# - Output Detected Objects: Detected_airplanes
# - Model Definition: airplane_15ep.emd
# - Extent: Extent of the region where model will run.
# - Cell Size: The model is very susceptible to the cell size. This model works well with cell sizes 0.5 to 1
# 
# 

# 

# The Detect Objects Using Deep Learning tool will return a vector layer with the airplanes detected in the chosen region, as seen below.

# 

# The trained model is provided [here](https://geosaurus.maps.arcgis.com/home/item.html?id=c0897e960591464e9b417f399a3d6d78). You can download the model and perform inferencing in ArcGIS Pro.

# ## Conclusion <a class="anchor" id="7"></a>

# In this notebook, we demonstrated how to use the Faster R-CNN model from the ArcGIS API for Python to detect airplanes in satellite imagery.

# ## References <a class="anchor" id="8"></a>

# [1] Dataset source https://www.cosmiqworks.org/rareplanes/


# ====================
# detecting-deforestation-using-kmeans-clustering-on-sentinel-imagery.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Detecting deforestation in the Amazon rainforest using unsupervised K-means clustering on satellite imagery

# ## Table of Contents <a class="anchor" id="0"></a>
# * [Introduction](#1) 
# * [Imports](#2)
# * [Connecting to ArcGIS](#3)
# * [Accessing & Visualizing the datasets](#4) 
# * [Data Preparation](#5) 
# * [Model Building](#9)
#     * [Data Preprocessing](#11)  
#     * [Model Initialization ](#12)
#     * [Learning Rate Search ](#13)
#     * [Model Training ](#14) 
#     * [Result Visualization](#16)
# * [Conclusion](#23)
# * [Summary of methods used](#24)
# * [Data resources](#25)

# ## Introduction <a class="anchor" id="1"></a>

# Deforestation around the world has reached a critical level, causing irreversible damage to environmental sustainability that is contributing to climate change around the world. Widespread forest fires, from the Amazon Basin in Brazil, to the west coast of the United States, are raging all year-round. This notebook will allow us to detect deforested areas in the Brazilian Amazon rainforest, using satellite imagery.

# ## Imports <a class="anchor" id="2"></a>

# In[1]:


get_ipython().run_line_magic('matplotlib', 'inline')

import pandas as pd
from datetime import datetime
from IPython.display import Image
from IPython.display import HTML
import matplotlib.pyplot as plt

from sklearn.preprocessing import MinMaxScaler
from datetime import datetime as dt

import arcgis
from arcgis.gis import GIS
from arcgis.learn import MLModel, prepare_tabulardata
from arcgis.raster import Raster

from fastai.vision import *


# ## Connecting to ArcGIS <a class="anchor" id="3"></a>

# In[2]:


gis = GIS("home")   
gis_enterp = GIS("https://pythonapi.playground.esri.com/portal", "arcgis_python", "amazing_arcgis_123")


# ## Accessing & Visualizing datasets  <a class="anchor" id="4"></a>
# 
# Here, we use Sentinel-2 imagery, which has a high resolution of 10m and 13 bands. This imagery is accessed from the ArcGIS Enterprise portal, where it is sourced from the AWS collection. 

# In[3]:


# get image
s2 = gis.content.get('fd61b9e0c69c4e14bebd50a9a968348c')
sentinel = s2.layers[0]
s2


# ## Data Preparation  <a class="anchor" id="5"></a>

# ## Define Area of Interest in the Amazon
# The area of interest is defined using the four latitude and longitude values from a certain region of the Amazon rainforest where a considerable area of forest has been deforested, as can be seen from the images above.

# In[4]:


#  extent in 3857 for amazon rainforest
amazon_extent = {
    "xmin": -6589488.51,
    "ymin": -325145.08,
    "xmax": -6586199.09,
    "ymax": -327024.74,
    "spatialReference": {"wkid": 3857}
}


# Here, we select all the scenes from the sentinel imagery containing the area of interest for our study.

# In[5]:


# The respective scene having the above area is selected
selected = sentinel.filter_by(where="(Category = 1) AND (cloudcover <=0.05)",
                              geometry=arcgis.geometry.filters.intersects(amazon_extent))
df = selected.query(out_fields="AcquisitionDate, GroupName, CloudCover, DayOfYear",
                   order_by_fields="AcquisitionDate").sdf

df['AcquisitionDate'] = pd.to_datetime(df['acquisitiondate'], unit='ms')
df


# The satellite imagery with the least cloud cover is selected and visualized for further processing.

# In[6]:


# The scene is selected with the least cloud cover and extracted using the amazon extent
amazon_scene = sentinel.filter_by('OBJECTID=1584818')
amazon_scene.extent = amazon_extent
amazon_scene


# In the above scene, the brown patches are the deforested areas that are to be identified. This selected scene is then published to the portal.

# In[7]:


# publish the scene to the portal
amazon_scene.save('amazon_scene'+ str(dt.now().microsecond), gis=gis_enterp)


# The published imagery of the Amazon rainforest is exported back to an image file on disk for further processing.

# In[8]:


raster_amazon_13bands = Raster("https://pythonapi.playground.esri.com/ra/rest/services/Hosted/amazon_scene_may26/ImageServer",
                               gis=gis_enterp,
                               engine="image_server")


# In[9]:


# visualizing the image 
raster_amazon_13bands.export_image(size=[3330,1880])


# ## Model Building <a class="anchor" id="9"></a>

# The first part of model building consists of defining the preprocessors, which will be used to scale the bands before feeding them into the model. The band names use the conventional naming method of the imagery name with an id number appended at the end as follows:

# Sentinel-2 imagery has 13 bands, of which 4 bands, namely the blue, green, red, and near infrared bands, we will use here for modelling. These bands work well for differentiating green forested areas from barren land. The band information, along with the band name and their respective ids, are obtained for selecting the required bands.    

# In[10]:


# get the band names and their ids, sentinel images have 13 bands
pd.DataFrame(amazon_scene.key_properties()['BandProperties'])


# In[11]:


# get the imagery name to define the band names
raster_amazon_13bands.name


# Here, the imagery name is 'Hosted/amazon_scene_april9'. Subsequently, the names of the blue, green, red, and near infrared bands would be 'Hosted/amazon_scene_april9_1', 'Hosted/amazon_scene_april9_2', 'Hosted/amazon_scene_april9_3', 'Hosted/amazon_scene_april9_7' respectively. These bands will be used for defining the preprocessors. 

# ### Data Pre-processing <a class="anchor" id="11"></a>

# In[12]:


from sklearn.preprocessing import MinMaxScaler


# The four bands are listed in the preprocessors for scaling, with the last item as the designated scaler as follows. 

# In[13]:


preprocessors = [('Hosted/amazon_scene_may26_1', 
                  'Hosted/amazon_scene_may26_2', 
                  'Hosted/amazon_scene_may26_3',
                  'Hosted/amazon_scene_may26_7', MinMaxScaler())]


# Here, in the explanatory raster, we pass the name of the explanatory raster and the selected bands by their id's — 1 for blue, 2 for green, 3 for red, and 7 for NIR, as follows:

# In[14]:


# Data is prepared for the MLModel using the selected scene and the preprocessors
data = prepare_tabulardata(explanatory_rasters=[(raster_amazon_13bands,(1,2,3,7))], preprocessors=preprocessors)


# In[15]:


# visualization of the data to be processed by the model
data.show_batch()


# ### Model Initialization <a class="anchor" id="12"></a>
# 
# Once the data is prepared, an unsupervised model of k-means clustering from scikit-learn is used for clustering the pixels into deforested areas and forested areas. The clustering model is passed inside MLModel as follows, with the number of clusters set as three in the parameters.

# In[16]:


from arcgis.learn import MLModel, prepare_tabulardata


# In[17]:


model = MLModel(data, 'sklearn.cluster.KMeans', n_clusters=3, init='k-means++', random_state=43)


# ### Model Training <a class="anchor" id="14"></a>
# Now the model is ready to be trained, and will label the pixels as being one of three classes, either forested, slightly deforested, or highly deforested.  

# In[18]:


# here model is trained which would label the pixels into the designated classes 
model.fit()


# In[19]:


# the labelled pixels can be visualized as follows with the last column returning the predicted labels by the model 
model.show_results()


# ### Deforestation Clusters Prediction<a class="anchor" id="21"></a>
# 
# Next, the trained model is used to predict clusters within the entire selected scene of the Amazon rainforest. This is passed as the explanatory raster, with the prediction type as raster and a local path provided for output. The `output_layer_name` parameter can also be used for publishing. 

# In[20]:


pred_new = model.predict(explanatory_rasters=[raster_amazon_13bands],
                         prediction_type='raster',
                         output_layer_name=('deforest_predicted2'+str(dt.now().microsecond)),
                         output_raster_path=r"/tmp/result5.tif")


# ## Result Visualization<a class="anchor" id="16"></a>
# 
# The resulting raster with the predicted classes of deforested areas and forested areas is now read back for visualization. The predicted local raster can be accessed here.

# In[21]:


amazon_predict = gis.content.get('b81b89aac4cd4e08bcd7cd400fac558f')
amazon_predict


# In[22]:


import os, zipfile


# In[23]:


filepath_new = amazon_predict.download(file_name=amazon_predict.name)
with zipfile.ZipFile(filepath_new, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath_new).parent)
output_path = Path(os.path.join(os.path.splitext(filepath_new)[0]))
output_path = os.path.join(output_path, "result5.tif")    


# In[24]:


raster_predict = Raster(output_path)


# In[25]:


raster_predict.export_image(size=[3290,1880])


# The model has correctly labelled the deforested areas in white, distinguishing them from the rest of the forested areas in black.
# 
# The boundaries of the detected deforested areas could be further extracted into polygons using the convert raster to feature function.

# ## Conclusion<a class="anchor" id="23"></a>
# 
# In this sample notebook we were able to detect deforestation in the Amazon rainforest using the unsupervised model of k-means clustering on satellite imagery. This was implemented via the MLModel framework, which exhibited the application of several unsupervised models from the scikit learn library on raster imagery.

# ### Summary of methods used <a class="anchor" id="24"></a>

# | Method | Description | Examples |
# | -| - |-|
# | prepare_tabulardata| prepare data including imputation, normalization and train-test split  |prepare data ready for fitting a  MLModel 
# | MLModel() | select the ML algorithm to be used for fitting  | any supervised and unsupervised models from scikit learn can be used
# | model.fit() | train a model   | training the unsupervised model with suitable input 
# | model.score() | find the appropriate model metric of the trained model  | returns suitable value after training the unsupervised MLModel
# | model.predict() | predict on a test set | predict values using the trained models on the trained data itself

# ### Data resources <a class="anchor" id="25"></a>

# | Dataset | Source | Link |
# | -| - |-|
# | sat imagery| sentinel2  |https://registry.opendata.aws/sentinel-2/|

# In[ ]:






# ====================
# detecting_mussel_farms_using_deep_learning.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Detecting Mussel Farms using Deep Learning
# > * 🔬 Data Science
# * 🥠 Deep Learning and Object Detection

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Export training data](#Export-training-data)
# * [Train the model](#Train-the-model)
#     * [Necessary imports](#Necessary-imports)
#     * [Get training data](#Get-training-data)
#     * [Prepare data](#Prepare-data)
#     * [Visualize training data](#Visualize-training-data)
#     * [Load model architecture](#Load-model-architecture)
#     * [Find an optimal learning rate](#Find-an-optimal-learning-rate)
#     * [Fit the model](#Fit-the-model)
#     * [Visualize results in validation set](#Visualize-results-in-validation-set)
#     * [Accuracy assessment](#Accuracy-assessment)
#     * [Save the model](#Save-the-model)
# * [Deploy model and detect mussel farms](#Deploy-model-and-detect-mussel-farms)
#     * [Model builder](#Model-builder)
#     * [Model inference](#Model-inference)
# * [Conclusion](#Conclusion)

# ## Introduction

# Mussels are aquatic animals with bivalved hard shells that are consumed by millions of people around the world. Mussels grow in fresh water rivers or lakes near their openings to saline waters of the ocean, as well as in some coastal intertidal regions. Mussel aquaculture involves floating rafts that have ropes suspended in the water on which the mussels are cultured. The farmers collect mussel seeds from nearby rocky shores during low tides and attach them to the ropes in their rafts. Later, these ropes are covered with mesh socks that collect the mussels once they are ripe.

# <figure>
#     <br>
#     <center>
#     <figcaption>Mussels growing on a rocky shore </figcaption>
#     </center>
# </figure>

# Spain is one of the biggest producer of mussels in the world. The Galicia region in Spain accounts for almost 90 percent of the mussel aquaculture in the country. Our study area for this notebook is Ria de Arousa, which is the biggest mussel farming area in Galicia region.

# <figure>
#     <br>
#     <center>
#     <figcaption>Mussel Farms in Ria de Arousa, Spain </figcaption>
#     </center>
# </figure>

# Monitoring of such aquacultures is an important aspect of maintaining the aquatic ecosystems. While sustainable farming can help the ecosystem, it's exploitation can degrade the environment quality and biodiversity. Therefore, it is important to monitor the growth of mussel farms in a region and their expansion into fragile areas. While physical surveys can be arduous and time consuming, satellite imagery and deep learning can help in monitoring mussel farming with much less effort. These analysis can also be helpful in comparing changes over longer periods of time.

# In this notebook, we will train a deep learning model to detect mussel farms in high-resolution imagery of the Ria De Arousa region of Spain.

# ## Export training data

# In[10]:


# Connect to GIS
from arcgis.gis import GIS
gis = GIS("home")


# The following imagery layer contains high resolution imagery of a part of the Ria De Arousa region. The spatial resolution of the imagery is 30 cm, and it contains 3 bands: Red, Green, and Blue. It is used as the 'Input Raster' for exporting the training data.

# In[2]:


training_raster = gis.content.get('f2b92eed10394e5eb3c7f135861937d9')
training_raster


# The following feature layer contains the bounding boxes for a few mussel farms in the Ria de Arousa region. It is used as the 'Input Feature Class' for exporting the training data.

# In[3]:


training_feature_layer = gis.content.get('ff6a48b3391c4a24b807af0eb08bb6c1')
training_feature_layer


# Training data can be exported by using the 'Export Training Data For Deep Learning' tool available in [ArcGIS Pro](https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/export-training-data-for-deep-learning.htm) and [ArcGIS Enterprise](https://developers.arcgis.com/rest/services-reference/export-training-data-for-deep-learning.htm). For this example, we prepared the training data in the 'PASCAL Visual Object Classes' format, using a 'chip_size' of 448px and a 'cell_size' of 0.3m, in ArcGIS Pro. The 'Input Raster' and the 'Input Feature Class' have been made available to export the required training data. We have also provided the exported training data in the next section, if you wish to skip this step.

# <figure>
#     <br>
#     <center>
#     <figcaption>Export Training Data for Deep Learning </figcaption>
#     </center>
# </figure> 

# ## Train the model

# ### Necessary imports

# In[1]:


import os
import glob
import zipfile
from pathlib import Path
from arcgis.learn import prepare_data, MMDetection


# ### Get training data

# We have already exported the data that can be directly used by following the steps below:

# In[7]:


training_data = gis.content.get('57cb821dedca4c5598e81c8d2d510c91')
training_data


# In[5]:


filepath = training_data.download(file_name=training_data.name)


# In[6]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[7]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# ### Prepare data
# 
# We will specify the path to our training data and a few hyperparameters. 
# 
# - `path`: path of the folder/list of folders containing training data.
# - `batch_size`: Number of images your model will train on each step inside an epoch. Depends on the memory of your graphic card.
# - `chip_size`: The same as the tile size used while exporting the dataset.

# In[4]:


data = prepare_data(path, batch_size=4, chip_size=448)


# ### Visualize training data
# 
# To get a sense of what the training data looks like, the `show_batch()` method will randomly pick a few training chips and visualize them.

# In[7]:


data.show_batch(rows=2)


# ### Load model architecture

# Through the integration of the MMDetection library, `arcgis.learn` allows the use of the Dynamic RCNN model, along with many other models. For more in-depth information on how to use MMDetection, please see [Use MMDetection with arcgis.learn](https://developers.arcgis.com/python/guide/use-mmdetection-with-arcgis-learn/).

# In[28]:


MMDetection.supported_models


# In[8]:


model = MMDetection(data, 'dynamic_rcnn')


# ### Find an optimal learning rate

# Learning rate is one of the most important hyperparameters in model training. The ArcGIS API for Python provides a learning rate finder that automatically chooses the optimal learning rate for you.

# In[5]:


lr = model.lr_find()
lr


# ### Fit the model 

# Next, we will train the model for a few epochs with the learning rate found above. Given the small size of the training dataset, we can train the model for 10 epochs.

# In[9]:


model.fit(10, lr=lr)


# As we can see, the training and validation losses were decreasing until the 9th epoch, before increasing slightly in the last epoch. As such, there could be room for more training.

# ### Visualize results in validation set

# It is a good practice to see the results of the model viz-a-viz ground truth. The code below picks random samples and shows us ground truth and model predictions, side by side. This enables us to preview the results of the model we trained.

# In[11]:


model.show_results()


# ### Accuracy assessment

# `arcgis.learn` provides the `average_precision_score()` method that computes the average precision of the model on the validation set for each class.

# In[12]:


model.average_precision_score()


# ### Save the model

# We will save the trained model as a 'Deep Learning Package' ('.dlpk' format). The Deep Learning package is the standard format used to deploy deep learning models on the ArcGIS platform. 
# 
# We will use the `save()` method to save the trained model. By default, it will be saved to the 'models' sub-folder within our training data folder.

# In[ ]:


model.save('musselfarms_mmd_dynamic_rcnn_10ep')


# ## Deploy the model and detect mussel farms

# We can now use the saved model to detect mussel farms in the entire Ria De Arousa region. Here, we have provided only a sample raster for Ria De Arousa. The following imagery layer contains high resolution imagery of a part of the Ria De Arousa region. The spatial resolution of the imagery is 30 cm and contains 3 bands: Red, Green, and Blue.

# In[11]:


sample_inference_raster = gis.content.get('d6f035f5de504c86855e0ee70e83ad0e')
sample_inference_raster


# ### Model builder

# To save time and resources, we will only run the model on the water body and not the surrounding land masses, as the mussel farms are only found in water. To extract a raster with only the water body, we created a water mask using the Sentinel-2 Views NDWI. The following model builder can be used to create a water mask and detect the mussel farms.

# In[4]:


model_builder = gis.content.get('2647a386f5a04917b74cc4f40a48f57f')
model_builder


# Tools used:
# - Copy Raster: Copies a region of the [Sentinel-2 Views NDWI](https://www.arcgis.com/home/item.html?id=112db40d3640473aacb0d1f891462496) hosted imagery and creates a subset to be processed further.
# - Greater Than Equal: Creates a binary raster with water pixels as 1 and others as 0.
# - Reclassify: Reclassifies the other pixels (value 0) in the binary raster as 'No Data'.
# - Raster to Polygon: Converts the raster into a feature layer with polygons representing the water mask.
# - Fill Gaps: Fills gaps smaller than 1500 square meters (approximate maximum area of a mussel farm) in the water mask.
# - Extract By Mask: Uses the water mask to clip the original raster and create a new raster containing only the water body.
# - Detect Objects Using Deep learning: Detects mussel farms on the clipped raster using the model we trained earlier.

# <figure>
#     <br>
#     <center>
#     <figcaption> Model Builder </figcaption>
#     </center>
# </figure> 

# We can use the following step if we want to detect mussel farms in any given imagery containing only the water body, without running the model builder. In this step, we will generate a feature layer with detected mussel farms using the 'Detect Objects Using Deep Learning' tool available in both [ArcGIS Pro](https://pro.arcgis.com/en/pro-app/latest/tool-reference/image-analyst/detect-objects-using-deep-learning.htm) and [ArcGIS Enterprise](https://developers.arcgis.com/rest/services-reference/enterprise/detect-objects-using-deep-learning.htm).

# <figure>
#     <br>
#     <center>
#     <figcaption>Detect Objects Using Deep Learning </figcaption>
#     </center>
# </figure>

# ### Results

# The model was run on the entire Ria De Arousa region and the results can be viewed here.

# In[6]:


fc = gis.content.get('477d756f79e4400fa91c5b220406d98c')
fc


# In[11]:


from arcgis.mapping import WebMap
wm_item = gis.content.get('36714d14f80649e99aaf702f3cec6455')


# In[ ]:


wm = WebMap(wm_item)
wm.add_layer(fc)
wm


# <figure>
# </figure> 

# ## Conclusion

# In this notebook, we saw how we can use deep learning and high-resolution satellite imagery to detect mussel farms. This can be an important task for monitoring and conservation purposes. We used a small sample as training data with one of the object detection models available through the MMDetection integration in `arcgis.learn`. We trained the deep learning model for a few iterations and then deployed it to detect all the mussel farms in the Ria De Arousa region. To save time and resources we used a model builder that clipped out a raster containing only the water body. This clipped raster was used to detect the farms. The results are highly accurate and almost all the mussel farms in the region have been detected.


# ====================
# detecting_palm_trees_using_deep_learning.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Detecting Palm Trees using Deep Learning
# 
# - 🔬 Data Science
# - 🥠 Deep Learning and Object Detection

# Coconuts and coconut products are an important commodity in the Tongan economy. Plantations, such as one in the town of Kolovai, have thousands of trees. Inventorying each of these trees by hand would require a lot of time and resources. Alternatively, tree health and location can be surveyed using remote sensing and deep learning.
# 
# In this notebook, we will train a deep learning model to detect palm trees in high-resolution imagery of the Kolovai region using the `arcgis.learn` module of the `ArGIS API for Python`.
# 

# 

# ## Export training data

# The first step is to find imagery that shows Kolovai, Tonga, and has a fine enough spatial and spectral resolution to identify trees. Once we have the imagery, we'll export training samples to a format that can be used by a deep learning model.

# ### Download the imagery

# Accurate and high-resolution imagery is essential when extracting features. The model will only be able to identify the palm trees if the pixel size is small enough to distinguish palm canopies. Additionally, to calculate tree health, we'll need an image with spectral bands that will enable you to generate a vegetation health index. You'll find and download the imagery for this study from OpenAerialMap, an open-source repository of high-resolution, multispectral imagery.
# 
# - Go to the [OpenAerialMap website](https://openaerialmap.org/).
# 
#     In the interactive map view, you can zoom, pan, and search for imagery available anywhere on the planet. The map is broken up into grids. When you point to a grid box, a number appears. This number indicates the number of available images for that box.
# 
# 
# - In the search box, type Kolovai and press Enter. In the list of results, click Kolovai.
# The map zooms to Kolovai. This is a town on the main island of Tongatapu with a coconut plantation.
# 
# 
# - If necessary, zoom out until you see the label for Kolovai on the map. Click the grid box directly over Kolovai.

# 

# - In the side pane, click Kolovai UAV4R Subset (OSM-Fit) by Cristiano Giovando.

# 

# - Click the download button to download the raw .tif file. Save the image to a location of your choice.
# 

# 

# Because of the file size, download may take a few minutes. The default name of the file is 5b1b6fb2-5024-4681-a175-9b667174f48c.

# The spatial resolution of the kolovai imagery is 9 cm, and it contains 3 bands: Red, Green, and Blue. It is used as the 'Input Raster' for exporting the training data. 

# ### Get palm labels

# In[1]:


# Connect to GIS
from arcgis.gis import GIS
gis = GIS("home")


# The following feature layaer collection contains 2 layers, labelled palm trees of a part of Kolovai region and a mask  that delineates the area where image chips will be created.  

# In[10]:


palm_label = gis.content.get('1bc2daa8960340ee92ea68ddb35ab4d4')
palm_label


# Training data can be exported by using the [`Export Training Data For Deep Learning`](https://pro.arcgis.com/en/pro-app/latest/tool-reference/image-analyst/export-training-data-for-deep-learning.htm) tool available in [ArcGIS Pro](https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/export-training-data-for-deep-learning.htm). For this example, we prepared the training data in the 'PASCAL Visual Object Classes' format, using a 'chip_size' of 448px and a 'cell_size' of 0.085, in ArcGIS Pro. The 'Input Raster' and the 'Input Feature Class' have been made available to export the required training data. We have also provided the exported training data in the next section, if you wish to skip this step.
# 
# 

# 

# ## Train DetREG model

# ### Necessary imports

# In[2]:


import os
import zipfile
from pathlib import Path
from arcgis.learn import prepare_data, DETReg


# ### Get training data

# In[15]:


training_data = gis.content.get('e7878a3fab0a400f9665d800972395f1')
training_data


# In[16]:


filepath = training_data.download(file_name=training_data.name)


# In[17]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[18]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))
data_path


# - `path`: path of the folder/list of folders containing training data.
# - `batch_size`: Number of images your model will train on each step inside an epoch. Depends on the memory of your graphic card.
# - `chip_size`: The same as the tile size used while exporting the dataset.
# - `class_mapping`: map label id to string

# In[3]:


data = prepare_data(data_path, batch_size=8, chip_size=448, class_mapping={'1': 'palm'})


# ### Visualize training data

# To get a sense of what the training data looks like, the `show_batch()` method will randomly pick a few training chips and visualize them.

# In[22]:


data.show_batch()


# In[23]:


data.classes


# ### Load model architecture

# `DetREG` model pretrains the entire object detection network, including the object localization and embedding components. During pretraining, `DETReg` predicts object localizations to match the localizations from an unsupervised region proposal generator and simultaneously aligns the corresponding feature embeddings with embeddings from a self-supervised image encoder. Through the integration of `DetREG` model in `argis.learn`, we could train a deep learning model with small training data ~50 images. 

# In[4]:


detreg_model = DETReg(data)


# In[6]:


detreg_model.lr_find()


# We will train the model for a few epochs with the learning rate 2e-5. 

# In[25]:


detreg_model.fit(epochs=100, lr=0.0001584893192461114)


# As we can see, the training and validation losses were decreasing until the 99th epoch, there could be room for more training.

# In[26]:


detreg_model.plot_losses()


# ### Visualize results in validation set

# It is a good practice to see the results of the model viz-a-viz ground truth. The code below picks random samples and shows us ground truth and model predictions, side by side. This enables us to preview the results of the model we trained.

# In[28]:


detreg_model.show_results(thresh=0.4)


# ### Accuracy assessment

# arcgis.learn provides the `average_precision_score()` method that computes the average precision of the model on the validation set for each class.

# In[27]:


detreg_model.average_precision_score()


# ### Save the model

# We will save the trained model as a 'Deep Learning Package' ('.dlpk' format). The Deep Learning package is the standard format used to deploy deep learning models on the ArcGIS platform.
# 
# We will use the `save()` method to save the trained model. By default, it will be saved to the 'models' sub-folder within our training data folder.

# In[24]:


detreg_model.save('palm_e100', publish=True)


# ## Detect palm trees with the trained deep learning model

# The bulk of the work in extracting features from imagery is preparing the data, creating training samples, and training the model. Now that these steps have been completed, we'll use the [trained model](https://www.arcgis.com/home/item.html?id=9406080ccb6b499b9e2651c7b36f969d) to detect palm trees in the desired imagery. Object detection is a process that typically requires multiple tests to achieve the best results. There are several parameters that you can alter to allow your model to perform best. To test these parameters quickly, we'll try detecting trees in a small section of the image. Once you're satisfied with the results, we'll extend the detection tools to the full image.

# 

# In[35]:


fc = gis.content.get('4ca014288f834385a7b97a2c4534b57d')
fc


# 

# ## Conclusion

# In this notebook, we saw how we can use `DetREG` deep learning model and high-resolution satellite imagery to detect palm tree. This can be an important task for monitoring and conservation purposes. We used only handful of images as training data and trained a pretty good modl with `DetREG` available in arcgis.learn. We trained the deep learning model for a few iterations and then deployed it to detect all the palm trees in the Kolovai imagery. The results are highly accurate and almost all the palm trees in the region have been detected.


# ====================
# detecting_settlements_using_supervised_classification_and_deep_learning.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Detecting settlements using supervised classification and deep learning

# - 🔬 Data Science
# - 🥠 Deep Learning and pixel-based classification

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Necessary imports](#Necessary-imports)
# * [Connect to your GIS](#Connect-to-your-GIS)
# * [Get the data for analysis](#Get-the-data-for-analysis)
# * [Filter satellite Imageries based on cloud cover and time duration](#Filter-satellite-Imageries-based-on-cloud-cover-and-time-duration)
# * [Visualize the mosaic and training points on map](#Visualize-the-mosaic-and-training-points-on-map)
# * [Classification](#Classification)
#  * [Mask settlements out of the classified raster](#Mask-settlements-out-of-the-classified-raster)
# * [Deep Learning](#Deep-Learning)
#  * [Export training data](#Export-training-data)
#  * [Prepare data](#Prepare-data)
#  * [Visualize training data](#Visualize-training-data)
#  * [Load model architecture](#Load-model-architecture)
#  * [Train the model](#Train-the-model)
#  * [Save the model](#Save-the-model)
#  * [Load an intermediate model to train it further](#Load-an-intermediate-model-to-train-it-further)
#  * [Preview results](#Preview-results)
# * [Deploy model and extract settlements](#Deploy-model-and-extract-settlements)
#  * [Generate a classified raster](#Generate-a-classified-raster)
# * [Conclusion](#Conclusion)
# * [References](#References)

# ## Introduction

# Deep-Learning methods tend to perform well with high amounts of data as compared to machine learning methods which is one of the drawbacks of these models. These methods have also been used in geospatial domain to detect objects [1,2] and land use classification [3] which showed favourable results, but labelled input satellite data has always been an effortful task. In that regards, in this notebook we have attempted to use the supervised classification approach to generate the required volumes of data which after cleaning was used to come through the requirement of larger training data for Deep Learning model.
# 
# For this, we have considered detecting settlements for Saharanpur district in Uttar Pradesh, India. Settlements have their own importance to urban planners and monitoring them temporally can lay the foundation to design urban policies for any government. 

# 
# 

# ## Necessary imports

# In[1]:


get_ipython().run_line_magic('matplotlib', 'inline')

import pandas as pd
from datetime import datetime
import matplotlib.pyplot as plt

import arcgis
from arcgis.gis import GIS
from arcgis.geocoding import geocode
from arcgis.learn import prepare_data, UnetClassifier
from arcgis.raster.functions import clip, apply, extract_band, colormap, mask, stretch
from arcgis.raster.analytics import train_classifier, classify, list_datastore_content


# ## Connect to your GIS

# In[5]:


gis = GIS(url='https://pythonapi.playground.esri.com/portal', username='arcgis_python', password='amazing_arcgis_123')


# ## Get the data for analysis

# Search for <b>Multispectral Landsat</b> layer in ArcGIS Online. We can search for content shared by users outside our organization by setting <b>outside_org</b> to True.

# In[6]:


landsat_item = gis.content.search('title:Multispectral Landsat', 'Imagery Layer', outside_org=True)[0]
landsat = landsat_item.layers[0]
landsat_item


# Search for India State Boundaries 2018 layer in ArcGIS Online. This layer has all the District boundaries for India at index - 2.

# In[7]:


boundaries = gis.content.search('India District Boundaries 2018', 'Feature Layer', outside_org=True)[2]
boundaries


# In[8]:


district_boundaries = boundaries.layers[2]
district_boundaries


# In[1]:


m = gis.map('Saharanpur, India')
m.add_layer(district_boundaries)
m.legend = True


# As this notebook is to detect settlements for Saharanpur district, you can filter the boundary for Saharanpur.

# In[16]:


area = geocode("Saharanpur, India", out_sr=landsat.properties.spatialReference)[0]
landsat.extent = area['extent']


# We want to detect settlements for Saharanpur district, so we will apply a query to the boundary layer, by setting "OBJECTID = 09132". The code below brings imagery and feature layer on the same extent.

# In[9]:


saharanpur = district_boundaries.query(where='ID=09132')   # query for Saharanur district boundary
saharanpur_geom = saharanpur.features[0].geometry          # Extracting geometry of Saharanpur district boundary   
saharanpur_geom['spatialReference'] = {'wkid':4326}        # Set the Spatial Reference 
saharanpur.features[0].extent = area['extent']             # Set the extent


# Get the training points for training the classifier. These are 212 points in total marked against 5 different classes namely <b>Urban</b>, <b>Forest</b>, <b>Agricultural</b>, <b>Water</b> and <b>Wasteland</b>. These points are marked using ArcGIS pro and pulished on the gis server.

# In[10]:


data = gis.content.search('classificationPointsSaharanpur', 'Feature Layer')[0]
data


# ## Filter satellite Imagery based on cloud cover and time duration

# In order to produce good results, it is important to select cloud free imagery from the image collection for a specified time duration. In this example we have selected all the images captured between 1 October 2018 to 31 December 2018 with cloud cover less than or equal to 5% for Saharanpur region.

# In[23]:


selected = landsat.filter_by(where="(Category = 1) AND (cloudcover <=0.05)",
                             time=[datetime(2018, 10, 1), 
                                   datetime(2018, 12, 31)],
                             geometry=arcgis.geometry.filters.intersects(area))
df = selected.query(out_fields="AcquisitionDate, GroupName, CloudCover, DayOfYear", 
                    order_by_fields="AcquisitionDate").sdf
df['AcquisitionDate'] = pd.to_datetime(df['AcquisitionDate'], unit='ms')
df


# We can now select the image dated 02 October 2018 from the collection using its OBJECTID which is "2360748"

# In[24]:


saharanpur_image = landsat.filter_by('OBJECTID=2360748') # 2018-10-02 


# In[27]:


m = gis.map('Saharanpur, India')
m.add_layer(apply(saharanpur_image, 'Natural Color with DRA'))
m.add_layer(saharanpur)
m.legend = True
m


# As our area of interest is larger and can not be enclosed within the tile we have chosen,  let's make a mosaic of two images to enclose the AOI. Mosaicking two rasters with OBJECTID - 2360748 and 2365229 using predefined [mosaic_by](https://developers.arcgis.com/python/api-reference/arcgis.raster.toc.html?highlight=mosaic_by#arcgis.raster.ImageryLayer.mosaic_by) function.

# In[32]:


landsat.mosaic_by(method="LockRaster",lock_rasters=[2360748,2365229])
mosaic = apply(landsat, 'Natural Color with DRA')
mosaic


# We will clip the mosaicked raster using the boundary of Saharanpur which is our area of interest using [clip](https://developers.arcgis.com/python/api-reference/arcgis.raster.functions.html?highlight=clip#clip) function.

# In[30]:


saharanpur_clip = clip(landsat, saharanpur_geom)
saharanpur_clip.extent = area
arcgis.env.analysis_extent = area
saharanpur_clip


# ## Visualize the mosaic and training points on map

# In[28]:


m = gis.map('Saharanpur, India')
m.add_layer(mosaic)
m.add_layer(saharanpur)
m.add_layer(data)
m.legend = True
m


# ## Classification

# With the Landsat layer and the training samples, we are now ready to train a support vector machine (SVM) model using [train_classifier](https://developers.arcgis.com/python/api-reference/arcgis.raster.analytics.html?highlight=train_classifier#arcgis.raster.analytics.train_classifier) function.

# In[ ]:


classifier_definition = train_classifier(input_raster=saharanpur_clip, 
                                         input_training_sample_json=data.layers[0].query().to_json, 
                                         classifier_parameters={"method":"svm",
                                                                "params":{"maxSampleClass":100}},
                                         gis=gis)


# Now we have the model, we are ready to apply the model to a landsat image of Saharanpur district to make prediction. This classifier will classify each pixel of the image into 5 different classes namely <b>Urban</b>, <b>Forest</b>, <b>Agricultural</b>, <b>Water</b> and <b>Wasteland</b>. We will use predefined [classify](https://developers.arcgis.com/python/api-reference/arcgis.raster.analytics.html?highlight=classify#arcgis.raster.analytics.classify) function to perform the classification.

# In[ ]:


classified_output = classify(input_raster=saharanpur_clip, 
                             input_classifier_definition=classifier_definition)


# Now let's visualize the classification result based on the colormap defined below.

# In[ ]:


cmap = colormap(classified_output.layers[0],
                 colormap=[[0, 255, 0, 0],
                          [1, 15, 242, 45],
                          [2, 255, 240, 10],
                          [3, 0, 0, 255],
                          [4, 176, 176, 167]])


# In[45]:


map2 = gis.map('Saharanpur, India')
map2.add_layer(cmap)
map2.legend = True
map2


# Here, '0', '1', '2', '3' and '4' represents pixels from <b>'Urban'</b>, <b>'Forest'</b>, <b>'Agricultural'</b>, <b>'Water'</b> and <b>'Wasteland'</b> class respectively.
# 
# We can see that our model is performing reasonably well and is able to clearly identify the urban settlements while comparing the prediction with the Landsat image.

# ### Mask settlements out of the classified raster

# We have a classified raster which has each of its pixel classified against 5 different classes. But we require the pixels belonging to <b>Urban</b> class only, so we will mask all the pixels belonging to that class.

# In[ ]:


classified_output = gis.content.search('Classify_AS81ZD')[0]
classified_output


# As we are concerned only with detecting settlement pixels, we will mask out others from the classified raster by selecting the pixels with a class value of '0' which represents <b>'Urban'</b> class.

# In[47]:


masked = colormap(mask(classified_output.layers[0],
                       no_data_values =[],
                       included_ranges =[0,0]),
                  colormap=[[0, 255, 0, 0]],
                  astype='u8')
masked.extent = area
masked


# We can save the masked raster containing the settlements to an imagery layer. This uses distributed raster analytics and performs the processing at the source pixel level and creates a new raster product.

# In[ ]:


masked = masked.save()


# We can now Convert the masked pixels to polygons using <b>'to_features'</b> function.

# In[ ]:


urban_item = masked.layers[0].to_features(output_name='masked_polygons' + str(datetime.now().microsecond) ,
                                          output_type='Polygon',
                                          field='Value',
                                          simplify=True,
                                          gis=gis)
urban_layer = urban_item.layers[0]


# In[48]:


m2 = gis.map('Saharanpur, India')
m2.legend = True
m2.add_layer(urban_item)
m2


# The settlement polygons have many false positives - mainly there are areas near river bed that are confused with settlements. 
# 
# These false positives were removed by post processing using ArcGIS Pro. The result obtained after cleaning up the supervised classification are shown below, which are the final detected settlements.

# In[ ]:


settlement_poly = gis.content.search('settlement_polygons')[0]
settlement_poly


# ## Deep Learning

# The deep learning approach for classification needs more training data. We will use the results that we obtained after cleaning up false positives from SVM predictions above, and exported training chips against each of those polygons using ArcGIS pro to train a deep learning model.

# ### Export training data

# Export training data using 'Export Training data for deep learning' tool, detailed documentation here
# 
# - Set Landsat imagery as 'Input Raster'.
# - Set a location where you want to export the training data, it can be an existing folder or the tool will create that for you.
# - Set the settlement training polygons as input to the 'Input Feature Class Or Classified Raster' parameter.
# - In the option 'Input Mask Polygons' we can set a mask layer to limit the tool to export training data for only those areas which have buildings in it, we created one by generating a grid of 200m by 200m on Urban Polygons layer's extent and dissolving only those polygons which contained settlements to a single multipolygon feature.
# - 'Tile Size X' & 'Tile Size Y' can be set to 224.
# - Select 'Classified Tiles' as the 'Meta Data Format' because we are training an 'Unet Model'.
# - In 'Environments' tab set an optimum 'Cell Size'. For this example, as we have performing the analysis on Landsat8 imagery, we used 30 cell size which meant 30m on a project coordinate system.

# 
# 

# ### Prepare data

# We would specify the path to our training data and a few hyper parameters.
# 
# - `path`: path of folder containing training data.
# - `batch_size`: No of images your model will train on each step inside an epoch, it directly depends on the memory of your graphic card. 8 worked for us on a 11GB GPU.
# - `imagery_type`: It is a mandatory input to enable model for multispectral data processing. It can be "landsat8", "sentinel2", "naip", "ms" or "multispectral".

# In[2]:


import os
from pathlib import Path

gis = GIS('home')


# In[3]:


training_data = gis.content.get('b9602a576cd54bc6a8e0279a290537af')
training_data


# In[4]:


filepath = training_data.download(file_name=training_data.name)


# In[5]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[6]:


data_path = Path(os.path.join(filepath.split('.')[0]))


# In[9]:


# Prepare Data
data = prepare_data(data_path,
                    batch_size=8,
                    imagery_type='landsat8')


# For our model, we want out of the 11 bands in Landsat 8 image, weights for the RGB bands to be initialized using ImageNet weights whereas the other bands should have their weights initialized using the red band values. We can do this by setting the "type_init" parameter in the environment as "red band".

# In[50]:


arcgis.env.type_init = 'red_band'


# ### Visualize training data

# The code below shows a few samples of our data with the same symbology as in ArcGIS Pro.
# 
# - `rows`: No. of rows we want to see the results for.
# - `alpha`: controls the opacity of labels (Classified imagery) over the background imagery
# - `statistics_type`: for dynamic range adjustment

# In[52]:


data.show_batch(rows=2, alpha=0.7, statistics_type='DRA')


# Here, the light blue patches show the labelled settlements.

# ### Load model architecture

# We will be using U-net [4], one of the well recognized image segmentation algorithm, for our land cover classification. U-Net is designed like an auto-encoder. It has an encoding path (“contracting”) paired with a decoding path (“expanding”) which gives it the “U” shape. However, in contrast to the autoencoder, U-Net predicts a pixelwise segmentation map of the input image rather than classifying the input image as a whole. For each pixel in the original image, it asks the question: “To which class does this pixel belong?”. U-Net passes the feature maps from each level of the contracting path over to the analogous level in the expanding path. These are similar to residual connections in a ResNet type model, and allow the classifier to consider features at various scales and complexities to make its decision.

# In[ ]:


# Create Unet Model
model = UnetClassifier(data)


# ### Train the model

# By default, the U-Net model uses ImageNet weights which are trained on 3 band imagery but we are using the multispectral support of ArcGIS API for Python to train the model for detecting settlements on 11 band Landsat 8 imagery, so unfreezing will make the backbone trainable so that it can learn to extract features from multispectral data. 
# 
# Before training the model, let's unfreeze it to train it from scratch. 

# In[54]:


model.unfreeze()


# Learning rate is one of the most important hyperparameters in model training. Here, we explore a range of learning rate to guide us to choose the best one.

# In[55]:


# Find Learning Rate
model.lr_find()


# Based on the learning rate plot above, we can see that the learning rate suggested by `lr_find()` for our training data is 0.0002754228703338166. We can use it to train our model.

# We would fit the model for 100 epochs. One epoch mean the model will see the complete training set once.

# In[56]:


# Training
model.fit(epochs=100, lr=0.00022908676527677726);


# In[11]:


model.mIOU()


# Here, we can see loss on both training and validation set decreasing, which shows how well the model generalizes on unseen data thereby preventing overfitting. Also, as we can see our model here is able to classify pixels to settlement or non settlement with an accuracy 0.98. Once we are satisfied with the accuracy, we can save the model.

# ### Save the model

# We would now save the model which we just trained as a 'Deep Learning Package' or '.dlpk' format. Deep Learning package is the standard format used to deploy deep learning models on the ArcGIS platform. UnetClassifier models can be deployed by the tool 'Classify Pixels Using Deep Learning' available in ArcGIS Pro as well as ArcGIS Enterprise. For this sample, we will be using this model in ArcGIS Pro to extract settlements.
# 
# We will use the `save()` method to save the model and by default it will be saved to a folder 'models' inside our training data folder itself.

# In[ ]:


model.save('100e')


# ### Load an intermediate model to train it further

# If we need to further train an already saved model we can load it again using the code below and repeat the earlier mentioned steps of training.

# In[ ]:


# Load Model from previous saved files 
# model.load('100e')


# ### Preview results

# The code below will pick a few random samples and show us ground truth and respective model predictions side by side. This allows us to validate the results of your model in the notebook itself. Once satisfied we can save the model and use it further in our workflow.

# In[57]:


model.show_results(rows=2)


# ### Deploy model and extract settlements

# The model saved in previous step can be used to extract classified raster using <b>'Classify Pixels Using Deep Learning'</b> tool. Further the classified raster is regularised and finally converted to a vector Polygon layer. The regularisation step uses advanced ArcGIS geoprocessing tools to remove unwanted artifacts in the output.

# ### Generate a classified raster

# In this step, we will generate a classified raster using <b>'Classify Pixels Using Deep Learning'</b> tool available in both ArcGIS Pro and ArcGIS Enterprise.
# 
# - `Input Raster`: The raster layer from which we want to extract settlements from.
# - `Model Definition`: It will be located inside the saved model in 'models' folder in '.emd' format.
# - `Padding`: The 'Input Raster' is tiled and the deep learning model classifies each individual tile separately before producing the final 'Output Classified Raster'. This may lead to unwanted artifacts along the edges of each tile as the model has little context to predict accurately. Padding as the name suggests allows us to supply some extra information along the tile edges, this helps the model to predict better.
# - `Cell Size`: Should be close to at which we trained the model, we specified that at the Export training data step .
# - `Processor Type`: This allows to control whether the system's 'GPU' or 'CPU' would be used in to classify pixels, by 'default GPU' will be used if available.
# - `Parallel Processing Factor`: This allows us to scale this tool, this tool can be scaled on both 'CPU' and 'GPU'. It specifies that the operation would be spread across how many 'cpu cores' in case of cpu based operation or 'no of GPU's' in case of GPU based operation.

# 
# 

# Output of this tool will be in form of a 'classified raster' containing both background and Settlements. The pixels having background class are shown in green color and pixels belonging to settlement class are in purple color.

# 
# 
#                                     
#                                             A subset of preditions by our Model

# Change the symbology of the raster by setting the second class of the classified pixels to <b>'No color'</b>.

# 
# 

# After this step, we will get a masked raster which will look like the screenshot below.

# 
# 

# We can now run <b>'Raster to Polygon'</b> tool in ArcGIS Pro to convert this masked raster to polygons.

# 
# 

# Results obtained from Deep Learning approach are ready to be analyzed.

# 
# 
#                  Deep Learning Results

# ## Conclusion

# This notebook shows how traditional algorithms like Support Vector Machines can be used to generate the necessary quality data for Deep Learning models which after cleaning can serve as the training data required by a DL model for generating accurate results at a much larger extent. 

# ## References 

# [1] https://developers.arcgis.com/python/sample-notebooks/detecting-swimming-pools-using-satellite-image-and-deep-learning/ 
# 
# [2] https://developers.arcgis.com/python/sample-notebooks/extracting-building-footprints-from-drone-data/
# 
# [3] https://developers.arcgis.com/python/sample-notebooks/land-cover-classification-using-unet/
# 
# [4] Olaf Ronneberger, Philipp Fischer, Thomas Brox: U-Net: Convolutional Networks for Biomedical Image Segmentation, 2015; [arXiv:1505.04597](https://arxiv.org/abs/1505.04597).


# ====================
# detecting_swimming_pools_using_automated_machine_learning.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Detecting Swimming Pools using Automated Deep Learning
# * 🔬 Data Science
# * 🥠 Deep Learning and Object Detection

# ## Table of Contents
# * [Introduction and objective](#Introduction-and-objective)
# * [Part 1 - Export training data](#Part-1---Export-training-data)
# * [Part 2 - model training](#Part-2---model-training)
#     * [Necessary imports](#Necessary-imports)
#     * [Prepare data that will be used for training](#Prepare-data-that-will-be-used-for-training)
#     * [Visualize training data](#Visualize-training-data)
#     * [Load model architecture](#Load-model-architecture)
#     * [Train the model](#Train-the-model)
#     * [Detect and visualize swimming pools in validation set](#Detect-and-visualize-swimming-pools-in-validation-set)
#     * [Save the model](#Save-the-model) 
# * [Part 3 - Model inference](#Part-3---Model-inference)
#     * [Visualize detected pools on map](#Visualize-detected-pools-on-map)
# * [Conclusion](#Conclusion)
# * [References](#References)

# ## Introduction and objective

# Deep Learning has achieved great success with state of the art results, but taking it to the field and solving real-world problems is still a challenge. Integration of the latest research in AI with ArcGIS opens up a world of opportunities. This notebook demonstrates an end-to-end deep learning workflow in using `ArcGIS API for Python`. The workflow consists of three major steps: (1) extract training data, (2) train a deep learning object detection model, (3) deploy the model for inference and create maps. To better illustrate this process, we choose detecting swmming pools in Redlands, CA using remote sensing imagery.

# ## Part 1 - Export training data

# To export training data, we need a labeled feature class that contains the bounding box for each object, and a raster layer that contains all the pixels and band information. In this swimming pool detection case, we have created feature class by hand labelling the bounding box of each swimming pool in Redlands using ArcGIS Pro and USA NAIP Imagery: Color Infrared as raster data.

# In[72]:


from arcgis.gis import GIS
gis = GIS('home')
ent_gis = GIS('https://pythonapi.playground.esri.com/portal')


# In[70]:


pool_bb = gis.content.get('0da0026a3a6d47dc8da0bcff6cf5bfb2')
pool_bb


# In[73]:


naip_item = ent_gis.content.get('2f8f066d526e48afa9a942c675926785')
naip_item


# With the feature class and raster layer, we are now ready to export training data using the 'Export Training Data For Deep Learning' tool in arcgis Pro. In addtion to feature class, raster layer, and output folder, we also need to speficy a few other parameters such as tile size (size of the image chips), stride size (distance to move in the X when creating the next image chip), chip format (TIFF, PNG, or JPEG), metadata format (how we are going to store those bounding boxes). 
# 
# Depending on the size of your data, tile and stride size, and computing resources, this opertation can take 15mins~2hrs in our experiment. Also, do not re-run it if you already run it once unless you would like to update the setting.

# 

# ## Part 2 - model training

# If you've already done part 1, you should already have both the training chips and swimming pool labels. Please change the path to your own export training data folder that contains "images" and "labels" folder.

# ### Necessary imports

# In[2]:


import os
from pathlib import Path
from arcgis.gis import GIS
from arcgis.learn import prepare_data, AutoDL, ImageryModel


# In[3]:


training_data = gis.content.get('73a29df69b344ce8b94fdb4c9df7103d')
training_data


# In[4]:


filepath = training_data.download(file_name=training_data.name)


# In[5]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[6]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# ### Prepare data that will be used for training

# In[9]:


data = prepare_data(data_path, 
                    batch_size=4, 
                    chip_size=448,
                    class_mapping={'0': 'pool'})
data.classes


# ### Visualize training data
# To get a sense of what the training data looks like, `arcgis.learn.show_batch()` method randomly picks a few training chips and visualize them.

# In[10]:


get_ipython().run_cell_magic('time', '', 'data.show_batch()\n')


# ### Load model architecture
# 

# In[11]:


model = AutoDL(data, total_time_limit=2)


# In[12]:


get_ipython().run_cell_magic('time', '', 'model.fit()\n')


# In[16]:


score = model.average_precision_score()
score.sort_values(by='pool', ascending=False)


# In[17]:


from arcgis.learn import ImageryModel


# In[57]:


model = ImageryModel()


# In[58]:


model.load(r'C:\Users\pri10421\AppData\Local\Temp\detecting_swimming_pools_using_satellite_image_and_deep_learning\models\AutoDL_atss_resnet34\AutoDL_atss_resnet34.emd', 
           data)


# In[59]:


lr = model.lr_find()


# ### Train the model

# In[60]:


model.fit(epochs=10, lr=lr)


# In[61]:


model.average_precision_score()


# In[62]:


model.fit(epochs=10)


# In[63]:


model.average_precision_score()


# ### Detect and visualize swimming pools in validation set
# Now we have the model, let's look at how the model performs. Here we plot out 5 rows of images and a threshold of 0.2. Threshold is a measure of probablity that a swimming pool exists. Higher value meas more confidence.

# In[66]:


model.show_results(thresh=0.2)


# As we can see, with only 20 epochs, we are already seeing reasonable results. Further improvment can be acheived through more sophisticated hyperparameter tuning. Let's save the model for further training or inference later. The model should be saved into a models folder in your folder. By default, it will be saved into your `data_path` that you specified in the very beginning of this notebook.

# In[64]:


model.save('PoolDetection_USA_20')


# ## Part 3 -  Model inference
# To test our model, let's get a raster image with some swimming pools.

# 

# ## Visualize detected pools on map

# In[79]:


predicted_result = gis.content.get('793d2060d14746d19ee4c45d3eda7724')
predicted_result


# In[88]:


result_map = gis.map('Redlands, CA')
result_map.add_layer(naip_item.layers[0])
result_map.add_layer(predicted_result.layers[0])
result_map = {'spatialReference': {'latestWkid': 3857, 'wkid': 102100},
              'xmin': -13044535.370791622,'ymin': 4045062.583115232,
              'xmax': -13042184.932171792,'ymax': 4046018.045968822}
result_map


# ## Conclusion
# In thise notebook, we have covered a lot of ground. In part 1, we discussed how to export training data for deep learning using ArcGIS Pro, we demonstrated how to prepare the input data, train a object detection model, visualize the results, as well as apply the model to an unseen image using the Detect Objects Using Deep Learning tool in ArcGIS Pro. 
# 
# ## References
# [1]
# Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu: “SSD: Single Shot MultiBox Detector”, 2015; <a href='http://arxiv.org/abs/1512.02325'>arXiv:1512.02325</a>.


# ====================
# detecting_swimming_pools_using_satellite_image_and_deep_learning.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Detecting Swimming Pools using Satellite Imagery and Deep Learning
# > * 🔬 Data Science
# * 🥠 Deep Learning and Object Detection

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Prerequisites" data-toc-modified-id="Prerequisites-1">Prerequisites</a></span></li><li><span><a href="#Introduction-and-objective" data-toc-modified-id="Introduction-and-objective-2">Introduction and objective</a></span></li><li><span><a href="#Part-1---export-training-data-for-deep-learning" data-toc-modified-id="Part-1---export-training-data-for-deep-learning-3">Part 1 - export training data for deep learning</a></span><ul class="toc-item"><li><span><a href="#Import-ArcGIS-API-for-Python-and-get-connected-to-your-GIS" data-toc-modified-id="Import-ArcGIS-API-for-Python-and-get-connected-to-your-GIS-3.1">Import ArcGIS API for Python and get connected to your GIS</a></span></li><li><span><a href="#Prepare-data-that-will-be-used-for-training-data-export" data-toc-modified-id="Prepare-data-that-will-be-used-for-training-data-export-3.2">Prepare data that will be used for training data export</a></span></li><li><span><a href="#Specify-a-folder-name-in-raster-store-that-will-be-used-to-store-our-training-data" data-toc-modified-id="Specify-a-folder-name-in-raster-store-that-will-be-used-to-store-our-training-data-3.3">Specify a folder name in raster store that will be used to store our training data</a></span></li><li><span><a href="#Export-training-data-using-arcgis.learn" data-toc-modified-id="Export-training-data-using-arcgis.learn-3.4">Export training data using <code>arcgis.learn</code></a></span></li></ul></li><li><span><a href="#Part-2---model-training" data-toc-modified-id="Part-2---model-training-4">Part 2 - model training</a></span><ul class="toc-item"><li><span><a href="#Visualize-training-data" data-toc-modified-id="Visualize-training-data-4.1">Visualize training data</a></span></li><li><span><a href="#Load-model-architecture" data-toc-modified-id="Load-model-architecture-4.2">Load model architecture</a></span></li><li><span><a href="#Train-a-model-through-learning-rate-tuning-and-transfer-learning" data-toc-modified-id="Train-a-model-through-learning-rate-tuning-and-transfer-learning-4.3">Train a model through learning rate tuning and transfer learning</a></span></li><li><span><a href="#Detect-and-visualize-swimming-pools-in-validation-set" data-toc-modified-id="Detect-and-visualize-swimming-pools-in-validation-set-4.4">Detect and visualize swimming pools in validation set</a></span></li></ul></li><li><span><a href="#Part-3---deployment-and-inference" data-toc-modified-id="Part-3---deployment-and-inference-5">Part 3 - deployment and inference</a></span><ul class="toc-item"><li><span><a href="#Locate-model-package" data-toc-modified-id="Locate-model-package-5.1">Locate model package</a></span></li><li><span><a href="#Model-inference" data-toc-modified-id="Model-inference-5.2">Model inference</a></span></li></ul></li><li><span><a href="#Visualize-detected-pools-on-map" data-toc-modified-id="Visualize-detected-pools-on-map-6">Visualize detected pools on map</a></span></li><li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-7">Conclusion</a></span></li><li><span><a href="#References" data-toc-modified-id="References-8">References</a></span></li></ul></div>

# ## Prerequisites
# 
# - Please refer to the prerequisites section in our [guide](https://developers.arcgis.com/python/guide/geospatial-deep-learning/) for more information. This sample demonstrates how to do export training data and model inference using [ArcGIS Image Server](https://www.esri.com/en-us/arcgis/products/arcgis-image-server). Alternatively, they can be done using [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview) as well.
# - If you have already exported training samples using ArcGIS Pro, you can jump straight to the training section. The saved model can also be imported into ArcGIS Pro directly.

# ## Introduction and objective

# Deep Learning has achieved great success with state of the art results, but taking it to the field and solving real-world problems is still a challenge. Integration of the latest research in AI with ArcGIS opens up a world of opportunities. This notebook demonstrates an end-to-end deep learning workflow in using ArcGIS API for Python. The workflow consists of three major steps: (1) extracting training data, (2) train a deep learning object detection model, (3) deploy the model for inference and create maps. To better illustrate this process, we choose detecting swmming pools in Redlands, CA using remote sensing imagery.

# ## Part 1 - export training data for deep learning

# ### Import ArcGIS API for Python and get connected to your GIS

# In[1]:


import arcgis
import sys
from arcgis import GIS, learn
from arcgis.raster import analytics
from arcgis.raster.functions import extract_band, apply, clip
from arcgis.raster.analytics import list_datastore_content
from arcgis.learn import SingleShotDetector, prepare_data, Model, list_models, detect_objects 
arcgis.env.verbose = True
from arcgis.geocoding import geocode


# In[2]:


gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# ### Prepare data that will be used for training data export

# To export training data, we need a _labeled feature class_ that contains the bounding box for each object, and a _raster layer_ that contains all the pixels and band information. In this swimming pool detection case, we have created feature class by hand labelling the bounding box of each swimming pool in Redlands using ArcGIS Pro and USA NAIP Imagery: Color Infrared as raster data.

# In[3]:


pool_bb = gis.content.get('3443553a9e8443eeb570ef1b973e44a8')
pool_bb


# In[4]:


pool_bb_layer = pool_bb.layers[0]
pool_bb_layer.url


# In[5]:


m = gis.map("Prospect Park, Redlands, CA")
m


# 

# In[6]:


m.basemap = 'gray'


# Now let's retrieve the NAIP image layer.

# In[7]:


naip_item = gis.content.get('2f8f066d526e48afa9a942c675926785')
naip_item


# In[8]:


naiplayer = naip_item.layers[0]
naiplayer


# In[9]:


m.add_layer(naiplayer)


# ###  Specify a folder name in raster store that will be used to store our training data

# Make sure a raster store is ready on your raster analytics image server. This is where where the output subimages, also called chips, labels and metadata files are going to be stored.

# In[11]:


ds = analytics.get_datastores(gis=gis)
ds


# In[12]:


ds.search()


# In[15]:


rasterstore = ds.get("/rasterStores/RasterDataStore")
rasterstore


# In[16]:


samplefolder = "pool_chips_test"
samplefolder


# ### Export training data using `arcgis.learn`

# With the feature class and raster layer, we are now ready to export training data using the export_training_data() method in arcgis.learn module. In addtion to feature class, raster layer, and output folder, we also need to speficy a few other parameters such as tile_size (size of the image chips), strid_size (distance to move in the X when creating the next image chip), chip_format (TIFF, PNG, or JPEG), metadata format (how we are going to store those bounding boxes). More detail can be found [here](https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/export-training-data-for-deep-learning.htm). 
# 
# Depending on the size of your data, tile and stride size, and computing resources, this opertation can take 15mins~2hrs in our experiment. Also, do not re-run it if you already run it once unless you would like to update the setting.

# In[14]:


pool_bb


# In[15]:


pool_bb_layer


# In[16]:


export = learn.export_training_data(input_raster=naiplayer,
                                           output_location=samplefolder,
                                           input_class_data=pool_bb_layer, 
                                           chip_format="PNG", 
                                           tile_size={"x":448,"y":448}, 
                                           stride_size={"x":224,"y":224}, 
                                           metadata_format="PASCAL_VOC_rectangles",                                        
                                           classvalue_field = "Id",
                                           buffer_radius = 6,
                                           context={"startIndex": 0, "exportAllTiles": False},
                                           gis = gis)

export


# Now let's get into the raster store and look at what has been generated and exported.

# In[17]:


samples = list_datastore_content(rasterstore.datapath + '/' + samplefolder + "/images", filter = "*png")
# print out the first five chips/subimages
samples['/rasterStores/RasterDataStore/pool_chips_test/images'][0:5]


# In[18]:


labels = list_datastore_content(rasterstore.datapath + '/' + samplefolder + "/labels", filter = "*xml")
# print out the labels/bounding boxes for the first five chips
labels['/rasterStores/RasterDataStore/pool_chips_test/labels'][0:5]


# We can also create a image layer using one of this images and look at what it looks like. Note that a chip may or may not have a bounding box in it and one chip might have multiple boxes as well.

# ## Part 2 - model training

# If you've already done part 1, you should already have both the training chips and swimming pool labels. Please change the path to your own export training data folder that contains "images" and "labels" folder.

# In[2]:


import os
from pathlib import Path

agol_gis = GIS('home')


# In[3]:


training_data = agol_gis.content.get('73a29df69b344ce8b94fdb4c9df7103d')
training_data


# In[4]:


filepath = training_data.download(file_name=training_data.name)


# In[5]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[6]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# In[20]:


data = prepare_data(data_path, {0:'Pool'}, batch_size=32)
data.classes


# ### Visualize training data
# To get a sense of what the training data looks like, `arcgis.learn.show_batch()` method randomly picks a few training chips and visualize them.

# In[21]:


get_ipython().run_cell_magic('time', '', 'data.show_batch()\n')


# ### Load model architecture
# Here we use Single Shot MultiBox Detector (SSD) [1], a well-recognized object detection algorithm, for swimming pool detection. A SSD model architecture using Resnet-34 as the base model has already been predefined in ArcGIS API for Python, which makes it easy to use.

# 
# <center>Architecture of a convolutional neural network with a SSD detector [1]</center>

# In[22]:


ssd = SingleShotDetector(data, grids=[5], zooms=[1.0], ratios=[[1.0, 1.0]])


# ### Train a model through learning rate tuning and transfer learning
# [Learning rate](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) is one of the most important hyperparameters in model training. Here we explore a range of learning rate to guide us to choose the best one.

# In[23]:


ssd.lr_find()


# Based on the learning rate plot above, we can see that the loss starts going down from 1e-4. Therefore, we set learning rate to be a range from 1e-4 to 3e-3, which means we will apply smaller rates to the first few layers and larger rates for the last few layers, and intermediate rates for middle layers, which is the idea of transfer learning. Let's start with 10 epochs for the sake of time.

# In[24]:


ssd.fit(10, lr=slice(1e-3, 3e-2))


# ### Detect and visualize swimming pools in validation set
# Now we have the model, let's look at how the model performs. Here we plot out 5 rows of images and a threshold of 0.3. Threshold is a measure of probablity that a swimming pool exists. Higher value meas more confidence.

# In[25]:


ssd.show_results(thresh=0.3)


# As we can see, with only 10 epochs, we are already seeing reasonable results. Further improvment can be acheived through more sophisticated hyperparameter tuning. Let's save the model for further training or inference later. The model should be saved into a models folder in your folder. By default, it will be saved into your `data_path` that you specified in the very beginning of this notebook.

# In[54]:


ssd.save('5x5-10-deploy', publish=True, gis=gis)


# ## Part 3 - deployment and inference

# In[27]:


detect_objects_model_package = gis.content.get('85bf36e8c5e24d6da21420e4f89f5ea7')
detect_objects_model_package


# Now we are ready to install the mode. Installation of the deep learning model item will unpack the model definition file, model file and the inference function script, and copy them to "trusted" location under the Raster Analytic Image Server site's system directory. 

# In[29]:


detect_objects_model = Model(detect_objects_model_package)


# In[58]:


detect_objects_model.install(gis=gis)


# In[31]:


detect_objects_model.query_info(gis=gis)


# ### Model inference
# To test our model, let's get a raster image with some swimming pools.

# In[32]:


out_objects = detect_objects(input_raster=naiplayer.url,
                             model=detect_objects_model,
                             output_name="pooldetection_full_redlands",
                             context={'cellSize': 0.42, 'processorType': 'GPU'},
                             gis=gis)

out_objects


# ## Visualize detected pools on map

# In[33]:


result_map = gis.map('Redlands, CA')
result_map.basemap='satellite'
result_map.add_layer(out_objects)
result_map


# ## Conclusion
# In thise notebook, we have covered a lot of ground. In part 1, we discussed how to export training data for deep learning using ArcGIS python API and what the output looks like. In part 2, we demonstrated how to prepare the input data, train a object detection model, visualize the results, as well as apply the model to an unseen image using the deep learning module in ArcGIS API for Python. Then we covered how to install and publish this model and make it production-ready in part 3.
# 
# ## References
# [1]
# Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu: “SSD: Single Shot MultiBox Detector”, 2015; <a href='http://arxiv.org/abs/1512.02325'>arXiv:1512.02325</a>.


# ====================
# detection_of_electric_utility_features_and_vegetation_encroachments_from_satellite_images_using_deep_learning.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Detection of electric utility features and vegetation encroachments from satellite images using deep learning
# > * 🔬 Data Science
# > * 🥠 Deep Learning and Object Detection

# ## Table of Contents
# 
# * [Introduction](#Introduction)
# * [Necessary imports and get connected to your GIS](#Necessary-imports-and-get-connected-to-your-GIS)
# * [Part 1 - Train the model and detect electric utility features](#Part-1---Train-the-model-and-detect-electric-utility-features)
#   * [Export electric utility training data for deep learning](#Export-electric-utility-training-data-for-deep-learning)
#   * [Load RetinaNet model architecture](#Load-RetinaNet-model-architecture)
#   * [Tuning for optimal learning rate](#Tuning-for-optimal-learning-rate)
#   * [Fit the model on the data](#Fit-the-model-on-the-data)
#   * [Unfreeze and fine tuning (optional)](#Unfreeze-and-fine-tuning-(optional))
#     * [Unfreeze model](#Unfreeze-model)
#     * [Optimal learning rate](#Optimal-learning-rate)
#     * [Fit model on the data](#Fit-model-on-the-data)
#   * [Save the electric utility detection model](#Save-the-electric-utility-detection-model)
#   * [Load an intermediate model to train it further](#Load-an-intermediate-model-to-train-it-further)
#   * [Visualize results in validation set](#Visualize-results-in-validation-set)
# * [Part 2 - Train the model and detect trees](#Part-2---Train-the-model-and-detect-trees)
#   * [Export trees training data for deep learning](#Export-trees-training-data-for-deep-learning)
#   * [Load RetinaNet model](#Load-RetinaNet-model)
#   * [Finding optimal learning rate](#Finding-optimal-learning-rate)
#   * [Fit the model on the tree data](#Fit-the-model-on-the-tree-data)
#   * [Save the tree detection model](#Save-the-tree-detection-model)
#   * [Visualize results in validation tree dataset](#Visualize-results-in-validation-tree-dataset)
# * [Part 3 - Deploy model and detect electric utility features & trees at scale](#Part-3---Deploy-model-and-detect-electric-utility-features-&-trees-at-scale)
# * [Part 4 - Near analysis to find possible vegetation encroachment near electric utility features](#Part-4---Near-analysis-to-find-possible-vegetation-encroachment-near-electric-utility-features)
# * [Conclusion](#Conclusion)
# * [References](#References)

# ## Introduction

# This sample notebook demonstrates how to efficiently map the electric utility features and trees in the imagery with possible locations of vegetation encroachment. Satellite imagery combined with machine learning leads to cost-effective management of the electric grids. This workflow consists of four major operations:
# 
# * Building and extracting training data for electric utility and trees using ArcGIS Pro
# * Training a deep learning model i.e. RetinaNet using arcgis.learn
# * Model inferencing at scale using ArcGIS Pro
# * Proximity analysis between detected objects (electric utility and trees) feature layers using ArcGIS Pro

# 

# <center> Example of electric utility object i.e transmission towers, distribution towers, Sub-station detection </center>

# ## Necessary imports and get connected to your GIS

# In[3]:


import arcgis
from arcgis import GIS
from arcgis.learn import RetinaNet, prepare_data


# In[4]:


gis = GIS("home")


# ## Part 1 - Train the model and detect electric utility features

# A electric utility feature layer consisting of manually labelled features, which will be used to define the location and label of each feature.

# In[5]:


electric_train_data = gis.content.get('8e703649c43041c1bb4985b16788aa44')
electric_train_data


# ### Export electric utility training data for deep learning
# 
# Training samples for electric utilities were manually labelled for **Hartford, Connecticut state**. The training data consisted of three classes i.e. Transmission towers, Distribution towers, Sub-stations.
# 
# Training data can be exported using the `Export training data for deeplearning` tool available in ArcGIS Pro and ArcGIS Image Server
# 
# - `Input Raster`: Imagery
# - `Input Feature Class or Classified Raster`: feature layer with labelled polygon
# - `Class Value Field`: field in the attributes containing class
# - `Tile Size X & Tile Size Y`: 256
# - `Stride X & Stride Y`: 128
# - `Reference System`: Map space
# - `Meta Data Format`: Pascal VOC (Visual Object Class)
# - `Environments`: Set optimum Cell Size, Processing Extent 

# 

# ```
# arcpy.ia.ExportTrainingDataForDeepLearning("Imagery", r"C:\ElectricUtility_deepLearn\Train_chip_lablel_data", "Training_electricObjects", , "TIFF", 256, 256, 128, 128, "ONLY_TILES_WITH_FEATURES", "PASCAL_VOC_rectangles", 0, "Classvalue", 0, None, 0)
# ```
# 
# After filling all details and running the Export Training Data For Deep Learning tool, a code like above will be generated and executed. This will create training data i.e. Image (.tif) and labels (.xml) with necessary files in the specified folder, ready to be used in upcoming steps.

# ### Train the model
# 
# We will select and train a model using `arcgis.learn` module in ArcGIS API for Python. `arcgis.learn` has deep learning tools and capabilities to accomplish the task in this study. As electric utility features are generally small and varied in appearance, RetinaNet model is used, which is one of the best one stage object detection model that works specifically well in case of small and dense objects.

# ### Prepare data
# 
# We will specify the path to our training data and a few hyperparameters. It also helps in transformations and data augmentations on the training data, which enables us to train better model with limited datasets.
# 
# - **path**: path of the folder containing training data.
# - **class_mapping**: allows for specifying text labels for the classes
# - **batch_size**: Number of images your model will train on each step inside an epoch, it directly depends on the memory of your graphic card. 20 worked for us on a 11GB GPU.
# 
# This function returns a fast.ai databunch, which will be used further to train the model.

# In[5]:


training_data = gis.content.get('01ca39eab00e4780b0517af11d971b31')
training_data


# In[ ]:


filepath = training_data.download(file_name=training_data.name)


# In[ ]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[ ]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# In[4]:


## Load the Data ##
data = prepare_data(data_path, class_mapping = {1:'Dist_tower',2:'Trans_tower',3:'Station'}, batch_size=8)


# In[5]:


## Check the classes in the loaded data ##
data.classes


# ### Visualize a few samples from your training data
# - **rows**: number of rows we want to see in the results

# In[5]:


## Visualize random training samples from the data ##
data.show_batch(rows = 3, alpha=1)


# The imagery chips have the bounding boxes marked out for electric utility feature type. 

# ### Load RetinaNet model architecture
# 
# The code below initiates a RetinaNet model with a pre-trained Resnet type backbone or other supported backbones. The model return types and bounding boxes for detected electric utility objects in the imagery.  

# In[6]:


## Load the model architecture with resnet152 backbone 
retinanet_model = RetinaNet(data)


# ### Tuning for optimal learning rate
# Learning rate is one of the most important hyperparameters during model training as too high/low may cause the model to never converge or learn. `arcgis.learn` leverages fast.ai’s learning rate finder to find an optimum learning rate for training models. We can use the `lr_find()` method to find the optimum learning rate at which can train a robust model fast enough. 

# In[7]:


## Tune the learning rate

lr = retinanet_model.lr_find()
print(lr)


# Based on the learning rate finder, the lr = 0.0005754399373371565, which could be used to train our model or if not specified it internally uses the learning rate finder to find optimal learning rate and uses it.

# ### Fit the model on the data
# To train the model, we use the `fit()` method. To start, we will use 20 epochs to train our model. Epoch defines how many times the model is exposed to the entire training set.

# In[9]:


retinanet_model.fit(epochs=20, lr=lr)


# Training data was split into training and validation set in prepare data step. `fit()` starts the training process and gives losses on training and validation sets. The losses help in assessing the generalizing capability of the model and also prevent overfitting. When a considerable decrease in losses in observed, the model could be saved for further training or inference.

# ### Unfreeze and fine tuning (optional)
# Frozen network layers pretrained on ImageNet, help in accelerating the training of the network. Unfreezing the backbone layers, helps to fine-tune the complete network architecture with our own data, leading to better generalization capability.
# 
# As per requirement, It can be executed or else we can continue with next step to [Save the electric utility detection model](#Save-the-electric-utility-detection-model).

# #### Unfreeze model

# In[11]:


retinanet_model.unfreeze()


# #### Optimal learning rate

# In[12]:


## Find optimal learning rate for the model with unfreezed backbone
lr = retinanet_model.lr_find()


# #### Fit model on the data

# In[22]:


## Fine-tune for around 10 epochs
retinanet_model.fit(epochs=10,lr=lr)


# ### Save the electric utility detection model
# We will save the model which we trained as a 'Deep Learning Package' ('.dlpk' format). Deep Learning package is the standard format used to deploy deep learning models on the ArcGIS platform. We will use the `save()` method to save the trained model. By default, it will be saved to the 'models' sub-folder within our training data folder.

# In[ ]:


retinanet_model.save("Retinanet_electric_model_e20")


# ### Load an intermediate model to train it further
# To retrain a saved model, we can load it again using the code below and train it further

# In[22]:


retinanet_model.load("Retinanet_model.pth")


# ### Visualize results in validation set
# It is a good practice to see results of the model viz-a-viz ground truth. The code below picks random samples and shows us ground truth and model predictions, side by side. This enables us to preview the results of the model within the notebook.

# In[23]:


retinanet_model.show_results(rows=8, thresh=0.35)


# ## Part 2 - Train the model and detect trees
# The same workflow is followed for tree detection from Export training data to detect objects. After training, the model is saved for inference in the next step or for further training. 

# ### Export trees training data for deep learning
# 
# Training samples for trees were manually labelled for **Hartford, Connecticut state**. The training data consisted of one class i.e. Trees.
# 
# Training data can be exported using the `Export training data for deeplearning` tool available in ArcGIS Pro and ArcGIS Image Server
# 
# - `Input Raster`: Imagery
# - `Input Feature Class or Classified Raster`: feature layer with labelled polygon
# - `Class Value Field`: field in the attributes containing class i.e tree
# - `Tile Size X & Tile Size Y`: 256
# - `Stride X & Stride Y`: 128
# - `Reference System`: Map space
# - `Meta Data Format`: Pascal VOC (Visual Object Class)
# - `Environments`: Set optimum Cell Size, Processing Extent 

# In[6]:


electric_train_data = gis.content.get('81ef094891e042ccb7f0742b34805f25')
electric_train_data


# ### Prepare data

# In[ ]:


tree_data_path = Path(os.path.join(os.path.splitext(filepath)[1]))


# In[3]:


tree_data = prepare_data(tree_data_path, batch_size=4)


# ### Visualize a few samples from your trees training data

# In[36]:


tree_data.show_batch(rows = 2)


# ### Load RetinaNet model

# In[4]:


## Load the model architecture with resnet152 backbone 

retinanet_tree_model = RetinaNet(tree_data, backbone='resnet152')


# ### Finding optimal learning rate

# In[9]:


lr = retinanet_tree_model.lr_find()
print(lr)


# ### Fit the model on the tree data

# In[10]:


retinanet_tree_model.fit(epochs=10, lr=lr, checkpoint= False)


# ### Visualize results in validation tree dataset

# In[14]:


retinanet_tree_model.show_results(rows=8, thresh=0.3)


# ### Save the tree detection model

# In[ ]:


retinanet_tree_model.save("Retinanet_tree_model_e10")


# ## Part 3 - Deploy model and detect electric utility features & trees at scale
# 
# We will use the saved model to detect objects using 'Detect Objects Using Deep Learning' tool available in both ArcGIS Pro and ArcGIS Enterprise. For this sample, we will use the high resolution satellite imagery to detect electric utility features. Detect objects using both the model i.e. electric utility and trees, to get two different feature layers.
# 
# - `Input Raster` : Imagery
# - `Output Detect Objects` : Detected_Results
# - `Model Definition` : Retinanet_electric_model_e20.emd or Retinanet_tree_model_e10 
# - `padding` : The 'Input Raster' is tiled and the deep learning model runs on each individual tile separately before producing the final 'detected objects feature class'. This may lead to unwanted artifacts along the edges of each tile as the model has little context to detect objects accurately. Padding as the name suggests allows us to supply some extra information along the tile edges, this helps the model to predict better.
# - `threshold ` : 0.5
# - `nms_overlap` : 0.1
# - `Cell Size` : Should be close to value with which we trained the model, we specified that at the Export Training Data step .

# 

# `arcpy.ia.DetectObjectsUsingDeepLearning(in_raster="Imagery",
# out_detected_objects=r"DetectedObjects",
# in_model_definition=r"\\models\Retinanet_model_e400\Retinanet_model_e400.emd",
# model_arguments ="padding 56;batch_size 4;threshold 0.5",
# run_nms="NMS",
# confidence_score_field="Confidence",
# class_value_field="Class",
# max_overlap_ratio=0.1,
# processing_mode="PROCESS_AS_MOSAICKED_IMAGE")`

# Detect Objects Using Deep Learning returns a feature class that can be further refined using the Definition query and Non Maximum Suppression tool. 
# 

# 

# <center> Detected electric utility feature layer</center>

# 

# <center> Detected trees feature layer </center>

# ## Part 4 - Near analysis to find possible vegetation encroachment near electric utility features
# After model inference on imagery, detected objects i.e. Electric utility and trees, in the imagery are saved in as separate feature layers. The near analysis tool in ArcGIS Pro is used to calculate distance and additional proximity information between the input features (electric utility) and the closest feature in another layer or feature class (Trees).
# 
# - `Input Features` : feature layer from detect object tool for electric utility
# - `Near Features` : feature layer from detect object tool for trees
# - `Search radius` : required distance or range of search
# - `Location` : check location parameter checkbox

# 

# Here the tool finds locations where trees are in the vicinity of 5 m near electric utility features for possible vegetation related outages. The input feature will have two more attribute x (near_x) and y co-ordinates (near_y) of the closest location of the near feature.

# 

# <center> Detected objects i.e electric utility, trees with markers representing proximity of trees to utility installations </center>

# The green and red bounding boxes are trees and electric utility respectively. The red anchor show the electric utility object in range of 5m of tree and possible location of vegetation related outage, while yellow show at safe distance.
# We have published the outputs from this sample as a hosted feature layer.

# ## Conclusion
# 

# The models available with `arcgis.learn` were able to detect and map the electric utility features at scale in the imagery. Further training the models with larger and better data can improve the results at a scale of country. 
# 
# The overlay of information from this workflow can assist electric utility industry in cost-effective and efficient management of the electric grid. Data science can help us derive insight from data, but communicating those insights is perhaps as important if not more. We used the ArcGIS Pro to publish the results as a hosted feature layer, which could be viewed as a web-map.

# 

# <center> Web-map of detected objects with encroachment grid locations </center>

# ## References
#     
# [1] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan: “Feature Pyramid Networks for Object Detection”, 2016; [http://arxiv.org/abs/1612.03144 arXiv:1612.03144].
# 
# [2] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He: “Focal Loss for Dense Object Detection”, 2017; [http://arxiv.org/abs/1708.02002 arXiv:1708.02002].


# ====================
# determining_site_suitability_for_oil_palm_plantation.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Determining site suitability for oil palm plantation 

# ## Table of Contents
# 
# - [Introduction](#introduction)
# - [Prerequisites](#prerequisites)
# - [Necessary Imports](#imports)
# - [Using Raster Function Template ](#rft)
#     - [Suitability factors taken into consideration in the RFT](#factors)
#     - [Uploading the RFT to ArcGIS Online using Content Manager](#upload)
#     - [Creating the RFT object using the RFT class from raster function template item. ](#create)
# - [Creating hosted imagery layers for analysis](#layers)
# - [Applying the raster function chain on the input layers to create the suitability raster ](#suitability)
#     - [Persisting the output in the GIS](#op)
# - [Conclusion](#conclusion)
# - [References](#references)

# ## Introduction <a class= 'anchor' id='introduction'></a> 

# Proper land suitability assessment for agriculture has to be carried out to determine the potentiality of a land area to provide the most favorable ecological requirements for a particular crop variety, which results in optimum crop development and maximum productivity. 
# 
# In this notebook, we determine the site suitability for oil palm development by: 
#  
#  - Creating hosted imagery layers. 
#  - Applying raster function chain using the RFT class from the arcgis.raster.functions module.
#  - Persisting the raster analysis in ArcGIS Online.
#  
# Note: This analysis can be conducted on ArcGIS Enterprise as well as on ArcGIS Online. In this notebook, the notebook was carried out using ArcGIS Online.

# ## Prerequisites<a class= 'anchor' id='prerequisites'></a> 

# - To run raster analysis functions on ArcGIS Online, you must have privileges to create and publish content and to perform Raster Analysis.
# For more information, refer [here](https://doc.arcgis.com/en/arcgis-online/analyze/perform-raster-analysis.htm).<br>
# 
# - For creating imagery layers using [copy_raster](https://developers.arcgis.com/python/api-reference/arcgis.raster.analytics.html#copy-raster) in ArcGIS Online, Azure library packages for Python (Azure SDK for Python - azure-storage-blob: 12.1<= version <=12.8) needs to be pre-installed. Refer https://docs.microsoft.com/en-us/azure/developer/python/azure-sdk-install
# 
# - An optional installation of graphviz 2.38 can be used to visualize the funciton chain of the raster function template.  

# ## Necessary Imports <a class= 'anchor' id='imports'></a> 

# In[1]:


import os
import zipfile

import arcgis
from arcgis.gis import GIS
from arcgis.raster.analytics import copy_raster
from arcgis.raster.functions import RFT


# In[2]:


gis = GIS("Home")


# ## Using Raster Function Template (RFT)<a class= 'anchor' id='rft'></a> 

# A raster function template is a model that consists of many function chains to produce a processing workflow. We can build the RFT using the function editor and raster functions pane.
# 
# The RFT can be created using [ArcGIS Pro](https://pro.arcgis.com/en/pro-app/latest/help/analysis/raster-functions/raster-function-template.htm) or [ArcGIS Enterprise](https://enterprise.arcgis.com/en/portal/latest/use/raster-function-editor.htm). 

# ###  Suitability factors taken into consideration in the RFT <a class= 'anchor' id='factors'></a> 
# 
# In order to carry out the suitability analysis, we need to consider some of the favorable environmental and crop criteria settings:
# 
# | Suitability  Considerations | Suitable | Not suitable |
# | --- | --- | --- |
# | Elevation |  0-1000m | >1000 m |
# | Slope | 0-30% | >30% |
# | Conservation area buffer | >1000 m | <1000m |
# | Soil type | Inceptisol, Oxisol, Alfisol, Ultisol, Spodosol, Entisol | Histosols |
# | Soil acidity | 1-7 | None |
# 
# 
# 
# Based on these criteria, a raster function template was created using ArcGIS Pro. You can acquire the RFT using the following item. 

# In[32]:


palm_oil_suitability_rft = gis.content.get('9fa702ab04f747be99607206c2d0eaaa')
palm_oil_suitability_rft


# The RFT can be further added to your own GIS for further analysis, by downloading the RFT and adding it to your GIS. 

# In[33]:


# assign variables to locations on the file system 
cwd = os.path.abspath(os.getcwd())
data_path = os.path.join(cwd, r'data')


# In[34]:


rft_path = palm_oil_suitability_rft.download(save_path=data_path)


# ### Uploading the RFT to ArcGIS Online using Content Manager <a class= 'anchor' id='upload'></a> 

# In[3]:


palm_oil_suitability_rft_item=gis.content.add(item_properties={"type":"Raster function template",
                                                          "title":"PalmOilSuitability",
                                                          "tags":"PalmOilSuitability_UsingFiveFactors"
                                                         },
                                              data=rft_path
                                              )


# ### Creating the RFT object using the RFT class from raster function template item. <a class= 'anchor' id='create'></a> 

# In[4]:


palm_oil_suitability_rft = RFT(palm_oil_suitability_rft_item, gis=gis)


# We can visualize the function chain and help document specific to this RFT in order to understand the input parameters and the functions involved.

# In[2]:


palm_oil_suitability_rft.draw_graph(graph_size='10, 15')


# A '?' before or after an RFT object will display the help document consisting of the parameters marked as public by the author of the RFT.

# In[12]:


get_ipython().run_line_magic('pinfo', 'palm_oil_suitability_rft')


# ## Creating hosted imagery layers for analysis <a class= 'anchor' id='layers'></a> 

# Hosted imagery layers can be used to manage, share, and analyze raster and imagery data. Hosted imagery layers can be used as an input to analysis tools, create custom workflows with raster functions, manage large collections of imagery, and include imagery layers in maps. 
# 
# The [copy_raster](https://developers.arcgis.com/python/api-reference/arcgis.raster.analytics.html#copy-raster) function is used here to create hosted imagery layers in ArcGIS Online from local raster datasets.
# 
# In the following cells, we will be creating hosted layers representing the elevation, conservation area buffer, slope, soil type and soil acidity. They will act as the input rasters to the RFT. 

# You can acquire the data for analysis by using the following item.

# In[43]:


analysis_data = gis.content.get('40b00dc5c8794059811d0cced66e9113')
analysis_data


# In[65]:


#downloading the data
analysis_data_path = analysis_data.download(save_path=data_path)
with zipfile.ZipFile(analysis_data_path, 'r') as zip_ref:
    zip_ref.extractall(data_path)


# In[5]:


elevation_wm_data_path = os.path.join(os.path.splitext(analysis_data_path)[0], r'elevation_wm')
elevation_wm_item = copy_raster(input_raster=elevation_wm_data_path,
                                output_name="elevation_palm_tiled",
                                raster_type_name="Raster Dataset",
                                tiles_only=True
                               )
elevation_wm_layer = elevation_wm_item.layers[0]


# In[4]:


mp = gis.map()
mp.extent = {'spatialReference': {'latestWkid': 3857, 'wkid': 102100},
 'xmax': 13940786.084357359,
 'xmin': 11533936.937714368,
 'ymax': 495805.2734337793,
 'ymin': -482588.6886162166}
mp.add_layer(elevation_wm_layer)
mp


# In[6]:


conser_wm_data_path = os.path.join(os.path.splitext(analysis_data_path)[0], r'conser_wm')
conser_wm_item = copy_raster(input_raster=conser_wm_data_path,
                             output_name="conser_palm_tiled", 
                             raster_type_name="Raster Dataset", 
                             tiles_only=True
                            )
conser_wm_layer = conser_wm_item.layers[0]


# In[5]:


mp.add_layer(conser_wm_layer)
mp


# In[28]:


slope_wm_path = os.path.join(os.path.splitext(analysis_data_path)[0], r'slope_wm')
slope_wm_item = copy_raster(input_raster=slope_wm_path,
                            output_name="slope_palm_tiled", 
                            raster_type_name="Raster Dataset",
                            tiles_only=True
                           )
slope_wm_layer = slope_wm_item.layers[0]


# In[7]:


mp.add_layer(slope_wm_layer)
mp


# In[30]:


soil_acid_wm_path = os.path.join(os.path.splitext(analysis_data_path)[0], r'soil_acid_wm')
soil_acid_wm_item = copy_raster(input_raster=soil_acid_wm_path,
                                output_name="soil_acid_palm_tiled",
                                raster_type_name="Raster Dataset",
                                tiles_only=True
                               )
soil_acid_wm_layer = soil_acid_wm_item.layers[0]


# In[8]:


mp.add_layer(soil_acid_wm_layer)
mp


# In[32]:


soil_type_wm_path = os.path.join(os.path.splitext(analysis_data_path)[0], r'soil_type_wm')
soil_type_wm_item = copy_raster(input_raster=soil_type_wm_path,
                                output_name="soil_type_palm_tiled",
                                raster_type_name="Raster Dataset",
                                tiles_only=True
                               )
soil_type_wm_layer = soil_type_wm_item.layers[0]


# In[9]:


mp.add_layer(soil_type_wm_layer)
mp


# ## Applying the raster function chain on the input layers to create the suitability raster <a class= 'anchor' id='suitability'></a> 

# In[32]:


suitability_output = palm_oil_suitability_rft(ElevRaster=elevation_wm_layer,
                                              SlopeRaster=slope_wm_layer , 
                                              ConsRaster=conser_wm_layer, 
                                              STypeRaster=soil_type_wm_layer, 
                                              SAcidRaster=soil_acid_wm_layer
                                             )


# ### Persisting the output in the GIS<a class= 'anchor' id='op'></a> 

# In[35]:


saved_output = suitability_output.save(output_name='palm_oil_suitability_saved_til', tiles_only=True)


# In[12]:


mp.add_layer(saved_output.layers[0])
mp


# In the output, the red areas denote places that are less suitable for oil palm plantation, while green areas represent places that are more suitable.

# ## Conclusion <a class= 'anchor' id='conclusion'></a> 

# In this notebook, we located potentially suitable areas for sustainable oil palm cultivation. This analysis can act as a first step in a site selection process for a certified sustainable plantation and can inform government officials and nongovernmental organizations (NGOs) in assessing land use policy options to support the expansion of sustainable palm oil production.

# ## References <a class= 'anchor' id='references'></a> 

# [1] [https://hub.arcgis.com/datasets/afe7ce49f19941fab8db58e2aab90910 ]


# ====================
# drive_time_analysis_for_opioid_epidemic.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Drive time analysis for opioid epidemic

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Drive-time-analysis-for-opioid-epidemic" data-toc-modified-id="Drive-time-analysis-for-opioid-epidemic-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Drive time analysis for opioid epidemic</a></span><ul class="toc-item"><li><ul class="toc-item"><li><span><a href="#Problem" data-toc-modified-id="Problem-1.0.1"><span class="toc-item-num">1.0.1&nbsp;&nbsp;</span>Problem</a></span></li><li><span><a href="#Data-Driven-Approach" data-toc-modified-id="Data-Driven-Approach-1.0.2"><span class="toc-item-num">1.0.2&nbsp;&nbsp;</span>Data Driven Approach</a></span></li><li><span><a href="#Steps-of-analysis" data-toc-modified-id="Steps-of-analysis-1.0.3"><span class="toc-item-num">1.0.3&nbsp;&nbsp;</span>Steps of analysis</a></span></li><li><span><a href="#1.-Reading-in-clinics-and-Oakland-county-census-block-group-(CBG)-boundaries" data-toc-modified-id="1.-Reading-in-clinics-and-Oakland-county-census-block-group-(CBG)-boundaries-1.0.4"><span class="toc-item-num">1.0.4&nbsp;&nbsp;</span>1. Reading in clinics and Oakland county census block group (CBG) boundaries</a></span></li><li><span><a href="#2.-Drive-time-analysis" data-toc-modified-id="2.-Drive-time-analysis-1.0.5"><span class="toc-item-num">1.0.5&nbsp;&nbsp;</span>2. Drive time analysis</a></span><ul class="toc-item"><li><span><a href="#Service-area-generation-for-the-visible-[5]-minutes-layer" data-toc-modified-id="Service-area-generation-for-the-visible-[5]-minutes-layer-1.0.5.1"><span class="toc-item-num">1.0.5.1&nbsp;&nbsp;</span>Service area generation for the visible [5] minutes layer</a></span></li></ul></li><li><span><a href="#3.-Spatially-join-the-drive_times_all_layer-layer-with-oakland_cbg-layer-to-obtain-drive-times-for-each-census-block-group" data-toc-modified-id="3.-Spatially-join-the-drive_times_all_layer-layer-with-oakland_cbg-layer-to-obtain-drive-times-for-each-census-block-group-1.0.6"><span class="toc-item-num">1.0.6&nbsp;&nbsp;</span>3. Spatially join the <code>drive_times_all_layer</code> layer with <code>oakland_cbg</code> layer to obtain drive times for each census block group</a></span></li><li><span><a href="#4.-Enrich-census-block-groups-with-suitable-data" data-toc-modified-id="4.-Enrich-census-block-groups-with-suitable-data-1.0.7"><span class="toc-item-num">1.0.7&nbsp;&nbsp;</span>4. Enrich census block groups with suitable data</a></span></li><li><span><a href="#5.-Visualize-results" data-toc-modified-id="5.-Visualize-results-1.0.8"><span class="toc-item-num">1.0.8&nbsp;&nbsp;</span>5. Visualize results</a></span></li><li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-1.0.9"><span class="toc-item-num">1.0.9&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></li></ul></li></ul></div>

# This notebook performs drive time analysis for clinics of opioid epidemic treatment and/or prevention centers in the county of Oakland, Michigan. 

# ### Problem
# 
# Drug overdoses are a leading cause of injury death in the United States, resulting in approximately __52,000 deaths in 2015. In the same year, the overdose death rate in non metropolitan areas of the country (17 per 100,000) surpassed the metropolitan area rate (16.2 per 100,000), which historically wasn't the case.__
# 
# While metro areas have more people, and consequently, a greater incidence of drug use and overdose, the one stark disadvantage in the non metro areas is the comparatively limited access to opioid epidemic prevention/treatment facilities and transit.

# ### Data Driven Approach
# 
# Here, we demonstrate an analysis of how several parts of Oakland county in Michigan are accessible from their opioid epidemic prevention and/or treatment centers within 5, 10, 15, 20 and 25 minutes of drive time. The areas of the county identified within those time bound service areas are then enriched with factors such as
# 
# >    Health insurance spending
# >
# >    Number of people with a Bachelor's Degree
# >
# >    Total Population in 2017
# 
# This provides a __better understanding of the population served or under-served by these treatment facilities and the plausible contributing factors.__

# ### Steps of analysis
# 
# 1. Get data for clinics and census block groups boundary (CBG) in Oakland County
# 2. Generate areas served based on time taken to drive to the clinics
# 3. Spatially join the service areas generated with the CBG to obtain drive time to closest clinic for each CBG.
# 4. Enrich each CBG with additional data to estimate influencing factors.
# 5. Map it!

# Let's get started by first importing necessary modules and connecting to our GIS

# In[1]:


import arcgis
from datetime import datetime
from arcgis.features import FeatureLayer


# In[6]:


# Connect to GIS
from arcgis.gis import GIS
gis = GIS('home') 


# ### 1. Reading in clinics and Oakland county census block group (CBG) boundaries

# In[2]:


# Layer URLS for the Opioid Epidemic Clinics in Oakland County and Census Block Groups of Oakland County
clinic_url = 'https://services8.arcgis.com/TWfU0bgvgUkCgHLa/arcgis/rest/services/Oakland_County_Licensed_Program_Listings/FeatureServer/0/'
oakland_cbg = 'https://services.arcgis.com/bkrWlSKcjUDFDtgw/arcgis/rest/services/Oakland_2010_census_bg/FeatureServer/0/'

# create feature layers
clinic_layer = FeatureLayer(clinic_url)
oakland_layer = FeatureLayer(oakland_cbg)


# In[3]:


# Query all clinics and census block groups as FeatureSets
clinic_features = clinic_layer.query()
print('Total number of rows in the clinic dataset: ' + str(len(clinic_features.features)))

oakland_features = oakland_layer.query()
print('Total number of rows in the oakland dataset: ' + str(len(oakland_features.features)))


# We notice that Oakland County, MI has __122 clinics__ and __934 CBG__

# In[4]:


# Convert to SpatialDataFrames
clinic = clinic_features.sdf
clinic.head()


# In[5]:


oakland = oakland_features.sdf
oakland.head()


# In[19]:


m = gis.map('Oakland, Michigan')
m


# In[8]:


m.add_layer(oakland_layer)
m.add_layer(clinic_layer)


# ### 2. Drive time analysis

# Service areas around the clinics need to be generated twice for two different sets of drive time inputs:
# 
# > 1. __`[5, 10, 15, 20, 25]`__ that is needed to get the drive time required from each CBG to the closest clinic. While adding more/less intervals is certainly an option, the choice of time intervals here is meant to span the range (closest-farthest clinic) as well as assume a range of safety, i.e. time within which a person can be driven/drive to a clinic to avoid delays.
# >
# > 2. __`[5]`__ that is needed purely for visualization purposes (as seen towards the end of the notebook). Feel free to modify this function parameter based on how you would like to visualize the map.

# Service area generation for the [5, 10, 15, 20, 25] minutes layer

# In[9]:


from arcgis.features.use_proximity import create_drive_time_areas


# In[10]:


# Generate service areas
result = create_drive_time_areas(clinic_layer, [5, 10, 15, 20, 25], 
                                 output_name="DriveTimeToClinics_All_2"+ str(datetime.now().microsecond), 
                                 overlap_policy='Dissolve')


# In[11]:


# Check type of result
type(result)


# In[12]:


# Share results for public consumption
result.share(everyone=True)


# In[13]:


# Convert it to a FeatureSet
drive_times_all_layer = result.layers[0]
driveAll_features = drive_times_all_layer.query()
print('Total number of rows in the service area (upto 25 mins) dataset: '+ str(len(driveAll_features.features)))


# This implies that only 5 clinics in this county can be driven to within 5 minutes.

# In[14]:


# Get results as a SpatialDataFrame
driveAll = driveAll_features.sdf
driveAll.head()


# In[20]:


m_drive_times = gis.map('Oakland, Michigan')
m_drive_times


# In[16]:


m_drive_times.add_layer(result.layers[0])


# #### Service area generation for the visible [5] minutes layer

# In[29]:


# Generate service areas
visible_result = create_drive_time_areas(clinic_layer, 
                                         [5], 
                                         output_name="DriveTimeToClinics_5mins"+str(datetime.now().microsecond), 
                                         overlap_policy='Dissolve')


# In[30]:


# Grant 'visible_result' public access
visible_result.share(everyone=True)


# ### 3. Spatially join the `drive_times_all_layer` layer with `oakland_cbg` layer to obtain drive times for each census block group

# In[31]:


from arcgis.features.analysis import join_features


# In[32]:


# Perform spatial join between CBG layer and the service areas created for all time durations
cbg_with_driveTime = join_features(oakland_cbg, 
                                   drive_times_all_layer, 
                                   spatial_relationship='Intersects', 
                                   output_name='Oakland_CBG_allDriveTimes'+str(datetime.now().microsecond))


# In[33]:


# Grant 'cbg_with_driveTime' public access
cbg_with_driveTime.share(everyone=True)


# In[34]:


# Converting it to a FeatureSet
drive_times_cbg = cbg_with_driveTime.layers[0]
drive_times_features = drive_times_cbg.query()
print('Total number of rows in the drive time (upto 25 mins) dataset: '+ str(len(drive_times_features.features)))


# This implies that the farthest clinic in the county is within a 25 minute drive time window

# In[37]:


# Convert it to a SpatialDataFrame
drive_time_cbg_df = drive_times_features.sdf
drive_time_cbg_df.head()


# ### 4. Enrich census block groups with suitable data

# The 3 geo-enrichment factors that we will be including in this study are:
# 
# > 1. 2017 Total Population (Analysis Variable - TOTPOP_CY)
# > 2. 2017 Health Insurance (Analysis Variable - X8002_X)
# > 3. 2017 Education : Bachelor's Degree (Analysis Variable - BACHDEG_CY)

# In[38]:


from arcgis.features.enrich_data import enrich_layer


# In[39]:


# Perform data enrichment
enriched_drive_time_cbg = enrich_layer(drive_times_cbg, 
                                       analysis_variables=['TOTPOP_CY', 'X8002_X', 'BACHDEG_CY'], 
                                       output_name="Enriched_DriveTime_CBG"+str(datetime.now().microsecond))


# In[40]:


# Check type of 'enriched_drive_time_cbg'
type(enriched_drive_time_cbg)


# In[41]:


# Grant 'enriched_drive_time_cbg' public access
enriched_drive_time_cbg.share(everyone=True)


# In[42]:


# Convert it to a Feature Set
enriched_driveTimes_layer = enriched_drive_time_cbg.layers[0]
enriched_driveTimes_features = enriched_driveTimes_layer.query()
print('Total number of rows in the drive time (upto 25 mins) dataset: '+ str(len(enriched_driveTimes_features.features)))


# In[43]:


# Convert it to a SpatialDataFrame
enriched_df = enriched_driveTimes_features.sdf
enriched_df.head()


# In[46]:


# Print column names for ease of use during visualization
enriched_df.columns


# ### 5. Visualize results

# In[2]:


# Create map of Oakland county
map1 = gis.map('Oakland, Michigan', zoomlevel=10)
map1


# In[50]:


map1.add_layer(enriched_driveTimes_layer, {'renderer':'ClassedSizeRenderer',
                                           'field_name':'X8002_X',
                                           'opacity':0.6})


# In[52]:


map1.add_layer(visible_result.layers[0], {'opacity':0.6})


# In[53]:


map1.add_layer(clinic_layer, {'opacity':0.75})


# Similar maps can be generated for the following variables too
# > 1. Travel Time (`ToBreak`)
# > 2. Population (`TOTPOP_CY`)
# > 3. Citizens with Bachelor's Degree (`BACHDEG_CY`)

# ### Conclusion
# 
# As we can see from the map, clinics are concentrated in a certain part of the county. A few counties have relatively larger population, as compared to their health care spending and population count with Bachelor's degrees and don't have clinics very close by (drime time greater than 10 mins). This subtly hints at disparity in location of clinics with population/affluence level of the county. 
# 
# This analysis also sets stage for bringing in data about patients of this epidemic, to do a socio-economic study of this epidemic.

# In[ ]:






# ====================
# extracting_building_footprints_from_drone_data.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Extracting Building Footprints From Drone Data
# > * 🔬 Data Science
# * 🥠 Deep Learning and pixel-based classification

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Export Training Data](#Export-Training-Data)
#  * [Rasterize Building Footprints](#Rasterize-Building-Footprints)
#  * [Reclassify the Rasterized Building Footprints](#Reclassify-the-Rasterized-Building-Footprints)
#  * [Convert to a Themetic Raster with Pixel Type 8bit unsigned](#Convert-to-a-Themetic-Raster-with-Pixel-Type-8bit-unsigned)
#  * [Export training data](#Export-training-data)
# * [Train the Model](#Train-the-Model)
#  * [Required Imports](#Required-Imports)
#  * [Prepare Data](#Prepare-Data)
#  * [Visualise few samples from your Training data](#Visualise-few-samples-from-your-Training-data)
#  * [Load an UnetClassifier model](#Load-an-UnetClassifier-model)
#  * [Find an Optimal Learning Rate](#Find-an-Optimal-Learning-Rate)
#  * [Fit the model](#Fit-the-model)
#  * [Save the model](#Save-the-model)
#  * [Load an Intermediate model to train it further](#Load-an-Intermediate-model-to-train-it-further)
#  * [Preview Results](#Preview-Results)
# * [Deploy Model and Extract Footprints](#Deploy-Model-and-Extract-Footprints)
#  * [Generate a Classified Raster using Classify Pixels Using Deep Learning tool](#Generate-a-Classified-Raster-using-Classify-Pixels-Using-Deep-Learning-tool)
#  * [Streamline Postprocessing Workflow Using Model Builder](#Streamline-Postprocessing-Workflow-Using-Model-Builder)
#  * [Final Output](#Final-Output)
# * [Limitations](#Limitations)
# 

# ## Introduction
# 
# Building footprints is a required layer in lot of mapping exercises, for example in basemap preparation, humantitarian aid and disaster management, transportation and a lot of other applications it is a critical component.Traditionally GIS analysts delineate building footprints by digitizing aerial and high resolution satellite imagery.
# 
# This sample shows how ArcGIS API for Python can be used to train a deep learning model to extract building footprints from drone data. The models trained can be used with ArcGIS Pro or ArcGIS Enterprise and even support distributed processing for quick results.
# 
# > For this sample we will be using data which originates from USAA and covers the region affected by Woolsey fires. The Imagery used here is an orthomosaic of data acquired just after the fires using an optical sensor on drone with a spatial resolution of 30cm.
# 

# 

# <center>A subset of Drone Imagery Used overlayed with Training Data (Building Footprint Layer).</center>

# In this workflow we will basically have three steps.
# * Export Training Data
# * Train a Model
# * Deploy Model and Extract Footprints

# ## Export Training Data
# 
# Training data can be exported by using the 'Export Training Data For Deep Learning' tool available in [ArcGIS Pro](https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/export-training-data-for-deep-learning.htm) as well as [ArcGIS Enterprise](https://developers.arcgis.com/rest/services-reference/export-training-data-for-deep-learning.htm). For this example we prepared training data in 'Classified Tiles' format using a 'chip_size' of 400px and 'cell_size' of 30cm in ArcGIS Pro. 
# 
# 

# ### Rasterize Building Footprints
# Rasterize building footprints using <a href="https://pro.arcgis.com/en/pro-app/tool-reference/conversion/polygon-to-raster.htm">'Polgon to Raster'</a> tool.
# <center>Rasterize Building Footprints</center>
# 

# ```python
# arcpy.conversion.PolygonToRaster("Building_Footprints", "OBJECTID", r"C:\sample\sample.gdb\Building_Footprints_PolygonToRaster", "CELL_CENTER", "NONE", 0.3)
# ```

# ### Reclassify the Rasterized Building Footprints
# 
# Convert all NoData values to '0' and other values to '1' using [Reclassify Raster](https://pro.arcgis.com/en/pro-app/tool-reference/3d-analyst/an-overview-of-the-raster-reclass-toolset.htm) tool.
#  After selecting input raster click on 'classify' and set only one class. For the class with 'valid data range' Set 'New' value to '1' for 'NoData' set 'New' value to '0'.
# <center>Reclassify Raster</center>

# ```python
# arcpy.ddd.Reclassify("Building_Footprints_PolygonToRaster", "Value", "1 1;NODATA 0", r"C:\sample\sample.gdb\Building_Footprints_PolygonToRaster_reclass", "DATA")
# ```

# ### Convert  to a Themetic Raster with Pixel Type '8bit unsigned'  
# Export the reclassified raster from the previous step to a '8bit unsigned' raster.
# <center>Export Raster</center>
# 

# ```python
# arcpy.management.CopyRaster(r"C:\sample\sample.gdb\Building_Footprints_PolygonToRaster_reclass", r"C:\sample\sample.gdb\Building_Footprints_PolygonToRaster_reclass_8bitu", '', None, '', "NONE", "NONE", "8_BIT_UNSIGNED", "NONE", "NONE", "GRID", "NONE", "CURRENT_SLICE", "NO_TRANSPOSE")
# ```

# Open 'Catalog Pane' > navigate to the raster you just exported > Right Click and select 'Properties'. In the 'Raster Properties' pane change the source type to 'Themetic'.
# <center>Set the Source Type of Raster to Thematic.</center>
# 

# 
# <center>A Subset of our Themetic Raster generated by the previous steps.</center>

# ### Export training data
# Export training data using 'Export Training data for deep learning' tool, [detailed documentation here](#https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/export-training-data-for-deep-learning.htm)
# - Set drone imagery as 'Input Raster'.
# - Set a location where you want to export the training data, it can be an existing folder or the tool will create that for you.
# - Set classified raster as 'Input Feature Class Or Classified Raster'.
# - In the option 'Input Mask Polygons' we can set a mask layer to limit the tool to export training data for only those areas which have buildings in it, we created one by generating a grid of 200m by 200m on building footprint layer's extent and dissolving only those polygons which contained buildings to a single multipolygon feature.
# - 'Tile Size X' & 'Tile Size Y' can be set 
# - Select 'Classified Tiles' as the 'Meta Data Format' because we are training an 'Unet Model'.
# - In 'Environments' tab set an optimum 'Cell Size' which is small enough to allow model to learn the texture of building roofs by the details and big enough to allow multiple buildings to fall in one tile and model can also understand the surrounding context of the buildings. For this example we used 0.3 cell size which meant 30cm on a project coordinate system. 
# <div>
# </div>

# ```python
# arcpy.ia.ExportTrainingDataForDeepLearning("Imagery", r"C:\sample\Data\Training Data 400px 30cm", "Building_Footprints_8bitu.tif", "TIFF", 400, 400, 0, 0, "ONLY_TILES_WITH_FEATURES", "Classified_Tiles", 0, None, 0, "grid_200m_Dissolve", 0, "MAP_SPACE", "NO_BLACKEN", "Fixed_Size")
# ```

# This will create all the necessary files needed for the next step in the 'Output Folder', and we will now call it our training data.

# ## Train the Model
# 
# This step would be done using jupyter notebook and documentation is available <a href="https://developers.arcgis.com/python/guide/install-and-set-up/"> here to install and setup environment</a>. 

# ### Required Imports
# 

# In[1]:


import os
from pathlib import Path

import arcgis
from arcgis.gis import GIS
from arcgis.learn import prepare_data


# ### Prepare Data
# 
# We would specify the path to our training data and a few hyper parameters.
# 
# `path`: path of folder containing training data.<br/>
# `chip_size`: Same as per specified while exporting training data<br/>
# `batch_size`: No of images your model will train on each step inside an epoch, it directly depends on the memory of your graphic card. 8 worked for us on a 11GB GPU.

# In[2]:


gis = GIS('home')


# In[3]:


training_data = gis.content.get('2f9058a37129460fac8a661c5d462a5f')
training_data


# In[4]:


filepath = training_data.download(file_name=training_data.name)


# In[5]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[6]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# In[5]:


# Prepare Data
data = prepare_data(path=data_path,
                    chip_size=400, 
                    batch_size=8)


# ### Visualise a few samples from your Training data
# The code below shows a few samples of our data with the same symbology as in ArcGIS Pro.
# 
# `rows`: No of rows we want to see the results for.<br>
# `alpha`: controls the opacity of labels(Classified imagery) over the drone imagery

# In[6]:


data.show_batch(rows=2, alpha=0.7)


# ### Load an `UnetClassifier` model
# 
# The Code below will create an `UnetClassifier` model, it is based on a state of art deep learning model architecture 'U-net'. This type of model is used where 'pixel wise segmention' or in GIS terminology 'imagery classification' is needed, by default this model will be loaded on a pretrained resnet backbone.  

# In[7]:


# Create Unet Model
model = arcgis.learn.models.UnetClassifier(data)


# ### Find an Optimal Learning Rate
# 
# Optimization in deep learning is all about tuning 'hyperparameters'. In this step we will find an 'optimum learning rate' for our model on the training data. Learning rate is a very important parameter, while training our model it will see the training data several times and adjust itself (the weights of the network). Too high learning rate will lead to the convergence of our model to an suboptimal solution and too low learning can slow down the convergence of our model. We can use the `lr_find()` method to find an optimum learning rate at which can train a robust model fast enough. 

# In[ ]:


# Find Learning Rate
model.lr_find()


# The above method passes small batches of data to the model at a range of learning rates while records the losses for the respective learning rate after trainig. The result is this graph where loss has been plotted at y-axis with the respective learning rate at x-axis. Looking at the above graph we can see that the loss continously decreases after 1e-06 or .000001 and continous falling untill  bumping back at 1e-02, we can now pick any value near to the center of this steepest decline. Now we will pick 1e-03 or .001 so that can be set the highest learning rate and lowest learning rate a tenth of it in the next step.

# ### Fit the model
# 
# We would fit the model for 30 `epochs`. One epoch mean the model will see the complete training set once and so on.

# In[13]:


# Training
model.fit(30, lr=slice(0.0001, 0.001))


# The full training data was split into training set and validation set at the [Prepare Data step](#Prepare-Data), by default the validation set is .2 or 20% of the full training data and the remaining 80% goes in the training set. Here we can see loss on both training and validatiaon set, it helps us to be aware about how well our model generalizes on data which it has never seen and prevent overfitting of the model. Also as we can see our model here is able to classify pixels to building or non building with an accuracy of 96%.

# ### Save the model
# 
# We would now save the model which we just trained as a 'Deep Learning Package' or '.dlpk' format. Deep Learning package is the standard format used to deploy deep learning models on the ArcGIS platform. `UnetClassifier` models can be deployed by the tool 'Classify Pixels Using Deep Learning' available in <a href="https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/classify-pixels-using-deep-learning.htm">ArcGIS Pro</a> as well as <a href="https://developers.arcgis.com/rest/services-reference/classify-pixels-using-deep-learning-.htm">ArcGIS Enterprise</a>. For this sample we will using this model in ArcGIS Pro to extract building footprints.
# 
# We will use the `save()` method to save the model and by default it will be saved to a folder 'models' inside our training data folder itself.

# In[2]:


# Save model to file
model.save('30e')


# ### Load an Intermediate model to train it further
# 
# If we need to further train an already saved model we can load it again using the code below and go back to [Train the model step](#Train-the-Model) and further train it.

# In[2]:


# Load Model from previous saved files 
# model.load('30e')


# ### Preview Results
# 
# The code below will pick a few random samples and show us ground truth and respective model predictions side by side. This allows us to preview the results of your model in the notebook itself, once satisfied we can save the model and use it further in our workflow.

# In[10]:


# Preview Results
model.show_results()


# ## Deploy Model and Extract Footprints
# 
# The deep learning package saved in previous step can be used to extract classfied raster using 'Classify Pixels Using Deep Learning' tool.
# Further the classfied raster is regularised and finally converted to a vector Polygon layer. The regularisation step uses advanced ArcGIS geoprocessing tools to remove unwanted artifacts in the output. 

# ### Generate a 'Classified Raster' using 'Classify Pixels Using Deep Learning' tool
# 
# In this step we will generate a classfied raster using 'Classify Pixels Using Deep Learning' tool available in both <a href="https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/classify-pixels-using-deep-learning.htm">ArcGIS Pro</a> and <a href="https://developers.arcgis.com/rest/services-reference/classify-pixels-using-deep-learning-.htm">ArcGIS Enterprise</a>.
# 
# - Input Raster: The raster layer from which we want to extract building footprints from.
# - Model Definition: It will be located inside the saved model in 'models' folder in '.emd' format.
# - padding: The 'Input Raster' is tiled and the deep learning model classifies each individual tile separately before producing the final 'Output Classified Raster'. This may lead to unwanted artifacts along the edges of each tile as the model has little context to predict accuratly. Padding as the name suggests allows us to supply some extra information along the tile edges, this helps the model to predict better. 
# - Cell Size: Should be close to at which we trained the model, we specified that at the [Export training data step]("#Export-training-data") .
# - Processor Type: This allows to control wether the system's 'GPU' or 'CPU' would be used in to classify pixels, by 'default GPU' will be used if available.
# - Parallel Processing Factor: This allows us to scale this tool, this tool can be scaled on both 'CPU' and 'GPU'. It specifies that the opration would be spread across how many 'cpu cores' in case of cpu based operation or 'no of GPU's' incase of GPU based operation.
# <div>
# </div>
# 

# ```python
# out_classified_raster = arcpy.ia.ClassifyPixelsUsingDeepLearning("Imagery", r"C:\sample\Data\Training Data 400px 30cm\models\30e\30e.emd", "padding 100;batch_size 8"); 
# out_classified_raster.save(r"C:\sample\sample.gdb\Imagery_ClassifyPixelsUsingD")
# ```

# Output of this tool will be in form of a 'classified raster' containing both background and building footprints.
# <center>A subset of preditions by our Model</center>

# ###  Streamline Postprocessing Workflow Using Model Builder
# 
# As postprocessing workflow below involves quite a few tools and the parameters set in these tools need to be experimented with to get optimum results. We would use model builder to streamline this for us and enable us iteratively change the parameters in this workflow.
# <center>Postprocessing workflow in Model Builder.</center>
# 
# 
# 

# Tools used and important parameters:
# - Majority Filter: Reduces noise in the classified raster, we would use 'four' neighbouring cells majority to smoothen the raster.
# - Raster to Polygon: Vectorizes raster to polygon based on cell values, we will keep the 'Simplify Polygons' and 'Create Multipart Features' options unchecked.
# - Eliminate: Removes selected records by merging them into nieghbouring features, we selected features below 10 sqm Area before using this tool to remove small unwanted noise that  Vectorizes raster to polygon based on cell values, we will keep the 'Simplify Polygons' and 'Create Multipart Features' options unchecked.
# - Buffer: To slightly increase coverage of buildings we would buffer them by '5cm'. Before buffering them we would select only building features this will remove background features from output of this tool.
# - Regularize Building Footprints: This tool will produce the final finished results by shaping them to how actual buildings look. 'Method' for regularisation would be 'Right Angles and Diagnol' and the parameters 'tolerance', 'Densification' and 'Precision' are '1.5', '1', '0.25' and '2' respectively. 
# 
# Attached below is the toolbox:

# In[3]:


from arcgis.gis import GIS
gis = GIS(username='api_data_owner')
gis.content.get('4bbb65755af24ff895581b39dc1d73e9')


# ### Final Output
# 
# The final output will be in form of a feature class.
# <center>A subset of Extracted Building Footprints after Post Processing.</center>

# 

# 

# <center>A few subsets of results overlayed on the drone imagery</center>

# ## Limitations
# 
# This type of model works well when buildings are seperated from each other by some distance. In some conditions where buildings are touching each other instance segmentation models should be tried as it would always give better results.

# In[ ]:






# ====================
# extracting_features_and_land_use_land_cover_using_panoptic_segmentation.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Extracting features and Land Use Land Cover using Panoptic Segmentation
# > *  Data Science
# > *  Deep Learning and Panoptic Segmentation

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item">
#     <li><span><a href="#Introduction" data-toc-modified-id="Introduction">Introduction</a></span></li>
#     <li><span><a href="#Export training data" data-toc-modified-id="Export training data">Export training data</a></span>
#     <li><span><a href="#Train the model" data-toc-modified-id="Train the model">Train the model</a></span>
#         <ul class="toc-item">
#             <li><span><a href="#Necessary imports" data-toc-modified-id="Necessary imports">Necessary imports</a></span></li>
#             <li><span><a href="#Get training data" data-toc-modified-id="Get training data">Get training data</a></span></li>
#             <li><span><a href="#Prepare data" data-toc-modified-id="Prepare data">Prepare data</a></span></li>
#             <li><span><a href="#Visualize training data" data-toc-modified-id="Visualize training data">Visualize training data</a></span></li>
#             <li><span><a href="#Load model architecture" data-toc-modified-id="Load model architecture">Load model architecture</a></span></li>
#             <li><span><a href="#Find an optimal learning rate" data-toc-modified-id="Find an optimal learning rate">Find an optimal learning rate</a></span></li>
#             <li><span><a href="#Fit the model" data-toc-modified-id="Fit the model">Fit the model</a></span></li>
#             <li><span><a href="#Visualize results in validation set" data-toc-modified-id="Visualize results in validation set">Visualize results in validation set</a></span></li>
#             <li><span><a href="#Accuracy assessment" data-toc-modified-id="Accuracy assessment">Accuracy assessment</a></span></li>
#             <li><span><a href="#Save the model" data-toc-modified-id="Save the model">Save the model</a></span></li></ul></li>
#     <li><span><a href="#Deploy the model" data-toc-modified-id="Deploy the model">Deploy the model</a></span>
#         <ul class="toc-item">
#             <li><span><a href="#Results" data-toc-modified-id="Results">Results</a></span></li>
#             <li><span><a href="#Visualize results using map widgets" data-toc-modified-id="Visualize results using map widgets">Visualize results using map widgets</a></span></li></ul></li>
#     <li><span><a href="#Conclusion" data-toc-modified-id="Conclusion">Conclusion</a></span></li></ul></div>

# ## Introduction

# Land cover classification is one of the most common tasks in remote sensing. It can have multiple applications and can be very useful in understanding an area, its geography, landscape, demographic, and more. Landcover classification can be further combined with building footprint extraction and object detection for even more detailed analysis. Currently, this is achieved through multiple models. However, using Panoptic Segmentation, we can detect objects of interest and also classify the pixels of a raster using a single model.
# 
# Panoptic Segmentation works with both types of classes - stuff and things. In deep learning jargon, stuff refers to the class that is indistinctive and uncountable, for example, vegetation, road, water, sky, etc. Most of the land cover classes will classify as stuff. Things refers to classes that are distinctive, for example, buildings, cars, pools, solar panels, etc., and are also referred to as instance classes.
# 
# Panoptic Segmentation models use three inputs during training - a raster, a feature layer or a classified raster that classifies each pixel in the raster into individual classes, and a feature layer containing polygons for the objects of interest (the instance classes). This notebook demonstrates how to use the Panoptic Segmentation model MaXDeepLab to classify Land Use Land Cover (LULC) and extract building footprints and cars using very high-resolution satellite imagery of Los Angeles County. The model has been trained using the ArcGIS Python API. The trained model can then be deployed in ArcGIS Pro or ArcGIS Enterprise to classify LULC and extract building footprints and cars.
# 
# 

# ## Export training data

# In[1]:


# Connect to GIS
from arcgis.gis import GIS
gis = GIS("home")
gis_ent = GIS('https://pythonapi.playground.esri.com/portal/')


# The following imagery layer contains very high resolution imagery of a part of LA County in California. The spatial resolution of the imagery is 7.5 cm (4 inches), and it contains 3 bands: Red, Green, and Blue. It is used as the 'Input Raster' for exporting the training data.

# In[2]:


training_raster = gis_ent.content.get('c3bb538699964697bae61570a77768cb')
training_raster


# To export training data for panoptic segmentation, we need two input layers.
# 1. A feature class or a classified raster for pixel classification.
# 2. A feature class with polygons for the instance classes to be detected.

# The following feature layer contains the polygons for the LULC classes for the corresponding imagery of LA County. It is used as the 'Input Feature Class' for exporting the training data.

# In[3]:


input_feature_class = gis.content.get('d64f11aea3ae47c19084f21c204ba318')
input_feature_class


# The following feature layer contains the polygons for the building footprints and cars and will be used as the 'Instance Feature Class' for exporting the training data.

# In[4]:


instance_feature_class = gis.content.get('22b83520db8f42ea98a5279b71684afd')
instance_feature_class


# Training data can be exported by using the [Export Training Data For Deep Learning](https://pro.arcgis.com/en/pro-app/latest/tool-reference/image-analyst/export-training-data-for-deep-learning.htm) tool available in [ArcGIS Pro](https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/export-training-data-for-deep-learning.htm) and [ArcGIS Enterprise](https://developers.arcgis.com/rest/services-reference/export-training-data-for-deep-learning.htm). For this example, we prepared the training data in ArcGIS Pro in the `Panoptic Segmentation` format using a `chip_size` of 512px and a `cell_size` of 0.32 ft. The `Input Raster`, the `Input Feature Class` and the `Instance Feature Class` have been made available to export the required training data. We have also provided a subset of the exported training data in the next section, if you want to skip this step.

# <figure>
#     <br>
#     <center>
#     <figcaption>Export Training Data for Deep Learning </figcaption>
#     </center>
# </figure>

# ## Train the model

# ### Necessary imports

# In[5]:


get_ipython().run_line_magic('matplotlib', 'inline')


# In[6]:


import os
import zipfile
from pathlib import Path
from arcgis.learn import prepare_data, MaXDeepLab


# ### Get training data

# We have already exported a smaller dataset and made it available so that it can be used by following the steps below. Exporting a larger dataset is suggested for better results.

# In[7]:


training_data = gis.content.get('69544aca740d42d3a3e36f7ad5ddb485')
training_data


# In[8]:


filepath = training_data.download(file_name=training_data.name)


# In[9]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[10]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# ### Prepare data

# We will specify the path to our training data and a few hyperparameters. 
# 
# - `data_path`: path of the folder/list of folders containing training data.
# - `batch_size`: Number of images your model will train on each step inside an epoch. Depends on the memory of your graphic card.
# - `chip_size`: The same as the tile size used while exporting the dataset. A smaller chip_size crops the tile.
# - `n_mask`: The maximum number of masks, i.e., the sum of unique instances of all things classes and total stuff classes, in a chip. The default value is 30. You can increase it if there are many objects in a single chip. A warning message will be displayed in some of the methods if the `n_mask` value is less than the number of objects in a chip. The function `compute_n_masks` can be used to calculate the maximum number of objects in any chip in a dataset. 
# 
# `batch_size`, `chip_size`, and `n_mask` will define the GPU memory requirements.

# In[11]:


data = prepare_data(data_path, batch_size=4, chip_size=512, n_masks=30)


# ### Visualize training data

# To get a sense of what the training data looks like, the `show_batch()` method will randomly pick a few training chips and visualize them. The chips are overlaid with masks representing the building footprints, cars, and the lulc classes in each image chip.

# In[12]:


data.show_batch(alpha=0.8)


# <figure>
#     <br>
# </figure>           

# ### Load model architecture

# `arcgis.learn` provides the `MaXDeepLab` model for panoptic segmentation task.

# In[13]:


model = MaXDeepLab(data)


# ### Find an optimal learning rate

# Learning rate is one of the most important hyperparameters in model training. The ArcGIS API for Python provides a learning rate finder that automatically chooses the optimal learning rate for you.

# In[14]:


lr = model.lr_find()
lr


# ### Fit the model 

# Next, we will train the model for a few epochs with the learning rate found above. We have trained the model using a large dataset for 20 epochs. If the loss continues to decrease, the model can be trained further until you start seeing overfitting.

# In[15]:


model.fit(20, lr=lr)


# We can see in the following plot that although the training loss was contantly reducing gradually, the validation loss became erratic after 12th epoch. The smallest validation loss was recorded for the 17th epoch.
# 
# MaXDeepLab has a compound loss function - a combination of multiple metrics. It has been observed that longer training with a  relatively flat validation loss curve can also improve results. A diverging training and validation loss curve will still reflect overfitting.

# In[16]:


model.plot_losses()


# ### Visualize results in validation set

# It is a good practice to see the results of the model viz-a-viz ground truth. The code below picks random samples and shows us ground truth and model predictions, side by side. This enables us to preview the results of the model we trained.

# In[17]:


model.show_results(alpha=0.8)


# ### Accuracy assessment

# MaXDeepLab provides the `panoptic_quality()` method that computes the panoptic quality, a combination of recognition and segmentation quality, of the model on the validation set.

# In[18]:


model.panoptic_quality()


# <figure>
# </figure>           

# ### Save the model

# We will save the trained model as a 'Deep Learning Package' ('.dlpk' format). The Deep Learning package is the standard format used to deploy deep learning models on the ArcGIS platform. 
# 
# We will use the `save()` method to save the trained model. By default, it will be saved to the 'models' sub-folder within our training data folder.

# In[19]:


model.save("la_panoptic_512px_ep21_lr001", publish=True)


# ## Deploy the model

# We can now use the saved model to detect buildings and cars and classify the LULC. Here, we have provided a very high resolution sample raster for LA County for inferencing. The spatial resolution of the imagery is 0.32 ft and contains 3 bands: Red, Green, and Blue.

# In[20]:


sample_inference_raster = gis_ent.content.get('0c8288d76d1c4301ab8b5efbad358f39')
sample_inference_raster


# We have provided a trained model to use for inferencing.

# In[21]:


trained_model = gis.content.get('4ef21be1d9fb4ced9eacb306558e8edd')
trained_model


# In this step, we will use the [Detect Objects Using Deep Learning](https://pro.arcgis.com/en/pro-app/latest/tool-reference/image-analyst/detect-objects-using-deep-learning.htm) tool available in both ArcGIS Pro and ArcGIS Enterprise to generate a feature layer with detected buildings and cars and a classified raster with the LULC.

# <figure>
#     <br>
#     <center>
#     <figcaption>Detect Objects Using Deep Learning</figcaption>
#     </center>
# </figure>

# ### Results

# The model was run on the sample inference raster and the results can be viewed here.

# In[22]:


fc = gis_ent.content.get('3b5033eb16044a089795682256e819ae')
fc


# In[23]:


ic = gis.content.get('ca935d3e3e614e2392485e07ab56b97f')
ic


# ### Visualize results using map widgets

# Two map widgets are created showing the inference raster and the results.

# In[24]:


map1 = gis_ent.map()
map1.add_layer(inference_raster)
map2 = gis.map()
map2.add_layer(fc)
map2.add_layer(ic)


# The maps are synchronized with each other using [MapView.sync_navigation](https://developers.arcgis.com/python/api-reference/arcgis.widgets.html#arcgis.widgets.MapView.sync_navigation) functionality. It helps in comparing the inference raster with the results. Detailed description about advanced map widget options can be referred [here](https://developers.arcgis.com/python/guide/advanced-map-widget-usage/).

# In[25]:


map2.sync_navigation(map1)


# In[26]:


from ipywidgets import HBox, VBox, Label, Layout


# Set the layout of the map widgets using [Hbox and Vbox](https://developers.arcgis.com/python/api-reference/arcgis.widgets.html?highlight=hbox).

# In[27]:


hbox_layout = Layout()
hbox_layout.justify_content = 'space-around'

hb1=HBox([Label('Raster'),Label('Results')])
hb1.layout=hbox_layout


# In[28]:


VBox([hb1,HBox([map1,map2])])


# <figure>
#     <br>
# </figure>

# ## Conclusion

# In this notebook we saw how the panoptic segmentation model MaXDeepLab can be used to classify pixels and detect objects at the same time. We learned how to create the data in the Panoptic Segmenation format and use it to train a model using the ArcGIS API for Python. The trained model was then used to generate results for a sample raster and was displayed in the notebook.


# ====================
# extracting_slums_from_satellite_imagery.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Extracting Slums from Satellite Imagery
# > * 🔬 Data Science
# * 🥠 Deep Learning and pixel-based classification

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Export Training Data](#Export-Training-Data)
#  * [Preprocessing Slum Boundaries](#Preprocessing-Slum-Boundaries)
#  * [Export training data](#Export-training-data)
# * [Train the Model](#Train-the-Model)
#  * [Prepare Data](#Prepare-Data)
#  * [Load an UnetClassifier model](#Load-an-UnetClassifier-model)
#  * [Find an Optimal Learning Rate](#Find-an-Optimal-Learning-Rate)
#  * [Fit the model](#Fit-the-model)
#  * [Save the model](#Save-the-model)
#  * [Load an Intermediate model to train it further](#Load-an-Intermediate-model-to-train-it-further)
#  * [Preview Results](#Preview-Results)
# * [Deploy Model and Extract Slum Boundaries](#Deploy-Model-and-Extract-Slum-Boundaries)
# * [Postprocessing](#Postprocessing)
# * [Conclusion](#Conclusion)
# 

# ## Introduction
# 
# Every geographical area has its own definition for slum, but slum is usually an area with substandard housing and inadequate services. Slum is an important issue in developing countries as people reside in hazardous living conditions in congested slums and it makes them vulnerable to natural, physical and social disasters. Owing to the sensitivity of the slums, authorities need to continuously track and monitor the growth of the slums.
# 
# This sample shows how we can extract the slum boundaries from satellite imagery using the learn module in ArcGIS API for Python.
# The model trained here can be deployed on ArcGIS Pro as well as ArcGIS Enterprise and even support distributed processing for quick results.
# 
# In this workflow we will basically have three steps.
# * Export Training Data using ArcGIS Pro
# * Train a Model using learn module in ArcGIS API for Python
# * Deploying the Model on ArcGIS Pro
# 
# >For this sample we will be using data from Dharavi area in Mumbai, India. For a comparative analysis we will be using imagery for 2004 and 2014 acquired using quickbird and worldview3 sensors with a spatial resolution of 60cm. 

# 

# <center>A subset of Imagery overlaid with Training Data (Slum Footprint Layer).</center>

# ## Export Training Data
# 
# We will export training data using the 'Export Training Data For Deep Learning' tool available in [ArcGIS Pro](https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/export-training-data-for-deep-learning.htm) as well as [ArcGIS Enterprise](https://developers.arcgis.com/rest/services-reference/export-training-data-for-deep-learning.htm). For this example we prepared training data in 'Classified Tiles' format using a 'chip_size' of 400px and 'cell_size' of 80cm in ArcGIS Pro.

# In[1]:


from arcgis.gis import GIS
gis = GIS('home')


# The item below is a feature layer of slum boundary for the year 2004.

# In[2]:


slum_boundary = gis.content.search('slum_ground_truth_2004', 'feature layer')[0]
slum_boundary


# Export training data using 'Export Training data for deep learning' tool, [detailed documentation here](#https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/export-training-data-for-deep-learning.htm)
# - Set the **satellite imagery** as 'Input Raster'.
# - Set a location where you want to export the training data, it can be an existing folder or the tool will create that for you.
# - Set **slum_boundary** as 'Input Feature Class Or Classified Raster'.
# - In the option 'Input Mask Polygons' we can set a mask layer to limit the tool to export training data for only those areas which have slum in it, we can create one by generating a grid on slum boundary layer's extent and dissolving only those polygons which contained slum to a single multipolygon feature.
# - '**Tile Size X**' & '**Tile Size Y**': **400**
# - Select 'Classified Tiles' as the 'Meta Data Format' because we are training an 'Unet Model'.
# - In 'Environments' tab set an optimum 'Cell Size' which is small enough to allow model to learn the texture and setting of slum as well as also understand the surrounding context of the slums. For this example we used 0.8 cell size which meant **80cm** on the project coordinate system. 

# <div>
# </div>

# ```python
# with arcpy.EnvManager(cellSize=0.8):
#     arcpy.ia.ExportTrainingDataForDeepLearning("Imagery", r"C:\training_data", "slum_boundary", "TIFF", 400, 400, 0, 0, "ONLY_TILES_WITH_FEATURES", "Classified_Tiles", 0, "code", 0, "grid_200m_Dissolve", 0, "MAP_SPACE", "PROCESS_AS_MOSAICKED_IMAGE", "NO_BLACKEN", "FIXED_SIZE")
# ```

# This will create all the necessary files needed for the next step in the 'Output Folder', and we will now call it our **training data**.

# ## Train the Model
# 
# As we have already exported our training data, we will now train our model using ArcGIS API for Python. We will be using `arcgis.learn` module which contains tools and deep learning capabilities. Documentation is available <a href="https://developers.arcgis.com/python/guide/install-and-set-up/"> here to install and setup environment</a>.

# ### Necessary Imports
# 

# In[3]:


import os
from pathlib import Path

import arcgis
from arcgis.learn import prepare_data


# ### Prepare Data
# 
# The `prepare_data` function takes in the training data and applies various types of transformations and augmentations. We would now use this function and specify the path to our training data and a few parameters to create a fastai databunch. 
# 
# `path`: path of folder containing training data.<br/>
# `chip_size`: Same as per specified while exporting training data<br/>
# `batch_size`: No of images your model will train on each step inside an epoch, it directly depends on the memory of your graphic card. 8 worked for us on a GPU with 11GB memory.

# In[4]:


training_data = gis.content.get('e95daeb4085d4afb8ca81d0a1b11dce2')
training_data


# In[5]:


filepath = training_data.download(file_name=training_data.name)


# In[6]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[7]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# In[8]:


# Prepare Data
data = prepare_data(data_path,
                    chip_size=400, 
                    batch_size=6)


# #### Visualize a few samples from your Training data
# To get a sense of what the training data looks like we will use the `show_batch()` method in arcgis.learn which randomly picks a few samples from the training data and visualizes them with the same symbology as in ArcGIS Pro.
# 
# `rows`: No of rows we want to see the results for.<br>
# `alpha`: controls the opacity of labels(Classified imagery) over the drone imagery

# In[9]:


data.show_batch(rows=3, alpha=0.5)


# ### Load an `UnetClassifier` model
# 
# The Code below will create a `UnetClassifier` model, it is based on a state of art deep learning model architecture 'U-net'. This type of model is used where '**pixel wise segmentation**' or in GIS terminology '**imagery classification**' is needed, by default this model will be loaded on a pretrained 'resnet' backbone.  

# In[10]:


# Create Unet Model
model = arcgis.learn.models.UnetClassifier(data)


# ### Find an Optimal Learning Rate
# 
# Optimization in deep learning is all about tuning 'hyperparameters', in this step we will find an optimum learning rate for our model on the training data. Learning rate is a very important parameter, while training our model it will see the training data several times and adjust itself (the weights of the network). Too high learning rate will lead to the convergence of our model to an suboptimal solution and too low learning can slow down the convergence of our model. We can use the `lr_find()` method to find an optimum learning rate at which can train a robust model fast enough. 

# In[11]:


# Find Learning Rate
model.lr_find()


# The above method passes small batches of data to the model at a range of learning rates while records the losses for the respective learning rate after training. The result is this graph where loss has been plotted at y-axis with the respective learning rate at x-axis. Looking at the above graph we can see that the loss continuously decreases after 1e-06 or .000001 and continuous falling until bumping back at 1e-02, we can now pick any value near to the center of this steepest decline. Now we will pick 1e-03 or .001 so that can be set the highest learning rate and lowest learning rate a tenth of it in the next step.

# ### Fit the model
# 
# We will fit our model for 25 epochs. One epoch mean the model will see the complete training set once and so on.

# In[12]:


# Training
model.fit(25, lr=0.0002)


# The full training data was split into training set and validation set at the [Prepare Data step](#Prepare-Data), by default the validation set is .2 or 20% of the full training data and the remaining 80% goes in the training set. Here we can see loss on both training and validation set, it helps us to be aware about how well our model generalizes on data which it has never seen and prevent overfitting of the model. As we can see that with only 25 epochs, we are already seeing reasonable results. Further improvement can be achieved through more sophisticated hyperparameter tuning. Let's save the model for further training or inference later. The model should be saved into a model’s folder in your folder. By default, it will be saved into your data path that you specified in the very beginning of this notebook.

# ### Save the model
# 
# We will save the model which we just trained as a '**Deep Learning Package**' or '.dlpk' format. Deep Learning package is the standard format used to deploy deep learning models on the ArcGIS platform. `UnetClassifier` models can be deployed by the tool 'Classify Pixels Using Deep Learning' available in <a href="https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/classify-pixels-using-deep-learning.htm">ArcGIS Pro</a> as well as <a href="https://developers.arcgis.com/rest/services-reference/classify-pixels-using-deep-learning-.htm">ArcGIS Enterprise</a>. For this sample we will using this model in ArcGIS Pro to extract building footprints.
# 
# We will use the `save()` method to save the model and by default it will be saved to a folder '**models**' inside our training data folder itself.

# In[13]:


# Save model to file
model.save('25e')


# ### Load an Intermediate model to train it further
# 
# If we need to further train an already saved model we can load it again using the code below and go back to [Train the model step](#Train-the-Model) and further train it.

# In[14]:


# Load Model from previous saved files 
# model.load('25e')


# ### Preview Results
# 
# The code below will pick a few random samples and show us ground truth and respective model predictions side by side. This allows us to preview the results of your model in the notebook itself, once satisfied we can save the model and use it further in our workflow.

# In[15]:


# Preview Results
model.show_results()


# ## Deploy Model and Extract Slum Boundaries
# 
# The deep learning package saved in previous step can be used to extract classified raster using '**Classify Pixels Using Deep Learning**' tool.
# Further the classified raster is converted to a vector Polygon layer.

# 
# #### Generate a Classified Raster using 'Classify Pixels Using Deep Learning tool'
# 
# In this step we will generate a classified raster using 'Classify Pixels Using Deep Learning' tool available in both [**ArcGIS Pro**](https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/classify-pixels-using-deep-learning.htm) and [**ArcGIS Enterprise**](https://developers.arcgis.com/rest/services-reference/classify-pixels-using-deep-learning-.htm).
# 
# - **Input Raster**: The raster layer from which we want to extract building footprints from.
# - **Model Definition**: It will be located inside the saved model in 'models' folder in '.emd' format.
# - **padding**: The 'Input Raster' is tiled and the deep learning model classifies each individual tile separately before producing the final '**Output Classified Raster**'. This may lead to unwanted artifacts along the edges of each tile as the model has little context to predict accurately. Padding as the name suggests allows us to supply some extra information along the tile edges, this helps the model to predict better. 
# - **Cell Size**: Should be close to at which we trained the model, we specified that at the [Export training data step]("#Export-training-data") .
# - **Processor Type**: This allows to control whether the system's '**GPU**' or '**CPU**' would be used in to classify pixels, by 'default GPU' will be used if available.
# - **Parallel Processing Factor**: This allows us to scale this tool, this tool can be scaled on both 'CPU' and 'GPU'. It specifies that the operation would be spread across how many '**cpu cores**' in case of cpu based operation or '**no of GPU's**' incase of GPU based operation.

# 

# ```python
# out_classified_raster = arcpy.ia.ClassifyPixelsUsingDeepLearning(r"Imagery\wv3_juris_2014_mpt2.img", r"C:\sample\Training Data\models\140e\140e.emd", "padding 100;batch_size 8"); 
# out_classified_raster.save(r"C:\sample\sample.gdb\wv3_juris_2014_mpt2_Classify")
# ```

# Output of this tool will be in form of a 'classified raster' containing both background and slum patches.
# <center>A subset of preditions by our Model</center>

# ### Postprocessing
# 
# #### Majority Filter
# 
# We will apply majority filter on the classified raster produced in previous step by our model, this will **Reduce noise** in the classified raster, we would use 'four' neighboring cells majority to smoothen the raster.

# ```python
# out_raster = arcpy.sa.MajorityFilter("slum_2004_pred", "FOUR", "MAJORITY"); 
# out_raster.save(r"C:\sample\sample.gdb\slum_2004_pred_maj")
# ```

# #### Raster to Polygon
# 
# We will convert the processed classified raster produced in previous step to a polygon feature class layer using the 'Raster to Polygon' tool.

# ```python
# arcpy.conversion.RasterToPolygon("slum_2014_pred_maj", r"C:\sample\sample.gdb\slums_detected_2014", "NO_SIMPLIFY", "Value", "SINGLE_OUTER_PART", None)
# ```

# #### Final Output
# 
# The final output will be in form of a feature class. We would first use a tool majority filter to smoothen prediction by our model and then use polygon to raster tool to vectorize our results.

# 

# <center> A subset of results overlaid on satellite imagery it was predicted on (Worldview 2014) by our model.</center>

# ### Conclusion
# 
# This type of workflows can be used by government bodies to continuously track and monitor the growth of slum areas, this will help decision makers take informed decisions. Slums are a wide subject and the definition may vary across geographical areas, thus the workflow has to be adapted for better results.

# #### An example of ArcGIS dashboard tracking slums

# In[16]:


gis = GIS('home')
gis.content.get('809e82a098264c38bfd608e7cc08d5f3')


# In[ ]:





# In[ ]:






# ====================
# feature_categorization_using_satellite_imagery_and_deep_learning.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Feature Categorization using Satellite Imagery and Deep Learning
# > * 🔬 Data Science
# * 🥠 Deep Learning and Feature Classifier

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Introduction and methodology" data-toc-modified-id="Introduction and methodology-1">Introduction and methodology</a></span></li><li><span><a href="#Part-1---Export-training-data-for-deep-learning" data-toc-modified-id="Part-1---Export-training-data-for-deep-learning-2">Part 1 - Export training data for deep learning</a></span><ul class="toc-item"><li><span><a href="#Import-ArcGIS-API-for-Python-and-get-connected-to-your-GIS" data-toc-modified-id="Import-ArcGIS-API-for-Python-and-get-connected-to-your-GIS-2.1">Import ArcGIS API for Python and get connected to your GIS</a></span></li><li><span><a href="#Prepare-data-that-will-be-used-for-training-data-export" data-toc-modified-id="Prepare-data-that-will-be-used-for-training-data-export-2.2">Prepare data that will be used for training data export</a></span></li><li><span><a href="#Specify-a-folder-name-in-raster-store-that-will-be-used-to-store-our-training-data" data-toc-modified-id="Specify-a-folder-name-in-raster-store-that-will-be-used-to-store-our-training-data-2.3">Specify a folder name in raster store that will be used to store our training data</a></span></li><li><span><a href="#Export-training-data-using-arcgis.learn" data-toc-modified-id="Export-training-data-using-arcgis.learn-2.4">Export training data using <code>arcgis.learn</code></a></span></li></ul></li><li><span><a href="#Part-2---Model-training" data-toc-modified-id="Part-2---Model-training-3">Part 2 - Model training</a></span><ul class="toc-item"><li><span><a href="#Visualize-training-data" data-toc-modified-id="Visualize-training-data-3.1">Visualize training data</a></span></li><li><span><a href="#Load-model-architecture" data-toc-modified-id="Load-model-architecture-3.2">Load model architecture</a></span></li><li><span><a href="#Train-a-model-through-learning-rate-tuning-and-transfer-learning" data-toc-modified-id="Train-a-model-through-learning-rate-tuning-and-transfer-learning-3.3">Train a model through learning rate tuning and transfer learning</a></span></li><li><span><a href="#Visualize-classification-results-in-validation-set" data-toc-modified-id="Visualize-classification-results-in-validation-set-3.4">Visualize classification results in validation set</a></span></li></ul></li><li><span><a href="#Part-3---Inference and post processing" data-toc-modified-id="Part-3---Inference and post processing-4">Part 3 - Inference and post processing</a></span></li><li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-5">Conclusion</a></span></li><li><span><a href="#Reference" data-toc-modified-id="Reference-6">Reference</a></span></li></ul></div>

# ## Introduction and methodology
# 
# This sample notebook demonstrates the use of deep learning capabilities in ArcGIS to perform feature categorization. Specifically, we are going to perform automated damage assessment of homes after the devastating [Woolsey fires](https://en.wikipedia.org/wiki/Woolsey_Fire). This is a critical task in damage claim processing, and using deep learning can speed up the process and make it more efficient. The workflow consists of three major steps: (1) extract training data, (2) train a deep learning **feature classifier** model, (3) make inference using the model. 

# 
# <center>Figure 1. Feature classification example </center>

# ### Methodology

# <center>Figure 2. Methodology </center>

# ## Part 1 - Export training data for deep learning
# 
# To export training data for feature categorization, we need two input data:
# - An input raster that contains the spectral bands,
# - A feature class that defines the location (e.g. outline or bounding box) and label of each building.

# ### Import ArcGIS API for Python and get connected to your GIS

# In[1]:


import os
from pathlib import Path

import arcgis
from arcgis import GIS
from arcgis import learn, create_buffers
from arcgis.raster import analytics, Raster
from arcgis.features.analysis import join_features
from arcgis.learn import prepare_data, FeatureClassifier, classify_objects,  Model, list_models
arcgis.env.verbose = True


# In[2]:


gis = GIS('home')
gis_ent = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# ### Prepare data that will be used for training data export

# A building footprints feature layer will be used to define the location and label of each building.

# In[3]:


building_footprints = gis.content.search('buildings_woolsey', item_type='Feature Layer Collection')[0]
building_footprints


# We will buffer the building footprints layer for 150m using [create_buffers](https://doc.arcgis.com/en/arcgis-online/analyze/create-buffers.htm) . With 150m buffer, when the training data will be exported it will cover the surrounding of houses which will help the model to understand the difference between damaged and undamaged houses.

# In[45]:


building_buffer = arcgis.create_buffers(building_footprints, 
                                        distances=[150],
                                        units='Meters', 
                                        dissolve_type='None', 
                                        ring_type='Disks', 
                                        side_type='Full', 
                                        end_type='Round', 
                                        output_name='buildings_buffer'+str(datetime.now().microsecond), 
                                        gis=gis_ent)
building_buffer


# In[46]:


building_buffer.share(everyone=True)


# An aerial imagery of West Malibu will be used as input raster that contains the spectral bands. This raster will be used for exporting the training data.

# In[115]:


input_raster = Raster("https://pythonapi.playground.esri.com/server/rest/services/raster_for_training_damage_classifier/ImageServer",
                        gis=gis_ent,
                        engine="image_server")
input_raster


# ### Specify a folder name in raster store that will be used to store our training data

# In[7]:


ds = analytics.get_datastores(gis=gis_ent)
ds


# In[134]:


ds.search(types="rasterStore") 


# In[135]:


rasterstore = ds.get('/rasterStores/RasterDataStore')
rasterstore


# In[136]:


samplefolder = "feature_classifier_sample"+str(datetime.now().microsecond)
samplefolder


# In[137]:


output_folder = rasterstore.datapath + "/" + samplefolder
output_folder


# ### Export training data using `arcgis.learn`
# 
# Now ready to export training data using the `export_training_data()` method in arcgis.learn module. In addtion to feature class, raster layer, and output folder, we also need to specify a few other parameters such as `tile_size` (size of the image chips), `stride_size` (distance to move each time when creating the next image chip), `chip_format` (TIFF, PNG, or JPEG), `metadata_format` (how we are going to store those training labels). Note that unlike Unet and object detection, the metadata is set to be `Labeled_Tiles` here. More detail can be found [here](https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/export-training-data-for-deep-learning.htm). 
# 
# Depending on the size of your data, tile and stride size, and computing resources, this operation can take a while. In our experiments, this took 15mins~2hrs. Also, do not re-run it if you already run it once unless you would like to update the setting.
# 
# We will export the training data for a small sub-region of our study area and the whole study area will be used for inferencing of results. We will create a map widget, zoom in to the western corner of our study area and get the extent of the zoomed in map. We will use this extent in `Export training data using deep learning` function.

# In[14]:


export = learn.export_training_data(input_raster=input_raster,
                                    output_location=output_folder,
                                    input_class_data=building_buffer.layers[0], 
                                    classvalue_field = "class_ecode",
                                    chip_format="TIFF", 
                                    tile_size={"x":600,"y":600}, 
                                    stride_size={"x":0,"y":0}, 
                                    metadata_format="Labeled_Tiles",                                        
                                    context={"startIndex": 0, "exportAllTiles": False, "cellSize": 0.1},
                                    gis = gis_ent)


# ## Part 2 - Model training

# If you've already done part 1, you should already have the training chips. Please change the path to your own export training data folder that contains "images" and "labels" folder.

# In[52]:


training_data = gis.content.get('14f3f9421c6b4aa3bf224479c0eaa4f9')
training_data


# In[16]:


filepath = training_data.download(file_name=training_data.name)


# In[17]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[18]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# In[19]:


data = prepare_data(data_path, {1:'Damaged', 2:'Undamaged'}, chip_size=600, batch_size=16)


# ### Visualize training data
# To get a sense of what the training data looks like, `arcgis.learn.show_batch()` method randomly picks a few training chips and visualize them.

# In[54]:


data.show_batch()


# ### Load model architecture

# Now the building classification problem has become a standard image classfication problem. By default `arcgis.learn` uses Resnet34 as its backbone model followed by a softmax layer. 

# 
# <center>Figure 2. Resnet34 architecture [1] </center>

# In[21]:


model = FeatureClassifier(data)


# ### Train a model through learning rate tuning and transfer learning
# [Learning rate](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) is one of the most important hyperparameters in model training. Here we explore a range of learning rates to guide us to choose the best one.

# In[218]:


# The users can visualize the learning rate of the model with comparative loss. This case - loss rate decreases, so we are detecting more n more objects as it learns more
lr = model.lr_find()
lr


# Based on the learning rate plot above, we can see that the loss going down most dramatically at 1e-2. Therefore, we set learning rate to be to 1e-2. Let's start with 10 epochs for the sake of time.

# In[11]:


model.fit(epochs=15, lr=1e-2)


# ### Visualize classification results in validation set
# Now we have the model, let's look at how the model performs.

# In[17]:


model.show_results(rows=10)


# As we can see, with only 15 epochs, we are already seeing reasonable results. Further improvment can be acheived through more sophisticated hyperparameter tuning. Let's save the model for further training or inference later. The model should be saved into a models folder in your folder. By default, it will be saved into your `data_path` that you specified in the very beginning of this notebook.

# In[23]:


model.save(r'damage_classifier_15e', publish=True, gis=gis_ent)


# ### Accuracy assessment
# 
# `arcgis.learn` provides the `plot_confusion_matrix()` function that plots a confusion matrix of the model predictions to evaluate the model's accuracy.

# In[22]:


model.plot_confusion_matrix()


# The confusion matrix validates that the trained model is learning to classify and differentiate between damaged and undamaged houses. The diagonal numbers show the number of scenes correctly classified as their respective categories

# ## Part 3 - Inference and post processing
# 
# Now we have the model ready, let's apply the model to a new feature class with a few new buildings and see how it performs.

# In[29]:


fc_model_package = gis_ent.content.search("finetuned_damage_classifier_model owner:api_data_owner", item_type='Deep Learning Package')[0]
fc_model_package


# Now we are ready to install the model. Installation of the deep learning model item will unpack the model definition file, model file and the inference function script, and copy them to "trusted" location under the Raster Analytic Image Server site's system directory.

# In[27]:


fc_model = Model(fc_model_package)


# In[28]:


fc_model.install()


# In[29]:


fc_model.query_info()


# We will use [Classify Objects Using Deep Learning](https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/classify-objects-using-deep-learning.htm) for inferencing the results. The parameters required to run the function are:
# - `in_raster`:The input raster dataset to classify. The input can be a single raster or multiple rasters in a mosaic dataset, an image service, or a folder of images.
# - `out_feature_class`: The output feature class that will contain geometries surrounding the objects from the input feature class, as well as a field to store the classification label.
# - `in_model_definition`: It contains the path to the deep learning binary model file, the path to the Python raster function to be used, and other parameters such as preferred tile size or padding.
# - `in_features`: input feature class represents a single object. If no input feature class is specified, the tool assumes that each input image contains a single object to be classified.
# - `class_label_field`: The name of the field that will contain the classification label in the output feature class. If no field name is specified, a new field called ClassLabel will be generated in the output feature class.

# In[34]:


raster_for_inferencing = gis_ent.content.search('88f5610bb5134db0921b1824bdc47000')[0]
raster_for_inferencing


# In[30]:


inferenced_lyr = classify_objects(input_raster=raster_for_inferencing,
                                  model = fc_model,
                                  model_arguments={'batch_size': 4}
                                  input_features=building_buffer.layers[0],
                                  output_name="inferenced_layer_fc"+str(datetime.now().microsecond),
                                  class_value_field='status',
                                  context={'cellSize': 0.5, 'processorType':'GPU'},
                                  gis=gis_ent)
inferenced_lyr


# We can load the inference layer into a spatially enabled dataframe to examine the inference result. As we can see, all sample buildings are classified correctly with high confidence.

# In[36]:


import pandas as pd
from arcgis.features import GeoAccessor, GeoSeriesAccessor

sdf = pd.DataFrame.spatial.from_layer(inferenced_lyr.layers[0])
sdf[['objectid', 'type', 'status', 'confidence_classifyobjects']].head(10)


# The next step is to join the `inferenced_lyr` with `building_footprints` so that the `building_footprints` layer will have the `status` column. We will use the output layer of [join features](https://pro.arcgis.com/en/pro-app/tool-reference/big-data-analytics/join-features.htm) function for visualization of results in map widget.

# In[44]:


final_lyr = arcgis.features.analysis.join_features(building_footprints, 
                                                   inferenced_lyr, 
                                                   attribute_relationship=['{"targetField":"building_i","operator":"equal","joinField":"building_i"}'],
                                                   join_operation='JoinOneToOne',  
                                                   output_name='bfootprint_withstatus'+str(datetime.now().microsecond),  
                                                   gis=gis_ent)
final_lyr


# We can see the results in the webmap, click on the webmap a new tab will open which will show the classified building footprint layer overlayed on the aerial imagery. You can also add your inferenced layer to this webmap.

# In[33]:


webmap = gis.content.search('woolsey_building_damage', outside_org=True)[0]
webmap


# ## Conclusion
# 
# In this notebook, we have covered a lot of ground. In part 1, we discussed how to export training data for deep learning using ArcGIS python API. In part 2, we demonstrated how to prepare the input data, train a feature classifier model, visualize the results, as well as apply the model to an unseen image. Then we covered how to installation of model, inferencing and post processing of inferenced results to make it production-ready in part 3. The same workflow can be applied to many other use cases. For example, when we know the locations of swimming pools, we can use it to indentify which ones are dirty and not being properly maintained.

# ## Reference
# 
# [1] Ruiz Pablo, Understanding and Visualizing ResNets,  https://towardsdatascience.com/understanding-and-visualizing-resnets-442284831be8, Accessed 2 September 2019.


# ====================
# fighting_california_forest_fires_using_spatial_analysis.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Fighting California forest fires using spatial analysis
# 
# The state of California has had a dangerous fire season in 2015 and 2016. A standard procedure while fighting these fires is identifying facilities that are at risk and evacuating them. Tasks such as this can be accomplished easily using **spatial analysis** tools available on your GIS. Spatial analysis tools allow overlaying the extent of fire and the locations of the facilities on a map and identifying the ones that fall within the fire's extent.
# 
# Thus, this sample demonstrates the application of spatial analysis tools such as **buffer and overlay**.
# 
# This notebook describes a scenario wherein an analyst automates the process of identifying facilities at risk from forest fires and sharing this information with other departments such as the fire department, etc.
# 
# **Note**: To run this sample, you need the ``pandas`` library in your conda environment. If you don't have the library, install it by running the following command from cmd.exe or your shell
# ```
# conda install pandas```

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Fighting-California-forest-fires-using-spatial-analysis" data-toc-modified-id="Fighting-California-forest-fires-using-spatial-analysis-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Fighting California forest fires using spatial analysis</a></span><ul class="toc-item"><li><span><a href="#Using-groups-to-share-items-and-collaborate" data-toc-modified-id="Using-groups-to-share-items-and-collaborate-1.1"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>Using groups to share items and collaborate</a></span></li><li><span><a href="#Visualize-the-fire-data" data-toc-modified-id="Visualize-the-fire-data-1.2"><span class="toc-item-num">1.2&nbsp;&nbsp;</span>Visualize the fire data</a></span></li><li><span><a href="#Create-a-buffer-of-4-miles-around-fire-boundaries" data-toc-modified-id="Create-a-buffer-of-4-miles-around-fire-boundaries-1.3"><span class="toc-item-num">1.3&nbsp;&nbsp;</span>Create a buffer of 4 miles around fire boundaries</a></span></li><li><span><a href="#Perform-overlay-analysis-to-extract-facilities-that-fall-within-the-fire-buffers" data-toc-modified-id="Perform-overlay-analysis-to-extract-facilities-that-fall-within-the-fire-buffers-1.4"><span class="toc-item-num">1.4&nbsp;&nbsp;</span>Perform overlay analysis to extract facilities that fall within the fire buffers</a></span></li><li><span><a href="#Read-analysis-results-as-a-pandas-dataframe-for-analysis" data-toc-modified-id="Read-analysis-results-as-a-pandas-dataframe-for-analysis-1.5"><span class="toc-item-num">1.5&nbsp;&nbsp;</span>Read analysis results as a pandas dataframe for analysis</a></span></li><li><span><a href="#Mapping-the-infrastructure-at-risk" data-toc-modified-id="Mapping-the-infrastructure-at-risk-1.6"><span class="toc-item-num">1.6&nbsp;&nbsp;</span>Mapping the infrastructure at risk</a></span></li><li><span><a href="#Sharing-the-result-of-the-analysis-as-a-web-map" data-toc-modified-id="Sharing-the-result-of-the-analysis-as-a-web-map-1.7"><span class="toc-item-num">1.7&nbsp;&nbsp;</span>Sharing the result of the analysis as a web map</a></span></li></ul></li></ul></div>

# In[1]:


import datetime
import arcgis
from arcgis.gis import GIS
from IPython.display import display
import pandas as pd

# create a Web GIS object
gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# ## Using groups to share items and collaborate
# 
# A group effort spanning several professionals and teams is required to meet challenges such as forest fires. Members of a web GIS can collaborate with each other by sharing web maps, layers, applications, etc. with each other. This sharing is accomplished by creating groups in the GIS and items shared to a group can be used by members of the group.
# 
# The code below lists the items shared with the 'LA County Emergency Management' group. This group contains a collection of maps, apps and layers that are published as the authoritative common map for the county of Los Angeles.

# In[49]:


# get our group
group = gis.groups.search('LA County Emergency Management')[0]
group


# In[50]:


# list items in the group
items = group.content()
for item in items: 
    display(item)


# ## Visualize the fire data
# Let us create a map widget to see the fire related information in it's geographic context:

# In[10]:


# create a map of our area of interest
m = gis.map('Los Angeles', 9)
m


# In[7]:


# add the active fires layer
fires = items[0]
m.add_layer(fires)


# In[8]:


# add our critical infrastructure layer
infra = items[1]
m.add_layer(infra)


# In[51]:


#we shall use this webmap item in the last section of the notebook.
webmapitem = items[2]


# The `fires` feature layer, that we just added to the map, contains the boundaries of the active forest fires in the region. For this demo scenario, this layer is being constantly updated by the Fire Department with inputs from fire fighters in the field and remote sensing data obtained from satellites.
# 
# The `infra` feature layer contains the locations of critical infrastructure facilities in the region. The objective of this script is to identify critical infrastructure facilities that are at risk due to the fires, and alert firefighters, county officials and others for allocating firefighting resources, planning evacuations, etc.
# 
# ## Create a buffer of 4 miles around fire boundaries
# 
# To identify the critical infrastructure resources that are the risk, let us create a buffer of 4 miles around the fire boundaries and consider all facilities that fall within this area at risk. The process of buffer expands the feature's boundaries in all directions. 
# 
# To perform buffers, we use the **`create_buffers`** function available in the **`arcgis.features.use_proximity`** module.
# As an input to the tool, we provide the fires layer as the layer to be buffered.
# 
# Feature analysis tools can return (in memory) feature collections as output for immediate consumption, or create a new feature service if the `output_name` parameter is specified.
# 
# We specify an output name (with a timestamp) for the service as we may want the buffered fire perimeters to be persisted for bookkeeping purposes, or be shared with others as feature layers or in web maps:

# In[17]:


# create a map of our area of interest
m1 = gis.map('Los Angeles', 9)
m1


# In[12]:


#add the active fires layer to the map
m1.add_layer(fires)


# In[13]:


#add the critical infrastructure layer to the map
m1.add_layer(infra)


# In[14]:


from arcgis.features.use_proximity import create_buffers

# buffer the active fire boundaries and add as new content

timestamp = '{:%Y_%m_%d_%H_%M_%S}'.format(datetime.datetime.now())
firebuffers = create_buffers(fires, [4], None, 'Miles', output_name="Fire_Buffers_" + timestamp )


# The output layer is private by default. To share it with particular groups, members within the org,  or everyone, we can use the `Item.share()` method. The code below shares it to everyone (public):

# In[15]:


# share the layer with public
firebuffers.share(True)


# Since the `output_name` parameter was specified, the tool created a new feature layer item as output. We can visualize this by adding it to the map above. Now we can observe more facilities falling within the buffered area.

# In[16]:


# add risk areas to map
m1.add_layer(firebuffers)


# ## Perform overlay analysis to extract facilities that fall within the fire buffers
# 
# To programmatically extract the locations that fall within the fire buffers, we use **`overlay_layers`** tool under `FeatureAnalysisTools` class just like we did for `create_buffers` tool earlier. The `overlay_layers` tool supports a few overlay types, here we use **`Intersect`** as we need to perform a spatial intersection to identify the facilities that are located within the fire boundaries. To learn more about this operation, refer to the [documentation](https://developers.arcgis.com/rest/analysis/api-reference/overlay-layers.htm).
# 
# We specify an output name (with a timestamp) for the service as we want to keep a record of the critical infrastructure within the risk boundaries and share it with others as feature layers or in web maps:

# In[18]:


from arcgis.features.manage_data import overlay_layers

# run analysis to determine critical infrastructure within the risk boundaries
riskinfra = overlay_layers(firebuffers, infra, 
                           
                        overlay_type="Intersect",
                        output_name="At_Risk_Infrastructure_" + timestamp)


# In[19]:


# set sharing on new analysis layer
riskinfra.share(True)


# The output of the overlay analysis is a Feature Layer Item. We can access the layers and tables in an Item represing GIS services using the `layers` and `tables` attributes. 

# In[20]:


riskinfra.layers


# ## Read analysis results as a pandas dataframe for analysis
# 
# 
# Let us read this results of the overlay analysis by querying it's layer for attribute data, and display the attribute table to get a sense of the results. Layers can be queried for attributes and geometry using the `query()` method.
# 
# The query we ran in the previous step returned a list of dictionaries representing the features. For further analysis or for visualizing the output data as a table, let us convert it into a [pandas](http://pandas.pydata.org/) [dataframe](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html).

# In[22]:


at_risk_facilities  = riskinfra.layers[0]
df = at_risk_facilities.query(as_df=True) # read the returned features as a Pandas dataframe
df


# From the data frame, we can see there are 7 features which fell within the fire buffers. To make this result usable, let us export a CSV with only the facility name and other critical details. This CSV can be considered as a valuable information product as a result of this analysis and can be shared with the teams assisting in organizing the firefighting efforts, evacuations, etc.

# In[23]:


# view simplified risk facilities table
df1_simplified = df[['name', 'cat1', 'post_id']]
df1_simplified


# In[24]:


# Export this to a csv file. This CSV can then be shared with fire fighters.
csv_file = r'at_risk_facilities.csv'
df1_simplified.to_csv(csv_file)


# ## Mapping the infrastructure at risk
# 
# Tables and charts aren't enough to convey the information gathered from this analysis. By plotting the  locations of the critical infrastructure at risk from the forest fires, we are able to identify patterns that can be of great value in fighting a catastrophe of this scale and prevent or minimize human, infrastructural and environmental loss:

# In[28]:


# create new map over aoi
m2 = gis.map('Los Angeles', 9)
m2


# In[26]:


#add the critical infrastructure layer to the map
m2.add_layer(infra)


# In[27]:


# add at risk facilities layer to fresh map
m2.add_layer(riskinfra)
             


# ## Sharing the result of the analysis as a web map
# 
# The different teams working on the firefighting, relief and rescue efforts need a current and updated web map of the critical infrastruture at risk from the fires. The code below fetches the web-map from the GIS and updates it's operational layer to the one from the analysis above. This enables all interested parties to refer to one common, constantly updated web-map for planning firefighting, rescue and relief efforts.

# In[52]:


#display the webmap item that we had obtained as part of the 'LA County Emergency Management' group's content in the first section of this notebook.
webmapitem


# In[53]:


# create a webmap from the item
webmap = arcgis.mapping.WebMap(webmapitem)


# In[54]:


# read the operational layers in our webmap
for lyr in webmap.layers:
    if lyr['id'].startswith('At_Risk_Infrastructure'):
        print(lyr['url'])


# In[55]:


new_url = at_risk_facilities.url
new_url


# In[56]:


# update the url of the 'At Risk Infrastructure' layer in the web map

for lyr in webmap.layers:
    if lyr['id'].startswith('At_Risk_Infrastructure'):
        lyr['url'] = new_url


# In[57]:


# save the updates back to the GIS
webmap.update()


# Web map items can also be visualized in the Jupyter Notebook as shown below. This enables use of web-maps for analysis and visualization within the notebook environment.

# In[61]:


#zoom in to the extents of Los Angeles in the map below, to view the webmapitem 
m3 = gis.map(webmapitem)
m3



# ====================
# finding_a_new_home.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Finding a New Home

# Buying or selling of a house can be a very stressful event in one's life. The process could be frustrating as it is lengthy, uncertain and needs a lot of examination. Through this workflow we will guide a couple (Mark and Lisa) who is interested in selling their home and relocating to a place nearest to both of their work places. In this case study, we will explore the current housing market, estimate average house prices in their area and hunt for a new one. We will download the <a href="https://www.zillow.com/research/data/">Zillow data</a> for their current home for our analysis.
# You can use your own data and follow along with this workflow which aims to help Mark and Lisa in finding their new home. 
# 
# ![](https://desktop.arcgis.com/en/analytics/case-studies/GUID-F5565467-6211-45D9-A099-669701C5D8E1-web.png)

# 
# The notebook is divided into two parts. In the first part, we will calculate the following:
# - Percentage of decrease/increase in house price since Mark and Lisa bought their home.
# - Suggested selling price for their home.
# - Whether their zip code is a buyerÃ¢â‚¬â„¢s market or sellerÃ¢â‚¬â„¢s market.
# - Average number of days it takes for homes to sell in their neighbourhood.
# 
# In the second part of the notebook, we will explore the investment potential of homes close to their work places. Based on how much a person is willing to spend commuting to work, we will create a drive-time buffer. This will narrow down the search areas. Zillow also provides data for market health and projected home value appreciation. Visualizing the zip codes by their market health will help them focus only on areas with good market health. Hence they will get a list of areas to choose from, for buying their new home.

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc">
#     <ul class="toc-item">
#         <li><span><a href="#Selling-your-home" data-toc-modified-id="Selling-your-home-1">Selling your home</a></span>
#           <ul class="toc-item">
#             <li><span><a href="#Determine-an-appropriate-selling-price" data-toc-modified-id="Determine-an-appropriate-selling-price-1.1">Determine an appropriate selling price</a></span></li>
#             <li><span><a href="#Get-additional-information-about-the-local-real-estate-market" data-toc-modified-id="Get-additional-information-about-the-local-real-estate-market-1.2">Get additional information about the local real estate market</a></span></li>
#             <li><span><a href="#Visualize-Results" data-toc-modified-id="Visualize-Results-1.3">Visualize Results</a></span></li>
#           </ul>
#         </li>
#         <li><span><a href="#House-Hunting" data-toc-modified-id="House-Hunting-2">House Hunting</a></span>
#           <ul class="toc-item">
#             <li><span><a href="#Find-all-ZIP-Codes-within-a-specified-drive-time-of-important-places" data-toc-modified-id="Find-all-ZIP-Codes-within-a-specified-drive-time-of-important-places-2.1">Find all ZIP Codes within a specified drive time to work</a></span></li>
#             <li><span><a href="#Map-market-health,-home-values,-and-projected-appreciation" data-toc-modified-id="Map-market-health,-home-values,-and-projected-appreciation-2.2">Map market health, home values, and projected appreciation</a></span></li>
#             <li><span><a href="#Find-the-best-cities-to-begin-house-hunting" data-toc-modified-id="Find-the-best-cities-to-begin-house-hunting-2.3">Find the best cities to begin house hunting</a></span></li>
#              <li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-2.4">Conclusion</a></span></li>  
#           </ul>
#         </li>
#     </ul>
# </div>

# ## Selling your home
# 
# Execute the following command to install the openpyxl library if not already. This package is used to read from any Excel or CSV files.
# ```
# !pip install openpyxl
# ```

# Also, when `matplotlib` is not present, run the following command to have it installed or upgraded:
# ```
# import sys 
# !{sys.executable} -m pip install matplotlib
# ```

# ### Determine an appropriate selling price

# 1) Download home sales time series data from Zillow at <a href="https://www.zillow.com/research/data/">www.zillow.com/research/data</a>.
# > Mark and Lisa have a 3-bedroom home, so we will select the **ZHVI 3-Bedroom time-series ($) ** data set at the ZIP Code level.

# 2) Prepare the Excel data as follows:
# 
# > a) Using Excel, open the **.csv** file.
# 
# > Notice that the **RegionName** field has ZIP Codes as numbers (if we sort the **RegionName** field we will notice the ZIP Codes for Massachusetts, for example, don't have leading zeros; 01001 is 1001). Also, notice the median home value columns are named using the year and month. The first data available is for April 1996 (**1996-04**).
# > b) Copy all the column headings and the one record with data for their ZIP Code to a new Excel sheet.

# > Apply a filter to the **RegionName** field. Mark and Lisa live in Crestline, California, so we will apply a filter for the 92325 ZIP Code.

# 

# > c) Select (highlight) fields starting with the month and year when they bought their home and continuing to the last month and year column in the Excel table. So, for example, since Mark and Lisa bought their home in December 2007, they highlight the the two rows from column **2007-01** to column **2018-08**.
# 
# > d) Copy (press Ctrl+C) the selected data and paste it, along with the column headings, to a new Excel sheet using **Paste Transposed** (right-click in the first cell of the new sheet to see the paste options; select **Paste Transposed**).
# This gives two columns of data.
# 
# > e) The first column has date values but only includes the year and month. In column **C**, create a proper date field.
# 
# > * Right-click column C and format the cells to be category **date**.
# > * In the first cell of column C, enter the following formula: **= DATEVALUE(CONCATENATE(A1, "-01"))**
# > * Drag the <a href="https://support.office.com/en-us/article/Fill-data-automatically-in-worksheet-cells-74e31bdd-d993-45da-aa82-35a236c5b5db">Autofill handle</a> down to the last data cell in the column.
# 

# > f) Insert a top row and type the column headings:
# 
# > **YYYYMM, Value, and date**.

# 

# > g) Rename the Excel sheet (probably called Sheet2 at present) something like <MyCity>AveSellingPrice and delete the other sheets (the first sheet contains a large amount of data that we won't be using further in the workflow).
#     
# > Mark and Lisa named their price Excel sheet **CrestlineAveSellingPrice**.
# 
# > h) Save this new sheet as an Excel workbook.
# 
# > Mark and Lisa named their Excel file **Crestline3BdrmAveSellingPrice.xlsx**.

# 3) Connect to your ArcGIS Online organization.

# In[1]:


from arcgis.gis import GIS


# In[2]:


import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime as dt


# In[3]:


gis = GIS('home')


# Use the boolean `has_arcpy` to flag whether `arcpy` is present on the local environement:

# In[2]:


has_arcpy = False


# In[4]:


try:
    import arcpy
    has_arcpy = True
    print("arcpy present")
except:
    print("arcpy not present")


# 4) Load the excel file for analysis.

# In[4]:


data = gis.content.search('finding_a_new_home owner:api_data_owner type: csv collection')[0]
data


# In[5]:


filepath = data.download(file_name=data.name)


# In[6]:


import os
import zipfile
from pathlib import Path
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[7]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))
data_path


# In[8]:


datapath = [os.path.abspath(os.path.join(data_path, p)) for p in os.listdir(data_path)]
datapath


# In[9]:


file_name1 = datapath[1]
data1 = pd.pandas.read_excel(file_name1)


# In[10]:


data1.head()


# In[11]:


data1.tail()


# In[12]:


data1.shape


# In[13]:


data1[['year','month','day']] = data1.date.apply(lambda x: pd.Series(
    x.strftime("%Y,%m,%d").split(","))) # split date into year, month, day


# In[14]:


data1.head()


# In[15]:


grpby_data1 = data1.groupby(['year']).mean()


# In[16]:


type(grpby_data1)


# 5) We will Create a graph showing how average home prices have changed since they bought their home.

# In[17]:


grpby_data1.reset_index(inplace=True)


# In[18]:


grpby_data1.head()


# In[19]:


grpby_data1.value


# In[20]:


grpby_data1.year


# In[21]:


plt.plot(grpby_data1.year, grpby_data1.value)
plt.title("average home prices (2007-2018)")
plt.xlabel("date")
plt.ylabel("average house price")


# 7) Determine an appropriate selling price based on home sales trends as follows:
# 
# > a) Determine the current average selling price and the average selling price when they bought their home. Divide the current average price by the beginning average price to see how much homes in their ZIP Code have appreciated or depreciated. When Mark and Lisa bought their home in December of 2007, 3-bedroom homes were selling for \$276,617. 

# In[22]:


price_initial = grpby_data1.iloc[0]


# In[23]:


price_initial


# In[24]:


price_current = grpby_data1.iloc[-1]


# In[25]:


price_current


# In[26]:


house_worth = price_current.value / price_initial.value


# In[27]:


house_worth


# This indicates that homes in Crestline are only worth 91 percent of what they were at the end of 2007.

# > b) We can get a rough estimate of what their home is worth by summing what they paid for their home plus what they invested in it, and multiplying that sum by the ratio computed above. Mark and Lisa, for example, paid \\$291,000 in 2007 and invested \\$100,000 in solid improvements (new kitchen, major landscaping, hardwood flooring, and so on). Multiplying (\\$291,000 + \\$100,000) by 0.91 gives a rough suggested selling price of \\$343,134.

# In[28]:


(price_initial.value + 100000)*house_worth


# ### Get additional information about the local real estate market

# If their home is part of a seller's market, they are more likely to get their asking price.

# 1) Download the Buyer-Seller Index data at the ZIP Code level from www.zillow.com/research/data. In **Home Listings and Sales** select data type as seller-buyer index and geography as zip codes.

# 2) Open the **.csv** file using Excel. Zillow reports ZIP Codes as numbers. We will need to pad the ZIP Code numbers with leading zeros so the Zillow data will link to the ArcGIS ZIP Code geometry.
# 
# Follow these steps:
# > a) Sort the RegionName column from smallest to largest so we will be able to see how the formula below works.
# 
# > b) Name a new column in the Excel table zipstring.
# 
# > c) In the first cell of the new column, enter the formula to pad each RegionName value with leading zeros, keeping the rightmost five characters: =RIGHT(CONCAT("00000",B2),5)
# 
# > d) Drag the Autofill handle down to the last data cell in the column.

# 

# 

# 3) Load the excel file for analysis

# In[29]:


file_name2 = datapath[0]
data2 = pd.read_excel(file_name2)


# In[30]:


data2.head()


# In[31]:


data2.dtypes


# In[32]:


data2.columns


# 4) Select **BuyerSellerIndex, DaysOnMarket, zipstring** fields

# In[33]:


cols = ['BuyerSellerIndex', 'DaysOnMarket', 'zipstring']


# In[34]:


selected_data2 = data2[cols]


# 5) Explore the data values as follows:
# > a) Sort on the DaysOnMarket field and notice the range. For the data Mark and Lisa downloaded, the range is 35 to 294.5 days.

# In[35]:


days_on_mrkt_df = selected_data2.sort_values(by='DaysOnMarket', axis=0)


# In[36]:


days_on_mrkt_df.DaysOnMarket.min()


# In[37]:


days_on_mrkt_df.DaysOnMarket.max()


# In[38]:


days_on_mrkt_df.head()


# In[39]:


days_on_mrkt_df.tail()


# > b) Sort on the BuyerSellerIndex field and notice the range of values. ZIP Codes with index values near 0 are part of a strong seller's market; ZIP Codes with index values near 10 are part of a strong buyer's market.

# In[40]:


buyer_seller_df = data2.sort_values(by='BuyerSellerIndex', axis=0)


# In[41]:


buyer_seller_df.head()


# In[42]:


buyer_seller_df.tail()


# In[43]:


buyer_seller_df.BuyerSellerIndex.min()


# In[44]:


buyer_seller_df.BuyerSellerIndex.max()


# > c) Filter the data to only show their home's ZIP code

# In[45]:


selected_data2[selected_data2['zipstring'] == 92325]


# Notice the average number of days. Determine if the home is part of a buyer's or seller's market. Mark and Lisa learn that their home is part of a buyer's market (9.6), and they can expect their home to be on the market approximately 79 days before it sells.

# In[46]:


selected_data2.rename(columns={"zipstring": "ZIP_CODE"}, inplace=True)


# In[47]:


selected_data2.shape


# In[48]:


selected_data2.dtypes


# In[49]:


selected_data2 = selected_data2.astype({"ZIP_CODE": int})


# In[50]:


selected_data2.dtypes


# 6) Search for the **United States ZIP Code Boundaries 2017** layer. We can specify the owner's name to get more specific results. To search for content from the Living Atlas, or content shared by other users on ArcGIS Online, set outside_org=True

# In[51]:


items = gis.content.search('United States ZIP Code Boundaries 2021 owner: esri_dm',
                           outside_org=True)


# Display the list of results.

# In[52]:


from IPython.display import display

for item in items:
    display(item)


# Select the desired item from the list.

# In[53]:


us_zip = items[1]


# In[54]:


us_zip


# Get the layer names from the item

# In[55]:


for lyr in us_zip.layers:
    print(lyr.properties.name)


# 7) We want to merge the zip_code layer with data2 to visualize the result on the map.

# In[56]:


us_zip_lyr = us_zip.layers[3]


# In[57]:


zip_df = pd.DataFrame.spatial.from_layer(us_zip_lyr)


# In[58]:


zip_df.head()


# In[59]:


zip_df.shape


# In[60]:


zip_df.dtypes


# In[61]:


zip_df = zip_df.astype({"ZIP_CODE": int})


# In[62]:


zip_df.dtypes


# In[63]:


merged_df = pd.merge(zip_df, selected_data2, on='ZIP_CODE')


# In[64]:


merged_df.shape


# In[65]:


merged_df.spatial.set_geometry('SHAPE')


# In[66]:


mergd_lyr = gis.content.import_data(merged_df,
                                    title='MergedLayer',
                                    tags='datascience')


# When arcpy is present, the `import_data` will upload the local SeDF (Spatially Enabled DataFrame) as a FGDB (File geodatabase) to your organization, and publish to a hosted feature layer; On the other hand, when arcpy is not present, then the `import_data` method would have the local SeDF upload to your organization as a shapefile, and then publish as a hosted Feature Layer. This minor difference will result in column/property name differences from what's defined in the original SeDF.
# 
# The `has_arcpy` flag is to be used in determine which naming convention the newly created Feature Layer would be conforming to, when we are adding the Feature Layer for display based on variables.

# 8) Create a map of the BuyerSellerIndex field using the following steps:

# ### Visualize Results

# In[67]:


m1 = gis.map('United States', 8)
m1


# In[6]:


cur_field_name = "BuyerSellerIndex"
if has_arcpy:
    if cur_field_name not in mergd_lyr.layers[0].properties.fields:
        cur_field_name = "buyer_seller_index"
else:
    cur_field_name = "BuyerSelle"


# In[68]:


m1.add_layer(mergd_lyr, {"renderer":"ClassedColorRenderer",
                         "field_name":cur_field_name,
                         "opacity":0.7
                  })


# 9) Create a map on DaysOnMarket field as follows:

# In[69]:


m2 = gis.map('Redlands, CA')
m2


# In[7]:


cur_field_name = "DaysOnMarket"
if has_arcpy:
    if cur_field_name not in mergd_lyr.layers[0].properties.fields:
        cur_field_name = "days_on_market"
else:
    cur_field_name = "DaysOnMark"


# In[70]:


m2.add_layer(mergd_lyr, {"renderer":"ClassedSizeRenderer",
                         "field_name":cur_field_name,
                         "opacity":0.7
              })


# ## House Hunting

# ### Find all ZIP Codes within a specified drive time of important places
# 
# 
# 1) Create an Excel table with columns for **Street, City, State**, and **Zip**. Add addresses for the locations you want to access from your new home. Mark and Lisa's table below has their current job addresses. They named their Excel file **ImportantPlaces.xlsx** and the Excel sheet **WorkLocations**.

# 2) Load the excel file for analysis

# In[71]:


file_name3 = datapath[2]
data3 = pd.read_excel(file_name3)


# In[72]:


data3.head()


# In[73]:


data3['Address'] = data3['Street'] + ' ' + data3['City'] + ' ' + data3['State']


# In[74]:


data3['Address']


# 3) Draw the address on map

# In[75]:


m3 = gis.map('Redlands, CA', 10)
m3


# In[76]:


from arcgis.geocoding import geocode
data3_addr1 = geocode(data3.Address[0])[0]
popup = { 
    "title" : "Lisa's job", 
    "content" : data3_addr1['address']
    }
m3.draw(data3_addr1['location'], popup,
        symbol = {"angle":0,"xoffset":0,"yoffset":0,
                  "type":"esriPMS", "url":"https://static.arcgis.com/images/Symbols/PeoplePlaces/School.png",
                  "contentType":"image/png","width":24,"height":24})


# In[77]:


from arcgis.geocoding import geocode
data3_addr2 = geocode(data3.Address[1])[0]
popup = { 
    "title" : "Mark's job", 
    "content" : data3_addr2['address']
    }
m3.draw(data3_addr2['location'], popup,
        symbol = {"angle":0,"xoffset":0,"yoffset":0,
                  "type":"esriPMS", "url":"https://static.arcgis.com/images/Symbols/PeoplePlaces/School.png",
                  "contentType":"image/png","width":24,"height":24})


# 4) We will Create buffer and enter the maximum time they are willing to spend commuting from their new home to their work.

# In[78]:


from arcgis.geoenrichment import BufferStudyArea, enrich


# In[79]:


marks = BufferStudyArea(area='4511 E Guasti Road, Ontario, CA 91761', 
                           radii=[45], units='Minutes', 
                           travel_mode='Driving')
lisas = BufferStudyArea(area='380 New York St Redlands CA 92373', 
                           radii=[45], units='Minutes', 
                           travel_mode='Driving')

drive_time_df = enrich(study_areas=[marks, lisas], data_collections=['Age'])


# In[80]:


drive_time_lyr = gis.content.import_data(drive_time_df,
                                         title="DriveTimeLayer")


# 5) Select the Dissolve buffer style to merge overlapping polygons.

# In[81]:


from arcgis.features.manage_data import dissolve_boundaries


# In[82]:


dissolved_lyr = dissolve_boundaries(drive_time_lyr)


# In[83]:


m_3 = gis.map('Redlands, CA', 9)
m_3


# In[84]:


m_3.add_layer(dissolved_lyr)


# In[85]:


m_3.draw(data3_addr1['location'], popup,
        symbol = {"angle":0,"xoffset":0,"yoffset":0,
                  "type":"esriPMS", "url":"https://static.arcgis.com/images/Symbols/PeoplePlaces/School.png",
                  "contentType":"image/png","width":24,"height":24})
m_3.draw(data3_addr2['location'], popup,
        symbol = {"angle":0,"xoffset":0,"yoffset":0,
                  "type":"esriPMS", "url":"https://static.arcgis.com/images/Symbols/PeoplePlaces/School.png",
                  "contentType":"image/png","width":24,"height":24})


# ### Map market health, home values, and projected appreciation

# 1) Download and prepare the Excel Market Health data as follows:
# 
# > a) From <a href="https://www.zillow.com/research/data/">www.zillow.com/reserach/data</a>, download ZIP Code level Market Health Index data.
# 
# > b) Open the .csv file using Excel and add the ZIPString column as a text field. Compute the values using =RIGHT(CONCAT("00000",B2),5). Drag the Autofill handle down to the last cell in the column to create text formatted ZIP Code values with leading zeros.
# 
# > c) Save the file as an Excel workbook. Close Excel.
# 
# > Mark and Lisa named their file MarketHealth.xlsx.

# 2) Load th Excel file for analysis.

# In[86]:


file_name4 = datapath[3]
data4 = pd.read_excel(file_name4)


# In[87]:


data4


# 3) Select **City, MarketHealthIndex, ZHVI, ForecastYoYPctChange**, and **zipstring** fields.

# In[88]:


col = ['City', 'MarketHealthIndex', 'ZHVI', 'ForecastYoYPctChange', 'zipstring']


# In[89]:


matket_health_index = data4[col]


# In[90]:


matket_health_index.head()


# In[91]:


matket_health_index.dtypes


# In[92]:


matket_health_index['MarketHealthIndex'].min()


# In[93]:


matket_health_index['MarketHealthIndex'].max()


# In[94]:


matket_health_index.rename(columns={"zipstring": "ZIP_CODE"},
                           inplace=True)


# In[95]:


matket_health_index[matket_health_index['City']=='Crestline']


# 4) Sort the table on the ZIP_CODE field so we can locate their ZIP Code. Make a note of the values for MarketHealthIndex, ZHVI, and ForecastYoYPctChange. In Crestline, for example, the market health index is fair: 6.4 on a scale that ranges from 0 to 10. The median home value for all homes (not just 3-bedroom homes) is $214,100. Homes are expected to appreciate 4.8 percent.

# In[96]:


zip_df.head()


# In[97]:


zip_df = zip_df.astype({"ZIP_CODE": int})


# In[98]:


health_df = pd.merge(zip_df, matket_health_index, on='ZIP_CODE')


# In[99]:


health_df.head()


# In[100]:


health_df.shape


# In[101]:


health_df.spatial.set_geometry('SHAPE')


# In[102]:


hlth_lyr = gis.content.import_data(health_df,
                                  title="MarketHealthLayer")


# In[103]:


m4 = gis.map('United States', 5)
m4


# In[8]:


cur_field_name = "MarketHealthIndex"
if cur_field_name not in hlth_lyr.layers[0].properties.fields:
    if has_arcpy:
        cur_field_name = "market_health_index"
    else:
        cur_field_name = "MarketHeal"


# In[104]:


m4.add_layer(hlth_lyr, {"renderer":"ClassedColorRenderer",
                        "field_name":cur_field_name,
                        "classificationMethod":'quantile',
                        "opacity":0.7
                        })


# In[105]:


market_hlth_lyr = hlth_lyr.layers[0]


# In[106]:


from arcgis.features.find_locations import find_centroids


# In[107]:


poly_to_point = find_centroids(market_hlth_lyr, output_name="HealthLyrPolygonToPoint" + str(dt.now().microsecond))


# In[108]:


from  arcgis.features.manage_data import overlay_layers


# In[109]:


zip_intersect = overlay_layers(drive_time_lyr, 
                               market_hlth_lyr, 
                               output_name="Market Health Data Within drive time Buffer" + str(dt.now().microsecond))


# In[110]:


zip_intersect


# 5) Notice how many ZIP Codes intersect the drive time buffer.

# In[111]:


zip_hlth_intersect = zip_intersect.layers[0]


# In[112]:


overlay_df = pd.DataFrame.spatial.from_layer(zip_hlth_intersect)


# In[113]:


overlay_df.shape


# In[114]:


m5 = gis.map('Redlands, CA', 9)
m5


# In[115]:


m5.add_layer(hlth_lyr, {"renderer":"ClassedColorRenderer",
                        "field_name":cur_field_name,
                        "classificationMethod":'quantile',
                        "opacity":0.7
                        })
m5.add_layer(drive_time_lyr)


# The larger the index, the darker the color, and the healthier the housing market is. They want to buy their new home in an area with a healthy housing market (rather than a location where there are vacancies, homes that aren't selling, and numerous foreclosures).

# This result has all the variables one should be interested in mapping, narrowed down to the ZIP Codes that are within an acceptable drive time to their work.

# In[116]:


m6 = gis.map('Redlands, CA', 9)
m6


# In[117]:


m6.add_layer(hlth_lyr, { "renderer":"ClassedColorRenderer",
                         "field_name":"ZHVI",
                         "classificationMethod":'quantile',
                         "opacity":0.7
              })
m6.add_layer(drive_time_lyr)


# The dark ZIP Codes have the most expensive average home value estimates.

# Similarly plot for the field **ForecastYoYPctChange**

# In[118]:


m7 = gis.map('Redlands, CA', 9)
m7


# In[10]:


cur_field_name2 = "ForecastYoYPctChange"
if cur_field_name2 not in hlth_lyr.layers[0].properties.fields:
    if has_arcpy:
        cur_field_name2 = "forecast_yo_y_pct_change"
    else:
        cur_field_name2 = "ForecastYo"


# In[119]:


m7.add_layer(hlth_lyr, {"renderer":"ClassedColorRenderer",
                        "field_name":cur_field_name2,
                        "classificationMethod":'quantile',
                        "opacity":0.7
              })
m7.add_layer(drive_time_lyr)


# The darkest ZIP Codes are expected to have the largest increase in home values over the next year.

# ### Find the best cities to begin house hunting

# 1) Narrow the search to ZIP Codes with home values in price range as follows:

# > Mark and Lisa will only sell their home for a loss if they can purchase a home valued the same as theirs with better investment potential. They will look for homes that cost about \\$340,000 in neighborhoods with median home values between \\$300,000 and \\$500,000. Being surrounded by homes that are valued higher than theirs is much better than being the most expensive home on the block. That's why Mark and Lisa extended their search to ZIP Codes with median home values as high as \$500,000.

# > They also want to narrow their search to areas belonging to healthy housing markets with expected home value appreciation. Mark and Lisa, for example, exclude ZIP Codes in unhealthy housing markets by filtering for MarketHealthIndex > 8. They also focus on ZIP Codes with expected home appreciation higher than Crestline by filtering for ForecastYoYPctChange > 0.060. Set filters to reflect your own criteria for the MarketHealthIndex and ForecastYoYPctChange fields.

# In[120]:


query_str = '((ZHVI > 350000) AND (ZHVI < 600000) AND (' + cur_field_name + ' > 8) AND (' + cur_field_name2 + '> 0.06)) AND (1=1)'

zip_hlth_intersect_df = zip_hlth_intersect.query(where=query_str).sdf
zip_hlth_intersect_df


# In[117]:


m9 = gis.map('United States')
m9


# In[122]:


m9.add_layer(zip_hlth_intersect,
             {"definition_expression": query_str,
             "classificationMethod":'quantile'})
m9.zoom_to_layer(zip_hlth_intersect)


# Notice that when the filter is applied, the map changes. ZIP Codes that don't meet the filter criteria are removed from the map and the colors change to reflect the recomputed Quantile classification.

# ### Conclusion

# The results show possible cities and ZIP Codes where they can explore homes for sale. We can use real estate websites such as <a href="https://www.zillow.com/">Zillow.com</a> or <a href="https://www.realtor.com/">Realtor.com</a> to see if they can find a home in their price range with the characteristics and qualities they're are looking for.


# ====================
# finding_grazing_allotments.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Monitoring hydrologic water quality in pasturelands through spatial overlay analysis

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc">
#     <ul class="toc-item">
#     <li><span><a href="#Introduction" data-toc-modified-id="Introduction-1">Introduction</a></span></li>
#     <li><span><a href="#Workflow" data-toc-modified-id="Workflow-1">Workflow</a></span></li>
#     <li><span><a href="#Necessary-Imports" data-toc-modified-id="Necessary-Imports-2">Necessary Imports</a></span></li>
#     <li><span><a href="#Connect-to-your-GIS" data-toc-modified-id="Connect-to-your-GIS-3">Connect to your GIS</a></span></li>
#     <li><span><a href="#Get-data-for-analysis" data-toc-modified-id="Get-data-for-analysis-4">Get data for analysis</a></span></li>
#     <li><span><a href="#Assigning-basin-information-to-allotments" data-toc-modified-id="Assigning-basin-information-to-allotments-5">Assigning basin information to allotments</a></span></li>
#     <ul class="tocSkip">
#         <li><span><a href="#Mapping-and-exploring-basin-overlay-results" data-toc-modified-id="Mapping-and-exploring-basin-overlay-results-5.1">Mapping and exploring basin overlay results</a></span></li>
#         <li><span><a href="#Display-grazing-allotments-within-a-particular-basin" data-toc-modified-id="Display-grazing-allotments-within-a-particular-basin-5.2">Display grazing allotments within a particular basin</a></span></li>
#         <li><span><a href="#Display-a-particular-grazing-allotment-to-see-which-basins-intersect-it" data-toc-modified-id="Display-a-particular-grazing-allotment-to-see-which-basins-intersect-it-5.3">Display a particular grazing allotment to see which basins intersect it</a></span></li>
#     </ul>
#     <li><span><a href="#Assigning-allotment-information-to-streams" data-toc-modified-id="Assigning-allotment-information-to-streams-6">Assigning allotment information to streams</a></span></li>
#     <ul>
#         <li><span><a href="#Mapping-and-searching-stream-overlay-results" data-toc-modified-id="Mapping-and-searching-stream-overlay-results-6.1">Mapping and searching stream overlay results</a></span></li>
#         <li><span><a href="#Display-a-stream-and-grazing-allotments-it-intersects" data-toc-modified-id="Display-a-stream-and-grazing-allotments-it-intersects-6.2">Display a stream and grazing allotments it intersects</a></span></li>
#     </ul>
#     <li>
#         <span><a href="#Conclusion" data-toc-modified-id="Conclusion-7">Conclusion</a></span></li>
#     </ul>
# </div>

# ### Introduction

# Much of the grazing in the state of Oregon occurs on federal lands. Grazing areas are divided into allotments by federal agencies. They issue permits or leases to ranchers for individual grazing allotments.
# 
# Studies by the state of Oregon Department of Environment Quality (DEQ) indicate that streams located in grazing areas are majorly polluted by sediments and animal waste. This is a substantial concern as it causes degradation of water quality, threatening human and ecological health. The department thus wants to inspect the effect of livestock grazing on the state’s water quality.
# 
# While federal agencies manage the grazing lands by allotments, the state's biologists monitor water quality by watersheds, or hydrologic basins (as the hydrologists refer to them). If a basin has water quality issues, then biologists who monitor water quality for watersheds or hydrologic basins could identify all grazing allotments that are in that basin. They can then work with federal agencies who manages the grazing allotments to ensure that permit holders are conforming to best practices.

# 

# <center>Grazing allotments and hydrologic basin boundaries. Many allotments fall in more than one hydrologic basin.</center>

# Since grazing allotments were not created with basin boundaries in mind,  an allotment can fall completely within a hydrologic basin, or can cross basin boundaries, falling in two or more basins.
# 
# This sample uses ArcGIS API for Python to find out which watershed, or watersheds, each grazing allotment falls in, for water quality monitoring. 
# 
# It demonstrates using tools such as `overlay_layers` to identify  allotments in a particular basin. This will assign each allotment to the hydrologic basins it falls within.
# 
# Moreover, in order to successfully identify the source of pollution in each basin, each basin is assigned the grazing allotment name and the number of streams within. This will help identify which allotment each segment of each stream passes through. If field tests find a water quality issue with a particular stream, biologists can link back to the federal database and get a report on each suspect allotment (the type and number of livestock, the owner information, the administrating office, and so on). The information will help them determine the source of pollution.

# ### Workflow

# 

# ### Necessary Imports

# In[1]:


get_ipython().run_line_magic('matplotlib', 'inline')

import pandas as pd
pd.set_option('mode.chained_assignment', None)
import matplotlib.pyplot as plt
from IPython.display import display
from datetime import datetime as dt

from arcgis.gis import GIS
from arcgis.features.manage_data import overlay_layers


# ### Connect to your GIS

# Connect to our GIS via an existing profile or creating a new connection by e.g. gis = GIS("https://www.arcgis.com", "arcgis_python", "P@ssword123")

# In[2]:


gis = GIS('home')


# ### Get data for analysis

# Search for **grazing allotments and watersheds** layer in ArcGIS Online.

# In[3]:


items = gis.content.search('grazing allotments and watersheds owner: api_data_owner', 
                           'Feature Layer')


# The code below displays the items.

# In[4]:


for item in items:
    display(item)


# We will use the first item for our analysis. Since the item is a Feature Layer Collection, accessing the layers property will give us a list of FeatureLayer objects.

# In[5]:


data = items[0]


# The code below cycles through the layers and prints their names.

# In[6]:


for lyr in data.layers:
    print(lyr.properties.name)


# Let us now get the layers and assign a variable to them.

# In[7]:


hydro_units = data.layers[0]


# In[8]:


counties = data.layers[1]


# In[9]:


grazing_allotments = data.layers[2]


# In[10]:


streams = data.layers[3]


# In[11]:


map1 = gis.map("Oregon")
map1


# In[12]:


map1.add_layer(hydro_units)
map1.add_layer(grazing_allotments)


# ### Assigning basin information to allotments

# In order to find out which hydrologic basins each grazing allotment is in, we will use `overlay_layers` tool. It combines two layers, an analysis layer and an overlay layer, into a new layer, creating new features and combining attributes from the two layers according to the overlay method selected. Overlay operations supported are Intersect, Union, and Erase.
# 
# We will overlay grazing allotments with hydrologic basins using the Intersect option.

# In[13]:


basin_overlay = overlay_layers(grazing_allotments,
                               hydro_units,
                               overlay_type='Intersect',
                               output_name="OverlayAllotmentWithBasin" + str(dt.now().microsecond))


# In[14]:


basin_overlay


# In[15]:


map2 = gis.map('Oregon')
map2


# In[16]:


map2.add_layer(basin_overlay.layers[0])


# The new features have all the attributes of the features in the input layers. In this case, the new allotment features are assigned the attributes-including the name and ID-of the hydrologic basin they fall within. Allotments that fall in two or more basins are split at the basin boundaries and the corresponding attributes assigned to each portion of the allotment.
# 
# 

# ### Mapping and exploring basin overlay results

# We will explore overlay results using both matplotlib and plot method of Spatially Enabled Dataframe.

# Let's read the overlay layer as Spatially Enabled DataFrame.

# In[17]:


sdf = pd.DataFrame.spatial.from_layer(basin_overlay.layers[0])


# In[18]:


slctd_cols = ['BASIN_NAME', 'allot_name', 'allot_no', 'REGION', 'HUC', 'SHAPE']


# In[19]:


basin_overlay_df = sdf[slctd_cols]


# In[20]:


basin_overlay_df.head()


# We will group the dataframe by 'BASIN_NAME'. To get the number of basins in each group we will use the [size()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.size.html) method. 

# In[21]:


grp_basin = basin_overlay_df.groupby('BASIN_NAME').size()


# In[22]:


grp_basin.head()


# In[23]:


grp_basin.nlargest(10)


# In[24]:


grp_basin.nlargest(10).plot(kind='barh')


# We see that Powder Basin intersected the largest number of grazing allotments, followed by Burnt and Lower John Day basins.

# We will now map grazing allotments by hydrologic basin.

# In[25]:


map3 = gis.map('oregon')
map3


# <center>Grazing allotments are color coded by the basin they are in. Allotment features are split where a basin boundary crosses them. Selecting an allotment feature displays the allotment and basin information--the basin information is associated with the feature.</center>

# In[26]:


basin_overlay_df.spatial.plot(kind='map',
                              map_widget=map3,
                              renderer_type='u',
                              col='BASIN_NAME') 


# ### Display grazing allotments within a particular basin

# Let's get a list of all basin names.

# In[27]:


basin_overlay_df.BASIN_NAME.str.strip().unique()


# We will apply a filter to visualize grazing allotments within a particular basin.

# In[28]:


john_day_df = basin_overlay_df[basin_overlay_df['BASIN_NAME'] == ' MIDDLE FORK JOHN DAY']


# In[29]:


john_day_df


# Let's plot the filtered results on map.

# In[30]:


map4 = gis.map('oregon', zoomlevel=8)
map4


# <center>Grazing allotments are filtered by a particular hydrologic basin (Middle Fork John Day). The map shows all the allotments (or portions of allotments) that are in the basin, and the table above lists them with the associated information.</center>

# In[31]:


map4.center = [44.88, -118.83]


# In[32]:


map4.add_layer(hydro_units)


# In[33]:


john_day_df.spatial.plot(map_widget=map4)


# ### Display a particular grazing allotment to see which basins intersect it

# We will filter grazing allotments using allotment number 04003.

# In[34]:


allot_df = basin_overlay_df[basin_overlay_df['allot_no'] == '04003']


# In[35]:


allot_df


# In[36]:


map5 = gis.map('oregon', zoomlevel=8)
map5


# Grazing allotments are filtered using a particular allotment number (04003). The map shows in which basins the allotment is.

# In[37]:


map5.center = [44.88, -118.83]


# In[38]:


map5.add_layer(hydro_units)


# In[39]:


allot_df.spatial.plot(map_widget=map5)


# ### Assigning allotment information to streams
# 

# We will again use `overlay_layers` tool, this time overlaying streams with grazing allotments (area features can be overlaid with line or point features as well as other area features).

# We will overlay streams with grazing allotments using the Intersect option. The output layer contains only those stream segments that cross a grazing allotment.

# In[40]:


stream_overlay = overlay_layers(streams,
                                grazing_allotments,
                                overlay_type='Intersect',
                                output_name="StreamOverlay" + str(dt.now().microsecond)
                               )


# In[41]:


stream_overlay


# Read the overlay layer as Spatially Enabled DataFrame.

# ### Mapping and exploring stream overlay results

# In[42]:


stdf = pd.DataFrame.spatial.from_layer(stream_overlay.layers[0])


# In[43]:


cols = ['allot_name', 'allot_no', 'HUC', 'PNAME', 'SHAPE' ]


# In[44]:


stream_overlay_df = stdf[cols]


# In[45]:


stream_overlay_df.head()


# In[46]:


st_grp = stream_overlay_df.groupby('PNAME').size()


# In[47]:


st_grp.nlargest(10).plot(kind='barh')


# John Day River is the third longest free flowing river in contiguous US.
# The plot shows that this river is mostly used for ranching.

# In[48]:


stream_overlay_df.groupby('allot_name').size().nlargest(10)


# Thirty-six (36) stream segments from 17 different streams pass through the 'THREE FINGERS' grazing allotment.

# In[49]:


map6 = gis.map('oregon')
map6


# <center>Map of streams overlaid by grazing allotments.</center>

# In[50]:


map6.add_layer(grazing_allotments)


# In[51]:


stream_overlay_df.spatial.plot(kind='map',
                               map_widget=map6,
                               renderer_type='u',
                               col='PNAME') 


# ### Display a stream and grazing allotments it intersects

# We will filter the analysis_layer for a specific stream—BRIDGE CR.

# In[52]:


creek_df = stream_overlay_df[stream_overlay_df['PNAME'] == 'BRIDGE CR']


# In[53]:


creek_df.head()


# In[54]:


map7 = gis.map('oregon')
map7


# This map shows in which allotment each stream segment falls.

# In[55]:


map7.add_layer(grazing_allotments)


# In[56]:


creek_df.spatial.plot(map_widget=map7)


# ### Conclusion

# Biologists can now identify which hydrologic basin and stream(s) intersect with which grazing allotments in an effort to identify sources of chronic water quality issues.


# ====================
# finding_hospitals_closest_to_an_incident.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Find hospitals closest to an incident

# The `network` module of the ArcGIS API for Python can be used to solve different types of network analysis operations. In this sample, we see how to find the hospital that is closest to an incident.

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Find-hospitals-closest-to-an-incident" data-toc-modified-id="Find-hospitals-closest-to-an-incident-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Find hospitals closest to an incident</a></span><ul class="toc-item"><li><span><a href="#Closest-facility" data-toc-modified-id="Closest-facility-1.1"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>Closest facility</a></span><ul class="toc-item"><li><span><a href="#Connect-to-your-GIS" data-toc-modified-id="Connect-to-your-GIS-1.1.1"><span class="toc-item-num">1.1.1&nbsp;&nbsp;</span>Connect to your GIS</a></span></li><li><span><a href="#Create-a-Network-Layer" data-toc-modified-id="Create-a-Network-Layer-1.1.2"><span class="toc-item-num">1.1.2&nbsp;&nbsp;</span>Create a Network Layer</a></span></li><li><span><a href="#Create-hospitals-layer" data-toc-modified-id="Create-hospitals-layer-1.1.3"><span class="toc-item-num">1.1.3&nbsp;&nbsp;</span>Create hospitals layer</a></span></li><li><span><a href="#Create-incidents-layer" data-toc-modified-id="Create-incidents-layer-1.1.4"><span class="toc-item-num">1.1.4&nbsp;&nbsp;</span>Create incidents layer</a></span></li></ul></li><li><span><a href="#Solve-for-closest-hospital" data-toc-modified-id="Solve-for-closest-hospital-1.2"><span class="toc-item-num">1.2&nbsp;&nbsp;</span>Solve for closest hospital</a></span></li><li><span><a href="#Analyze-the-results-in-a-table" data-toc-modified-id="Analyze-the-results-in-a-table-1.3"><span class="toc-item-num">1.3&nbsp;&nbsp;</span>Analyze the results in a table</a></span><ul class="toc-item"><li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-1.3.1"><span class="toc-item-num">1.3.1&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></li></ul></li></ul></div>

# ## Closest facility

# The closest facility solver provides functionality for finding out the closest locations to a particular input point. This solver would be useful in cases when you have an incident and need to find the closest facility or need to get information on the travel time and the distance to each of the facilities from an incident point for reporting purposes.
# 
# ![](http://desktop.arcgis.com/en/arcmap/latest/extensions/network-analyst/GUID-96C273DB-6A24-4D42-AADA-975A33B44F3D-web.png)
# 
# When finding closest facilities, you can specify how many to find and whether the direction of travel is toward or away from them. The closest facility solver displays the best routes between incidents and facilities, reports their travel costs, and returns driving directions.

# ### Connect to your GIS
# As a first step, you would need to establish a connection to your organization which could be an ArcGIS Online organization or an ArcGIS Enterprise.

# In[1]:


from IPython.display import HTML
import pandas as pd
from arcgis.gis import GIS

#connect to your GIS
my_gis = GIS("home")


# ### Create a Network Layer
# To perform any network analysis (such as finding the closest facility, the best route between multiple stops, or service area around a facility), you would need to create a `NetworkLayer` object. In this sample, since we are solving for closest facilities, we need to create a `ClosestFacilityLayer` which is a type of `NetworkLayer`.
# 
# To create any `NetworkLayer` object, you would need to provide the URL to the appropriate network analysis service. Hence, in this sample, we provide a `ClosestFacility` URL to create a `ClosestFacilityLayer` object. 
# 
# Since all ArcGIS Online organizations already have access to those routing services, you can access this URL through the `GIS` object's `helperServices` property. If you have your own ArcGIS Server based map service with network analysis capability enabled, you would need to provide the URL for this service.
# 
# Let us start by importing the `network` module

# In[2]:


import arcgis.network as network


# Access the analysis URL from the `GIS` object

# In[3]:


analysis_url = my_gis.properties.helperServices.closestFacility.url
analysis_url


# Create a `ClosestFacilityLayer` object using this URL

# In[4]:


cf_layer = network.ClosestFacilityLayer(analysis_url, gis=my_gis)


# ### Create hospitals layer
# In this sample, we will be looking for the closest hospital (facility) to an incident location. Even though we are interested in finding out the closest one, it would still be helpful to get the information on the distance and travel time to all of them for reference purposes.
# 
# In the code below, we need to geocode the hospitals' addresses as well as do the reverse geocode for the incident location which has been supplied in the latitude/longitude format.
# 
# To perform the geocode operations, we import the `geocoding` module of the ArcGIS API.

# In[5]:


from arcgis import geocoding


# In this sample, we geocode addresses of hospitals to create the facility layer. In your workflows, this could any feature layer. Create a list of hospitals in Rio de Janeiro, Brazil.

# In[6]:


hospitals_addresses = ['Estrada Adhemar Bebiano, 339 Del Castilho, Rio de Janeiro RJ, 21051-370, Brazil',
                       'R. José dos Reis Engenho de Dentro, Rio de Janeiro RJ, 20750-000, Brazil',
                       'R. Dezessete, s/n Maré, Rio de Janeiro RJ, 21042-010, Brazil',
                       'Rua Dr. Miguel Vieira Ferreira, 266 Ramos, Rio de Janeiro RJ, Brazil']


# Loop through each address and geocode it. The geocode operation returns a list of matches for each address. We pick the first result and extract the coordinates from it and construct a `Feature` object out of it. Then we combine all the `Feature`s representing the hospitals into a `FeatureSet` object.

# In[7]:


from arcgis.features import Feature, FeatureSet


# In[8]:


hosp_feat_list = []

for address in hospitals_addresses:
    hit = geocoding.geocode(address)[0]
    hosp_feat = Feature(geometry=hit['location'], attributes=hit['attributes'])

    hosp_feat_list.append(hosp_feat)


# Construct a `FeatureSet` using each hospital `Feature`.

# In[9]:


hospitals_fset = FeatureSet(features=hosp_feat_list, 
                            geometry_type='esriGeometryPoint', 
                            spatial_reference={'wkid' : 4326, 'latestWkid': 4326})


# Lets draw our hospitals on a map

# In[10]:


from IPython.display import HTML


# In[11]:


map1 = my_gis.map('Rio de Janeiro, Brazil')
map1.basemap = 'arcgis-light-gray'
map1


# ![image](https://user-images.githubusercontent.com/13968196/234745611-6b2b4636-25e1-43cf-a60d-328e8b6a3aae.png)
# 

# In[12]:


map1.draw(hospitals_fset, symbol={"type": "esriSMS","style": "esriSMSSquare",
                                  "color": [76,115,0,255],"size": 8,})


# ### Create incidents layer
# Similarly, let us create the incient layer

# In[13]:


incident_coords = '-43.281206,-22.865676'
reverse_geocode = geocoding.reverse_geocode({"x": incident_coords.split(',')[0], 
                                              "y": incident_coords.split(',')[1]})

incident_feature = Feature(geometry=reverse_geocode['location'], 
                           attributes=reverse_geocode['address'])


# In[14]:


incident_fset = FeatureSet([incident_feature], geometry_type='esriGeometryPoint',
                          spatial_reference={'wkid' : 4326, 'latestWkid': 4326})


# Let us add the incident to the map

# In[16]:


map1.draw(incident_fset, symbol={"type": "esriSMS","style": "esriSMSCircle","size": 8})


# ## Solve for closest hospital
# By default the closest facility service would return only the closest location, so we need to specify explicitly the `default_target_facility_count` parameter as well as `return_facilities`.
# 

# In[17]:


result = cf_layer.solve_closest_facility(incidents=incident_fset,
                                        facilities=hospitals_fset,
                                        default_target_facility_count=4,
                                        return_facilities=True,
                                        impedance_attribute_name='TravelTime',
                                        accumulate_attribute_names=['Kilometers','TravelTime'])


# Let us inspect the result dictionary

# In[18]:


result.keys()


# Let us use the `routes` dictionary to construct line features out of the routes to display on the map

# In[19]:


result['routes'].keys()


# In[20]:


result['routes']['features'][0].keys()


# Construct line features out of the routes that are returned.

# In[21]:


line_feat_list = []
for line_dict in result['routes']['features']:
    f1 = Feature(line_dict['geometry'], line_dict['attributes'])
    line_feat_list.append(f1)


# In[22]:


routes_fset = FeatureSet(line_feat_list, 
                         geometry_type=result['routes']['geometryType'],
                         spatial_reference= result['routes']['spatialReference'])


# Add the routes back to the map. The route to the closest hospital is in red

# In[23]:


allocation_line_symbol_4 = {'type': 'esriSLS', 'style': 'esriSLSSolid',
                                'color': [0,0,255,100], 'width': 6}
map1.draw(routes_fset, symbol = allocation_line_symbol_4)


# ![image](https://user-images.githubusercontent.com/13968196/234746249-1dddbca5-11b5-44d5-97a9-79aeb47837b9.png)

# In[24]:


map1.clear_graphics()


# ## Analyze the results in a table
# Since we parsed the routes as a `FeatureSet`, we can display the attributes easily as a `pandas` `DataFrame`.

# In[25]:


df1 = routes_fset.sdf
df1


# Let us add the hospital addresses and incident address to this table and display only the relevant columns

# In[26]:


df1['facility_address'] = hospitals_addresses
df1['incident_address'] = [incident_feature.attributes['Match_addr'] for i in range(len(hospitals_addresses))]


# In[27]:


df1[['facility_address','incident_address','Total_Miles','Total_TravelTime']]


# ### Conclusion
# Thus using the `network` module of the ArcGIS API for Python, you can solve for closest facilities from an incident location.


# ====================
# finding_routes_for_appliance_delivery_with_VRP_solver.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Finding routes for appliance delivery with Vehicle Routing Problem Solver
# 

# <h2>Table of Contents<span class="tocSkip"></span></h2>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Finding-routes-for-appliance-delivery-with-Vehicle-Routing-Problem-Solver" data-toc-modified-id="Finding-routes-for-appliance-delivery-with-Vehicle-Routing-Problem-Solver-1">Finding routes for appliance delivery with Vehicle Routing Problem Solver</a></span><ul class="toc-item"><li><ul class="toc-item"><li><span><a href="#Create-orders-layer-with-csv-file:" data-toc-modified-id="Create-orders-layer-with-csv-file:-1.0.1">Create orders layer with csv file:</a></span></li><li><span><a href="#Create-routes-layer-with-csv-file:" data-toc-modified-id="Create-routes-layer-with-csv-file:-1.0.2">Create routes layer with csv file:</a></span></li><li><span><a href="#Create-depots-layer-with-csv-file:" data-toc-modified-id="Create-depots-layer-with-csv-file:-1.0.3">Create depots layer with csv file:</a></span></li><li><span><a href="#Draw-the-depots-and-orders-in-map." data-toc-modified-id="Draw-the-depots-and-orders-in-map.-1.0.4">Draw the <code>depots</code> and <code>orders</code> in map.</a></span></li><li><span><a href="#Create-depots-layer-by-geocoding-the-location" data-toc-modified-id="Create-depots-layer-by-geocoding-the-location-1.0.5">Create depots layer by geocoding the location</a></span></li><li><span><a href="#Create-routes-layer-with-JSON-file" data-toc-modified-id="Create-routes-layer-with-JSON-file-1.0.6">Create routes layer with JSON file</a></span></li></ul></li><li><span><a href="#Solve-VRP" data-toc-modified-id="Solve-VRP-1.1">Solve VRP</a></span></li><li><span><a href="#Result" data-toc-modified-id="Result-1.2">Result</a></span></li><li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-1.3">Conclusion</a></span></li></ul></li></ul></div>

# The network module of the ArcGIS API for Python can be used to solve different types of network analysis operations. For example, an appliance delivery company wants to serve multiple customers in a day using several delivery drivers, a health inspection company needs to schedule inspection visits for the inspectors. The problem that is common to these examples is called vehicle routing problem (VRP). Each such problem requires to determine which orders should be serviced by which vehicle/driver and in what sequence, so the total operating cost is minimized and the routes are operational. In addition, the VRP solver can solve more specific problems because numerous options are available, such as matching vehicle capacities with order quantities, giving breaks to drivers, and pairing orders so they are serviced by the same route.
# 
# In this sample, we find a solution for an appliance delivery company to find routes for two vehicles which start from the warehouse and return to the same, at the end of the working day. We will find optimized routes given a set of orders to serve and a set of vehicles with constraints. All we need to do is pass in the `depot` locations, `route` (vehicle/driver) details, the `order` locations, and specify additional properties for `orders`.  The VRP service will return `routes` with directions. Once you have the results, you can add the `routes` to a map, display the turn-by-turn directions, or integrate them further into your application. To learn more about the capabilities of the routing and directions services, please visit the [documentation](http://desktop.arcgis.com/en/arcmap/latest/extensions/network-analyst/vehicle-routing-problem.htm#).

# **Note** :If you run the tutorial using ArcGIS Online, 2 [credits](http://pro.arcgis.com/en/pro-app/tool-reference/appendices/geoprocessing-tools-that-use-credits.htm#ESRI_SECTION2_5FAF16DB9F044F78AF164A22752A9A7F) will be consumed.

# As a first step, let's import required libraries and establish a connection to your organization which could be an ArcGIS Online organization or an ArcGIS Enterprise. If you dont have an ArcGIS account, [get ArcGIS Trial](https://www.esri.com/en-us/arcgis/trial).

# In[1]:


import arcgis
from arcgis.gis import GIS
import pandas as pd
import datetime
import getpass
from IPython.display import HTML

from arcgis import geocoding
from arcgis.features import Feature, FeatureSet
from arcgis.features import GeoAccessor, GeoSeriesAccessor


# In[2]:


my_gis = GIS('home')


# To solve the Vehicle Routing Problem, we need `orders` layer with stop information, `depots` layer with the warehouse location information from where the routes start and `routes` table with constraints on routes like maximum total time the driver can work etc.
# To provide this information to the service, different types of inputs are supported as listed below:
# 
# - Csv file
# - Address information to geocode or reverse geocode
# - Json file with attribute names and values
# 
# Let us see how to get the layer from each of these input types.

# ### Create orders layer with csv file:
# Use this parameter to specify the orders the routes should visit. An order can represent a delivery (for example, furniture delivery), a pickup (such as an airport shuttle bus picking up a passenger), or some type of service or inspection (a tree trimming job or building inspection, for instance). When specifying the orders, you can specify additional properties for orders using attributes, such as their names, service times, time windows, pickup or delivery quantities etc.
# 
# Orders csv file has an `Address` column. So, we convert orders csv to a SpatialDataFrame using the address values to geocode order locations.To perform the geocode operations, we imported the geocoding module of the ArcGIS API.

# In[3]:


orders_csv = "data/orders.csv"

# Read the csv file and convert the data to feature set
orders_df = pd.read_csv(orders_csv)
orders_sdf = pd.DataFrame.spatial.from_df(orders_df, "Address")
orders_fset = orders_sdf.spatial.to_featureset()


# In[4]:


orders_sdf.head(3)


# ### Create routes layer with csv file:

# A route specifies vehicle and driver characteristics. A route can have start and end depot service times, a fixed or flexible starting time, time-based operating costs, distance-based operating costs, multiple capacities, various constraints on a driver’s workday, and so on. When specifying the routes, you can set properties for each one by using attributes. Attributes in the csv are explained below.
# 
# `Name`- The name of the route
# 
# `StartDepotName`- The name of the starting depot for the route. This field is a foreign key to the Name field in Depots.
# 
# `EndDepotName`-  The name of the ending depot for the route. This field is a foreign key to the Name field in the Depots class.
# 
# `EarliestStartTime`- The earliest allowable starting time for the route. 
# 
# `LatestStartTime`- The latest allowable starting time for the route.
# 
# `Capacities`- The maximum capacity of the vehicle.
# 
# `CostPerUnitTime`- The monetary cost incurred per unit of work time, for the total route duration, including travel times as well as service times and wait times at orders, depots, and breaks.
# 
# `MaxOrderCount`- The maximum allowable number of orders on the route.
# 
# `MaxTotalTime`- The maximum allowable route duration.
# 
# `AssignmentRule`- This field specifies the rule for assigning the order to a route for example, Exclude, Preserve route, Preserve rooute with relative sequence, override etc, please read [documentation](https://developers.arcgis.com/python/api-reference/arcgis.network.analysis.html#solve-vehicle-routing-problem) for more details. 
# 
# To get a featureset from dataframe, we convert the CSV to a pandas data frame using read_csv function. Note that in our CSV, `EarliestStartTime` and `LatestStartTime` values are represented as strings denoting time in the local time zone of the computer. So we need to parse these values as date-time values which we accomplish by specifying to_datetime function as the datetime parser.
# 
# When calling `arcgis.network.analysis.solve_vehicle_routing_problem` function we need to pass the datetime values in milliseconds since epoch. The `routes_df` dataframe stores these values as datetime type. We convert from datetime to int64 datatype which stores the values in nano seconds. We then convert those to milliseconds.
# 

# In[12]:


routes_csv = "data/routes.csv"

# Read the csv file
routes_df = pd.read_csv(routes_csv, parse_dates=["EarliestStartTime", "LatestStartTime"], date_parser=pd.to_datetime)
routes_df["EarliestStartTime"] = routes_df["EarliestStartTime"].astype("int64") / 10 ** 6
routes_df["LatestStartTime"] = routes_df["LatestStartTime"].astype("int64") / 10 ** 6
routes_fset = arcgis.features.FeatureSet.from_dataframe(routes_df)


# In[6]:


routes_df


# ### Create depots layer with csv file:
# Use this parameter to specify a location that a vehicle departs from at the beginning of its workday and returns to, at the end of the workday. Vehicles are loaded (for deliveries) or unloaded (for pickups) at depots at the start of the route.
# 
# Depots CSV file has depot locations in columns called `Latitude` (representing Y values) and `Longitude` (representing X values). So, we convert depots CSV to a SpatialDataFrame using the X and Y values. We need to reverse geocode these locations as these are provided in latitude/longitude format. Depots must have `Name` values because the `StartDepotName` and `EndDepotName` attributes of the `routes` parameter reference the names you specify here. 

# In[7]:


depots_csv = "data/depots.csv"

# Read the csv file and convert the data to feature set
depots_df = pd.read_csv(depots_csv)
depots_sdf = pd.DataFrame.spatial.from_xy(depots_df, "Longitude", "Latitude")
depots_sdf = depots_sdf.drop(axis=1, labels=["Longitude", "Latitude"])
depots_fset = depots_sdf.spatial.to_featureset()
depots_sdf


# 
# ### Draw the `depots` and `orders` in map.
# To visualize the `orders` and `depots` layer in the map, create a map as done below. The layer can also be drawn with suitable symbol, size and color with draw service in map. The green dots represent stops location to service and the orange square represents the depot from where `routes` should start.

# In[42]:


# Create a map instance to visualize inputs in map
map_view_inputs = my_gis.map('San Fransisco, United States', zoomlevel=10)
map_view_inputs


# In[38]:


# Visualize order and depot locations with symbology
map_view_inputs.draw(orders_fset, symbol={"type": "esriSMS","style": "esriSMSCircle","color": [76,115,0,255],"size": 8})
map_view_inputs.draw(depots_fset, symbol={"type": "esriSMS","style": "esriSMSSquare","color": [255,115,0,255], "size": 10})


# Alternatively, if the input data is provided in different formats, we could still create layers from those data. In practice, there might not always be csv files for all the inputs. It could be just a list of addresses or the inputs are feed from an automated script or server output in JSON format from REST API.
# 
# Let's see how to convert each of input type to featureset to provide as input directly to the VRP service.
# 
# ### Create depots layer by geocoding the location
# 
# If we know address of depot, we would geocode the address to get the feature set to input that to the VRP solver as follows.

# In[8]:


depot_geocoded_fs = geocoding.geocode("2-98 Pier 1, San Francisco, California, 94111", 
                                      as_featureset=True, max_locations=1)
depot_geocoded_fs.sdf


# ### Create routes layer with JSON file
# If we have json files for inputs, without needing to parse it, we could feed that to from_json service of FeatureSet and we would get feature set to input to the [VRP service](https://developers.arcgis.com/rest/network/api-reference/vehicle-routing-problem-service.htm#ESRI_SECTION3_E65DBEE0F0BF46EB946A63A2537137C5).

# In[14]:


import json
with open("data/Routes_fset.json") as f:
    df1 = json.load(f)
routes_string = json.dumps(df1)    
routes_fset2 = FeatureSet.from_json(routes_string)
routes_fset2


# In[10]:


routes_string


# ## Solve VRP
# Once you have all the inputs as featuresets, you can pass inputs converted from different formats. For example, depot could be a featureset geocoded from address, `orders` and `routes` could be read from csv files to convert to featureset. As result to the solve, we get routes with geometry. If we need driving directions for navigation, `populate_directions` must be set to true. 

# In[13]:


get_ipython().run_cell_magic('time', '', 'today = datetime.datetime.now()\nfrom arcgis.network.analysis import solve_vehicle_routing_problem\nresults = solve_vehicle_routing_problem(orders= orders_fset,\n                                        depots = depots_fset,\n                                        routes = routes_fset, \n                                        save_route_data=\'true\',\n                                        populate_directions=\'true\',\n                                        travel_mode="Driving Time",\n                                        default_date=today)\n\nprint(\'Analysis succeeded? {}\'.format(results.solve_succeeded))\n')


# ## Result
# Let's have a look at the output routes in the dataframe. Description about the fields returned for each route can be read in the [documentation](https://developers.arcgis.com/rest/network/api-reference/vehicle-routing-problem-service.htm#ESRI_SECTION3_E65DBEE0F0BF46EB946A63A2537137C5). Some fields shown in the output are:
# 
# `Name`- The name of the route.
# 
# `OrderCount`- The number of orders assigned to the route.
# 
# `StartTime`- The starting time of the route. 
# 
# `EndTime`- The ending time of the route.
# 
# `StartTimeUTC`- The start time of the route in UTC time.
# 
# `EndTimeUTC`- The end time of the route in UTC time.
# 
# `TotalCost`- The total operating cost of the route, which is the sum of the following attribute values: FixedCost, RegularTimeCost, OvertimeCost, and DistanceCost.
# 
# `TotalDistance`- The total travel distance for the route. 
# 
# `TotalTime`- The total route duration. 
# 
# `TotalTravelTime`- The total travel time for the route. 
# 
# The time values returned from VRP service are in milliseconds from epoch, we need to convert those to datetime format. From the output table, Route1 starts at 8 am, the total time duration for the route is 126 minutes and the total distance for the route is around 60 miles and Route1 services 4 orders.

# In[18]:


# Display the output routes in a pandas dataframe.
out_routes_df = results.out_routes.sdf
out_routes_df[['Name','OrderCount','StartTime','EndTime','TotalCost','TotalDistance','TotalTime','TotalTravelTime','StartTimeUTC','EndTimeUTC']]


# The information about stops sequence and which routes will service the stops, let's read out_stops output table. Relevant attributes from the table are:
# 
# `Name`- The name of the stop.
# 
# `RouteName`- The name of the route that makes the stop.
# 
# `Sequence`- The relative sequence in which the assigned route visits the stop.
# 
# `ArriveTime`- The time of day when the route arrives at the stop. The time-of-day value for this attribute is in the time zone in which the stop is located.
# 
# `DepartTime`- The time of day when the route departs from the stop. The time-of-day value for this attribute is in the time zone in which the stop is located.
# 
# Similar to routes table, it has time values in milliseconds from epoch, we will convert those values to date time format. 
# 
# In the table, Route1 starts at 2019-01-17 08:00:00.000 from Warehouse, and arrives at the stop 6 located at '10264 San Pablo Ave, El Cerrito, CA 94530', at 2019-02-01 08:28 local time.

# In[24]:


out_stops_df = results.out_stops.sdf
out_stops_df[['Name','RouteName','Sequence','ArriveTime','DepartTime']].sort_values(by=['RouteName',
                                                                                       'Sequence'])


# Now, let us visualize the results on a map.

# In[25]:


# Create a map instance to visualize outputs in map
map_view_outputs = my_gis.map('San Fransisco, United States', zoomlevel=10)
map_view_outputs


# In[21]:


#Visusalize the inputsn with different symbols
map_view_outputs.draw(orders_fset, symbol={"type": "esriSMS",
                                           "style": "esriSMSCircle",
                                           "color": [76,115,0,255],"size": 8})
map_view_outputs.draw(depots_fset, symbol={"type": "esriSMS",
                                           "style": "esriSMSSquare",
                                           "color": [255,115,0,255], "size": 10})

#Visualize the first route
out_routes_flist = []
out_routes_flist.append(results.out_routes.features[0])
out_routes_fset = []
out_routes_fset = FeatureSet(out_routes_flist)
map_view_outputs.draw(out_routes_fset, 
                      symbol={"type": "esriSLS",
                              "style": "esriSLSSolid",
                              "color": [0,100,240,255],"size":10})

#Visualize the second route
out_routes_flist = []
out_routes_flist.append(results.out_routes.features[1])
out_routes_fset = []
out_routes_fset = FeatureSet(out_routes_flist)
map_view_outputs.draw(out_routes_fset, 
                      symbol={"type": "esriSLS",
                              "style": "esriSLSSolid",
                              "color": [255,0,0,255],"size":10})


# Save the route data from result to local disk, which would then be used to upload to online portal to share with drivers eventually and share the routes in ArcGIS online on the portal. Individual routes are saved as route layers which could then be opened in navigator with directions(if you solve with 'populate_directions'=true')

# In[26]:


route_data = results.out_route_data.download('.')
route_data


# In[29]:


route_data_item = my_gis.content.add({"type": "File Geodatabase"}, route_data)
route_data_item


# Create route layers from the route data. This will create route layers in the online portal which could then be shared with drivers, so they would be able to open this in navigator.

# In[18]:


route_layers = arcgis.features.analysis.create_route_layers(route_data_item, 
                                                            delete_route_data_item=True)
for route_layer in route_layers:
    route_layer.share(org=True)
    display(route_layer.homepage)
    display(route_layer)


# ## Conclusion
# The network module of the ArcGIS API for Python allows you to solve a Vehicle Routing Problem and other network problems with necessary business constraints. Learn more about how to solve VRP with business constraints [here](https://developers.arcgis.com/python/api-reference/arcgis.network.analysis.html#solve-vehicle-routing-problem).


# ====================
# finding_suitable_spots_for_AED_devices_using_raster_analytics.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Finding suitable spots for placing heart defibrillator equipments in public
# In this sample, we will observe how site [suitability analyses](https://en.wikipedia.org/wiki/Suitability_analysis) can be performed using the ArcGIS API for Python. The objective of this sample is to find locations in the city of Philadelphia that are suitable for placing [AED (Automated External Defibrillator)](https://en.wikipedia.org/wiki/Automated_external_defibrillator) for public emergencies.

# 
# <center><i>Image of an AED device attached to a wall at San Diego Convention Center during Esri UC</i></center>

# The criteria for suitable places are those that have high incidence of [OHCA (Out of Hospital Cardiac Arrests)](http://www.sca-aware.org/sca-news/aha-releases-latest-statistics-on-out-of-hospital-cardiac-arrest) and be accessible to public, such as commercial areas.
# 
# As inputs, we start with geocoded OCHA (Out-of-Hospital Cardiac Arrest) point data, along with a few base layers for the city of Pittsburgh published as feature layers. As output, we need to generate a list of locations that have a high incidence of heart-attacks and located within commercial areas, allowing easy access at times of emergencies.

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Getting-set-up" data-toc-modified-id="Getting-set-up-1">Getting set up</a></span><ul class="toc-item"><li><span><a href="#Load-input-datasets" data-toc-modified-id="Load-input-datasets-1.1">Load input datasets</a></span></li><li><span><a href="#Outline-of-the-analysis" data-toc-modified-id="Outline-of-the-analysis-1.2">Outline of the analysis</a></span></li></ul></li><li><span><a href="#Create-a-600-feet-buffer-around-commercial-areas" data-toc-modified-id="Create-a-600-feet-buffer-around-commercial-areas-2">Create a 600 feet buffer around commercial areas</a></span><ul class="toc-item"><li><span><a href="#Select-commercial-zones" data-toc-modified-id="Select-commercial-zones-2.1">Select commercial zones</a></span></li><li><span><a href="#Create-buffers-around-commercial-zones" data-toc-modified-id="Create-buffers-around-commercial-zones-2.2">Create buffers around commercial zones</a></span></li><li><span><a href="#Create-a-density-map-to-find-areas-of-high-heart-attack-incidence" data-toc-modified-id="Create-a-density-map-to-find-areas-of-high-heart-attack-incidence-2.3">Create a density map to find areas of high heart attack incidence</a></span><ul class="toc-item"><li><span><a href="#Calculate-density" data-toc-modified-id="Calculate-density-2.3.1">Calculate density</a></span></li></ul></li></ul></li><li><span><a href="#Reclassify-the-density-raster" data-toc-modified-id="Reclassify-the-density-raster-3">Reclassify the density raster</a></span><ul class="toc-item"><li><span><a href="#Convert-units-from-sqmile-to-city-blocks" data-toc-modified-id="Convert-units-from-sqmile-to-city-blocks-3.1">Convert units from sqmile to city blocks</a></span></li></ul></li><li><span><a href="#Perform-overlay-analysis" data-toc-modified-id="Perform-overlay-analysis-4">Perform overlay analysis</a></span></li><li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-5">Conclusion</a></span></li></ul></div>

# ## Getting set up

# In[2]:


from arcgis.gis import GIS
from arcgis.mapping import WebMap
from arcgis.widgets import MapView
from arcgis.features import FeatureCollection, use_proximity
from datetime import datetime


# In[3]:


gis = GIS(url='https://pythonapi.playground.esri.com/portal', username='arcgis_python', password='amazing_arcgis_123')


# ### Load input datasets

# In[5]:


ohca_item = gis.content.get('a5719916dff4442789a59680c25a4284')
ohca_item


# In[8]:


ohca_map_item = gis.content.get('b8b6cf2bcbeb4903a5372b7f4cbfb252')
ohca_map_item


# Let us take a look at the layers available in this item

# In[9]:


for lyr in ohca_item.layers:
    print(lyr.properties.name)


# Let us display the Web Map item to view these layers on a map.

# In[38]:


map1 = MapView(item=ohca_map_item)
map1.legend=True
map1


# ### Outline of the analysis
# The idea of this analysis is to find places suitable for placing the AED devices. Based on prior knowledge we happen to know areas that are commercial, accessible to public and showing a high incidence of out-of-hospital cardiac arrests are good candidates. We will build the suitability model by performing these steps:
# 
#  * use **Zoning** polygon layer to identify commercial areas and build a `600` feet buffer around them
#  * perform density analysis on **Heart attack incidence** point layer
#  * perform spatial overlay to find overlapping locations

# ## Create a 600 feet buffer around commercial areas
# The `Zoning` feature layer contains polygon features that represent different zones such as commercial, residential etc. We need to select those features that correspond to commercial zones and create a buffer of `600` feet around them. The `600` feet area roughly corresponds to two-blocks, a walk able distance in case of an emergency.

# ### Select commercial zones
# To select the commercial zones using a query, we need to know what columns and values are available. Hence, let us construct a small query that gives the first few rows / features.

# In[12]:


zoning_flayer = ohca_item.layers[2]
zoning_sdf = zoning_flayer.query(result_record_count=5, return_all_records=False, as_df=True)


# In[13]:


zoning_sdf


# The column `zoning_grouped` contains zoning categories. We are intersted in those polygons that correspond to the `Commercial` category.

# In[14]:


zoning_commercial_fset = zoning_flayer.query(where="zoning_grouped = 'Commercial'")
commercial_zone_df = zoning_commercial_fset.sdf
commercial_zone_df.head(5)[['name','zoning_grouped']] #display the first 5 results


# In[15]:


commercial_zone_df.shape


# Let us draw the selected polygons on a map

# In[55]:


zone_map = gis.map("Pittsburgh, PA")
zone_map


# In[17]:


zone_map.draw(zoning_commercial_fset)


# Thus, from `965` zoning polygons, we have narrowed down to `317`.
# 
# ### Create buffers around commercial zones
# 
# The ArcGIS API for Python allows you to define definition queries or filters on Feature Layers. When you run a spatial analysis on those layers, only the features that fit the filter criteria you specified will be used. Thus, you can use the 'where' clause you used earlier (to get commercial zones) to set as a filter on the `zoning_flayer` and pass that as the input to the `create_buffers` tool. The advantage of this workflow is, you are not sending the features from the local `FeatureSet` object to the tool, instead, you are asking to the tool to get the features directly from the feature layer which is **colocated** with the tool. This paradigm of colocating the compute with the data is highly preferred to improve efficiency and scalability of your analyses.

# In[18]:


# create a filter using the where clause from earlier
zoning_flayer.filter = "zoning_grouped = 'Commercial'"


# In[19]:


# create a timestamp to create a unique output
timestamp=datetime.now().strftime('%d_%m_%Y_%H_%M_%S')

# create buffers
commercial_buffers = use_proximity.create_buffers(input_layer=zoning_flayer,
                                                  distances=[600],units='Feet', 
                                                  dissolve_type='Dissolve',
                                                 output_name=f'commercial_buffers_{timestamp}')
commercial_buffers


# Draw the results on the `commercial_zone_map` created above

# In[20]:


zone_map.add_layer(commercial_buffers)


# ### Create a density map to find areas of high heart attack incidence
# To calculate the density, we use `calculate_density` tool available under the `raster` module and provide the `Heart attack incidence` feature layer as its input. This layer has a column named `num_incidence` that additionally specifies the number of heart attacks that happened at each point location. Below we bring up a few of the features to get an idea.

# In[21]:


ha_incidence = ohca_item.layers[0] #the first layer in the input feature layer collection
ha_incidence_fset = ha_incidence.query(result_record_count=10, return_all_records=False)
ha_incidence_fset.sdf.head(10)


# #### Calculate density

# In[22]:


from arcgis.raster.analytics import calculate_density
from arcgis.raster.functions import *


# In[23]:


# create a timestamp to create a unique output
timestamp=datetime.now().strftime('%d_%m_%Y_%H_%M_%S')

ha_density = calculate_density(ha_incidence, count_field='num_incidence', 
                               output_cell_size={'distance':150,'units':'feet'},
                               output_name = f'ha_density_{timestamp}')
print(ha_density)


# Let us display the density raster on a map

# In[11]:


density_map = gis.map("Pittsburgh, PA", zoomlevel=11)
density_map


# Use the `stretch` [raster function](http://pro.arcgis.com/en/pro-app/help/data/imagery/stretch-function.htm) to enhance the density layer before adding it to the map:

# In[25]:


density_layer = ha_density.layers[0]

stretch_rf = stretch(density_layer, stretch_type='StdDev',num_stddev=2)
colormap_rf = colormap(stretch_rf, colormap_name='Gray')


# In[26]:


density_map.add_layer(colormap_rf, {"opacity":0.5})


# From the `density_map`, we see certain regions (in shades of white) have a higher density of heart attack incidences compared to the rest.

# ## Reclassify the density raster
# Calculate density tool returns the number of incidences per sq.mile. We are interested in the number of heart attacks at a larger scale of about 5 square blocks. In Pittsburgh, each block spans about 300 ft in length, thus 5 sq. blocks cover an area of 1500 x 1500 sq.feet. We apply `remap` raster function to convert the density from sq. miles to that in 5 block area

# In[30]:


import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')
import numpy as np


# Plot the histogram to view actual density values and its distribution. The `histograms` property of the `ImageryLayer` object returns you histogram of each of its bands.

# In[31]:


density_hist = density_layer.histograms


# Construct the X axis such that it ranges from min value to max value of the pixel range in the image.

# In[32]:


x = np.linspace(density_hist[0]['min'], density_hist[0]['max'], num=density_hist[0]['size'])


# In[33]:


fig = plt.figure(figsize=(10,8))
ax = fig.add_axes([0,0,1,1])
ax.bar(x,density_hist[0]['counts'])
ax.set_title("Histogram of heart attack density layer")
ax.set_xlabel("Heart attacks per sq. mile")
ax.set_ylabel("Number of pixels")

ax2 = fig.add_axes([0.25,0.2,0.7,0.7])
ax2.bar(x[-200:], density_hist[0]['counts'][-200:])
ax2.set_title("Histogram of heart attack density layer - zoomed")
ax2.set_xlabel("Heart attacks per sq. mile")
ax2.set_ylabel("Number of pixels")


# ### Convert units from sqmile to city blocks
# The inset histogram chart has the histogram zoomed to view the distribution in the upper end of the density spectrum. We are interested in selecting those regions that have a heart attack of at least 5 per 5 block area. To achieve this, we need to convert the density from square miles to 5 square blocks.

# In[34]:


conversion_value = (1500*1500)/(5280*5280)
density_5blocks = density_layer * conversion_value #raster arithmetic
density_5blocks


# Let us remap this continuous density raster to a binary layer representing whether a pixel represents high enough density or not.

# In[35]:


density_classified_color = colormap(density_5blocks, colormap_name='Random',astype='u8')
density_classified_color


# Next, we classify the density raster such that pixels that have heart attacks greater than 5 get value 1 and rest become 'no data' pixels.

# In[36]:


#remap pixel values to create a binary raster
density_classified = remap(density_5blocks, input_ranges=[5,16], output_values=[1],astype='u8',no_data_ranges=[0,5])
density_classified_viz = colormap(density_classified, colormap_name='Random', astype='u8')
density_classified_viz


# Through classification, we have determined there are 3 hotspots in our density raster. Let us overlay this on a map to see which areas these hotspots correspond to.

# In[111]:


density_map2 = gis.map("Pittsburgh, PA")
density_map2


# In[112]:


density_map2.add_layer(density_classified_viz)


# ## Perform overlay analysis

# The site selection condition requires two inputs, the heart attack density layer (which we created earlier) and the accessibility layer (from the buffer analysis). To perform overlay, we need to convert the buffers layer to a raster layer of matching cell size as that of the density raster layer. To perform this conversion we use the `convert_feature_to_raster` method.

# In[37]:


from arcgis.raster.analytics import convert_feature_to_raster


# In[38]:


# create a timestamp to create a unique output
timestamp=datetime.now().strftime('%d_%m_%Y_%H_%M_%S')

# convert zoning buffer polygon to a raster layer of matching cell size
buffer_raster = convert_feature_to_raster(commercial_buffers.layers[0],
                                          output_cell_size={'distance':150, 'units':'feet'},
                                          output_name=f'buffer_raster_{timestamp}')

print(buffer_raster)


# Query the layer to quickly visualize it as an image

# In[39]:


buffer_raster = buffer_raster.layers[0]
buffer_raster


# The `raster` module of the Python API provides numerous raster functions. Of which we use the [bitwise_and](http://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/local-function.htm) local function which returns an image with pixels that match in both the input rasters.

# In[40]:


bool_overlay = bitwise_and([buffer_raster,density_classified])
bool_overlay


# Let us overlay this final result on a map to visualize the regions that are suitable to locating new AED devices.

# In[4]:


map3 = gis.map("Carnegie Mellon University, PA")
map3


# In[2]:


map3.add_layer(bool_overlay)


# ## Conclusion
# Thus, in this sample, we observed how site-suitability analyses can be performed using ArcGIS and the ArcGIS API for Python. We started with the requirements for placing new AED devices as -- high intensity of cardiac arrests and proximity to commercial areas. Using a combination of feature analysis and raster analysis, we were able to process and extract the suitable sites. The analyst could convert the results from raster to vector, perform a centroid operation on the polygons, followed by reverse geocode to get the addresses of these 3 suitable locations for reporting and further action.


# ====================
# finetuning_pre-trained_building_footprint_model.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Finetuning Pre-trained Building Footprint Model

# * 🔬 Data Science
# * 🥠 Deep Learning and Instance Segmentation

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Load training data](#Load-training-data)
#  * [Visualize training data](#Visualize-training-data)
# * [Model finetuning](#Model-finetuning)
#  * [Load a pre-trained building footprint model](#Load-a-pre-trained-building-footprint-model)
#  * [Test-the-model-on-our-dataset](#Test-the-model-on-our-dataset)
#  * [Train the model](#Train-the-model)
#  * [Visualize detected building footprints](#Visualize-detected-building-footprints)
#  * [Save model](#Save-model)
# * [Model inference](#Model-inference)

# ## Introduction

# ArcGIS Living Atlas hosts a variety of [pre-trained models](https://livingatlas.arcgis.com/en/browse/?q=dlpk). While these models work well on geography that the model's training data was exported from, they may not perform well on other geographies.
# 
# However, we can improve the performance of these models on different geographies by finetuning the model on our own training data. When compared to training a similar model from scratch, this process will save time, is computationally less intensive, and will provide more accurate results.

# In this workflow, we will perform three broad steps.
# 
# - Load the training data
# - Finetune a pre-trained model
# - Deploy the model and extract footprints
# 
# 

# This workflow requires deep learning dependencies to be installed. Documentation is available [here](https://developers.arcgis.com/python/guide/install-and-set-up/) that outlines how to install and setup an appropriate environment.

# ## Load training data

# In[1]:


from arcgis.gis import GIS
gis = GIS('home')
portal = GIS('https://pythonapi.playground.esri.com/portal')


# In[2]:


training_data = gis.content.get('5351aca735604197ac8d8ede45f6cc4b')
training_data


# In[5]:


filepath = training_data.download(file_name=training_data.name)


# In[12]:


import zipfile
from pathlib import Path
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[17]:


data_path = Path(filepath).parent / 'building_footprints'


# In[6]:


from arcgis.learn import prepare_data
data = prepare_data(data_path, 
                    batch_size=16, 
                    chip_size=400)


# ### Visualize training data

# To get a sense of what the training data looks like, use the `show_batch()` method to randomly pick a few training chips and visualize them. The chips are overlaid with masks representing the building footprints in each image chip.

# In[20]:


data.show_batch(rows=4)


# ## Model finetuning

# ### Load a pre-trained building footprint model

# We can search ArcGIS Living Atlas for [Pre-trained models](https://livingatlas.arcgis.com/en/browse/?q=dlpk). 
# 
# From a model's page on living atlas, we can either directly download the model from the page or find the itemid in the URL to download it using the ArcGIS Python API as follows.

# In[7]:


model_item = gis.content.get('a6857359a1cd44839781a4f113cd5934')
model_item


# Next, we download the model.

# In[28]:


model_path = model_item.download(file_name=model_item.name)


# Once the model is downloaded, we can then load the model.

# In[9]:


from arcgis.learn import MaskRCNN
model = MaskRCNN.from_model(model_path, data)


# ### Test the model on our dataset

# In this sample, our dataset has been curated from Kuwait, which has a very different geography when compared to the data from the United States that was used to train the 'Building Footprint Extraction - USA' model.
#   
# We will run the {model}.`show_results()` method to check the performance of the model on our dataset, and as the model has not yet been trained on the Kuwaiti data, it is expected that the model will not perform well.

# In[10]:


model.show_results()


# 
# The learning rate is one of the most important hyperparameters in training a model. We will use the `lr_find()` method to find an optimal learning rate that will allow us to fine tune the model.

# In[31]:


lr = model.lr_find()
lr


# ### Train the model

# Next, we will use the learning rate suggested above to train our model for 10 epochs.

# In[11]:


model.fit(10,lr=lr)


# In[ ]:





# ## Visualize detected building footprints

# The `model.show_results()` method can be used to display the detected building footprints. Each detection is visualized as a mask by default.

# In[12]:


model.show_results()


# We can set the mode parameter to `bbox_mask` to visualize both masks and bounding boxes.

# In[13]:


model.show_results(mode='bbox_mask')


# ### Save the model

# As we can see, with 10 epochs, we are already seeing reasonable results. More improvements can be achieved by training the model further or by adding more training data. Let's save the model, so that it can be used for inference or further training. By default, it will be saved into the `path` that you specified in the very beginning of this notebook, in the `prepare_data` call.

# In[ ]:


model.save('Building_footprint_10epochs')


# ## Model inference

# The saved model can now be used to extract building footprint masks using the 'Detect Objects Using Deep Learning' tool available in ArcGIS Pro or ArcGIS Enterprise. For this sample, we will use high satellite imagery to detect footprints.

# 

# 

# You can also achieve this using arcpy.

# ```python
# with arcpy.EnvManager(
#     extent="799956.322438235 3234305.33935078 801796.850070329 3235284.04198516", 
#     cellSize=0.3, 
#     processorType="GPU"
# ):
#     arcpy.ia.DetectObjectsUsingDeepLearning(
#         "im_2019_12_05", 
#         r"C:\building_footprints\building_footprints.gdb\building_footprints", 
#         r"C:\building_footprints\Building_footprint_10epochs\Building_footprint_10epochs.emd", 
#         "padding 100;batch_size 4;threshold 0.9;return_bboxes False;tile_size 400", 
#         "NMS", 
#         "Confidence", 
#         "Class", 
#         0, 
#         "PROCESS_AS_MOSAICKED_IMAGE"
#     )
# ```

# The output of the model is a layer of detected building footprints that need to be post-processed using the [Regularize Building Footprints](https://pro.arcgis.com/en/pro-app/tool-reference/3d-analyst/regularize-building-footprint.htm) tool. This tool normalizes the footprint of building polygons by eliminating undesirable artifacts in their geometry. The post-processed building footprints are shown below:

# 

# <center>A subset of detected building footprints


# ====================
# flood_inundation_mapping_using_sar_data_and_deep_learning.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Flood inundation mapping and monitoring using SAR data and deep learning
# > * 🔬 Data Science
# > * 🥠 Deep Learning and pixel classification

# ## Table of Contents
# 
# * [Introduction](#1)
# * [Necessary imports](#2)
# * [Connect to your GIS](#3)
# * [Export training data](#4)
# * [Model training](#5)
#   * [Get training data](#6)
#   * [Prepare data](#7)
#   * [Visualize training data](#8)
#   * [Load model architecture](#9)
#   * [Train the model](#10)
#   * [Visualize results in validation set](#11)
#   * [Evaluate model performance](#12)
#   * [Save the model](#13)
# * [Model inferencing](#14)
# * [Results visualization](#15)
#   * [Flood inundation mapping](#16)
#   * [Estimation of flood inundated area](#17)
#   * [Infrastructural inundation assessment](#18)
# * [Conclusion](#19)

# ## Introduction<a class="anchor" id="1"></a>

# Flooding is one of the most frequent and costly forms of natural disasters. They often strike without warning and can occur when large volumes of water fall in a short time, causing flash floods. Flood mapping is typically performed using the following methods:
# 
# - Aerial observations
# - Ground surveys
# 
# However, when flooding is widespread, these methods become prohibitively expensive and time consuming. Furthermore, aerial observation and optical imagery can often prove difficult, if not impossible, due to obstructive weather conditions. During flooding conditions, clouds can prevent the use of optical satellite imagery for visualization and analysis. In these instances, synthetic-aperture radar (SAR) allows us to penetrate through clouds and hazy atmospheric conditions to continuously observe and map flooding.
# 
# In 2019, severe flooding occurred in the Midwest of the United States. Also known as the Great Flood of 2019, 14 million people were affected across multiple states. In this analysis, we will perform flood mapping and infrastructural inundation mapping of the St. Peters region of Missouri, which was one of the affected areas during the flood.

# ## Necessary imports<a class="anchor" id="2"></a>

# In[1]:


import os
from datetime import datetime
from pathlib import Path

from arcgis.gis import GIS
from arcgis.learn import prepare_data, UnetClassifier
from arcgis.raster import Raster, convert_raster_to_feature
from arcgis.features.manage_data import overlay_layers
from arcgis.features.analysis import dissolve_boundaries


# ## Connect to your GIS<a class="anchor" id="3"></a>

# In[1]:


from arcgis import GIS
gis = GIS('home')
gis_ent = GIS('https://pythonapi.playground.esri.com/portal')


# ## Export training data<a class="anchor" id="4"></a>

# Here, we convert the Sentinel-1 GRD VH polarization band to a 3 band raster using [Export Raster](https://pro.arcgis.com/en/pro-app/2.8/help/data/imagery/export-or-convert-raster-datasets.htm). Under the `Render Settings` section, once `Use Renderer` is checked, `Force RGB` will be enabled.

# 

# The resulting raster is generated from the Sentinel-1 GRD VH imagery using traditional histogram thresholding technique. The raster contains two classes, `permanent waterbodies` and `flood water`. This raster will be used as a `Classified Raster` in the `Export Training Data Using Deep Learning` tool.

# In[3]:


input_raster = gis_ent.content.get("b2d15ba81b65442180e7b1d1b9b708f9")
input_raster


# The feature layer contains two classes: `1 = Permanent Waterbodies` and `2 = Flood Water`. The feature layer will be used as the `Input Feature Class` in the `Export Training Data For Deep Learning` tool.

# In[4]:


label_raster = gis_ent.content.get("2d52064d4eb54559b75ff4451cb6d52b")
label_raster


# The polygon feature class will be used as `Input Mask Polygons` in the `Export Training Data For Deep Learning` tool to delineate the area where image chips will be created.

# In[5]:


aoi = gis_ent.content.get("d378a12e00a24815a306965e1917601d")
aoi


# The [Export Training Data For Deep Learning](https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/export-training-data-for-deep-learning.htm) tool is used to prepare training data for training a deep learning model. The tool is available in both `ArcGIS Pro` and `ArcGIS Enterprise`.

# 

# Next, we will utilize Jupyter Notebooks. Documentation on how to install and setup the necessary environment is available [here](https://developers.arcgis.com/python/guide/install-and-set-up/).

# ## Model training<a class="anchor" id="5"></a>

# ### Get training data<a class="anchor" id="6"></a>

# We have already exported the data, and it can be directly downloaded using the following steps:

# In[6]:


training_data = gis.content.get('c4f58fd8e21743d69c82a93b30c8b873')
training_data


# In[7]:


filepath = training_data.download(file_name=training_data.name)


# In[8]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[9]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# ### Prepare data<a class="anchor" id="7"></a>

# The prepare_data function takes a training data path as input and creates a fast.ai databunch with specified parameters, like transformation, batch size, split percentage, etc.

# In[10]:


data = prepare_data(data_path, batch_size=4, chip_size=400)


# ### Visualize training data<a class="anchor" id="8"></a>

# To get a sense of what the training data looks like, the `arcgis.learn.show_batch()` method will randomly select a few training chips and visualize them.

# In[11]:


data.show_batch(rows=3)


# ### Load model architecture<a class="anchor" id="9"></a>

# `arcgis.learn` provides the UnetClassifier model for per pixel classification that is based on a pretrained convnet, like ResNet, that acts as the `backbone`. More details about UnetClassifier can be found [here](https://developers.arcgis.com/python/api-reference/arcgis.learn.toc.html#unetclassifier).

# In[12]:


# Create U-Net Model
unet = UnetClassifier(data, backbone='resnet34')


# In[13]:


unet.unfreeze()


# ### Train the model<a class="anchor" id="10"></a>

# In[14]:


lr = unet.lr_find()
lr


# We are using the suggested learning rate above to train the model for 400 epochs.

# In[15]:


unet.fit(100, lr=lr)


# We have trained the model for a further 300 epochs to improve model performance. For the sake of time, the cell below is commented out.

# In[16]:


# model.fit(300)


# ### Visualize results in validation set<a class="anchor" id="11"></a>

# It's a good practice to see results of the model vis-a-vis ground truth. The code below picks random samples and shows us ground truths and model predictions, side by side. This enables us to preview the results of the model within the notebook.

# In[17]:


unet.show_results(rows=2, alpha=0.9)


# ### Evaluate model performance<a class="anchor" id="12"></a>

# In[18]:


unet.accuracy()


# As we have 2 classes (`1=permanent waterbodies` and `2=flood water`) for this segmentation task, we need to perform an accuracy assessment for each class. To achieve this, ArcGIS API for Python provides the `per_class_metrics` function that calculates a precision, recall, and f1 score for each class.

# In[19]:


unet.per_class_metrics()


# ### Save the model<a class="anchor" id="13"></a>

# We will save the model that we trained as a 'Deep Learning Package' ('.dlpk' format). A Deep Learning Package is the standard format used to deploy deep learning models on the ArcGIS platform.
# 
# We will use the `save()` method to save the trained model. By default, it will be saved to the 'models' sub-folder within the training data folder.

# In[20]:


unet.save('flood_model', publish=True)


# ## Model inferencing<a class="anchor" id="14"></a>

# Using ArcGIS Pro, we can use the trained model on a test image/area to classify permanent waterbodies and flood inundated areas in the SAR satellite image.
# 
# After training the `UnetClassifier` model and saving the weights for classifying images, we can use the [Classify Pixels Using Deep Learning tool](https://pro.arcgis.com/en/pro-app/latest/tool-reference/image-analyst/classify-pixels-using-deep-learning.htm) tool available in [ArcGIS pro](https://pro.arcgis.com/en/pro-app/latest/tool-reference/image-analyst/classify-pixels-using-deep-learning.htm) and [ArcGIS Enterprise](https://enterprise.arcgis.com/en/portal/latest/use/classify-pixels-using-deep-learning.htm) for inferencing.

# In[21]:


flood_model = gis.content.get('86d5806943024257a8a15fe17296b19b')
flood_model


# In[2]:


raster_for_inferencing = gis.content.get('c6abe978dc854c20bf0664f6f7b43290')
raster_for_inferencing


# 

# `with arcpy.EnvManager(processorType="GPU"):
#  out_classified_raster = arcpy.ia.ClassifyPixelsUsingDeepLearning("sentinel1_3band_inference_raster", "https://deldev.maps.arcgis.com/sharing/rest/content/items/86d5806943024257a8a15fe17296b19b", "padding 100;batch_size 8;predict_background True;tile_size 400", "PROCESS_AS_MOSAICKED_IMAGE", None); out_classified_raster.save(r"C:\Users\shi10484\Documents\ArcGIS\Projects\flood2\flood2.gdb\inferenced_results")`

# ## Results visualization<a class="anchor" id="15"></a>

# The classified output raster is generated using ArcGIS Pro. The output raster is published on the portal for visualization.

# In[23]:


sar_ras2 = gis.content.get('427cd9a47eb544c59c1e965a56e72550')
inf_ras2 = gis.content.get('a7f2c8f23aa448d28fc14ec99b325ca8')


# In[24]:


from arcgis.raster import colormap
inf_cmap2 = colormap(inf_ras2.layers[0], colormap=[[1, 7, 42, 108],[2, 0, 206, 209]])


# ### Create map widgets
# Three map widgets are created showing flood inundation in different regions.

# In[25]:


map1 = gis.map('St Peters, USA', 11)
map1.basemap='satellite'
map2 = gis.map('St Peters, USA', 11)
map2.add_layer(sar_ras2)
map3 = gis.map()
map3.add_layer(sar_ras2)
map3.add_layer(inf_cmap2)


# ### Set the map layout

# In[26]:


from ipywidgets import HBox, VBox, Label, Layout


# In[27]:


map1.sync_navigation(map2)
map2.sync_navigation(map3)


# [Hbox and Vbox](https://developers.arcgis.com/python/api-reference/arcgis.widgets.html?highlight=hbox) were used to set the layout of map widgets.

# In[28]:


hbox_layout = Layout()
hbox_layout.justify_content = 'space-around'

hb1,hb2=HBox([Label('True Colour Image'),Label('Sentinel-1 Imagery'),Label('Predictions'),]),\
                HBox([Label('True Colour Image'),Label('Sentinel-1 Imagery'),Label('Predictions')])
hb1.layout,hb2.layout=hbox_layout,hbox_layout


# ### Flood inundation mapping<a class="anchor" id="16"></a>

# The resulting predictions are provided as a map for better visualization. The results show the spatial distribution of flood water in the Midwestern US during the 2019 floods. Sentinel-1 VV imagery of May 2019 are used for the analysis. In the map widgets, it can be seen that the trained `UNetClassifier` model is able to identify permanent waterbodies and flood water, as well as differentiate between the two. The deep blue color represents permanent waterbodies and the cyan color represents flood water.

# In[29]:


VBox([hb1,HBox([map1,map2,map3])])


# 

# Three map widgets were created. The left widget displays natural color high resolution satellite imagery prior to flooding, the middle widget displays the sentinel-1 imagery during the flood event, and the right map widget displays the predictions of the trained UnetClassifier model. In the maps, St Louis city can be seen where the Illinois river and the Mississippi river converge. The model is able to identify river channels and differentiate from the flood water. The True Color Imagery can be used for visual interpretation for model accuracy.

# ### Estimation of flood inundated area (sq. km)<a class="anchor" id="17"></a>

# The pixel size of the raster is required to calculate the area of flood inundated areas. We will use the `<raster>.properties.pixelSizeX` and `<raster>.properties.pixelSizeY` functions to find the Pixel size of the raster.

# In[30]:


## Cellsize
ras2_cellsize_x = inf_ras2.layers[0].properties.pixelSizeX
ras2_cellsize_y = inf_ras2.layers[0].properties.pixelSizeY
print(ras2_cellsize_x, ras2_cellsize_y)


# To calculate the area of land under flood water, we will use the `<raster>.attribute_table()` function to find the count of pixels per flood water class.

# In[31]:


inf_ras2.layers[0].attribute_table()


# This study requires the calculation of the area of land under flood water in terms of square km. The raster uses the projected coordinate system (3857), which has pixels in meters.

# In[32]:


## area in square kilometers
area_ras2_flood_water = (5219135*(ras2_cellsize_x*ras2_cellsize_y)/1000000)
area_ras2_flood_water


# ### Infrastructural inundation assessment<a class="anchor" id="18"></a>

# The inferenced raster will be used to assess the infrasruture inundated in flood water.

# In[33]:


The inferenced raster will be used to assess the infrasruture inundated in flood water.flood_raster = Raster("https://tiledimageservices6.arcgis.com/SMX5BErCXLM7eDtY/arcgis/rest/services/st_louis_flood_water/ImageServer",
                      gis=gis2,
                      engine="image_server")


# The LULC raster for St. Louis is generated using the [Land Cover Classification (Sentinel-2)](https://deldev.maps.arcgis.com/home/item.html?id=afd124844ba84da69c2c533d4af10a58) pretrained model to assess the inundated areas per the LULC class.

# In[34]:


lulc_raster = Raster("https://tiledimageservices6.arcgis.com/SMX5BErCXLM7eDtY/arcgis/rest/services/lulc_st_louis/ImageServer",
                        gis=gis2,
                        engine="image_server")


# The flood raster will be converted to polygons using [convert_raster_to_feature](https://developers.arcgis.com/rest/services-reference/enterprise/convert-raster-to-feature.htm). We then use the import_data function to create a new feature layer containing the flood water polygons.

# In[35]:


flood_poly = convert_raster_to_feature(flood_raster, 
                                    field='Value', 
                                    output_type='Polygon', 
                                    simplify=False, 
                                    output_name='flood_st_louis_poly'+str(datetime.now().microsecond), 
                                    gis=gis)

## Create dataframe from feature layer and get water polygons
dfm1 = flood_poly.layers[0].query('gridcode=2').sdf 

## Convert dataframe to feature layer
flood_poly = gis.content.import_data(dfm1, title='flood_water_poly'+str(datetime.now().microsecond))


# Next, the LULC raster will be converted to polygons using [convert_raster_to_feature](https://developers.arcgis.com/rest/services-reference/enterprise/convert-raster-to-feature.htm). After getting the LULC polygons, we remove NODATA and Water polygons and use the `import_data` function to create a new feature layer containing the correct LULC polygons.

# In[36]:


lulc_poly = convert_raster_to_feature(lulc_raster, 
                                    field='Value', 
                                    output_type='Polygon', 
                                    simplify=False, 
                                    output_name='lulc_st_louis_poly'+str(datetime.now().microsecond), 
                                    gis=gis)

## Create dataframe from feature layer and get water polygons
dfm2 = lulc_poly.layers[0].query('gridcode > 0 And gridcode < 5').sdf 

## Convert dataframe to feature layer
lulc_polygon = gis.content.import_data(dfm2, title='lulc_poly'+str(datetime.now().microsecond))


# To get the LULC classes for the flood inundated areas, we will use the [overlay_layers](https://developers.arcgis.com/rest/analysis/api-reference/overlay-layers.htm) function.

# In[37]:


inundated_lulc = overlay_layers(lulc_polygon.layers[0], 
                                flood_poly.layers[0], 
                                output_name='inundated_lulc'+str(datetime.now().microsecond),
                                gis=gis)


# After getting the LULC classes for the flood inundated areas, we will dissolve the polygons on the basis of `gridcode`. The output feature layer will have the combined area of each class in `square miles` units.

# In[38]:


lulc_dissolve = dissolve_boundaries(inundated_lulc, 
                               dissolve_fields=['gridcode'], 
                               output_name='dissolved_lulc'+str(datetime.now().microsecond),
                               gis=gis,
                               multi_part_features=True)


# The resulting feature layer has a column for the area per class, but the corresponding LULC class name is missing. We will add the class names to the dataframe using the code below.

# In[39]:


dfm4 = lulc_dissolve.layers[0].query().sdf
lulc_classes = ['Artificial surfaces', 'Agricultural areas', 'Forest and semi natural areas', 'Wetlands']
dfm4['LULC_Classes'] = lulc_classes
dfm5 = dfm4[['gridcode','AnalysisArea', 'LULC_Classes']].copy()
dfm5.rename(columns={'AnalysisArea': 'Area_in_square_miles'}, inplace=True)
dfm5


# ## Conclusion<a class="anchor" id="19"></a>

# In this notebook, we have demonstrated how to use a UnetClassifier model with ArcGIS API for Python to extract flood water and permanent waterbodies. In Part 1, we covered how Sentinel-1 SAR data can be used for flood inundation mapping and monitoring. This process involved steps to prepare the input data, train a pixel-based classification model, visualize the results, generate accuracy metrics, and inferencing results on a test raster/area. Finally, in Part 2, we demonstrated the flood water inundated area in square kilometers and an infrastructural inundation assessment.


# ====================
# forecasting_air_temperature_in_california_using_rescnn_model.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Forecasting Air Temperature in California using ResCNN model

# * [Introduction](#1) 
# * [Importing libraries](#2)
# * [Connecting to your GIS](#3)
# * [Accessing & visualizing the dataset](#4) 
# * [Time series data preprocessing](#5) 
#     * [Converting into time series format ](#6)
#     * [Data types of time series variables ](#7)
#     * [Checking autocorrelation of time dependent variables ](#8)
#     * [Creating dataset for prediction ](#9)
#     * [Train - Test split of time series dataset ](#10)       
# * [Time series model building](#11)
#     * [Data preprocessing ](#12)  
#     * [Model initialization ](#13)
#     * [Learning rate search ](#14)
#     * [Model training ](#15) 
# * [Air temperature forecast & validation](#16)   
#     * [Forecasting Using the trained TimeSeriesModel ](#17)
#     * [Estimate model metrics for validation ](#18)
#     * [Result visualization ](#19)
# * [Conclusion](#20)
# * [Summary of methods used](#21)
# * [References](#22)
# * [Data resources](#23)

# ## Introduction <a class="anchor" id="1"></a>

# A rise in air temperature is directly correlated with Global warming and change in climatic conditions and is one of the main factors in predicting other meteorological variables, like streamflow, evapotranspiration, and solar radiation. As such, accurate forecasting of this variable is vital in pursuing the mitigation of environmental and economic destruction. Including the dependency of air temperature in other variables, like wind speed or precipitation, helps in deriving more precise predictions. In this study, the deep learning TimeSeriesModel from arcgis.learn is used to predict monthly air temperature for two years at a ground station at the Fresno Yosemite International Airport in California, USA. The dataset ranges from 1948-2015. Data from January 2014 to November 2015 is used to validate the quality of the forecast.
# 
# Univariate time series modeling is one of the more popular applications of time series analysis. This study includes multivariate time series analysis, which is a bit more convoluted, as the dataset contains more than one time-dependent variable. The TimeSeriesModel from arcgis.learn includes backbones, such as InceptionTime, ResCNN, ResNet and FCN, which do not need fine-tuning of multiple hyperparameters before fitting the model. Here is the schematic flow chart of the methodology:

# 

# ## Importing libraries <a class="anchor" id="2"></a>

# In[1]:


get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib.pyplot as plt

import numpy as np
import pandas as pd
from pandas.plotting import autocorrelation_plot as aplot

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
import sklearn.metrics as metrics

from arcgis.gis import GIS
from arcgis.learn import TimeSeriesModel, prepare_tabulardata
from arcgis.features import FeatureLayer, FeatureLayerCollection


# ## Connecting to your GIS <a class="anchor" id="3"></a>

# In[2]:


gis = GIS('home')


# ## Accessing & visualizing the dataset <a class="anchor" id="4"></a>

# The data used in this sample study is a multivariate monthly time series dataset recorded at a ground station in the Fresno Yosemite International Airport, California, USA. It ranges from January 1948 to November 2015.

# In[3]:


# Location of the ground station
location = gis.map(location="Fresno Yosemite International California", zoomlevel=12)
location


# 

# In[4]:


# Access the data table
data_table = gis.content.get("8c58e808aabd40408f7bc4eeac64fffb")
data_table


# In[5]:


# Visualize as pandas dataframe
climate_data = data_table.tables[0]
climate_df = climate_data.query().sdf
climate_df.head()


# The dataframe above contains columns for station ID (STATION), station name (NAME), Date (DATE), Wind speed (AWND), precipitation (PRCP), possible sunshine (PSUN), snow cover (SNOW), average temperature (TAVG), maximum temperature (TMAX), minimum temperature (TMIN), total sunshine (TSUN), and peak wind gust speed (WSFG).

# In[6]:


climate_df.shape


# Next, the dataset is prepared by dropping the variables for station, possible sunshine, snow cover, maximum temperature, minimum temperature, total sunshine, and peak wind gust speed. Then, the dataset is narrowed to the data from 1987 on, to avoid missing values.

# In[7]:


climate_df = climate_df.drop(
    ["ObjectId", "STATION", "NAME", "PSUN", "SNOW", "TSUN",'TMAX', 'TMIN', "WSFG"], axis=1
)


# In[8]:


climate_df.columns


# In[9]:


# Selecting dataset from year 1987 to get continous data without NAN values
selected_df = climate_df[climate_df.DATE > "1987"]
selected_df.head()


# Here, **TAVG** is our variable to be predicted, with **PRCP** and **AWND** being the predictors used, showing their influence on temperature.

# In[10]:


selected_df.shape


# ## Time series data preprocessing<a class="anchor" id="5"></a>   
# The preprocessing of the data for multivariate time series modeling involves the following steps:

# ### Converting into time series format<a class="anchor" id="6"></a>
# The dataset is now transformed into a time series data format by creating a new index that will be used by the model for processing the sequential data.

# In[11]:


final_df = selected_df.reset_index()
final_df = final_df.drop("index", axis=1)
final_df.head()


# ###  Data types of time series variables<a class="anchor" id="7"></a> 
# Here we check the data types of the variables.

# In[12]:


final_df.info()


# The time-dependent variables should of the type float. If a time-dependent variable is not of a float data type, then it needs to be changed to float. Here, Windspeed (AWND) is converted from object dtype to float64, as shown in the next cell.

# In[13]:


final_df["AWND"] = final_df["AWND"].astype("float64")
final_df.head()


# ### Checking autocorrelation of time dependent variables<a class="anchor" id="8"></a> 
# The next step will determine if the time series sequence is autocorrelated. To ensure that our time series data can be modeled well, the strength of correlation of the variable with its past data must be estimated.

# In[14]:


variables = ["AWND", "PRCP", "TAVG"]
for variable in variables:
    plt.figure(figsize=(20, 2))
    autocorr = aplot(final_df[variable], color="blue")
    plt.title(variable)


# The plots are showing a significant correlation of the data with its immediate time-lagged terms, and that it gradually decreases over time as the lag increases.

# ### Creating dataset for prediction<a class="anchor" id="9"></a> 
# Here, in the original dataset, the variable predict column of Average Temperature (TAVG) is populated with NaNs for the forecasting period of 2014-2015. This format is required for the `model.predict()` function in time series analysis, which will fill up the NaN values with forecasted temperatures.

# In[15]:


predict_df = final_df.copy()
predict_df.loc[predict_df["DATE"] > "2013-12-01", "TAVG"] = None
predict_df.tail()


# ### Train - Test split of time series dataset<a class="anchor" id="10"></a> 
# Out of these 27 years(1987-2015), 25 years of data is used for training the model, with the remaining 23 months (2014-2015) being used for forecasting and validation. As we are splitting timeseries data, we set shuffle=False to keep the sequence intact and we set a test size of 23 months for validation.

# In[16]:


test_size = 23
train, test = train_test_split(final_df, test_size=test_size, shuffle=False)


# In[17]:


train


# ## Time series model building<a class="anchor" id="11"></a> 
# After the train and test sets are created, the training set is ready for modeling.

# ### Data preprocessing <a class="anchor" id="12"></a>

# In this example, the dataset contains 'AWND' (Windspeed), 'PRCP' (Precipitation), and 'TAVG' (Average Air temperature) as time-dependent variables leading to a multivariate time series analysis at a monthly time scale. These variables are used to forecast the next 23 months of air temperature for the months after the last date in the training data, or, in other words, these multiple explanatory variables are used to predict the future values of the dependent air temperature variable.
# 
# Once the variables are identified, the preprocessing of the data is performed by the `prepare_tabulardata` method from the `arcgis.learn` module in the ArcGIS API for Python. This function takes either a non-spatial data frame, a feature layer, or a spatial data frame containing the dataset as input and returns a TabularDataObject that can be fed into the model. By default, `prepare_tabulardata` scales/normalizes the numerical columns in a dataset using StandardScaler.
# The primary input parameters required for the tool are:
# 
# - <span style='background :lightgrey' >input_features</span> : Takes the spatially enabled dataframe as a feature layer in this model
# - <span style='background :lightgrey' >variable_predict</span> : The field name of the forecasting variable
# - <span style='background :lightgrey' >explanatory_variables</span> : A list of the field names that are used as time-dependent variables in multivariate time series
# - <span style='background :lightgrey' >index_field</span> : The field name containing the timestamp that will be used as the index field for the data and to visualize values on the x-axis in the time series

# In[18]:


data = prepare_tabulardata(
    train,
    variable_predict="TAVG",
    explanatory_variables=["AWND", "PRCP"],
    index_field="DATE",
    seed=42,
)


# In[19]:


# Visualize the entire timeseries data
data.show_batch(graph=True)


# In[20]:


# Here sequence length is used as 12 which also indicates the seasonality of the data
seq_len = 12


# Next, we visualize the timeseries in batches. Here, we will pass the sequence length as the batch length.

# In[21]:


data.show_batch(rows=4, seq_len=seq_len)


# ### Model initialization <a class="anchor" id="13"></a>
# 
# This is an important step for fitting a time series model. Here, along with the input dataset, the backbone for training the model and the sequence length are passed as parameters. Out of these three, the sequence length has to be selected carefully. The sequence length is usually the cycle of the data, which in this case is 12, as it is monthly data and the pattern repeats after 12 months. In model initialization, the data and the backbone are selected from the available set of InceptionTime, ResCNN, Resnet, and FCN.

# In[22]:


tsmodel = TimeSeriesModel(data, seq_len=seq_len, model_arch="ResCNN")


# ### Learning rate search<a class="anchor" id="14"></a>
# Here, we find the optimal learning rate for training the model.

# In[23]:


lr_rate = tsmodel.lr_find()


# ### Model training <a class="anchor" id="15"></a>
# 
# The model is now ready for training. To train the model, the `model.fit` method is used and is provided with the number of epochs for training and the learning rate suggested above as parameters:

# In[24]:


tsmodel.fit(100, lr=lr_rate)


# To check the quality of the trained model and whether the model needs more training, we generate a train vs validation loss plot below:

# In[25]:


tsmodel.plot_losses()


# Next, the predicted values of the model and the actual values are printed for the training dataset.

# In[26]:


tsmodel.show_results(rows=5)


# ## Air temperature forecast & validation <a class="anchor" id="16"></a>

# ### Forecasting using the trained TimeSeriesModel <a class="anchor" id="17"></a>
# During forecasting, the model uses the dataset prepared above with NaN values as input, with the `prediction_type` set as `dataframe`.

# In[27]:


# Checking the input dataset
predict_df.tail(23)


# In[28]:


df_forecasted = tsmodel.predict(predict_df, prediction_type="dataframe")


# In[29]:


# Final forecasted result returned by the model
df_forecasted


# Next, we format the results into actual vs predicted columns.

# In[30]:


result_df = pd.DataFrame()
result_df["DATE"] = test["DATE"]
result_df["Airtemp_actual"] = test["TAVG"]
result_df["Airtemp_predicted"] = df_forecasted["TAVG_results"][-23:]
result_df = result_df.set_index(result_df.columns[0])
result_df


# ### Estimate model metrics for validation <a class="anchor" id="18"></a>
# The accuracy of the forecasted values is measured by comparing the forecasted values against the actual values for the 23 months chosen for testing.

# In[31]:


r2 = r2_score(result_df["Airtemp_actual"], result_df["Airtemp_predicted"])
mse = metrics.mean_squared_error(
    result_df["Airtemp_actual"], result_df["Airtemp_predicted"]
)
rmse = metrics.mean_absolute_error(
    result_df["Airtemp_actual"], result_df["Airtemp_predicted"]
)
print(
    "RMSE:     ",
    round(np.sqrt(mse), 4),
    "\n" "MAE:      ",
    round(rmse, 4),
    "\n" "R-Square: ",
    round(r2, 2),
)


# A considerably high r-square value of .91 indicates a high similarity between the forecasted values and the actual values. Furthermore, the RMSE error of 3.661 is quite low, indicating a good fit by the model.

# ## Result visualization<a class="anchor" id="19"></a>
# Finally, the actual and forecasted values are plotted to visualize their distribution over the validation period, with the orange line representing the forecasted values and the blue line representing the actual values.

# In[32]:


plt.figure(figsize=(20, 5))
plt.plot(result_df)
plt.ylabel("Air Temperature")
plt.legend(result_df.columns.values, loc="upper right")
plt.title("Forecasted Air Temperature")
plt.show()


# ## Conclusion<a class="anchor" id="20"></a>

# The study conducted a multivariate time series analysis using the Deep learning TimeSeriesModel from the arcgis.learn library and forecasted the monthly Air temperature for a station in California. The model was trained with 25 years of data (1987-2013) that was used to forecast a period of 2 years (2014-2015) with high accuracy. The independent variables were wind speed and precipitation. The methodology included preparing a times series dataset using the prepare_tabulardata() method, followed by modeling, predicting, and validating the test dataset. Usually, time series modeling requires fine-tuning several hyperparameters for properly fitting the data, most of which has been internalized in this Model, leaving the user responsible for configuring only a few significant parameters, like the sequence length.

# ## Summary of methods used <a class="anchor" id="21"></a>

# | Method | Description | Examples |
# | -| - |-|
# | prepare_tabulardata| prepare data including imputation, scaling and train-test split  |prepare data ready for fitting a  Timeseries Model 
# | model.lr_find()| finds an optimal learning rate  | finalize a good learning rate for training the Timeseries model
# | TimeSeriesModel() | Model Initialization by selecting the TimeSeriesModel algorithm to be used for fitting  | Selected Timeseries algorithm from Fastai time series regression can be used
# | model.fit() | trains a model with epochs & learning rate as input  | training the Timeseries model with suitable input 
# | model.predict() | predicts on a test set | forecast values using the trained models on the test input

# ## References<a class="anchor" id="22"></a>
# - Jenny Cifuentes et.al., 2020. "Air Temperature Forecasting Using Machine Learning Techniques: A Review" 
# https://doi.org/10.3390/en13164215
# 
# - Xuejie, G. et.al., 2001. "Climate change due to greenhouse effects in China as simulated by a regional climate model" https://doi.org/10.1007/s00376-001-0036-y
# 
# - "gsom-gsoy_documentation" https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/gsom-gsoy_documentation.pdf
# 
# - "Prediction task with Multivariate Time Series and VAR model" https://towardsdatascience.com/prediction-task-with-multivariate-timeseries-and-var-model-47003f629f9

# ## Data resources <a class="anchor" id="23"></a>

# | Dataset | Source | Link |
# | -| - |-|
# |Global Summary of the Month|NOAA Climate Data Online |https://www.ncdc.noaa.gov/cdo-web/search|


# ====================
# forecasting_monthly_rainfall_in_california_using_deeplearning_timeseries_model_from_arcgis_learn.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Forecasting monthly rainfall in California using Deep Learning Time Series techniques

# ## Table of Contents <a class="anchor" id="0"></a>
# * [Introduction](#1) 
# * [Imports](#2)
# * [Connecting to ArcGIS](#3)
# * [Accessing & Visualizing the datasets](#4) 
# * [Timeseries Data Preparation](#5) 
#     * [Sequencing Timeseries data](#7)
#     * [Datatypes of timeseries variable](#8)
#     * [Checking autocorrelation of timeseries variable](#10)   
#     * [Train - Test split of timeseries dataset](#6)       
# * [Timeseries Model Building](#9)
#     * [Data Preprocessing](#11)  
#     * [Model Initialization ](#12)
#     * [Learning Rate Search ](#13)
#     * [Model Training ](#14) 
# * [Rainfall Forecast & Validation](#15)   
#     * [Forecasting Using the trained Timeseries Model](#26)
#     * [Estimate model metrics for validation](#27)
#     * [Result Visualization](#28)
# * [Conclusion](#23)
# * [Summary of methods used](#24)
# * [Data resources](#25)

# ## Introduction <a class="anchor" id="1"></a>

# Forest fires of historical proportions are ravaging various parts of California, started by a long and continuous period of drought. To help in dealing with this growing environmental emergency, utilizing prior knowledge of rainfall is critical. In this sample study, the deepelarning timeseries model from ArcGIS learn will be used to predict monthly rainfall for a whole year at a certain location in the Sierra Nevada foothills, around 30 miles north east of Fresno California, using the location's historic rainfall data. Data from January to December of 2019 will be used to validate the quality of the forecast.
# 
# Weather forecasting is a popular field for the application of timeseries modelling. There are various approaches used for solving timeseries problems, including classical statistical methods, such as ARIMA group of models, machine learning models, and deep learning models. The current implementation of the ArcGIS learn timeseries model uses state of the art convolutional neural networks backbones especially curated for timeseries datasets. These include InceptionTime, ResCNN, Resnet, and FCN. What makes timeseries modeling unique is that, in the classical methodology of ARIMA, multiple hyperparameters require fine tuning before fitting the model, while with the current deep learning technique, most of the parameters are learned by the model itself from the data.

# ## Imports <a class="anchor" id="2"></a>

# In[2]:


get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib.pyplot as plt

import numpy as np
import pandas as pd
import math
from datetime import datetime as dt
from IPython.display import Image, HTML

from sklearn.model_selection import train_test_split

from arcgis.gis import GIS
from arcgis.learn import TimeSeriesModel, prepare_tabulardata
from arcgis.features import FeatureLayer, FeatureLayerCollection


# ## Connecting to ArcGIS <a class="anchor" id="3"></a>

# In[3]:


gis = GIS("home")


# ## Accessing & Visualizing datasets  <a class="anchor" id="4"></a> 

# The dataset used in this sample study is a univariate timeseries dataset of total monthly rainfall from a fixed location of 1 sqkm area in the state of California, ranging from the January 1980 to December 2019.

# The following cell downloads the California rainfall dataset:

# In[4]:


url = 'https://services7.arcgis.com/JEwYeAy2cc8qOe3o/arcgis/rest/services/cali_precipitation/FeatureServer/0'
table = FeatureLayer(url)


# Next, we preprocess and sort the downloaded dataset by datetime sequence.

# In[8]:


cali_rainfall_df1 = table.query().sdf
cali_rainfall_df1 = cali_rainfall_df1.drop("ObjectId", axis=1)
cali_rainfall_df1_sorted = cali_rainfall_df1.sort_values(by='date')
cali_rainfall_df1_sorted.head()


# In[9]:


cali_rainfall_df1_sorted.shape


# In[10]:


cali_rainfall_df1_sorted.info()


# ## Timeseries Data Preparation<a class="anchor" id="5"></a>   
# Preparing the data for timeseries modeling consists of the following three steps:

# ###  Sequencing Timeseries data<a class="anchor" id="7"></a>

# The first step consist of establishing the sequence of the timeseries data, which is done by creating a new index that is used by the model for processing the sequential data. 

# In[11]:


# The first step consist of reindexing the timeseries data into a sequential series  
cali_rainfall_reindexed = cali_rainfall_df1_sorted.reset_index()
cali_rainfall_reindexed = cali_rainfall_reindexed.drop("index", axis=1)
cali_rainfall_reindexed.head()


# ###  Datatypes of timeseries variable<a class="anchor" id="8"></a> 
# The next step validates the data types of the variables.

# In[12]:


# check the data types of the variables
cali_rainfall_reindexed.info()


# Here, the timeseries variable is a float, which should be the expected data type. If the variable is not of a float data type, then it needs to be changed to a float data type, as demonstrated by the commented out line in the next cell.

# In[ ]:


#cali_rainfall_reindexed['prcp_mm_'].astype('float64')


# ###  Checking autocorrelation of timeseries variable<a class="anchor" id="10"></a>
# The most important step in this process is to determine if the timeseries sequence is autocorrelated. To ensure that our timeseries data can be modeled well, the strength of correlation of the variable with its past data must be estimated.

# In[13]:


from pandas.plotting import autocorrelation_plot


# In[14]:


plt.figure(figsize=(20,5))
autocorrelation_plot(cali_rainfall_reindexed["prcp_mm_"])
plt.show()


# The plot above shows us that there is significant correlation of the data with its immediate time lagged terms, and that it gradually decreases over time as the lag increases. 

# ### Train - Test split of timeseries dataset<a class="anchor" id="6"></a>

# The dataset above has 480 data samples each representing monthly ranifall of california for 40 years(1980-2019). Out of this 39 years(1980-2018) of data will be used for training the model and the rest 1 year or a total of 12 months of data are held out for validation. Accordingly the dataset is now split into train and test in the following.   

# In[15]:


# Splitting timeseries data retaining the original sequence by keeping shuffle=False, and test size of 12 months for validation 
test_size = 12
train, test = train_test_split(cali_rainfall_reindexed, test_size = test_size, shuffle=False)


# In[16]:


train 


# ## Model Building <a class="anchor" id="9"></a>

# Once the dataset is divided into the training and test dataset, the training data is ready to be used for modeling.

# ### Data Preprocessing <a class="anchor" id="11"></a>

# In this example, the data used is a univariate timeseries of total monthly rainfall in millimeters. This single variable will be used to forecast the 12 months of rainfall for the months subsequent to the last date in the training data, or put simply, a single variable will be used to predict the future values of that same variable. In the case of a multivariate timeseries model, there would be a list of multiple explanatory variables.
# 
# Once the variables are identified, the preprocessing of the data is performed by the `prepare_tabulardata` method from the `arcgis.learn` module in the ArcGIS API for Python. This function will take either a non spatial dataframe, a feature layer, or a spatial dataframe containing the dataset as input, and will return a TabularDataObject that can be fed into the model. 
# 
# The primary input parameters required for the tool are:
# 
# - <span style='background :lightgrey' >input_features</span> : non spatial dataframe, feature layer, or spatial dataframe containing the primary dataset and the explanatory variables, if there are any
# - <span style='background :lightgrey' >variable_predict</span> : field name containing the y-variable to be forecasted from the input feature layer/dataframe
# - <span style='background :lightgrey' >explanatory_variables</span> : list of the field names as 2-sized tuples containing the explanatory variables as mentioned above. Since there are none in this example, it is not required here 
# - <span style='background :lightgrey' >index_field</span> : field name containing the timestamp

# At this point, preprocessors could be used for scaling the data using a scaler as follows, depending on the data distribution.

# In[17]:


#from sklearn.preprocessing import MinMaxScaler


# In[18]:


#preprocessors = [('prcp_mm_', MinMaxScaler())]
#data = prepare_tabulardata(train, variable_predict='prcp_mm_', index_field='date', preprocessors=preprocessors)


# In this example, preprocessors are not used, as the data is normalized by default.  

# In[19]:


data = prepare_tabulardata(train, variable_predict='prcp_mm_', index_field='date', seed=42)


# In[20]:


# Visualize the entire timeseries data
data.show_batch(graph=True)


# In[21]:


# Here sequence length is used as 12 which also indicates the seasonality of the data
seq_len=12


# In[22]:


# visualize the timeseries in batches, here the sequence length is mentioned which would be treated as the batch length
data.show_batch(rows=4,seq_len=seq_len)


# ### Model Initialization <a class="anchor" id="12"></a>
# 
# This is the most significant step for fitting a timeseries model. Here, along with the data, the backbone for training the model and the sequence length are passed as parameters. Out of these three, the sequence length has to be selected carefully, as it can make or break the model. The sequence length is usually the cycle of the data, which in this case is 12, as it is monthly data and the pattern repeats after 12 months.

# In[23]:


# In model initialization, the data and the backbone is selected from the available set of InceptionTime, ResCNN, Resnet, FCN
ts_model = TimeSeriesModel(data, seq_len=seq_len, model_arch='InceptionTime')


# ### Learning Rate Search<a class="anchor" id="13"></a>

# In[24]:


# Finding the learning rate for training the model
l_rate = ts_model.lr_find()


# ### Model Training <a class="anchor" id="14"></a>
# 
# Finally, the model is now ready for training. To train the model, the `model.fit` method is called and is provided the number of epochs for training and the estimated learning rate suggested by `lr_find` in the previous step:

# In[25]:


ts_model.fit(100, lr=l_rate)


# In[26]:


# the train vs valid losses is plotted to check quality of the trained model, and whether the model needs more training 
ts_model.plot_losses()


# In[27]:


# the predicted values by the trained model is printed for the test set
ts_model.show_results(rows=5)


# The figures above display the training and the validation of the prediction attained by the model while training.

# ## Rainfall Forecast & Validation <a class="anchor" id="15"></a>

# ### Forecasting Using the trained Timeseries Model <a class="anchor" id="26"></a>
# During forecasting, the model will use the same training dataset as input and will use the last sequence length number of terms from that dataset's tail to predict the rainfall for the number of months specified by the user.   

# In[29]:


from datetime import datetime  


# In[30]:


# checking the training dataset
train


# Forecasting requires the format of the date column to be in datetime. If the date column is not in the datetime format, it can be changed to datetime by using the `pd.to_datetime()` method. 

# In[31]:


# checking if the datatype of the 'date' column is in datetime format
train.info()


# In this example, the date column is already in the required datetime format.

# In[32]:


train.tail(5)


# Finally the predict function is used to forecast for a period of the 12 months subsequent to the last date in the training dataset. As such, this will be forecasting rainfall for the 12 months of 2019, starting from January of 2019.

# In[33]:


# Here the forecast is returned as a dataframe, since it is non spatial data, mentioned in the 'prediction_type'     
sdf_forecasted = ts_model.predict(train, prediction_type='dataframe', number_of_predictions=test_size)


# In[34]:


# final forecasted result returned by the model
sdf_forecasted


# In[35]:


# Formating the result into actual vs the predicted columns
sdf_forecasted = sdf_forecasted.tail(test_size)
sdf_forecasted = sdf_forecasted[['date','prcp_mm__results']]
sdf_forecasted['actual'] = test[test.columns[-1]].values
sdf_forecasted = sdf_forecasted.set_index(sdf_forecasted.columns[0])
sdf_forecasted.head()


# ### Estimate model metrics for validation <a class="anchor" id="27"></a>
# The accuracy of the forecasted values is measured by comparing the forecasted values against the actual values of the 12 months.

# In[36]:


from sklearn.metrics import r2_score
import sklearn.metrics as metrics


# In[37]:


r2_test = r2_score(sdf_forecasted['actual'],sdf_forecasted['prcp_mm__results'])
print('R-Square: ', round(r2_test, 2))


# A considerably high r-squared value indicates a high similarity between the forecasted and the actual sales values.

# In[38]:


mse_RF_train = metrics.mean_squared_error(sdf_forecasted['actual'], sdf_forecasted['prcp_mm__results'])
print('RMSE: ', round(np.sqrt(mse_RF_train), 4))

mean_absolute_error_RF_train = metrics.mean_absolute_error(sdf_forecasted['actual'], sdf_forecasted['prcp_mm__results'])
print('MAE: ', round(mean_absolute_error_RF_train, 4))


# The error terms of RMSE and MAE in the forecasting are 32.28mm and 25.55mm respectively, which are quite low.

# ## Result Visualization<a class="anchor" id="28"></a>
# 
# Finally, the actual and forecasted values are plotted to visualize their distribution over the 12 months period, with the blue lines indicating forecasted values and orange line showing the actual values.

# In[39]:


plt.figure(figsize=(20,5))
plt.plot(sdf_forecasted)
plt.ylabel('prcp_mm__results')
plt.legend(sdf_forecasted.columns.values,loc='upper right')
plt.title( 'Rainfall Forecast')
plt.show()


# ## Conclusion<a class="anchor" id="23"></a>

# The newly implemented deeplearning timeseries model from the arcgis.learn library was used to forecast monthly rainfall for a location of 1 sqkm in California, for the period of January to December 2019, which it was able to model with a high accuracy. The notebook elaborates on the methodology of applying the model for forecasting time series data. The process includes first preparing a timeseries dataset using the prepare_tabulardata() method, followed by modeling and fitting the dataset. Usually, timeseries modelling requires fine tuning several hyperparameters for properly fitting the data, most of which has been internalized in this current implementation, leaving the user responsible for configuring only a few significant parameters, like the sequence length.   

# ### Summary of methods used <a class="anchor" id="24"></a>

# | Method | Description | Examples |
# | -| - |-|
# | prepare_tabulardata| prepare data including imputation, normalization and train-test split  |prepare data ready for fitting a  Timeseries Model 
# | model.lr_find()| find an optimal learning rate  | finalize a good learning rate for training the Timeseries model
# | TimeSeriesModel() | Model Initialization by selecting the Timeseries Deeplearning algorithm to be used for fitting  | Selected Timsereis algorithm from Fastai timeseries regression can be used
# | model.fit() | train a model with epochs & learning rate as input  | training the Timeseries model with sutiable input 
# | model.score() | find the model metric of R-squared of the trained model  | returns R-squared value after training the Timeseries Model 
# | model.predict() | predict on a test set | forecast values using the trained models on test input 

# ### Data resources <a class="anchor" id="25"></a>

# | Dataset | Source | Link |
# | -| - |-|
# |California Daily weather data|MODIS - Daily Surface Weather Data on a 1-km Grid for North America, Version 3|https://daac.ornl.gov/DAYMET/guides/Daymet_V3_CFMosaics.html|


# ====================
# forecasting_pm2.5_using_big_data_analysis.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Forecasting PM2.5 using big data analysis

# ## Table of Contents
# * [Prerequisites](#Prerequisites)
# * [Introduction](#Introduction)
# * [Necessary imports](#Necessary-imports)
# * [Connect to your ArcGIS Enterprise organization](#Connect-to-your-ArcGIS-Enterprise-organization)
# * [Ensure your GIS supports GeoAnalytics](#Ensure-your-GIS-supports-GeoAnalytics)
# * [Prepare data](#Prepare_data)
#     * [Create a big data file share](#Create-a-big-data-file-share)
# * [Get data for analysis](#Get-data-for-analysis)
#     * [Search for big data file shares](#Search-for-big-data-file-shares)
#     * [Search for feature layers](#Search-for-feature-layers)
# * [Uncover patterns in data](#Uncover-patterns-in-data)
#     * [Describe data](#Describe-data)
#     * [Commonly used methods of measurement](#Commonly-used-methods-of-measurement)
#     * [Average PM 2.5 value by county](#Average-PM-2.5-value-by-county)
# * [Prepare time series data](#Prepare-time-series-data)   
# * [Predict PM2.5 using Facebook's Prophet model](#Predict-PM2.5-using-Facebook's-Prophet-model)
# * [Visualize result on Dashboard](#Visualize-result-on-Dashboard)
# * [Conclusion](#Conclusion)
# * [References](#References)

# ## Prerequisites

# - The tools available in the ArcGIS API for Python geoanalytics module require an ArcGIS Enterprise licensed and configured with the linux based [ArcGIS GeoAnalytics server](http://enterprise.arcgis.com/en/server/latest/get-started/windows/configure-the-portal-with-arcgis-geoanalytics-server.htm).
# - When ArcGIS GeoAnalytics Server is installed on Linux, additional configuration steps are required before using the RunPythonScript operation. Install and configure Python 3.6 for Linux on each machine in your GeoAnalytics Server site, ensuring that Python is installed into the same directory on each machine. For this analysis, you also need to install FB Prophet library in the same environment. Then, update the [ArcGIS Server Properties](https://developers.arcgis.com/rest/enterprise-administration/server/serverproperties.htm) on your GeoAnalytics Server site with the pysparkPython property. The value of this property should be the path to the Python executable on your GeoAnalytics Server machines, for example, {"pysparkPython":"path/to/environment"}.
# 

# ## Introduction

# While the `arcgis.geoanalytics` module offers powerful spatial analysis tools, the pyspark package includes dozens of non-spatial distributed tools for classification, prediction, clustering, and more.
# Using the power of distributed compute, we can analyze large datasets much faster than other non-distributed systems.
# 
# This notebook demonstrates the capability of spark-powered geoanalytics server to forecast hourly [PM2.5](https://www.epa.gov/pm-pollution/particulate-matter-pm-basics) given the historic time series data for more than one time-dependent variable. The most common factors in the weather environment used in this analysis are PM2.5, PM10, wind speed, wind direction, and relative humidity. The levels of these pollutants are measured by the US Environmental Protection Agency (EPA), which controls overall air quality. We have used the [dataset](https://aqs.epa.gov/aqsweb/airdata/download_files.html#Raw) provided by EPA. 
# With Spark we will learn how to customize and extend our analysis capabilities by:
# 
# - Querying and summarizing your data using SQL
# - Turning analysis workflows into pipelines of GeoAnalytics tools
# - Modeling data with included machine learning libraries
# 
# The forecasted result will then be displayed on a dashboard using the recently introduced `arcgis.dashboard` module in ArcGIS API for Python. 
# 

# Note:
# - The ability to perform big data analysis is only available on ArcGIS Enterprise licensed with a GeoAnalytics server and not yet available on ArcGIS Online.

# ## Necessary imports

# In[2]:


from datetime import datetime as dt
import pandas as pd

import arcgis
from arcgis.gis import GIS
from arcgis.geoanalytics.manage_data import run_python_script


# ## Connect to your ArcGIS Enterprise organization

# In[3]:


gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# ## Ensure your GIS supports GeoAnalytics
# After connecting to the Enterprise organization, we need to ensure an ArcGIS Enterprise GIS is set up with a licensed GeoAnalytics server. To do so, we will call the `is_supported()` method.

# In[4]:


arcgis.geoanalytics.is_supported()


# ## Prepare data
# To register a file share or an HDFS, we need to format datasets as subfolders within a single parent folder and register the parent folder. This parent folder becomes a datastore, and each subfolder becomes a dataset. Our folder hierarchy would look like below:

# 

# Learn more about preparing your big data file share datasets [here](https://enterprise.arcgis.com/en/server/latest/get-started/windows/what-is-a-big-data-file-share.htm).

# The [`get_datastores()`](https://developers.arcgis.com/python/api-reference/arcgis.geoanalytics.toc.html?highlight=get_datastores#arcgis.geoanalytics.get_datastores) method of the geoanalytics module returns a [`DatastoreManager`](https://developers.arcgis.com/python/api-reference/arcgis.gis.toc.html#datastoremanager) object that lets you search for and manage the big data file share items as Python API [`Datastore`](https://developers.arcgis.com/python/api-reference/arcgis.gis.toc.html#datastore) objects on your GeoAnalytics server.

# In[5]:


bigdata_datastore_manager = arcgis.geoanalytics.get_datastores()
bigdata_datastore_manager


# We will register air quality data as a big data file share using the [`add_bigdata()`](https://developers.arcgis.com/python/api-reference/arcgis.gis.toc.html#arcgis.gis.DatastoreManager.add_bigdata) function on a `DatastoreManager` object. 
# 
# When we register a directory, all subdirectories under the specified folder are also registered with the server. Always register the parent folder (for example, \\machinename\mydatashare) that contains one or more individual dataset folders as the big data file share item. To learn more, see [register a big data file share](https://enterprise.arcgis.com/en/server/latest/manage-data/windows/registering-your-data-with-arcgis-server-using-manager.htm#ESRI_SECTION1_0D55682C9D6E48E7857852A9E2D5D189)
# 
# Note: 
# You cannot browse directories in ArcGIS Server Manager. You must provide the full path to the folder you want to register, for example, \\myserver\share\bigdata. Avoid using local paths, such as C:\bigdata, unless the same data folder is available on all nodes of the server site.

# In[ ]:


# data_item = bigdata_datastore_manager.add_bigdata("air_quality_17_18_19", r"/mnt/network/data")


# In[6]:


bigdata_fileshares = bigdata_datastore_manager.search()
bigdata_fileshares


# ## Get data for analysis

# Adding a big data file share to the Geoanalytics server adds a corresponding [big data file share item](https://enterprise.arcgis.com/en/portal/latest/use/what-is-a-big-data-file-share.htm) on the portal. We can search for these types of items using the item_type parameter.

# In[11]:


aqs_data = gis.content.search("bigDataFileShares_GA_Data", item_type = "big data file share")[0]
aqs_data


# Querying the layers property of the [item](https://developers.arcgis.com/python/api-reference/arcgis.gis.toc.html#item) returns a featureLayer representing the data. 

# In[12]:


air_lyr = aqs_data.layers[0]


# In[13]:


air_lyr.properties


# Now we'll search for a feature layer depicting all the counties in the United States. We'll use the county polygons later in the notebook.

# In[15]:


usa_counties = gis.content.get('5c6ef8ef57934990b543708f815d606e')


# In[16]:


usa_counties


# We will use the first [item](https://developers.arcgis.com/python/api-reference/arcgis.gis.toc.html#item) for our analysis. Since the item is a [Feature Layer Collection](https://developers.arcgis.com/python/api-reference/arcgis.features.toc.html#featurelayercollection), accessing the layers property will give us a list of [Feature layer](https://developers.arcgis.com/python/api-reference/arcgis.features.toc.html#featurelayer) objects.

# In[17]:


usa_counties_lyr = usa_counties.layers[0]


# ## Uncover patterns in data

# ### Describe data

# The [`describe_dataset`](https://developers.arcgis.com/python/api-reference/arcgis.geoanalytics.summarize_data.html#describe-dataset) method provides an overview of big data. By default, the tool outputs a [table](https://developers.arcgis.com/python/api-reference/arcgis.features.toc.html#table) layer containing calculated field statistics and a dictionary outlining geometry and time settings for the input layer.
# 
# Optionally, the tool can output a ``feature layer`` representing a sample set of features using the ``sample_size`` parameter, or a single polygon feature layer representing the input feature layers' extent by setting the ``extent_output`` parameter to True. 

# In[18]:


from arcgis.geoanalytics.summarize_data import describe_dataset


# In[19]:


description = describe_dataset(input_layer=air_lyr,
                               extent_output=True,
                               sample_size=1000,
                               output_name="Description of air quality data" + str(dt.now().microsecond),
                               return_tuple=True)


# In[20]:


description.output_json


# We can also use sql queries to return a subset of records by leveraging the ArcGIS API for Python's Feature Layer object itself. When you run a [`query()`](https://developers.arcgis.com/python/api-reference/arcgis.features.toc.html#arcgis.features.FeatureLayer.query) on a FeatureLayer, you get back a FeatureSet object. Calling the `sdf` property of the FeatureSet returns a Spatially Enabled DataFrame object.

# In[21]:


description.sample_layer.query().sdf


# In[15]:


m1 = gis.map('USA')
m1


# <center>Locations of the Air pollution monitors

# In[24]:


m1.add_layer(description.sample_layer)


# In[25]:


m1.zoom_to_layer(description.sample_layer)


# ### Commonly used methods of measurement

# The 'Method Name' attribute contains information about the type of instrument used for measurement. 'Parameter Name' attribute tells about the name or description assigned in AQS to the parameter measured by the monitor. For more details, read [here](https://aqs.epa.gov/aqsweb/airdata/FileFormats.html#_hourly_data_files).
# 
# The function below groups data by both these parameters. This will help us know the most common type of instrument used to measure each type of parameter.

# In[28]:


def measurement_type():
    from datetime import datetime as dt
    # Load the big data file share layer into a DataFrame
    df = layers[0]
    out = df.groupBy('Method Name','Parameter Name').count()
    out.write.format("webgis").save("common_method_type" + str(dt.now().microsecond)) # Write the final result to our datastore.


# The [`run_python_script`](https://developers.arcgis.com/python/api-reference/arcgis.geoanalytics.manage_data.html?highlight=run%20python%20script#arcgis.geoanalytics.manage_data.run_python_script) method executes a Python script directly in an ArcGIS GeoAnalytics server site . The script can create an analysis pipeline by chaining together multiple GeoAnalytics tools without writing intermediate results to a data store. The tool can also distribute Python functionality across the GeoAnalytics server site.
# 
# When using the geoanalytics and pyspark packages, most functions return analysis results as Spark DataFrame memory structures. You can write these data frames to a data store or process them in a script. This lets you chain multiple geoanalytics and pyspark tools while only writing out the final result, eliminating the need to create any bulky intermediate result layers.
# 
# 

# In[ ]:


run_python_script(code=measurement_type, layers=[air_lyr])


# The result is saved as a feature layer. We can Search for the saved item using the search() method. Providing the search keyword same as the name we used for writing the result will retrieve the layer.

# In[30]:


method_item = gis.content.search('common_method_type')[0]


# Accessing the `tables` property of the item will give us the tables object. We will then use `query()` method to read the table as spatially enabled dataframe.

# In[34]:


method_df = method_item.tables[0].query(as_df=True)


# Sort the values in the decreasing order of the count field.

# In[35]:


method_df.sort_values(by='count', ascending=False)


# The table above shows that Ozone is measure using ULTRA VIOLET ABSORPTION method. We can filter the dataframe and search for the ones we are interested in.

# ### Average PM 2.5 value by county

# The function below filters the data by rows that give information about PM2.5 pollutant. To find the average PM2.5 value of each county, we will use [`join_features`](https://developers.arcgis.com/rest/services-reference/join-features.htm) tool. Finally, we will write the output to the datastore.

# In[36]:


def average():
    from datetime import datetime as dt
    df = layers[0]
    df = df.filter(df['Parameter Name'] == 'PM2.5 - Local Conditions')
    res = geoanalytics.join_features(target_layer=layers[1], 
                                     join_layer=df, 
                                     join_operation="JoinOneToOne",
                                     summary_fields=[{'statisticType' : 'mean', 'onStatisticField' : 'Sample Measurement'}],
                                     spatial_relationship='Contains')
    res.write.format("webgis").save("average_pm_by_county" + str(dt.now().microsecond))


# In[ ]:


run_python_script(average, [air_lyr, usa_counties_lyr])


# In[40]:


average_pm_by_county = gis.content.search('average_pm_by_county')[0]


# In[41]:


average_pm_by_county


# In[ ]:


avg_pm = average_pm_by_county.layers[0]


# In[20]:


avg_pm.query(as_df=True).columns


# In[35]:


m2 = gis.map('USA')
m2


# In[32]:


m2.add_layer(avg_pm, {"type": "FeatureLayer",
                      "renderer":"ClassedColorRenderer",
                      "field_name":"MEAN_Sample_Measurement",
                      "class_breaks": 6})


# In[33]:


m2.zoom_to_layer(avg_pm)
m2.legend=True


# ## Prepare time series data

# We have observed that the data is spread across US and comes from multiple stations. So we will create a dataframe that contains data points from one station. Additionally, for the purpose of sample, we will only use 2017 and 2018 data to train our model and foreacst on 2019 data. 
# 
# The function below creates a column that gives a unique station id to each row of the data. We will then filter by one station id. The timeseries data is expected to have time-dependent variables as attributes of dataframe. For this, we will pivot the table along 'Parameter Name' column. 

# In[17]:


def data_processsing():
    from datetime import datetime as dt
    import pyspark.sql.functions as F
    from pyspark.sql.functions import concat, col, lit
    # Load the big data file share layer into a DataFrame.
    df = layers[0] #converts feature layer to spark dataframe
    cols = ['Site Num', 'County Code', 'State Code', 'Date Local', 'Time Local', 'Parameter Name', 'Sample Measurement']
    df = df.select(cols) #create a subset of the dataset with only selected columns
    df = df.withColumn('Site_Num', F.lpad(df['Site Num'], 4, '0'))
    df = df.withColumn('County_Code', F.lpad(df['County Code'], 3, '0'))
    df = df.withColumn('State_Code', F.lpad(df['State Code'], 2, '0'))
    df = df.withColumn('unique_id', F.concat(F.col('State_Code'), F.col('County_Code'), F.col('Site_Num')))
#     drop_cols = ['Site_Num', 'County_Code', 'State_Code', 'Site Num', 'County Code', 'State Code']
    df = df.drop('Site_Num', 'County_Code', 'Staate_Code', 'Site Num', 'County Code', 'State Code')
    df = df.withColumn('datetime', concat(col("Date Local"), lit(" "), col("Time Local")))
#     drop_cols = ['Time Local', 'Date Local']
    df = df.drop('Time Local', 'Date Local')
    df = df.filter(df.unique_id == df.first().unique_id) #filter by only one station
    # group the dataframe by datetime,unique_id field and pivot the table to get variables needed for prediction as columns  
    df = df.groupby(df['datetime'], df['unique_id']).pivot("Parameter Name").avg("Sample Measurement")

    df.write.format("webgis").save("timeseries_data_17_18_19_1station" + str(dt.now().microsecond))


# In[ ]:


run_python_script(code=data_processsing, layers=[air_lyr], gis=gis)


# In[45]:


data = gis.content.search('timeseries_data_17_18_19_1station')[0]


# In[36]:


data


# In[41]:


series_data = data.tables[0]


# In[42]:


series_data.query(as_df=True)[:5]


# ## Predict PM2.5 using Facebook's Prophet model

# The functon below uses [pyspark pandas UDF](https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/udf-python-pandas#:~:text=A%20pandas%20user%2Ddefined%20function,%2Da%2Dtime%20Python%20UDFs.) function [fb-prophet model](https://facebook.github.io/prophet/) to foreast pm2.5  hourly value for the month of 2019 January.

# In[68]:


def predict_pm25():
    #imports
    from arcgis.gis import GIS
    gis = GIS(profile="your_enterprise_portal")
    from datetime import datetime as dt
    from pyspark.sql.functions import concat, col, lit
    import pandas as pd
    import numpy as np
    from fbprophet import Prophet
    from pyspark.sql.functions import pandas_udf, PandasUDFType
    from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, FloatType, TimestampType
    import warnings
    warnings.filterwarnings('ignore')
    
    df1 = layers[0] #converts laayer into spark dataframe
    cols = ['Outdoor_Temperature', 'Ozone', 'PM10_Total_0_10um_STP',
        'PM2_5___Local_Conditions',
        'Wind_Direction___Resultant',
        'Wind_Speed___Resultant', 'datetime']
    df1 = df1.select(cols) #filter data by columns needed
    df1 = df1.withColumn('flag', lit(1))
    schema = StructType([StructField('ds', TimestampType(), True), #schema of the resulting dataframe
                         StructField('yhat_lower', FloatType(), True),
                         StructField('yhat_upper', FloatType(), True),
                         StructField('yhat', FloatType(), True),
                         StructField('y', FloatType(), True)])
    
    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)
    def forecast_pm25(df):
        #prepare data 
        df['Date'] = df['datetime'].astype('datetime64[ns]')
        df['year'] = df['Date'].dt.year
        df.set_index('Date', inplace=True) 
        df.sort_index(inplace=True)
        v = pd.date_range(start='2016-12-31 23:00:00', periods=18265, freq='H', closed='right') #get date range
        newdf = pd.DataFrame(index=v) 
        # Fill missing dates 
        historical=pd.merge(newdf, df, how='left', left_index=True, right_index=True)
        historical.interpolate(method='time', inplace=True)
        historical.reset_index(inplace=True)
        historical.rename(columns={'index': 'ds', 'PM2_5___Local_Conditions': 'y'}, inplace=True)
        historical.fillna(0, inplace=True)
        # handle zero and negative values for pm
        for i,item in enumerate(historical['y']):
            if item<=0:
                historical['y'].iloc[i]=historical['y'].iloc[i-1]
            else:
                historical['y'].iloc[i]=item
                
        for i,item in enumerate(historical['PM10_Total_0_10um_STP']):
            if item<=0:
                historical['PM10_Total_0_10um_STP'].iloc[i]=historical['PM10_Total_0_10um_STP'].iloc[i-1]
            else:
                historical['PM10_Total_0_10um_STP'].iloc[i]=item        
         
        for i,item in enumerate(historical['Wind_Speed___Resultant']):
            if item<=0:
                historical['Wind_Speed___Resultant'].iloc[i]=historical['Wind_Speed___Resultant'].iloc[i-1]
            else:
                historical['Wind_Speed___Resultant'].iloc[i]=item
        
        for i,item in enumerate(historical['Wind_Direction___Resultant']):
            if item<=0:
                historical['Wind_Direction___Resultant'].iloc[i]=historical['Wind_Direction___Resultant'].iloc[i-1]
            else:
                historical['Wind_Direction___Resultant'].iloc[i]=item
        # split data into train and test        
        train_df = historical[historical.year != 2019]
        test_df = historical[historical.year == 2019]
        test_df.drop(columns='y', inplace=True)        
        # train model    
        m = Prophet(daily_seasonality=True,
                    weekly_seasonality=True)
        m.add_regressor('PM10_Total_0_10um_STP')
        m.add_regressor('Wind_Speed___Resultant')
        m.add_regressor('Wind_Direction___Resultant')
        m.fit(train_df);
        # predict on test data
        forecast = m.predict(test_df)
        # save plots locally
        plot1 = m.plot(forecast);
        plot2 = m.plot_components(forecast);
        plot1.savefig(r'/home/ags/localdatastore/fbdata/forecast.png')
        plot2.savefig(r'/home/ags/localdatastore/fbdata/cmponents.png')
        # Uncomment the following lines if you want to publish and visualize your graphs as as item.
#         gis = GIS('https://machinename/portal', 'username', 'password')
#         gis.content.add(item_properties={"type": "Image", "title": "Forecast Plot"}, data=r"/home/ags/localdatastore/fbdata/forecast.png")
#         gis.content.add(item_properties={"type": "Image", "title": "Forecase Components2"}, data=r"/home/ags/localdatastore/fbdata/cmponents.png")
        # create df with actual and predicted fields
        cmp_df = forecast.set_index('ds')[['yhat', 'yhat_lower', 'yhat_upper']].join(historical.set_index('ds'))
        cmp_df.reset_index(inplace=True)
        cmp_df = cmp_df[['ds', 'yhat_lower', 'yhat_upper', 'yhat', 'y']]
        return cmp_df
    res = df1.groupby(['flag']).apply(forecast_pm25)

    res.write.format("webgis").save("predicted_results_on_test_data" + str(dt.now().microsecond))


# In[ ]:


run_python_script(code=predict_pm25, layers=[series_data])


# In[6]:


predicted_item = gis.content.search('predicted_results_on_test_data854829')[0]


# In[7]:


predicted_item


# In[7]:


predicted_df = predicted_item[0].tables[0].query().sdf


# In[8]:


predicted_df.columns


# In[9]:


predicted_df.head()


# In the above table, y attribute shows the actual value of PM2.5 and yhat shows the values predicted by the trained model.

# ## Visualize result on Dashboard

# The `arcgis.apps` module includes [Dashboard](https://developers.arcgis.com/python/api-reference/arcgis.apps.dashboard.html) submodule to create dashboards programmatically. The dashboard submodule contains classes for different widgets which can be configured and be used to publish a dashboard.
# 
# 
# We want to visualize our predicted results on a dashboard. To learn more about Dashboards module, visit guide [here](../guide/authoring-arcgis-dashboards/).

# In[39]:


from arcgis.apps.dashboard import SerialChart, add_column, add_row
from arcgis.apps.dashboard import Dashboard
from arcgis.apps.dashboard import SerialChart


# In[50]:


chart = SerialChart(predicted_item,  #Create a serial chart
                    categories_from="features", 
                    title="Forecast of PM2.5 for Janumary 2019") 

chart.data.category_field = "ds" #set category field

chart.category_axis.title = "datetime" #set title for x axis

chart.value_axis.title = "pm2.5" #set title for y axis

#set fields to visualize on y axis
chart.data.add_value_field('y', line_color='#CB4335') 
chart.data.add_value_field('yhat', line_color='#2980B9')
chart.data.add_value_field('yhat_upper', line_color='#CACFD2')
chart.data.add_value_field('yhat_lower', line_color='#CACFD2')

chart.legend.visibility = True 
chart.legend.placement = "side"
chart.category_axis.minimum_period = 'hours'
chart.data.labels = False
chart.data.hover_text = True
chart.data.hover_text = True
chart


# In[47]:


agol_gis = GIS('home')


# In[48]:


dashboard = Dashboard() #creates a Dashboard object
dashboard.layout = add_row([a_chart]) #adds one chat to the Dashboard
dashboard.save('pm2.5_dashboard_2019_jan',  #publishes the dashboard to the portal
               gis=agol_gis)


# ## Conclusion

# In this notebook, we learnt how to use spark with geoanalytics server in order to carry out distributed computation of big data analysis. 

# ## References

# - https://towardsdatascience.com/pyspark-forecasting-with-pandas-udf-and-fb-prophet-e9d70f86d802
# - https://stackoverflow.com/questions/61509033/forecasting-with-facebook-prophet-using-pandas-udf-in-spark
# - https://databricks.com/blog/2020/01/27/time-series-forecasting-prophet-spark.html


# ====================
# generating_lst_from_multispectral_imagery_using_pix2pix.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Generating Land Surface Temperature from multispectral imagery using Pix2Pix
# > * 🔬 Data Science
# > * 🥠 Deep Learning and image translation

# ## Table of Contents
# 
# * [Introduction](#Introduction)
# * [Necessary imports](#Necessary-imports)
# * [Connect to your GIS](#Connect-to-your-GIS)
# * [Export image domain data](#Export-image-domain-data)
# * [Methodology](#Methodology)
# * [Model training](#Model-training)
#   * [Prepare data](#Prepare-data)
#   * [Load model architecture](#Load-model-architecture)
#   * [Tuning for optimal learning rate](#Tuning-for-optimal-learning-rate)
#   * [Fit the model](#Fit-the-model)
#   * [Visualize results in validation set](#Visualize-results-in-validation-set)
#   * [Save the model](#Save-the-model)
#   * [Compute evaluation metrics](#Compute-evaluation-metrics)
# * [Model inferencing](#Model-inferencing)
# * [Results visualization](#1)
# * [Conclusion](#Conclusion)

# ## Introduction 

# Land Surface Temperature (LST) plays an important role in the Earth’s climate system. It represents the process of energy exchange, affecting both water content and vegetation growth rate. Traditionally, LST from Landsat-8 is calculated using a chain of formulas that are complex and demanding of resources. Fortunately, deep learning models provide an efficient way to compute and predict LST. In this study, we propose an approach to predicting LST from Landsat 8 imagery using the [Pix2Pix](https://developers.arcgis.com/python/guide/how-pix2pix-works/) deep learning model. The LST will be computed on a thermal band (band 10) from a single Landsat 8 image. The calculated LST will then be used to train an image translation Pix2Pix model. The model will then be capable of translating Landsat-8 multispectral imagery to LST, allowing the predictions to be used for multitemporal monitoring of LST.

# ## Necessary imports

# In[1]:


import os
from pathlib import Path

from arcgis import GIS
from arcgis.learn import Pix2Pix, prepare_data


#  ## Connect to your GIS

# In[2]:


gis = GIS('home')


# ## Export image domain data

# A stacked raster of Landsat-8 bands has been created using bands 1-7 and band 10. This mosaic will be used as our `input_raster` for the training data.

# In[3]:


landsat_mosaic = gis.content.get('952468e0942c4b6893006cb267cbf040')
landsat_mosaic


# The raster for Land Surface Temperature is generated using the thermal band (band 10). This raster will be used as the `Additional Input Raster` for the training data.

# In[4]:


lst_raster = gis.content.get('ad29f8ab93354e77bcb22ba83f9a846a')
lst_raster


# ## Methodology

# 

# The diagram above encapsulates the overall methodology used in the estimation of the Land Surface Temperature from multispectral imagery using deep learning.

# The data will be exported in the “Export_Tiles” metadata format, which is available in the [Export Training Data For Deep Learning tool](https://pro.arcgis.com/en/pro-app/latest/tool-reference/image-analyst/export-training-data-for-deep-learning.htm). This tool is available in [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview) and [ArcGIS Image Server](https://www.esri.com/en-us/arcgis/products/arcgis-image-server). The various inputs required by the tool are described below:
# 
# - `Input Raster`: landsat_composite_raster
# 
# - `Additional Input Raster`: lst_raster
# 
# - `Tile Size X & Tile Size Y`: 256
# 
# - `Stride X & Stride Y`: 128
# 
# - `Meta Data Format`: 'Export_Tiles' (as we are training a `Pix2Pix` model).
# 
# - `Environments`: Set optimum `Cell Size`, `Processing Extent`.

# 

# Inside the exported data folder, the 'Images' and 'Images2' folders contain all of the image tiles from the two domains exported from `landsat_composite_raster` and `lst_raster` respectively.

# ## Model training

# Alternatively, we have provided a subset of training data containing a few samples that follow the same directory structure mentioned above and that provides the rasters used for exporting the training dataset. This data can be used to run the experiments.

# In[5]:


training_data = gis.content.get('11ebeb485c2d44898b32b91b105f8de6')
training_data


# In[6]:


filepath = training_data.download(file_name=training_data.name)


# In[7]:


#Extract the data from the zipped image collection
import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# ### Prepare data

# In[8]:


output_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# In[9]:


data = prepare_data(output_path, dataset_type="Pix2Pix", batch_size=8)


# ### Visualize a few samples from your training data

# To get a sense of what the training data looks like, the `arcgis.learn.show_batch()` method randomly picks a few training chips and visualizes them. Below, the images displayed on the left are Landsat-8 rasters, and the images on the right are the corresponding LST rasters.

# In[10]:


data.show_batch()


# ### Load model architecture

# In[11]:


model = Pix2Pix(data)


# ### Tuning for optimal learning rate

# Learning rate is one of the most important hyperparameters in model training. ArcGIS API for Python provides a learning rate finder that automatically chooses the optimal learning rate for you.

# In[12]:


lr = model.lr_find()
lr


# ### Fit the model

# Next, the model is trained for 30 epochs with the suggested learning rate.

# In[13]:


model.fit(30, lr)


# Here, with 30 epochs, we can see reasonable results, as both the training and validation losses have gone down considerably, indicating that the model is learning to translate between imagery domains.

# ### Save the model

# Next, we will save the trained model as a 'Deep Learning Package' ('.dlpk' format). The Deep Learning package format is the standard format used to deploy deep learning models on the ArcGIS platform.
# 
# We will use the save() method to save the trained model. By default, it will be saved to the 'models' sub-folder within our training data folder.

# In[14]:


model.save("pix2pix_model", publish=True, overwrite=True)


# ### Visualize results in validation set

# It is a good practice to see the results of the model viz-a-viz the ground truth. The code below selects random samples and displays the ground truth and model predictions side by side. This enables us to preview the results of the model within the notebook.

# In[15]:


model.show_results(rows=4)


# ### Compute evaluation metrics

# To objectively assess the synthesized image quality obtained from the model generators, we will quantitatively evaluate the results using the `Structural Similarity (SSIM) Index` and the `Peak Signal-to-Noise Ratio (PSNR)`.
# 
# The SSIM index measures the structural information similarity between images, with 0 indicating no similarity and 1 indicating complete similarity. The `SSIM value` for the trained model is `0.94`.
# 
# The PSNR measures image distortion and noise level between images. A 20 dB or higher PSNR indicates that the image is of good quality. The `PSNR value` for the trained model is `20.9`.

# In[16]:


model.compute_metrics()


# ## Model inferencing

# After training the `Pix2Pix` model and saving the weights for translating images, we can use the [Classify Pixels Using Deep Learning tool](https://pro.arcgis.com/en/pro-app/latest/tool-reference/image-analyst/classify-pixels-using-deep-learning.htm), available in both `ArcGIS pro` and `ArcGIS Enterprise`, for inferencing at scale.
# 

# In[17]:


model_for_inferencing = gis.content.get('a94c729d87774b5e92a25d344e25364f')
model_for_inferencing


# 

# `with arcpy.EnvManager(processorType="GPU"): 
# out_classified_raster = arcpy.ia.ClassifyPixelsUsingDeepLearning("composite_raster_for_inferencing", r"C:\path\to\model.dlpk", "padding 64;batch_size 8;predict_background True; tile_size 224", "PROCESS_AS_MOSAICKED_IMAGE", None); out_classified_raster.save(r"C:\sample\sample.gdb\predicted_lst")`

# ## Results visualization<a class="anchor" id="1"></a>

# Here, the LST for the months of April and July are generated using ArcGIS Pro. The output rasters are then published on our portal for visualization. 

# In[18]:


inferenced_results_april = gis.content.get('b52f79a31c994dc0ac30aec41733e564')
inferenced_results_april


# In[19]:


from arcgis.raster.functions import colormap, stretch
inf_apr_lyr = inferenced_results_april.layers[0]
stretch_rs_apr = colormap(stretch(inf_apr_lyr, 
                                  stretch_type='PercentClip', 
                                  min=0, 
                                  max=255),
                          colormap_name="Condition Number")


# In[20]:


inferenced_results_july = gis.content.get('d02bc8947de64aee9a448a215a70bc94')
inferenced_results_july


# In[21]:


inf_july_lyr = inferenced_results_july.layers[0]
stretch_rs_july = colormap(stretch(inf_july_lyr, 
                                  stretch_type='PercentClip', 
                                  min=0, 
                                  max=255),
                          colormap_name="Condition Number")


# ### Create map widgets
# Next, two map widgets are created showing the Landsat 8 mosaic and Inferenced LST raster.

# In[22]:


map1 = gis.map('Iowa, USA', 13)
map1.add_layer(stretch_rs_apr)
map2 = gis.map('Iowa, USA', 13)
map2.add_layer(stretch_rs_july)
map3 = gis.map('Iowa, USA', 13)
map3.basemap = 'satellite'
map1.zoom_to_layer(stretch_rs_apr)


# ### Synchronize web maps
# 
# Once created, the maps can be synchronized with each other using the [MapView.sync_navigation](https://developers.arcgis.com/python/api-reference/arcgis.widgets.html#arcgis.widgets.MapView.sync_navigation) functionality. By syncing the two map widgets, we can more easily compare the inferenced results with the DSM. A more detailed description of advanced map widget options can be found [here](https://developers.arcgis.com/python/guide/advanced-map-widget-usage/).

# In[23]:


map1.sync_navigation(map2)
map2.sync_navigation(map3)


# In[24]:


map1.zoom_to_layer(stretch_rs_apr)


# ### Set the map layout

# In[25]:


from ipywidgets import HBox, VBox, Label, Layout


# [Hbox and Vbox](https://developers.arcgis.com/python/api-reference/arcgis.widgets.html?highlight=hbox) were used to set the layout of map widgets.

# In[26]:


hbox_layout = Layout()
hbox_layout.justify_content = 'space-around'

hb1,hb2,hb3=HBox([Label('April')]),\
            HBox([Label('July')]),\
            HBox([Label('RGB Imagery')])
hb1.layout,hb2.layout,hb3.layout=hbox_layout,hbox_layout,hbox_layout


# ### Results

# The predictions are provided as a map for better visualization.

# In[27]:


VBox([hb1,HBox([map1]),hb2,HBox([map2]), hb3,HBox([map3])])


# 

# In[28]:


map2.zoom_to_layer(stretch_rs_apr)


# In the maps above, the land surface temperature increases from dark green to red. The corn crop is usually sown in late April or early May, therefore, in the month of April, there will be little to no crop. This can be seen in April's prediction, where most of the area is yellow or orange in color, indicating a high land surface temperature. However, in the month of July, the crop is fully grown, resulting in the predictions for most of the area being dark or light green in color.

# ## Conclusion

# In this notebook, we have demonstrated how to use a `Pix2Pix` model using `ArcGIS API for Python` to translate imagery from one domain to another.


# ====================
# generating_rgb_imagery_from_digital_surface_model_using_pix2pix.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Generating rgb imagery from digital surface model using Pix2Pix
# > * 🔬 Data Science
# > * 🥠 Deep Learning and image translation

# ## Table of Contents
# 
# * [Introduction](#Introduction)
# * [Necessary imports](#Necessary-imports)
# * [Connect to your GIS](#Connect-to-your-GIS)
# * [Export image domain data](#Export-image-domain-data)
# * [Model training](#Model-training)
#   * [Prepare data](#Prepare-data)
#   * [Load Pix2Pix model architecture](#Load-Pix2Pix-model-architecture)
#   * [Tuning for optimal learning rate](#Tuning-for-optimal-learning-rate)
#   * [Fit the model](#Fit-the-model)
#   * [Visualize results in validation set](#Visualize-results-in-validation-set)
#   * [Save the model](#Save-the-model)
#   * [Compute evaluation metrics](#Compute-evaluation-metrics)
# * [Model inferencing](#Model-inferencing)
#   * [Inference on a single imagery chip](#Inference-on-a-single-imagery-chip)
#   * [Generate raster using classify pixels using deep learning tool](#Generate-raster-using-classify-pixels-using-deep-learning-tool)
# * [Results visualization](#1)
# * [Conclusion](#Conclusion)
# * [References](#References)

# ## Introduction 

# In this notebook, we will focus on using Pix2Pix [[1](https://scholar.google.com/scholar_url?url=http://openaccess.thecvf.com/content_cvpr_2017/papers/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.pdf&hl=en&sa=T&oi=gsb-gga&ct=res&cd=0&d=16757839449706651543&ei=k33QX4T8CMaOmgGMxpwI&scisig=AAGBfm15PJpxzp8tFaG4j8UcCAbsv_is5Q)], which is one of the famous and sucessful deep learning models used for paired image-to-image translation. In geospatial sciences, this approach could help in wide range of applications traditionally not possible, where we may want to go from one domain of images to another.
# 
# The aim of this notebook is to make use of `arcgis.learn` Pix2Pix model to translate or convert the gray-scale DSM to a RGB imagery. For more details about model and its working refer [How Pix2Pix works ?](https://developers.arcgis.com/python/guide/how-pix2pix-works/) in guide section.

# ## Necessary imports

# In[1]:


import os, zipfile
from pathlib import Path
from os import listdir
from os.path import isfile, join

from arcgis import GIS
from arcgis.learn import Pix2Pix, prepare_data


#  ## Connect to your GIS

# In[2]:


# gis = GIS('home')
ent_gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# ## Export image domain data

# For this usecase, we have a high-resolution NAIP airborne imagery in the form of IR-G-B tiles and lidar data converted into DSM, collected over St. George, state of utah by state of utah and partners [[5](https://portal.opentopography.org/datasetMetadata?otCollectionID=OT.092018.6341.1)] with same spatial resolution of 0.5 m. We will export that using “Export_Tiles” metadata format available in the `Export Training Data For Deep Learning` tool. This tool is available in [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview) as well as [ArcGIS Image Server](https://www.esri.com/en-us/arcgis/products/arcgis-image-server). The various inputs required by the tool, are described below.
# 
# - `Input Raster`: DSM
# - `Additional Input Raster`: NAIP airborne imagery
# - `Tile Size X & Tile Size Y`: 256
# - `Stride X & Stride Y`: 128
# - `Meta Data Format`: 'Export_Tiles' as we are training a `Pix2Pix` model.
# - `Environments`: Set optimum `Cell Size`, `Processing Extent`.

# Raster's used for exporting the training dataset are provided below

# In[3]:


naip_domain_b_raster = ent_gis.content.get('a55890fcd6424b5bb4edddfc5a4bdc4b')
naip_domain_b_raster


# In[4]:


dsm_domain_a_raster = ent_gis.content.get('aa31a374f889487d951e15063944b921')
dsm_domain_a_raster


# 

# Inside the exported data folder, 'Images' and 'Images2' folders contain all the image tiles from two domains exported from DSM and drone imagery respectively. Now we are ready to train the `Pix2Pix` model.

# ## Model training

# Alternatively, we have provided a subset of training data containing a few samples that follows the same directory structure mentioned above and also provided the rasters used for exporting the training dataset. You can use the data directly to run the experiments.

# In[5]:


training_data = gis.content.get('2a3dad36569b48ed99858e8579611a80')
training_data


# In[6]:


filepath = training_data.download(file_name=training_data.name)


# In[7]:


#Extract the data from the zipped image collection

with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# ### Prepare data

# In[8]:


output_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# In[9]:


data = prepare_data(output_path, dataset_type="Pix2Pix", batch_size=5)


# ### Visualize a few samples from your training data

# To get a sense of what the training data looks like, `arcgis.learn.show_batch()` method randomly picks a few training chips and visualize them. On the left are some DSM's (digital surface model) with the corresponding RGB imageries of various locations on the right.

# In[10]:


data.show_batch()


# ### Load Pix2Pix model architecture

# In[11]:


model = Pix2Pix(data)


# ### Tuning for optimal learning rate

# Learning rate is one of the most important hyperparameters in model training. ArcGIS API for Python provides a learning rate finder that automatically chooses the optimal learning rate for you.

# In[12]:


lr = model.lr_find()


# 2.5118864315095795e-05

# ### Fit the model

# The model is trained for around a few epochs with the suggested learning rate.

# In[13]:


model.fit(30, lr)


# Here, with 30 epochs, we can see reasonable results — both training and validation losses have gone down considerably, indicating that the model is learning to translate between domain of imageries.

# ### Save the model

# We will save the model which we trained as a 'Deep Learning Package' ('.dlpk' format). Deep Learning package is the standard format used to deploy deep learning models on the ArcGIS platform. 
# 
# We will use the save() method to save the trained model. By default, it will be saved to the 'models' sub-folder within our training data folder.

# In[14]:


model.save("pix2pix_model_e30", publish =True)


# ### Visualize results in validation set

# It is a good practice to see results of the model viz-a-viz ground truth. The code below picks random samples and shows us ground truth and model predictions, side by side. This enables us to preview the results of the model within the notebook.

# In[15]:


model.show_results()


# ### Compute evaluation metrics

# The Frechet Inception Distance score, or FID for short, is a metric that calculates the distance between feature vectors calculated for real and generated images. Lower scores indicate the two groups of images are more similar, or have more similar statistics, with a perfect score being 0.0 indicating that the two groups of images are identical.

# In[16]:


model.compute_metrics()


# ## Model inferencing

# ### Inference on a single imagery chip

# We can translate DSM to RGB imagery with the help of predict() method. 
# 
# Using predict function, we can apply the trained model on the image chip kept for validation, which we want to translate. 
# 
# - `img_path`: path to the image file.

# In[17]:


valid_data = gis.content.get('f682b16bcc6d40419a775ea2cad8f861')
valid_data


# In[18]:


filepath2 = valid_data.download(file_name=valid_data.name)


# In[19]:


# Visualize the image chip used for inferencing 
from fastai.vision import open_image
open_image(filepath2)


# In[20]:


#Inference single imagery chip
model.predict(filepath2)


# ### Generate raster using classify pixels using deep learning tool

# After we trained the `Pix2Pix` model and saved the weights for translating image and we could use the `classify pixels using deep learning` tool avialable in both `ArcGIS pro` and `ArcGIS Enterprise` for inferencing at scale. 

# In[21]:


test_data = ent_gis.content.get('86bed58f977c4c0aa39053d93141cdb1')
test_data


# 

# `out_classified_raster = arcpy.ia.ClassifyPixelsUsingDeepLearning("Imagery", r"C:\path\to\model.emd", "padding 64;batch_size 2"); 
# out_classified_raster.save(r"C:\sample\sample.gdb\predicted_img2dsm")`

# ## Results visualization<a class="anchor" id="1"></a>

# The RGB output raster is generated using ArcGIS Pro. The output raster is published on the portal for visualization. 

# In[22]:


inferenced_results = ent_gis.content.get('30951690103047f096c6339398593d79')
inferenced_results


# ### Create map widgets
# Two map widgets are created showing DSM and Inferenced RGB raster. 

# In[23]:


map1 = ent_gis.map('Washington Fields', 13)
map1.add_layer(test_data)
map2 = ent_gis.map('Washington Fields', 13)
map2.add_layer(inferenced_results)


# ### Synchronize web maps
# 
# The maps are synchronized with each other using [MapView.sync_navigation](https://developers.arcgis.com/python/api-reference/arcgis.widgets.html#arcgis.widgets.MapView.sync_navigation) functionality. It helps in comparing the inferenced results with the DSM. Detailed description about advanced map widget options can be referred [here](https://developers.arcgis.com/python/guide/advanced-map-widget-usage/).

# In[24]:


map2.sync_navigation(map1)


# ### Set the map layout

# In[25]:


from ipywidgets import HBox, VBox, Label, Layout


# [Hbox and Vbox](https://developers.arcgis.com/python/api-reference/arcgis.widgets.html?highlight=hbox) were used to set the layout of map widgets.

# In[26]:


hbox_layout = Layout()
hbox_layout.justify_content = 'space-around'

hb1=HBox([Label('DSM'),Label('RGB results')])
hb1.layout=hbox_layout


# ### Results

# The predictions are provided as a map for better visualization.

# In[27]:


VBox([hb1,HBox([map1,map2])])


# 

# In[28]:


map2.zoom_to_layer(inferenced_results)


# ## Conclusion

# In this notebook, we demonstrated how to use `Pix2Pix` model using `ArcGIS API for Python` in order to translate imagery of one domain to the another domain.

# ## References

# - [1]. Isola, Phillip, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. "Image-to-image translation with conditional adversarial networks." In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1125-1134. 2017.
# - [2]. Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. "Generative adversarial nets." In Advances in neural information processing systems, pp. 2672-2680. 2014.
# - [3]. https://stephan-osterburg.gitbook.io/coding/coding/ml-dl/tensorfow/chapter-4-conditional-generative-adversarial-network/acgan-architectural-design
# - [4]. Kang, Yuhao, Song Gao, and Robert E. Roth. "Transferring multiscale map styles using generative adversarial networks." International Journal of Cartography 5, no. 2-3 (2019): 115-141.
# - [5]. State of Utah and Partners, 2019, Regional Utah high-resolution lidar data 2015 - 2017: Collected by Quantum Spatial, Inc., Digital Mapping, Inc., and Aero-Graphics, Inc. and distributed by OpenTopography, https://doi.org/10.5069/G9RV0KSQ. Accessed: 2020-12-08


# ====================
# glacial_terminus_extraction_using_hrnet.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Glacial Terminus Extraction using HRNet
# > * 🔬 Data Science
# > * 🥠 Deep Learning and Segmentation

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Necessary imports](#Necessary-imports)
# * [Connect to your GIS](#Connect-to-your-GIS)
# * [Download training data](#Download-training-data)
# * [Train the model](#Train-the-model)
#  * [Prepare data](#Prepare-data)
#  * [Visualize training data](#Visualize-training-data)
#  * [Load model architecture](#Load-model-architecture)
#  * [Find an optimal learning rate](#Find-an-optimal-learning-rate)
#  * [Fit the model](#Fit-the-model)
#  * [Visualize results in validation set](#Visualize-results-in-validation-set)
#  * [Accuracy assessment](#Accuracy-assessment)
#  * [Save the model](#Save-the-model)
# * [Model inference](#Model-inference)
# * [Results](#Results)
# * [Conclusion](#Conclusion)

# ## Introduction

# With the change in global climate, glaciers all over the world are experiencing an increasing mass loss, resulting in changing calving fronts. This calving front delineation is important for monitoring the rate of glacial mass loss. Currently, most calving front delineation is done manually, resulting in excessive time consumption and under-utilization of satellite imagery.
# 
# Extracting calving fronts from satellite images of marine-terminating glaciers is a two-step process. The first step involves segmenting the front using different segmentation techniques, and the second step involves post-processing mechanisms to extract the terminus line. This notebook presents the use of an HRNet model from the `arcgis.learn` module to accomplish the first task of segmenting calving fronts. We have used data provided in the [CALFIN](https://github.com/daniel-cheng/CALFIN) repository. The training data includes 1600+ Greenlandic glaciers and 200+ Antarctic glaciers/ice shelves images from Landsat (optical) and Sentinel-1 (SAR) satellites.

# ## Necessary imports

# In[1]:


import os
import glob
import zipfile
from pathlib import Path

from arcgis.gis import GIS
from arcgis.learn import MMSegmentation, prepare_data


# ## Connect to your GIS

# In[14]:


# Connect to GIS
gis = GIS("home")


# ## Download training data 

# In[3]:


training_data = gis.content.get('cc750295180a487aa7af67a67cadff78')
training_data


# The data size is approximately 6.5 GBs and may take some time to download.

# In[4]:


filepath = training_data.download(file_name=training_data.name)


# In[5]:


with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[6]:


output_path = os.path.join(os.path.splitext(filepath)[0])


# In[7]:


output_path = glob.glob(output_path)


# ## Train the model

# `arcgis.learn` provides an HRNet model through the integration of the `MMSegmentation` class. For more in-depth information on MMSegmentation, see this guide - [Using MMSegmentation with arcgis.learn](https://developers.arcgis.com/python/guide/using-mmsegmentation-with-arcgis-learn/).

# ### Prepare data

# Next, we will specify the path to our training data and a few hyperparameters.
# 
# - `path`: path of the folder/list of folders containing the training data.
# - `batch_size`: The number of images your model will train on for each step of an epoch. This will directly depend on the memory of your graphics card.

# In[4]:


data = prepare_data(path=output_path, dataset_type='Classified_Tiles', batch_size=24)


# ### Visualize training data

# To get a sense of what the training data looks like, the `arcgis.learn.show_batch()` method will randomly select training chips and visualizes them.
# - `rows`: Number of rows to visualize

# In[5]:


data.show_batch(5, alpha=0.7)


# ### Load model architecture

# In[6]:


model = MMSegmentation(data, 'hrnet')


# ### Find an optimal learning rate

# [Learning rate](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) is one of the most important hyperparameters in model training. `ArcGIS API for Python` provides a learning rate finder that automatically chooses the optimal learning rate for you.

# In[7]:


lr = model.lr_find()


# ### Fit the model 

# Next, we will train the model for a few epochs with the learning rate found in the previous step. For the sake of time, we will start with 30 epochs.

# In[8]:


model.fit(30, lr)


# As we can see, the training and validation losses are continuing to decrease, indicating that the model is still learning. This suggests that there is more room for training, and as such, we chose to train the model for a total of 170 epochs to achieve better results.

# ### Visualize results in validation set

# It is a good practice to see the results of the model viz-a-viz ground truth. The code below picks random samples and visualizes the ground truth and model predictions side by side. This enables us to preview the results of the model we trained for 170 epochs within the notebook.

# In[9]:


model.show_results(5, thresh=0.1, aplha=0.1)


# ### Accuracy assessment

# `arcgis.learn` provides the `mIOU()` method that computes the mean IOU (Intersection over Union) on the validation set for each class.

# In[10]:


model.mIOU()


# In[11]:


model.per_class_metrics()


# ### Save the model

# We will save the model which we trained as a 'Deep Learning Package' ('.dlpk' format). The Deep Learning package is the standard format used to deploy deep learning models on the ArcGIS platform.
# 
# We will use the `save()` method to save the trained model. By default, it will be saved to the 'models' sub-folder within our training data folder.

# In[16]:


model.save("Glaciertips_hrnet_30e", publish=True)


# The saved model in this notebook can be downloaded from [this](https://geosaurus.maps.arcgis.com/home/item.html?id=2f4454094f974f74b1e67432bcaf564d) link.

# ## Model inference

# In this step, we will generate a classified raster using the 'Classify Pixels Using Deep Learning' tool available in both `ArcGIS Pro` and `ArcGIS Enterprise`.
# 
# - `Input Raster`: The raster layer you want to classify.
# - `Model Definition`: Located inside the saved model in the 'models' folder in '.emd' format.
# - `Padding`: The 'Input Raster' is tiled, and the deep learning model classifies each individual tile separately before producing the final 'Output Classified Raster'. This may lead to unwanted artifacts along the edges of each tile, as the model has little context to predict accurately. Padding allows us to supply extra information along the tile edges, thus helping the model to make better predictions.
# - `Cell Size`: Should be close to the size used to train the model.
# - `Processor Type`: Allows you to control whether the system's 'GPU' or 'CPU' will be used to classify pixels. By default, 'GPU' will be used if available.

# 
# 

# It is advised to zoom in to the right extent of the area of interest in order to avoid/reduce noise from the results as the model is not trained to be generalized to work across the globe.

# ## Results 

# The gif below was achieved with the model trained in this notebook and visualizes the segmented calving front for Rink Isbrae, a major West Greenland outlet glacier.
# 
# <p align="center"></p>

# ## Conclusion 

# In this notebook, we have demonstrated how to use models supported by the `MMSegmentation` class in `arcgis.learn` to perform segmentation tasks. We trained an `HRNet` model to segment calving fronts for the Risk Isbrae glacier. With this trained model, this segmentation task can now be performed in regular intervals to monitor glacial mass loss.


# ====================
# historical_wildfire_analysis.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Historical Wildfire Analysis
# 
# 
# Spatial wildfire occurrence data, referred to as the **Fire Program Analysis fire-occurrence database (FPA FOD)**, for the United States, 1992-2015 was obtained from https://www.fs.usda.gov/rds/archive/Product/RDS-2013-0009.4/. The dataset contains data on:
# * 1.88 Million US Wildfires
# * 24 years of geo-referenced wildfire records

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Historical-Wildfire-Analysis" data-toc-modified-id="Historical-Wildfire-Analysis-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Historical Wildfire Analysis</a></span><ul class="toc-item"><li><ul class="toc-item"><li><span><a href="#Given-this-data,-can-we-answer-the-following-questions?" data-toc-modified-id="Given-this-data,-can-we-answer-the-following-questions?-1.0.1"><span class="toc-item-num">1.0.1&nbsp;&nbsp;</span>Given this data, can we answer the following questions?</a></span></li><li><span><a href="#Exploratory-Data-Analysis" data-toc-modified-id="Exploratory-Data-Analysis-1.0.2"><span class="toc-item-num">1.0.2&nbsp;&nbsp;</span>Exploratory Data Analysis</a></span></li><li><span><a href="#Have-wildfires-become-more-frequent-over-time?" data-toc-modified-id="Have-wildfires-become-more-frequent-over-time?-1.0.3"><span class="toc-item-num">1.0.3&nbsp;&nbsp;</span>Have wildfires become more frequent over time?</a></span></li><li><span><a href="#What-are-the-causes-of-wildfire?" data-toc-modified-id="What-are-the-causes-of-wildfire?-1.0.4"><span class="toc-item-num">1.0.4&nbsp;&nbsp;</span>What are the causes of wildfire?</a></span></li><li><span><a href="#Does-the-day-of-week-matter?" data-toc-modified-id="Does-the-day-of-week-matter?-1.0.5"><span class="toc-item-num">1.0.5&nbsp;&nbsp;</span>Does the day of week matter?</a></span></li></ul></li><li><span><a href="#Mapping-and-Spatial-Analysis" data-toc-modified-id="Mapping-and-Spatial-Analysis-1.1"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>Mapping and Spatial Analysis</a></span></li><li><span><a href="#Can-Machine-Learning-categorize-the-cause-of-the-fire?" data-toc-modified-id="Can-Machine-Learning-categorize-the-cause-of-the-fire?-1.2"><span class="toc-item-num">1.2&nbsp;&nbsp;</span>Can Machine Learning categorize the cause of the fire?</a></span><ul class="toc-item"><li><span><a href="#Feature-Engineering:-Use-geoenrichment-to-add-features" data-toc-modified-id="Feature-Engineering:-Use-geoenrichment-to-add-features-1.2.1"><span class="toc-item-num">1.2.1&nbsp;&nbsp;</span>Feature Engineering: Use geoenrichment to add features</a></span></li><li><span><a href="#Fit-Random-Forest-classifier-and-check-accuracy" data-toc-modified-id="Fit-Random-Forest-classifier-and-check-accuracy-1.2.2"><span class="toc-item-num">1.2.2&nbsp;&nbsp;</span>Fit Random Forest classifier and check accuracy</a></span></li></ul></li></ul></li></ul></div>

# ### Given this data, can we answer the following questions?
# * **Are wildfires increasing over time?**
# * **Where are the fire hot spots?**
# * **Can Machine Learning categorize the cause of the fire given the size, location and date?**
# 
# That would give investigators a data driven way to prioritize cases for further investigation.

# In[1]:


from arcgis import *
import pandas as pd
import numpy as np


# In[2]:


gis = GIS(profile='deldev.maps')


# In[9]:


from arcgis.features import GeoAccessor, GeoSeriesAccessor

wildfires_df = pd.DataFrame.spatial.from_featureclass(r'C:\demo\FPA_FOD_20170508.gdb\Fires')


# In[10]:


wildfires_df.shape


# In[11]:


df = wildfires_df[['FIRE_YEAR', 'DISCOVERY_DATE', 'DISCOVERY_DOY',
       'STAT_CAUSE_DESCR', 'FIRE_SIZE', 'STATE', 'FIPS_CODE', 'FIPS_NAME', 'SHAPE']]


# In[12]:


df.head()


# ### Exploratory Data Analysis

# In[158]:


df['x'] = df.SHAPE.apply(lambda g: g['x'])
df['y'] = df.SHAPE.apply(lambda g: g['y'])
smdf = wildfires_df[['FIRE_YEAR', 'DISCOVERY_DATE', 'DISCOVERY_DOY',
       'STAT_CAUSE_DESCR', 'FIRE_SIZE', 'STATE', 'FIPS_CODE', 'FIPS_NAME', 'x', 'y']]
smdf.to_pickle('fires.pkl')


# In[135]:


pd.options.mode.chained_assignment = None 

df['MONTH'] = pd.DatetimeIndex(df['DISCOVERY_DATE']).month
df['DAY_OF_WEEK'] = df['DISCOVERY_DATE'].dt.dayofweek
df_arson = df[df['STAT_CAUSE_DESCR']=='Arson']
dfa = df_arson['DAY_OF_WEEK'].value_counts()
df_lightning = df[df['STAT_CAUSE_DESCR']=='Lightning']
dfl = df_lightning['DAY_OF_WEEK'].value_counts()


# In[ ]:


get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib.pyplot as plt

plt.style.use('ggplot')
sorted_df = df.sort_values('FIRE_YEAR')
fire_freq = sorted_df.groupby('FIRE_YEAR').count()
x = np.asarray(fire_freq.axes[0])
y = np.asarray(df.groupby('FIRE_YEAR').size())


# In[247]:


lyr = gis.content.search('Demographic Profile')[0]
lyr


# ### Have wildfires become more frequent over time?

# In[148]:


fig, ax = plt.subplots(figsize=(10,6))
ax.bar(x, y, color='coral')
plt.title('Number of fires by year', y=-0.2)
fit1 = np.polyfit(x, y, deg=1)
ax.plot(x, x*fit1[0] + fit1[1], color='teal', linewidth=4);


# ### What are the causes of wildfire?

# In[55]:


distribution = df['STAT_CAUSE_DESCR'].value_counts()


# In[143]:


plt.figure(figsize=(6,6))
plt.title('Distribution by cause', y=-0.15)
plt.pie(distribution, labels=list(distribution.index[:-2]) + ['', '']);
plt.axis('equal');


# ### Does the day of week matter?

# In[152]:


ind = np.arange(7) 
width = 0.35       

fig, ax   = plt.subplots(figsize=(10,6))
arson     = ax.bar(ind, dfa.sort_index(), width, color='coral')
lightning = ax.bar(ind + width, dfl.sort_index(), width, color='teal')

ax.set_title('Wildfires by day of week', y=-0.15)
ax.set_xticklabels(('', 'Mon', 'Tues', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'))
ax.legend((arson[0], lightning[0]), ('Arson', 'Lightning'), loc=2);


# ## Mapping and Spatial Analysis

# In[ ]:


wildfires = gis.content.import_data(wildfires_df)


# In[ ]:


from arcgis.features.analysis import find_hot_spots
hotspots = find_hot_spots(wildfires, 20, 'Miles', output_name='Hot Spots US_Historical_Wildfires_20')


# In[153]:


hotspots = gis.content.get('8f9e60e114c749a9864e4adb9643d096')


# In[154]:


usmap = gis.map('United States')
usmap.add_layer(hotspots)
usmap


# ## Can Machine Learning categorize the cause of the fire?
# 
# Since **arson** is a significant preventable cause for many wildfires, it would be instructive to see if Machine Learning techniques can be used to classify if a fire was caused due to arson given some basic data that is available when the fire is first discovered or when investigations begin. This can be framed as a **binary classification** problem.
# 
# Such a technique could give investigators a data driven way to prioritize cases for further investigation.

# In[13]:


from sklearn.model_selection import train_test_split
from sklearn import tree, preprocessing
import sklearn.ensemble as ske
import pandas as pd
import matplotlib.pyplot as plt

get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', 'IPCompleter.greedy=True')


# In[162]:


df = pd.read_pickle('fires.pkl')

df['MONTH'] = pd.DatetimeIndex(df['DISCOVERY_DATE']).month
df['DAY_OF_WEEK'] = df['DISCOVERY_DATE'].dt.dayofweek

ca_fires = df[df.STATE=='CA']
ca_fires['ARSON'] = ca_fires['STAT_CAUSE_DESCR'].apply(lambda x: 1 if x == 'Arson' else 0) 

pdf = ca_fires[ ['FIRE_YEAR', 'DISCOVERY_DOY', 'FIPS_CODE', 'FIRE_SIZE', 'x', 'y', 'MONTH', 'DAY_OF_WEEK', 'ARSON'] ]
pdf = pdf.dropna()


# ### Feature Engineering: Use geoenrichment to add features

# In[206]:


analysis_variables = [
    'TOTPOP_CY',  # 2016 Population: Total Population (Esri)
    'HHPOP_CY',   # 2016 Household Population (Esri)
    'FAMPOP_CY',  # 2016 Family Population (Esri)
    'DIVINDX_CY', # 2016 Diversity Index (Esri)
    'TOTHH_CY',   # 2016 Total Households (Esri)
    'AVGHHSZ_CY', # 2016 Average Household Size (Esri)

    'MALES_CY',   # 2016 Gender: Male Population (Esri)
    'FEMALES_CY', # 2016 Gender: Female Population (Esri)
    
    'MEDAGE_CY',  # 2016 Age: Median Age (Esri)
    
    'AVGFMSZ_CY', # 2016 Income: Average Family Size (Esri)
    'MEDHINC_CY', # 2016 Income: Median Household Income (Esri)
    'AVGHINC_CY', # 2016 Income: Average Household Income (Esri)
        
    'EDUCBASECY', # 2016 Educational Attainment Base (Esri)
    'NOHS_CY',    # 2016 Education: Less than 9th Grade (Esri)
    'SOMEHS_CY',  # 2016 Education: 9-12th Grade/No Diploma (Esri)
    'HSGRAD_CY',  # 2016 Education: High School Diploma (Esri)
    'GED_CY',     # 2016 Education: GED/Alternative Credential (Esri)
    'SMCOLL_CY',  # 2016 Education: Some College/No Degree (Esri)
    'ASSCDEG_CY', # 2016 Education: Associate's Degree (Esri)
    'BACHDEG_CY', # 2016 Education: Bachelor's Degree (Esri)
]


# In[239]:


from arcgis.geoenrichment import *

usa = Country.get('US')
counties = usa.subgeographies.states['California'].counties

enrich_df = enrich(list(counties.values()), analysis_variables=analysis_variables)

pdf['StdGeographyID'] = '06' + pdf['FIPS_CODE']
merged = pd.merge(pdf, enrich_df, on='StdGeographyID')
cols = ['FIRE_YEAR', 'DISCOVERY_DOY', 'FIPS_CODE', 'FIRE_SIZE', 'x', 'y', 'MONTH', 'DAY_OF_WEEK', 'ARSON'] + analysis_variables
mdf = merged[cols]

mdf.head()


# In[ ]:


m = gis.map('California')
m 


# In[245]:


lyr = gis.content.import_data(enrich_df, title='Demographic Profile')


# In[248]:


m.add_layer(lyr, {'renderer':'ClassedColorRenderer', 'field':'AVGHINC_CY'})


# ### Fit Random Forest classifier and check accuracy

# In[240]:


X = mdf.drop(['ARSON'], axis=1).values
y = mdf['ARSON'].values
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=42)

clf_rf = ske.RandomForestClassifier(n_estimators=200)
clf_rf = clf_rf.fit(X_train, y_train)

print(clf_rf.score(X_test, y_test))


# Summary: Given some basic data, available when a fire is first discovered, it is possible to predict with over 90% accuracy if the fire was the result of arson.

# 
# **Citations**
# 
# Short, Karen C. 2017. Spatial wildfire occurrence data for the United States, 1992-2015 [FPA_FOD_20170508]. 4th Edition. Fort Collins, CO: Forest Service Research Data Archive. https://doi.org/10.2737/RDS-2013-0009.4


# ====================
# how-much-green-is-Delhi-as-on-21-oct-2022.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # How much green is Delhi as on 13 Oct 2022?

#  <h1>Table of Contents<span class="tocSkip"></span></h1>
# 
# <div class="toc">
#     <ul class="toc-item">
#         <li><span><a href="#Introduction" data-toc-modified-id="Introduction-1">Introduction</a></span></li>
#         <li><span><a href="#Necessary-Imports" data-toc-modified-id="Necessary-Imports-1">Necessary Imports</a></span></li>
#         <li><span><a href="#Connect-to-your-GIS" data-toc-modified-id="Connect-to-your-GIS-2">Connect to your GIS</a></span></li>
#         <li><span><a href="#Get-the-data-for-analysis" data-toc-modified-id="Get-the-data-for-analysis-3">Get the data for analysis</a></span></li>
#         <li><span><a href="#Extracting-Landsat-imagery-for-Delhi-Region" data-toc-modified-id="Extracting-Landsat-imagery-for-Delhi-Region-4">Extracting Landsat imagery for Delhi Region</a></span></li>
#         <li><span><a href="#Filter-imageries-based-on-cloud-cover-and-Acquisition-Date" data-toc-modified-id="Filter-imageries-based-on-cloud-cover-and-Acquisition-Date-5">Filter imageries based on cloud cover and Acquisition Date</a></span></li>
#         <li><span><a href="#Creating-NDVI-composite-for-the-filtered-imagery" data-toc-modified-id="Creating-NDVI-composite-for-the-filtered-imagery-6">Creating NDVI composite for the filtered imagery</a></span></li>
#         <li><span><a href="#Masking-NDVI-composite-for-green-area-calculation" data-toc-modified-id="Masking-NDVI-composite-for-green-area-calculation-7">Masking NDVI composite for green area calculation</a></span></li>
#         <li><span><a href="#Visualising-results-on-map" data-toc-modified-id="Visualising-results-on-map-8">Visualising results on map</a></span></li>
#         <li><span><a href="#Area-Derivation" data-toc-modified-id="Area-Derivation-9">Area Derivation</a></span></li>
#         <li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-10">Conclusion</a></span></li>
#         <li><span><a href="#References" data-toc-modified-id="References-11">References</a></span></li>
#     </ul>
# </div>

# ## Introduction

# The India State of Forests Report (ISFR) 2021 [1], showed that the green cover of Delhi has increased from 21.88% in 2019 to 23.06% in 2021, of its geographical area as observed from satellite imageries of Delhi [2]. This was a welcome news for the city struggling with severe pollution and rising population, which makes it necessary to monitor the city's green cover and keep the city liveable. 
# 
# This sample shows the capabilities of spectral indices such as Normalized Difference Vegetation index (NDVI) for the calculation of green cover in Delhi on 21 October 2022 using Landsat 8 imagery.

# ## Necessary Imports

# In[1]:


get_ipython().run_line_magic('matplotlib', 'inline')

import pandas as pd
from datetime import datetime
from IPython.display import Image
from IPython.display import HTML
import matplotlib.pyplot as plt

import arcgis
from arcgis.gis import GIS
from arcgis.raster.functions import apply, clip, remap, colormap
from arcgis.geocoding import geocode


# ## Connect to your GIS

# In[2]:


gis = GIS("home")


# ## Get the data for analysis

# Here we're getting the multispectral landsat imagery item in ArcGIS Online.

# In[3]:


landsat_item = gis.content.get('d9b466d6a9e647ce8d1dd5fe12eb434b')
landsat = landsat_item.layers[0]
landsat_item


# Search for <b>India State Boundaries 2022</b> layer in ArcGIS Online. This layer has all the state boundaries for India. The boundary of Delhi can be filtered from the layer, as this notebook focuses on the city's green cover.

# In[4]:


boundaries = gis.content.get('b66401aa25074b098aaa571b10c1b21f')
state_boundaries = boundaries.layers[1]


# In[5]:


area = geocode("New Delhi, India", out_sr=landsat.properties.spatialReference)[0]
landsat.extent = area['extent']


# ## Extracting Landsat imagery for Delhi Region

# In State Boundary layer, OBJECTID for Delhi is 7 which is used below. Also, it is important to add extent to the geometry of selected boundary.

# In[6]:


delhi = state_boundaries.query(where='OBJECTID=7')
delhi_geom = delhi.features[0].geometry
delhi_geom['spatialReference'] = {'wkid':3857}
delhi.features[0].extent = area['extent']


# ## Filter imageries based on cloud cover and Acquisition Date

# In order to have good result, it is important to select cloud free imagery from the image collection for a specified time duration. In this example we have selected all the imageries captured between 1 October, 2022 to 31 December, 2022 with cloud cover less than or equal to 5% for Delhi.

# In[7]:


from datetime import datetime

selected = landsat.filter_by(where="(Category = 1) AND (cloudcover <=0.05)",
                             time=[datetime(2022, 10, 1), datetime(2022, 12, 31)],
                             geometry=arcgis.geometry.filters.intersects(delhi_geom))

df = selected.query(out_fields="AcquisitionDate, GroupName, CloudCover", 
                    order_by_fields="AcquisitionDate").sdf
df['AcquisitionDate'] = pd.to_datetime(df['AcquisitionDate'], unit='ms')
df


# Selecting imagery dated  21 October, 2022 since the <i>OBJECTID<i> value in the query below is for that date.

# In[8]:


delhi_image = landsat.filter_by('OBJECTID=3866751') # 2022-10-21


# Applying Natural color to verify the quality of image.

# In[9]:


apply(delhi_image, 'Natural Color with DRA')


# ## Creating NDVI composite for the filtered imagery

# In the Landsat layer properties, pre defined "NDVI Raw" function is applied to get the NDVI composite.

# In[10]:


ndvi_colorized = apply(delhi_image, 'NDVI Raw')
ndvi_colorized


# Clipping the NDVI composite for Delhi and setting the extent of the generated raster.

# In[11]:


delhi_clip = clip(raster=ndvi_colorized, geometry= delhi_geom)
delhi_clip.extent = area['extent']
delhi_clip


# ## Masking NDVI composite for green area calculation

# "remap" function is used to define the NDVI range for agricultural land and forest. The NDVI values between 0.4 - 0.5 represents agricultural land whereas NDVI values between 0.5 - 1 shows forest/ tree cover [2].

# In[12]:


threshold_val = 0.5
masked = colormap(remap(delhi_clip, 
                        input_ranges=[0.4,threshold_val,     # agricultural land
                                     threshold_val, 1],      # forest area/ tree cover
                        output_values=[1, 2]),
                        colormap=[[1, 124, 252, 0], [2, 0, 102, 0]], astype='u8')

Image(masked.export_image(bbox=area['extent'], size=[1200,450], f='image'))


# ## Visualising results on map

# In[13]:


m = gis.map('New Delhi, India', 10)
m.add_layer(masked)
m.legend = True
m


# ![How-much-green-is-Delhi_output.PNG](attachment:How-much-green-is-Delhi_output.PNG)

# Here, the pixels with light green color having a value of "1" represent agricultural land whereas the pixels with dark green color having a value of "2" represent forest area/ tree cover.

# ## Area Derivation

# Now we will calculate the total green cover of Delhi. It is important to note that pixels belonging to agricultural land as well as forest area/ tree cover are considered as green cover in the calculation.

# In[14]:


pixx = (delhi_clip.extent['xmax'] - delhi_clip.extent['xmin']) / 1200
pixy = (delhi_clip.extent['ymax'] - delhi_clip.extent['ymin']) / 450

res = masked.compute_histograms(delhi_clip.extent,
                               pixel_size={'x':pixx, 'y':pixy})
numpix = 0
histogram = res['histograms'][0]['counts'][0:]
for i in histogram[1:]:
    numpix += i


# In[15]:


sqmarea = numpix * pixx * pixy # in sq. m
acres = 0.00024711 * sqmarea   # in acres
HTML('<h3>Total Green cover in Delhi is ~ <i>{}%</i> of the total \
     geographical area of Delhi.</h3>'.format(int((acres/419004.17)*100)))


# In[16]:


plt.title('Green Cover', y=-0.1)
plt.pie(histogram, labels=['Non green','Agricultural Land', 'Forest Cover']);
plt.axis('equal');


# The pie chart clearly shows the distribution of agricultural land, forest cover and non green areas like urban areas, water, etc. Here, the non green areas contribute around 74% of Delhi's land and remaining 26% goes with forest cover and agricultural land. 

# ## Conclusion

# In this study, green cover of Delhi is calculated using Landsat 8 imagery for 21 October, 2022. Normalised Difference Vegetation Index (NDVI) is used for the calculation of green areas, which is a well known and widely accepted spectral index for vegetation studies.
# 
# NDVI is used here compared to the other spectral indices because it is easy to compute and requires only two bands. Pixels with NDVI values greater than 0.4 and less than 0.5 are considered as agricultural land, parks, etc and the pixels with more than or equal to 0.5 NDVI values are considered as tree cover/ forest areas.
# 
# Finally, the area of the pixels of agricultural land and forest class is calculated to estimate overall green cover of Delhi.
# 
# The study shows how the green land cover of an area could be easily computed in few lines of code using Esri's predefined NDVI layer and Landsat 8 imagery from ArcGIS server. 

# ## References

# [1] https://fsi.nic.in/forest-report-2021-details
# 
# [2] https://economictimes.indiatimes.com/news/india/environmentalists-question-govt-report-on-delhis-forest-cover/articleshow/92164943.cms?from=mdr
# 
# [3] https://earthobservatory.nasa.gov/features/MeasuringVegetation


# ====================
# identifying-country-names-from-incomplete-house-addresses.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Identifying country names from incomplete house addresses

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc">
# <ul class="toc-item">
# <li><span><a href="#Introduction" data-toc-modified-id="Introduction-1">Introduction</a></span></li>
# <li><span><a href="#Prerequisites" data-toc-modified-id="Prerequisites-2">Prerequisites</a></span></li>
# <li><span><a href="#Imports" data-toc-modified-id="Imports-3">Imports</a></span></li>
# <li><span><a href="#Data-preparation" data-toc-modified-id="Data-preparation-4">Data preparation</a></span></li>
# <li><span><a href="#TextClassifier-model" data-toc-modified-id="TextClassifier-model-5">TextClassifier model</a></span></li>
# <ul class="toc-item">
# <li><span><a href="#Load-model-architecture" data-toc-modified-id="Load-model-architecture-5.1">Load model architecture</a></span></li>
# <li><span><a href="#Model-training" data-toc-modified-id="Model-training-5.2">Model training</a></span></li>    
# <li><span><a href="#Validate-results" data-toc-modified-id="Validate-results-5.3">Validate results</a></span></li>
# <li><span><a href="#Model-metrics" data-toc-modified-id="Model-metrics-5.4">Model metrics</a></span></li>    
# <li><span><a href="#Get-misclassified-records" data-toc-modified-id="Get-misclassified-records-5.5">Get misclassified records</a></span></li>
# <li><span><a href="#Saving-the-trained-model" data-toc-modified-id="Saving-the-trained-model-5.6">Saving the trained model</a></span></li>
# </ul>
# <li><span><a href="#Model-inference" data-toc-modified-id="Model-inference-6">Model inference</a></span></li>
# <li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-7">Conclusion</a></span></li>
# <li><span><a href="#References" data-toc-modified-id="References-8">References</a></span></li>
# </ul></div>

# # Introduction

# [Geocoding](https://en.wikipedia.org/wiki/Geocoding) is the process of taking input text, such as an **address** or the name of a place, and returning a **latitude/longitude** location for that place. In this notebook, we will be picking up a dataset consisting of incomplete house addresses from 10 countries. We will build a classifier using `TextClassifier` class of `arcgis.learn.text` module to predict the country for these incomplete house addresses. 
# 
# The house addresses in the dataset consist of text in multiple languages like English, Japanese, French, Spanish, etc. The dataset is a small subset of the house addresses taken from [OpenAddresses data](http://results.openaddresses.io/) 
# 
# **A note on the dataset**
# - The data is collected around 2020-05-27 by [OpenAddresses](http://openaddresses.io).
# - The data licenses can be found in `data/country-classifier/LICENSE.txt`.

# # Prerequisites

# - Data preparation and model training workflows using arcgis.learn have a dependency on [transformers](https://huggingface.co/transformers/v3.0.2/index.html). Refer to the section **"Install deep learning dependencies of arcgis.learn module"** [on this page](https://developers.arcgis.com/python/guide/install-and-set-up/#Install-deep-learning-dependencies) for detailed documentation on the installation of the dependencies.
# 
# - **Labeled data**: For `TextClassifier` to learn, it needs to see documents/texts that have been assigned a label. Labeled data for this sample notebook is located at `data/country-classifier/house-addresses.csv`
# 
# - To learn more about how `TextClassifier` works, please see the guide on [Text Classification with arcgis.learn](https://developers.arcgis.com/python/guide/text-classification).

# # Imports

# In[1]:


import os
import zipfile
import pandas as pd
from pathlib import Path
from arcgis.gis import GIS
from arcgis.learn import prepare_textdata
from arcgis.learn.text import TextClassifier


# In[2]:


gis = GIS('home')


# # Data preparation
# 
# Data preparation involves splitting the data into training and validation sets, creating the necessary data structures for loading data into the model and so on. The `prepare_data()` function can directly read the training samples and automate the entire process.

# In[3]:


training_data = gis.content.get('ab36969cfe814c89ba3b659cf734492a')
training_data


# In[4]:


filepath = training_data.download(file_name=training_data.name)


# In[5]:


with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[6]:


DATA_ROOT = Path(os.path.join(os.path.splitext(filepath)[0]))


# In[7]:


data = prepare_textdata(DATA_ROOT, "classification", train_file="house-addresses.csv", 
                        text_columns="Address", label_columns="Country", batch_size=64)


# The `show_batch()` method can be used to see the training samples, along with labels.

# In[11]:


data.show_batch(10)


# # TextClassifier model

# `TextClassifier` model in `arcgis.learn.text` is built on top of [Hugging Face Transformers](https://huggingface.co/transformers/v3.0.2/index.html) library. The model training and inferencing workflow are similar to computer vision models in `arcgis.learn`. 
# 
# Run the command below to see what backbones are supported for the text classification task.

# In[12]:


print(TextClassifier.supported_backbones)


# Call the model's `available_backbone_models()` method with the backbone name to get the available models for that backbone. The call to **available_backbone_models** method will list out only few of the available models for each backbone. Visit [this](https://huggingface.co/transformers/pretrained_models.html) link to get a complete list of models for each backbone.

# In[13]:


print(TextClassifier.available_backbone_models("xlm-roberta"))


# ## Load model architecture

# Invoke the `TextClassifier` class by passing the data and the backbone you have chosen. The dataset consists of house addresses in multiple languages like Japanese, English, French, Spanish, etc., hence we will use a [multi-lingual transformer backbone](https://huggingface.co/transformers/v3.0.2/multilingual.html) to train our model.

# In[14]:


model = TextClassifier(data, backbone="xlm-roberta-base")


# ## Model training

# The `learning rate`[[1]](#References) is a **tuning parameter** that determines the step size at each iteration while moving toward a minimum of a loss function, it represents the speed at which a machine learning model **"learns"**. `arcgis.learn` includes a learning rate finder, and is accessible through the model's `lr_find()` method, that can automatically select an **optimum learning rate**, without requiring repeated experiments.

# In[15]:


model.lr_find()


# Training the model is an iterative process. We can train the model using its `fit()` method till the validation loss (or error rate) continues to go down with each training pass also known as an epoch. This is indicative of the model learning the task.

# In[17]:


model.fit(epochs=6, lr=0.001)


# ## Validate results

# Once we have the trained model, we can see the results to see how it performs.

# In[19]:


model.show_results(15)


# ### Test the model prediction on an input text

# In[20]:


text = """1016, 8A, CL RICARDO LEON - SANTA ANA (CARTAGENA), 30319"""
print(model.predict(text))


# ## Model metrics
# 
# To get a sense of how well the model is trained, we will calculate some important metrics for our `text-classifier` model. First, to find how accurate[[2]](#References) the model is in correctly predicting the classes in the dataset, we will call the model's `accuracy()` method.

# In[21]:


model.accuracy()


# Other important metrics to look at are Precision, Recall & F1-measures [[3]](#References). To find `precision`, `recall` & `f1` scores per label/class we will call the model's `metrics_per_label()` method.

# In[22]:


model.metrics_per_label()


# ## Get misclassified records
# 
# Its always a good idea to see the cases where your model is not performing well. This step will help us to:
# - Identify if there is a problem in the dataset.
# - Identify if there is a problem with text/documents belonging to a specific label/class.  
# - Identify if there is a class imbalance in your dataset, due to which the model didn't see much of the labeled data for a particular class, hence not able to learn properly about that class.
# 
# To get the **misclassified records** we will call the model's `get_misclassified_records` method.

# In[23]:


misclassified_records = model.get_misclassified_records()


# In[24]:


misclassified_records.style.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\
        .set_properties(**{'text-align': "left"}).hide_index()


# ## Saving the trained model
# 
# Once you are satisfied with the model, you can save it using the save() method. This creates an Esri Model Definition (EMD file) that can be used for inferencing on unseen data.

# In[25]:


model.save("country-classifier")


# # Model inference
# 
# The trained model can be used to classify new text documents using the predict method. This method accepts a string or a list of strings to predict the labels of these new documents/text.

# In[26]:


text_list = data._train_df.sample(15).Address.values
result = model.predict(text_list)

df = pd.DataFrame(result, columns=["Address", "CountryCode", "Confidence"])

df.style.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\
        .set_properties(**{'text-align': "left"}).hide_index()


# # Conclusion

# In this notebook, we have built a text classifier using `TextClassifier` class of `arcgis.learn.text` module. The dataset consisted of house addresses of 10 countries written in languages like English, Japanese, French, Spanish, etc. To achieve this we used a [multi-lingual transformer backbone](https://huggingface.co/transformers/v3.0.2/multilingual.html) like `XLM-RoBERTa` to build a classifier to predict the country for an input house address. 

# # References

# [1] [Learning Rate](https://en.wikipedia.org/wiki/Learning_rate)
# 
# [2] [Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision)
# 
# [3] [Precision, recall and F1-measures](https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-and-f-measures)


# ====================
# identifying-suitable-sites-for-als-clinics-using-location-allocation-analysis.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Identifying suitable sites for new ALS clinics using location allocation analysis

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Identifying-suitable-sites-for-new-ALS-clinics-using-location-allocation-analysis" data-toc-modified-id="Identifying-suitable-sites-for-new-ALS-clinics-using-location-allocation-analysis-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Identifying suitable sites for new ALS clinics using location allocation analysis</a></span><ul class="toc-item"><li><span><a href="#Clinic-access-for-the-chronically-ill" data-toc-modified-id="Clinic-access-for-the-chronically-ill-1.1"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>Clinic access for the chronically ill</a></span></li><li><span><a href="#Access-the-analysis-data" data-toc-modified-id="Access-the-analysis-data-1.2"><span class="toc-item-num">1.2&nbsp;&nbsp;</span>Access the analysis data</a></span></li><li><span><a href="#Map-the-challenge" data-toc-modified-id="Map-the-challenge-1.3"><span class="toc-item-num">1.3&nbsp;&nbsp;</span>Map the challenge</a></span></li><li><span><a href="#Introduction-to-Location-Allocation" data-toc-modified-id="Introduction-to-Location-Allocation-1.4"><span class="toc-item-num">1.4&nbsp;&nbsp;</span>Introduction to Location-Allocation</a></span><ul class="toc-item"><li><span><a href="#ArcGIS-Network-Analyst" data-toc-modified-id="ArcGIS-Network-Analyst-1.4.1"><span class="toc-item-num">1.4.1&nbsp;&nbsp;</span>ArcGIS Network Analyst</a></span></li><li><span><a href="#Location-Allocation-Analysis" data-toc-modified-id="Location-Allocation-Analysis-1.4.2"><span class="toc-item-num">1.4.2&nbsp;&nbsp;</span>Location-Allocation Analysis</a></span></li><li><span><a href="#The-solve_location_allocation-tool" data-toc-modified-id="The-solve_location_allocation-tool-1.4.3"><span class="toc-item-num">1.4.3&nbsp;&nbsp;</span>The <code>solve_location_allocation</code> tool</a></span></li></ul></li><li><span><a href="#Analysis-Preparation" data-toc-modified-id="Analysis-Preparation-1.5"><span class="toc-item-num">1.5&nbsp;&nbsp;</span>Analysis Preparation</a></span></li><li><span><a href="#Analysis:-Where-could-we-add-one-new-ALS-clinic-to-reach-the-greatest-number-of-ALS-patients-who-currently-have-poor-access-to-an-existing-clinic?" data-toc-modified-id="Analysis:-Where-could-we-add-one-new-ALS-clinic-to-reach-the-greatest-number-of-ALS-patients-who-currently-have-poor-access-to-an-existing-clinic?-1.6"><span class="toc-item-num">1.6&nbsp;&nbsp;</span>Analysis: Where could we add one new ALS clinic to reach the greatest number of ALS patients who currently have poor access to an existing clinic?</a></span></li><li><span><a href="#Analysis:-Where-could-we-add-new-ALS-clinics-to-reach-50%-of-patients-who-currently-must-drive-for-over-90-minutes-to-reach-their-nearest-clinic?" data-toc-modified-id="Analysis:-Where-could-we-add-new-ALS-clinics-to-reach-50%-of-patients-who-currently-must-drive-for-over-90-minutes-to-reach-their-nearest-clinic?-1.7"><span class="toc-item-num">1.7&nbsp;&nbsp;</span>Analysis: Where could we add new ALS clinics to reach 50% of patients who currently must drive for over 90 minutes to reach their nearest clinic?</a></span></li></ul></li><li><span><a href="#Conclusions" data-toc-modified-id="Conclusions-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Conclusions</a></span></li></ul></div>

# ## Clinic access for the chronically ill
# 
# Location is everything for the chronically ill.  For patients with amyotrophic lateral sclerosis (ALS), visits to clinics are exhausting full-day engagements involving sessions with highly trained specialists from several disciplines.  Patients with long drives to their nearest clinic may also face the additional hardship of having to plan for travel days to and from the clinic as well as for food and lodging.  
# 
# This notebook demonstrates how ArcGIS can perform network analysis to identify potential sites for new ALS clinics in California to improve access for patients who do not live near a clinic.
# 
# <blockquote><b>Note:</b> The examples in this notebook is intended to serve only as a technology demonstration.  The analysis results should not be used for any planning purposes as the data for ALS patient locations are fictional.  The ALS clinic locations were obtained from data that was publically available in October 2017.</blockquote>

# ## Access the analysis data

# In[ ]:


# Import the ArcGIS API for Python
from arcgis import *
from IPython.display import display

# Connect to the web GIS.
gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# In[ ]:


# Search for content tagged with 'ALS'
search_als = gis.content.search('tags: ALS', 'Feature Layer')
for item in search_als:
    display(item)


# ## Map the challenge
# 
# Many ALS patients in California have to drive for over 90 minutes to reach their nearest clinic.  For the purposes of this notebook, those patients are considered to have poor clinic access. The next block of code displays a map that displays the locations of those ALS patients who have poor clinic access.  
# 
# Map Legend:
# 
# <p>Locations of ALS patients that must travel for over 90 minutes to access a clinic.
# <p>Existing ALS clinics.
# <p>90 minute drive times from existing ALS clinics.
# <p>Candidate cities for new ALS clinics.

# In[ ]:


# Display the titles of the items returned by the search.
for item in search_als:
    print(item.title)


# In[2]:


# Create a map of California.
map1 = gis.map("State of California, USA")
map1.basemap = 'dark-gray'
display(map1)


# In[ ]:


# Add the ALS content to the map.
for item in search_als:
    if item.title == 'ALS_Clinics_CA':
        als_clinics = item       
    if item.title == 'ALS_Patients_CA':
        patient_locations = item        
    if item.title == 'ALS_Clinic_City_Candidates_CA':
        city_candidates = item    
    elif item.title == 'ALS_Clinic_90minDriveTime_CA':
        drive_times = item
        
map1.add_layer(drive_times)
map1.add_layer(als_clinics)
map1.add_layer(patient_locations)
map1.add_layer(city_candidates)


# ## Introduction to Location-Allocation
# ### ArcGIS Network Analyst
# 
# [ArcGIS Network Analyst](http://pro.arcgis.com/en/pro-app/help/analysis/networks/what-is-network-analyst-.htm) helps organizations run their operations more efficiently and improve strategic decision making by performing analysis on the travel costs between their facilities and demand locations to answer questions such as:
# 
# * What is the quickest way to get from point A to point B?
# * What houses are within five minutes of our fire stations?
# * What markets do our businesses cover?
# * Where should we open a new branch of our business to maximize market share?
# 
# To address these questions, ArcGIS Network Analyst creates origin-destination (OD) cost matrices along a travel network such as roads, to find solutions to reduce the overall costs of travel.
# 
# ![title](http://esri.github.io/arcgis-python-api/notebooks/nbimages/04_als_odcost.png)
# 
# To perform network analysis, you need a network dataset, which models a transportation network.  In the ArcGIS API for Python, the network dataset is accessed through a network service hosted in ArcGIS Online or Portal for ArcGIS.
# 
# ### Location-Allocation Analysis
# 
# The goal of [location-allocation](http://pro.arcgis.com/en/pro-app/help/analysis/networks/location-allocation-analysis-layer.htm) is to locate facilities in a way that supplies the demand points most efficiently.  As the name suggests, location-allocation is a two-fold problem that simultaneously locates facilities and allocates demand points to the facilities.
# 
# ![title](http://esri.github.io/arcgis-python-api/notebooks/nbimages/04_als_location_allocation_schematic.png)
# 
# ### The `solve_location_allocation` tool
# 
# In this notebook, we will use the [solve_location_allocation](http://esri.github.io/arcgis-python-api/apidoc/html/arcgis.network.analysis.html#solve-location-allocation) tool to find the best locations for new ALS clinics in California.  Inputs to this tool include [FeatureSet](http://esri.github.io/arcgis-python-api/apidoc/html/arcgis.features.toc.html#featureset)s containing the following data:
# * facilities
# * demand points.  
# 
# For the examples in this notebook, the facilities are a set of candidate locations for new ALS clinics.  These locations could be actual addresses.  However, for these examples, the candidate locations are cities that could potentially host new ALS clinics and are represented by their centroid points. The candidate cities were pre-selected based on the following criteria:
# * they are within California
# * they are outside of the 90 minute drive time areas from existing ALS clinics
# * they have populations of at least 60,000, and are therefore assumed to have sufficient health care facilities and professionals to support an ALS clinic
# 
# The analyses in this notebook could lead to further multi-criteria analysis to identify specific  locations within a city or other geographic area.  For examples of Jupyter Notebooks with multi-criteria suitability analysis see [Calculating cost surfaces using weighted overlay analysis](https://developers.arcgis.com/python/sample-notebooks/calculating-cost-surfaces-using-weighted-overlay-analysis/) and [Finding suitable spots for placing heart defibrillator equipments in public](https://developers.arcgis.com/python/sample-notebooks/finding-suitable-spots-for-aed-devices-using-raster-analytics/)
# 
# For the demand points, we will use point locations of fictional ALS patients.  These locations are aggregated to zip code centroids and contain an estimated number of ALS patients based on the total population within each zip code.
# 
# The output of the tool is a named tuple which contains the following data:
# * solve_succeeded (`bool`)
# * output_facilities (`FeatureSet`)
# * output_demand_points (`FeatureSet`)
# * output_allocation_lines (`FeatureSet`)
# 
# To prepare to run the `solve_location_allocation` tool we will first perform the following steps:
# 1. Extract the required input `FeatureSets` for the tool from `FeatureLayerCollection` items in the GIS.
# 2. Define a function to extract, symbolize, and display the output results from the `solve_location_allocation` tool.

# ## Analysis Preparation

# In[ ]:


# Extract the required input data for the analyses.

# ALS clinic city candidates FeatureSet
city_candidates_fset = city_candidates.layers[0].query()

# ALS patient locations FeatureSet
patient_locations_fset = patient_locations.layers[0].query()

# Display the ALS patients FeatureSet in a pandas Dataframe
patient_locations_sdf = patient_locations_fset.sdf
patient_locations_sdf.head()


# Note that the `num_patients` field contains the numbers of ALS patients at each location. We can use the `num_patients` field to "weight" the analysis in favor of candidate cities with the greatest numbers of patients near them.
# 
# To accomplish this, add a column called `weight` to the `SpatialDataFrame` object and remove the old `num_patients` column.

# In[ ]:


# create the 'weight' column
patient_locations_sdf['weight'] = patient_locations_sdf['num_patients']

# drop the 'num_patients' column
patient_locations_sdf2 = patient_locations_sdf.drop('num_patients', axis=1)

# convert SpatialDataFrame back to a FeatureSet
patient_locations_weighted = patient_locations_sdf2.spatial.to_featureset()
patient_locations_weighted.sdf.head()


# In[ ]:


# Define a function to display the output analysis results in a map
import time

def visualize_locate_allocate_results(map_widget, solve_locate_allocate_result, zoom_level):
    # The map widget
    m = map_widget
    # The locate-allocate analysis result
    result = solve_locate_allocate_result
    
    # 1. Parse the locate-allocate analysis results
    # Extract the output data from the analysis results
    # Store the output points and lines in pandas dataframes
    demand_df = result.output_demand_points.sdf
    lines_df = result.output_allocation_lines.sdf

    # Extract the allocated demand points (patients) data.
    demand_allocated_df = demand_df[demand_df['DemandOID'].isin(lines_df['DemandOID'])]
    demand_allocated_fset = features.FeatureSet.from_dataframe(demand_allocated_df)

    # Extract the un-allocated demand points (patients) data.
    demand_not_allocated_df = demand_df[~demand_df['DemandOID'].isin(lines_df['DemandOID'])]
    demand_not_allocated_fset = features.FeatureSet.from_dataframe(demand_not_allocated_df)

    # Extract the chosen facilities (candidate clinic sites) data.
    facilities_df = result.output_facilities.sdf[['Name', 'FacilityType', 
                                                 'Weight','DemandCount', 'DemandWeight', 'SHAPE']]
    facilities_chosen_df = facilities_df[facilities_df['FacilityType'] == 3]
    facilities_chosen_fset = features.FeatureSet.from_dataframe(facilities_chosen_df)

    # 2. Define the map symbology
    # Allocation lines
    allocation_line_symbol_1 = {'type': 'esriSLS', 'style': 'esriSLSSolid',
                                'color': [255,255,255,153], 'width': 0.7}

    allocation_line_symbol_2 = {'type': 'esriSLS', 'style': 'esriSLSSolid',
                                'color': [0,255,197,39], 'width': 3}

    allocation_line_symbol_3 = {'type': 'esriSLS', 'style': 'esriSLSSolid',
                                'color': [0,197,255,39], 'width': 5}
    
    allocation_line_symbol_4 = {'type': 'esriSLS', 'style': 'esriSLSSolid',
                                'color': [0,92,230,39], 'width': 7}
    
    # Patient points within 90 minutes drive time to a proposed clinic location.
    allocated_demand_symbol = {"angle":0,"xoffset":0,"yoffset":0,"type":"esriPMS",
                                "url":"https://static.arcgis.com/images/Symbols/Firefly/FireflyA7.png",
                                "contentType":"image/png","width":24,"height":24}

    # Patient points outside of a 90 minutes drive time to a proposed clinic location.
    unallocated_demand_symbol = {"angle":0,"xoffset":0,"yoffset":0,"type":"esriPMS",
                                "url":"https://static.arcgis.com/images/Symbols/Firefly/FireflyB3.png",
                                "contentType":"image/png","width":24,"height":24}

    # Selected clinic
    selected_facilities_symbol = {"angle":0,"xoffset":0,"yoffset":0,"type":"esriPMS",
                                 "url":"https://static.arcgis.com/images/Symbols/State-Government/State-Health-Clinics.png",
                                 "contentType":"image/png","width":24,"height":24}
    
    # 3. Display the analysis results in the map
    # Add a slight delay for drama. 
    time.sleep(1.5)  
    # Display the patient-clinic allocation lines.
    m.draw(shape=result.output_allocation_lines, symbol=allocation_line_symbol_4)
    m.draw(shape=result.output_allocation_lines, symbol=allocation_line_symbol_2)
    m.draw(shape=result.output_allocation_lines, symbol=allocation_line_symbol_1)
 
    # Display the locations of patients within the specified drive time to the selected clinic(s).
    m.draw(shape=demand_allocated_fset, symbol=allocated_demand_symbol)

    # Display the locations of patients outside the specified drive time to the selected clinic(s).
    m.draw(shape = demand_not_allocated_fset, symbol = unallocated_demand_symbol)

    # Display the chosen clinic site.
    m.draw(shape=facilities_chosen_fset, symbol=selected_facilities_symbol)

    # Zoom out to display all of the allocated patients points.
    m.zoom = zoom_level


# ## Analysis: Where could we add one new ALS clinic to reach the greatest number of ALS patients who currently have poor access to an existing clinic?
# 
# To answer this question, we will use the **"Maximize Coverage"** problem type.  This problem type chooses facilities such that as many demand points as possible are allocated to facilities within the impedance cutoff.  In this example, the impedance cutoff is defined as a drive time of 90 minutes.

# In[ ]:


# Identify the city which has the greatest number of patients within a 90 minute drive time.
result1 = network.analysis.solve_location_allocation(problem_type='Maximize Coverage',
    travel_direction='Demand to Facility',
    number_of_facilities_to_find='1',
    demand_points=patient_locations_weighted,
    facilities=city_candidates_fset,
    measurement_units='Minutes',
    default_measurement_cutoff=90
)
print('Analysis succeeded? {}'.format(result1.solve_succeeded))


# In[ ]:


# Display the analysis results in a pandas dataframe.
result1.output_facilities.sdf[['Name', 'FacilityType', 'DemandCount', 'DemandWeight']]


# The selected facility, i.e. ALS city candidate, is assigned the value "3" in the `FacilityType` field.  The `DemandCount` and `DemandWeight` fields indicate the number of demand points (patient locations) and total number of patients at those locations which are allocated to the facility.
# 
# Now, let us visualize the results on a map.

# In[3]:


# Display the analysis results in a map.

# Create a map of Visalia, California.
map2 = gis.map('Visalia, CA')
map2.basemap = 'dark-gray'
display(map2)

# Call custom function defined earlier in this notebook to 
# display the analysis results in the map.
visualize_locate_allocate_results(map2, result1, zoom_level=7)


# ## Analysis: Where could we add new ALS clinics to reach 50% of patients who currently must drive for over 90 minutes to reach their nearest clinic?
# 
# To answer this question we will use the **"Target Market Share"** problem type.  This problem type chooses the minimum number of facilities necessary to capture a specific percentage of the total market share.

# In[ ]:


# Identify where to open new clinics that are within a 90 minute drive time
# of 50% of ALS patients who currently have to drive for over 90 minutes to reach their nearest clinic.

result2 = network.analysis.solve_location_allocation(
    problem_type='Target Market Share', 
    target_market_share=70,
    facilities=city_candidates_fset, 
    demand_points=patient_locations_weighted,
    travel_direction='Demand to Facility',
    measurement_units='Minutes', 
    default_measurement_cutoff=90
)

print('Solve succeeded? {}'.format(result2.solve_succeeded))


# In[ ]:


# Display the analysis results in a table.
result2.output_facilities.sdf[['Name', 'FacilityType', 'DemandCount', 'DemandWeight']]


# The `solve_location_allocation` tool selected two of the candidate cities to host new ALS clinics. If ALS clinics are established in both of those cities, 50% of the patients who must currently drive for over 90 minutes to reach an existing clinic would be able to access one of the new clinics in 90 minutes or less.
# 
# Now, let us visualize the results on a map.

# In[4]:


# Display the analysis results in a map.

# Create a map of Visalia, California
map3 = gis.map('Visalia, CA', zoomlevel=7)
map3.basemap = 'dark-gray'

# Display the map and add the analysis results to it
from IPython.display import display
display(map3)
visualize_locate_allocate_results(map3, result2, zoom_level=6)


# # Conclusions
# 
# The ArcGIS Network Analysis toolset is a versatile set of tools that assist strategic decision making to reduce the costs of travel.  This examples in this notebook only scratch the surface of the situations where you could use these tools.  By performing your analysis using Python and the Jupyter Notebook, you also document your methodology in a form that can be easily shared via email to colleagues and stake holders, enabling them to reproduce your results or repeat the analyses in an iterative fashion with different parameters or different input datasets.  You can also [export](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html) the output tables from the solve_location_allocation tool by to CSV files to share with stake holders via email.


# ====================
# image_captioning_using_deep_learning.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Image Captioning Using Deep Learning

# * 🔬 Data Science
# * 🥠 Deep Learning and Image Captioning

# ## Table of Contents
# * [Introduction and objective](#Introduction-and-objective)
# * [Necessary imports](#Necessary-imports)
# * [Prepare data that will be used for training](#Prepare-data-that-will-be-used-for-training)
# * [Model training](#Model-training)
#  * [Visualize training data](#Visualize-training-data)
#  * [Load model architecture](#Load-model-architecture)
#  * [Train the model](#Train-the-model)
#  * [Visualize results on validation set](#Visualize-results-on-validation-set)
#  * [Evaluate model performance](#Evaluate-model-performance)
#  * [Save the model](#Save-the-model) 
#  * [Prediction on test image](#Prediction-on-test-image)
# * [Model inference](#Model-inference)
# * [Results](#Results)
# * [Conclusion](#Conclusion)
# * [References](#References)

# ## Introduction and objective

# Image caption, a concise textual summary that describes the content of an image, has applications in numerous fields such as scene classification, virtual assistants, image indexing, social media, for visually impaired persons and more. 
# Deep learning has been achieving superhuman level performance in computer vision tasks ranging from object detection to natural language processing. [`ImageCaptioner`](https://developers.arcgis.com/python/guide/how-image-captioning-works/), which is a combination of both image and text, is a deep learning model that generates image captions of remote sensing image data.
# 
# This sample shows how ArcGIS API for Python can be used to train `ImageCaptioner` model using Remote Sensing Image Captioning Dataset (RSICD) [1]. It is a publicly available dataset for remote sensing image captioning task. RSICD contains more than ten thousands remote sensing images which are collected from Google Earth, Baidu Map, MapABC and Tianditu. The images are fixed to 224X224 pixels with various resolutions. The total number of remote sensing images is 10921, with five sentences descriptions per image. The below screenshot shows an example of this data:

# 

# The trained model can be deployed on `ArcGIS Pro` or `ArcGIS Enterprise` to generate captions on a high satellite resolution imagery. 

# ## Necessary imports

# In[1]:


from pathlib import Path
import os, json

from arcgis.learn import prepare_data, ImageCaptioner
from arcgis.gis import GIS


# In[2]:


gis = GIS('home')


# ## Prepare data that will be used for training

# We need to put the RSICD dataset in a specific format, i.e., a root folder containing a folder named "*images*" and the JSON file containing the annotations named "*annotations.json*". The specific format of the json can be seen [here](https://github.com/201528014227051/RSICD_optimal/blob/master/dataset_rsicd.json). 

# <figure>
#     <br>
#     <center>
#     <figcaption>Folder structure for RSICD dataset. A root folder containing "<i>images</i>" folder and "<i>annotations.json</i>" file.</figcaption>
#     </center>
# </figure>

# ## Model training

# Let's set a path to the folder that contains training images and their corresponding labels.

# In[3]:


training_data = gis.content.get('8c4fc46930a044a9b20bb974d667e074')
training_data


# In[4]:


filepath = training_data.download(file_name=training_data.name)


# In[5]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[6]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# We'll use the `prepare_data` function to create a databunch with the necessary parameters such as `batch_size`, and `chip_size`. A complete list of parameters can be found in the [API reference](https://esri.github.io/arcgis-python-api/apidoc/html/arcgis.learn.html#prepare-data).

# In[7]:


data = prepare_data(data_path, 
                    chip_size=224,
                    batch_size=4,
                    dataset_type='ImageCaptioning')


# ### Visualize training data

# To visualize and get a sense of the training data, we can use the `data.show_batch` method.

# In[8]:


data.show_batch()


# ### Load model architecture

# `arcgis.learn` provides us image captioning model which are based on pretrained convnets, such as ResNet, that act as the backbones. We will use `ImageCaptioner` with the backbone parameters as `Resnet50` to create our image captioning model. For more details on `ImageCaptioner` check out [How image_captioning works?](https://developers.arcgis.com/python/guide/how-image-captioning-works/) and the [API reference](https://developers.arcgis.com/python/api-reference/arcgis.learn.toc.html#imagecaptioner).

# In[9]:


ic = ImageCaptioner(data, backbone='resnet50')


# We will use the `lr_find()` method to find an optimum learning rate. It is important to set a learning rate at which we can train a model with good accuracy and speed.

# In[10]:


lr = ic.lr_find()


# ### Train the model

# We will now train the `ImageCaptioner` model using the suggested learning rate from the previous step. We can specify how many epochs we want to train for. Let's train the model for 100 epochs. 

# In[11]:


ic.fit(100, lr, early_stopping=True)


# ### Visualize results on validation set

# To see sample results we can use the `show_results` method. This method displays the chips from the validation dataset with ground truth (left) and predictions (right). This visual analysis helps in assessing the qualitative results of the trained model.

# In[12]:


ic.show_results()


# ### Evaluate model performance

# To see the quantitative results of our model we will use the `bleu_score` method. Bilingual Evaluation Understudy Score(BLEU’s): is a popular metric that measures the number of sequential words that match between the predicted and the ground truth caption. It compares n-grams of various lengths from 1 through 4 to do this. A perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0.  summarizes how close the generated text is to the expected text.

# In[13]:


ic.bleu_score()


# ### Save the model

# Let's save the model by giving it a name and calling the `save` method, so that we can `load` it later whenever required. The model is saved by default in a directory called `models` in the `data_path` initialized earlier, but a custom path can be provided.

# In[14]:


ic.save('image-captioner-33epochs')


# ### Prediction on test image

# We can perform inferencing on a small test image using the `predict` function. 

# In[15]:


ic.predict(r'\image-captioner\test_img.tif')


# Now that we are satisfied with the model performance on a test image, we are ready to perform model inferencing on our desired images. In our case, we are interested in inferencing on high resolution satellite image.

# ## Model inference

# Before using the model for inference we need to make some changes in the model_name>.emd file. You can learn more about this file [here](https://github.com/Esri/raster-deep-learning/blob/master/docs/writing_model_definition.md).
# 
# By default, CropSizeFixed is set to 1. We want to change the CropSizeFixed to 0 so that the size of tile cropped around the features is not fixed. the below code will edit the emd file with CropSizeFixed:0 information.

# In[ ]:


with open(
    os.path.join(data_path, "models", "image-captioner-33epochs", "image-captioner-33epochs" + ".emd"), "r+"
) as emd_file:
    data = json.load(emd_file)
    data["CropSizeFixed"] = 0
    emd_file.seek(0)
    json.dump(data, emd_file, indent=4)
    emd_file.truncate()


# In order to perform inferencing in `ArcGIS Pro`, we need to create a feature class on the map using `Create Feature Class` or `Create Fishnet` tool.
# 
# The Feature Class and the trained model has been provided for reference. You could directly download these files to run perform model inferencing on desired area.

# - Feature Class : ["link"](https://www.arcgis.com/home/item.html?id=a037a6fdd3254208bb4d0b14d31fe970)
# - Image Captioner Model: ["Link"](https://www.arcgis.com/home/item.html?id=a4c1a0bc68794aeba60becf57f8b4a0e)

# 

# ```python
# with arcpy.EnvManager(extent="-13049125.3076102 4033595.5228646 -13036389.0790898 4042562.3896354", cellSize=1, processorType="GPU"):
# arcpy.ia.ClassifyObjectsUsingDeepLearning("Inferencing_Image", r"C:\Users\Admin\Documents\ImgCap\captioner.gdb\Classified_ImageCaptions", r"D:\image-captioner-33epochs\image-captioner-33epochs.emd", "California_Features", '', "PROCESS_AS_MOSAICKED_IMAGE", "batch_size 1;beam_width 5;max_length 20", "Caption")
# ```

# ## Results

# We selected an area unseen (by the model) and generated some features using the `Create Feature Class` tool. We then used our model to generate captions. Below are the results that we have achieved.

# 

# ## Conclusion

# In this notebook, we demonstrated how to use the `ImageCaptioner` model from the `ArcGIS API for Python` to generate image captions using RSICD as training data.

# ## References

# - [1] Xiaoqiang Lu, Binqiang Wang, Xiangtao Zheng, Xuelong Li: “Exploring Models and Data for Remote Sensing Image Caption Generation”, 2017; <a href='http://arxiv.org/abs/1712.07835'>arXiv:1712.07835</a>. DOI: <a href='https://ieeexplore.ieee.org/document/8240966'>10.1109/TGRS.2017.2776321</a>.
# - https://developers.arcgis.com/python/guide/how-image-captioning-works/


# ====================
# image_scene_classification_using_feature_classifier.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Image scene classification using FeatureClassifier
# 
# > * 🔬 Data Science
# > * 🥠 Deep Learning and Object classification

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Necessary imports](#Necessary-imports)
# * [Download & setting up training data](#Download-&-setting-up-training-data)
# * [Train the model](#Train-the-model)
#  * [Prepare data](#Prepare-data)
#  * [Visualize training data](#Visualize-training-data)
#  * [Load model architecture](#Load-model-architecture)
#  * [Find an optimal learning rate](#Find-an-optimal-learning-rate)
#  * [Fit the model](#Fit-the-model)
#  * [Visualize results in validation set](#Visualize-results-in-validation-set)
#  * [Accuracy assessment](#Accuracy-assessment)
#  * [Save the model](#Save-the-model)
# * [Model inference](#Model-inference)
# * [Results](#Results)
# * [Conclusion](#Conclusion)

# ## Introduction

# In this sample notebook, we will be using the `ArcGIS API for Python` for training an object classification model on image data from an external source and using that model for inferencing in ArcGIS Pro.
# 
# For this example, we will be using the [RESISC45 Dataset](https://arxiv.org/abs/1703.00121), which is a publicly available benchmark for Remote Sensing Image Scene Classification (RESISC) created by Northwestern Polytechnical University (NWPU). This dataset contains 31,500 images covering 45 scene classes, with 700 images in each class.
# 
# We will be using this dataset to train a `FeatureClassifier` model that will classify satellite image tiles in the 45 scene classes specified in the dataset.
# 
# 

# ## Necessary imports

# In[1]:


import os, json
from arcgis.learn import prepare_data, FeatureClassifier


# ## Download & setting up training data 

# Since the [RESISC45 Dataset](https://arxiv.org/abs/1703.00121) is publically available, we will download the data from the [Tensorflow website](https://www.tensorflow.org/datasets/catalog/resisc45).
# The name of the dataset we will be downloading is <b>NWPU-RESISC45.rar</b> <br>

# After the data has been downloaded, follow the steps below to prepare the model for the `FeatureClassifier`. 
# - Extract the .rar file
# - Create a folder named <b>images</b> and move all the 45 folders (correspoding to each class in the dataset) into the images folder

# Next, we will create an data_path variable containing the path of the images folder.

# In[2]:


data_path = os.path.join(os.getcwd(), "NWPU-RESISC45")


# ## Train the model

# `arcgis.learn` provides the ability to determine the class of each feature in the form of a `FeatureClassifier` model. To learn more about how it works and its potential use cases, see this guide - ["How feature classifier works?"](https://developers.arcgis.com/python/guide/how-feature-categorization-works/).
# 

# ### Prepare data

# Here, we will specify the path to our training data and a few hyperparameters.
# 
# - `path`: path of the folder/list of folders containing training data.
# - `dataset_type` : The type of dataset getting passed to the Feature Classifier.
# - `batch_size`: Number of images your model will train on each step inside an epoch. <b>This directly depends on the memory of your graphic card</b>. 128 worked for us on a 32GB GPU.
# 
# Since we are using the dataset from external source for training our `FeatureClassifier`, we will be using <b>Imagenet</b> as `dataset_type`.

# In[3]:


data = prepare_data(
    path=data_path, dataset_type="Imagenet", batch_size=128, val_split_pct=0.2
)


# ### Visualize training data

# To get a sense of what the training data looks like, the `show_batch()` method randomly picks a few training chips and visualizes them.
# - `rows`: Number of rows to visualize

# In[4]:


data.show_batch(rows=5)


# ### Load model architecture

# In[5]:


model = FeatureClassifier(data, oversample=True)


# ### Find an optimal learning rate

# [Learning rate](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) is one of the most important hyperparameters in model training. The `ArcGIS API for Python` provides a learning rate finder that automatically chooses the optimal learning rate for you.

# In[6]:


lr = model.lr_find()


# ### Fit the model 

# We will train the model for a few epochs with the learning rate we have found. For the sake of time, we can start with 20 epochs.

# In[7]:


model.fit(20, lr=lr)


# Here only after 20 epochs both training and validation losses have decreased considerably, indicating that the model is learning to classify image scenes.

# ### Visualize results in the validation set

# It is a good practice to see the results of the model viz-a-viz ground truth. The code below picks random samples and shows us ground truth and model predictions side by side. This enables us to preview the results of the model within the notebook.

# In[8]:


model.show_results(rows=4)


# Here, with only 20 epochs, we can see reasonable results.

# ### Accuracy assessment

# `arcgis.learn` provides the `plot_confusion_matrix()` function that plots a confusion matrix of the model predictions to evaluate the model's accuracy.

# In[9]:


model.plot_confusion_matrix()


# The confusion matrix validates that the trained model is learning to classify scenes to different classes. The diagonal numbers show the number of scenes correctly classified as their respective categories.

# ### Save the model

# Now, we will save the model that we trained as a 'Deep Learning Package' ('.dlpk' format). A Deep Learning package is the standard format used to deploy deep learning models on the ArcGIS platform.
# 
# We will use the `save()` method to save the trained model. By default, it will be saved to the 'models' sub-folder within our training data folder.

# In[13]:


model_name = "Nwpu_model1"
model.save(model_name)


# ## Model inference

# Before using the model for inference, we need to make some changes in the <b>model_name.emd</b> file. You can learn more about this file [here](https://github.com/Esri/raster-deep-learning/blob/master/docs/writing_model_definition.md).

# By default, in the EMD file, the `CropSizeFixed` is set to 1. We need to change the `CropSizeFixed` to 0 so that the size of tiles cropped around the feature are not fixed.

# In[22]:


with open(
    os.path.join(data_path, "models", model_name, model_name + ".emd"), "r+"
) as emd_file:
    data = json.load(emd_file)
    data["CropSizeFixed"] = 0
    emd_file.seek(0)
    json.dump(data, emd_file, indent=4)
    emd_file.truncate()


# For us to perform inferencing in ArcGIS Pro, we need to create a feature class on the map using either the `Create Feature Class` tool or the `Create Fishnet` tool, for an area that has not already seen by the model.
# 
# We have also provided the Feature Class and the Model trained on the NWPU Dataset for reference. You can directly download these to run your own experiments from the links below.
# - [Feature Class](https://pythonapi.playground.esri.com/portal/home/item.html?id=bb6645d6c7664a4c9d878d03e611107d)
# - [NWPU Model](https://pythonapi.playground.esri.com/portal/home/item.html?id=5e18839f2fee4449ad9b486c0bf2cbb0)
# 

# Now, we will use the `Classify Objects Using Deep Learning tool` for inferencing the results. The parameters required to run the function are:
# 
# - `Input Raster`: High_Resolution_Imagery
# - `Input Features`: Output from the `Create Feature Class` or `Create Fishnet` tool.
# - `Output CLassified Objects Feature Class`: Output feature class.
# - `Model Definition`: Emd file of the model that we trained.
# - `Class Label Field`: Field name that will contain the detected class number.
# - `Environments`: Set optimum `Cell Size`, `Processing Extent` and `Processor Type`.
# 
# We have investigated and found that a `Cell Size` of 1m/pixel works best for this model.  

# 

# ## Results 

# We selected an area that had not been seen by the model and generated the features in it using the `Create Feature Class` tool. We then used our model for classification. Below are the results.

# 

# We also created a fishnet using the `Create Fishnet` tool that we then fed to our model for classification. We can use this technique to create preliminary data about the image. Based on the output, we can make inferences about the image, such as the total of residential areas, industrial areas in an image, etc. Below is the map that we created from the results.

# 

# ## Conclusion 

# In this notebook, we demonstrated how to use the `FeatureClassifier` model from the `ArcGIS API for Python` to classify image scenes using training data from an external source.

# ## References

# - Citation : `@article{cheng2017remote,
#   title={Remote sensing image scene classification: Benchmark and state of the art},
#   author={Cheng, Gong and Han, Junwei and Lu, Xiaoqiang},
#   journal={Proceedings of the IEEE},
#   volume={105},
#   number={10},
#   pages={1865--1883},
#   year={2017},
#   publisher={IEEE}
# }`


# ====================
# increase-image-resolution-using-superresolution.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Increase Image Resolution using SuperResolution

# ## Table of Contents
# * [Introduction](#2)
# * [Export Training Data](#3)
# * [Model Training](#4)
#     * [Necessary Imports](#5)
#     * [Visualize training data](#6)
#     * [Train a model](#7)
#     * [Visualize results on validation set](#8)
#     * [Save the model](#9)
# * [Deploy the Model](#10)
#     * [Prediction and Upscaling](#11)
#     * [Model inference on ArcGIS Pro](#12)
# * [Conclusion](#13)
# * [References](#14)

# ## Prerequisites

# - Please refer to the prerequisites section in our [guide](https://developers.arcgis.com/python/guide/geospatial-deep-learning) for more information. This sample demonstrates how to export training data and model inference using [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview). Alternatively, they can be done using [ArcGIS Image Server](https://www.esri.com/en-us/arcgis/products/arcgis-image-server) as well.
# - If you have already exported training samples using ArcGIS Pro, you can jump straight to the training section. The saved model can also be imported into ArcGIS Pro directly.

# ## Introduction <a class="anchor" id="2"></a>

# High resolution imagery is desirable for both visualization and image interpretation. However, high resolution imagery is expensive to procure. This sample notebook demonstrates how the `SuperResolution` model in `arcgis.learn` module can be used to increase image resolution. This model uses deep learning to add texture and detail to low resolution satellite imagery and turn it into higher resolution imagery.
# 
# We first start with high resolution aerial imagery to train the model. The data preparation step first downsamples the higher resolution imagery to create lower resolution, blurred imagery. The `SuperResolution` model uses this training data and learns how to upsample the lower resolution imagery and produce realistic high resolution images that closely resemble the higher quality images that we started with. We then use the trained `SuperResolution` model to produce simulated high resolution aerial imagery from relatively lower resolution satellite imagery.

# ## Export Training Data <a class="anchor" id="3"></a>

# We will be using ArcGIS Pro to find the area where high resolution imagery. To simplify our job, we have already created polygon representing the extent of high resolution imagery. We can add the polygon from [here](https://www.arcgis.com/home/item.html?id=f783fe21206a4492b9fe42f585ebc13c#overview).
# 
# Training data can be exported by using the [Export Training Data For Deep Learning](https://desktop.arcgis.com/en/arcmap/latest/tools/spatial-analyst-toolbox/export-training-data-for-deep-learning.htm) tool available in [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview) as well as [ArcGIS Image Server](https://enterprise.arcgis.com/en/).
# 
# - **Input Raster:** Imagery
# - **Image Format:** JPEG Format
# - **Tile Size X & Tile Size Y:** 512
# - **Meta Data Format:** Export Tiles
# - In 'Environments' tab set an optimum 'Cell Size' (0.1 in our case). 
# - Set the extent same as the polygon layer which we have added.

# ```python
# arcpy.ia.ExportTrainingDataForDeepLearning("Imagery", r"C:\sample\Data\Hi_res_superres_data", "JPEG", 512, 512, 0, 0, "Export Tiles", 0, "ecode", 75, None, 0)
# ```
# 
# 
# 
# After filling all details and running the `Export Training Data For Deep Learning` tool, a code like above will be generated and executed. That will create all the necessary files needed for the next step in the 'Output Folder', and we will now call it our **training data**.

# 

# ## Model Training <a class="anchor" id="4"></a>

# We will train our model using `arcgis.learn` module within ArcGIS API for Python. `arcgis.learn` contains tools and deep learning capabilities required for this study. A detailed documentation to install and setup the environment is available [here](https://developers.arcgis.com/python/guide/install-and-set-up/).

# ### Necessary Imports <a class="anchor" id="5"></a>

# In[1]:


import os
from pathlib import Path

from arcgis.gis import GIS
from arcgis.learn import SuperResolution, prepare_data


# We will now use the `prepare_data()` function to apply various types of transformations and augmentations on the training data. These augmentations enable us to train a better model with limited data and also prevent the model from overfitting.  
# `prepare_data()`: Takes 4 parameters:  
# - `path`: Path of folder containing training data.  
# - `batch_size`: No. of images your model will train on each step inside an epoch, it directly depends on the memory of your graphic card, size of the images you are training on and the type of model which you are working with. For this sample a batch size of **8** worked for us on a GPU with 8GB memory.  
# - `dataset_type`: To infer the supported dataset type, in our case 'superres'.
# - `downsample_factor`: Factor to degrade the quality of image by resizing and adding compression artifacts in order to create labels.  
# **Note:The quality of degraded image should be similar to the image on which we are going to do inferencing.**
# 
# This function returns a data object which can be fed into a model for training.

# In[2]:


gis = GIS('home')


# In[3]:


training_data = gis.content.get('abc0812aa82c4fe681662e5ba495b6b8')
training_data


# In[4]:


filepath = training_data.download(file_name=training_data.name)


# In[5]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[6]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# In[ ]:


data = prepare_data(data_path, 
                    batch_size=8, 
                    dataset_type="superres", 
                    downsample_factor=8)


# ### Visualize training data <a class="anchor" id="6"></a>

# To make sense of training data we will use the `show_batch()` method in `arcgis.learn`. `show_batch()` randomly picks few samples from the training data and visualizes them.

# In[ ]:


data.show_batch()


# The imagery chips above show images which we have been downsampled in `prepare_data` and corresponding high resolution images with them.
# `data.show_batch()` shows a batch of images from our training data. We can visualize the the low resolution training data generated using `prepare_data` function on left along with the original data on the right. You can degrade the image quality more by increasing `downsample_factor` in `prepare_data`.

# `arcgis.learn` provides the `SuperResolution` model for increasing image resolution, which is based on a pretrained convnet, like `ResNet` that acts as the 'backbone'. 

# In[ ]:


superres_model = SuperResolution(data)


# We will use the `lr_find()` method to find an optimum learning rate. It is important to set a learning rate at which we can train a model with good accuracy and speed.

# In[ ]:


superres_model.lr_find()


# ### Train a model <a class="anchor" id="7"></a>

# We will now train the `SuperResolution model` using the suggested learning rate from the previous step. We can specify how many epochs we want to train for. Let's train the model for 10 epochs.

# In[ ]:


superres_model.fit(10, lr=0.0001584893192461114)


# After the training is complete, we can view the plot with training and validation losses.

# In[ ]:


superres_model.learn.recorder.plot_losses()


# ### Visualize results on validation set <a class="anchor" id="8"></a>

# This method displays the chips from the validation dataset with downsampled chips (left), predicted chips (middle) and ground truth (right). This visual analysis helps in assessing the qualitative results of the trained model.

# In[ ]:


superres_model.show_results()


# ### Save the model <a class="anchor" id="9"></a>

# Let's save the model by giving it a name and calling the `save()` method, so that we can load it later whenever required. By default, the model gets saved in training/data/models directory, however a custom path can optionally be provided to save model at desired location.

# In[ ]:


superres_model.save("superres_model")


# ## Deploy the Model <a class="anchor" id="10"></a>

# ### Prediction and Upscaling <a class="anchor" id="11"></a>

# We can upscale and improve the details of a single image chip with the help of `predict()` method.
# 
# Using `predict` function, we can apply the trained model on image to increase its resolution while also upscaling. It can be done by additionally providing `height` and `width` parameters.
# 

# In[ ]:


superres_model.predict(img_path=r"arcgis/home/data/super-res-test-img.jpeg", height=256, width=256)


# In the above step, we are upscaling an image with dimensions 32x32 to 256x256.
# 
# The model is not only generating a high resolution image, but is also removing the [compression artifacts](https://en.wikipedia.org/wiki/Compression_artifact) from a low resolution image chip.
# 
# **Note**: The model is not reconstructing your photo exactly as it would have been in upscaled version. It is hallucinating the perceptual details based on its training from example images. 

# ### Model inference on ArcGIS Pro <a class="anchor" id="12"></a>

# We will use saved model to improve the image quality using `Classify Pixels Using Deep Learning` tool available in both ArcGIS Pro and ArcGIS Image Server. For this sample we will deploy the model in Redlands,CA region where low resolution imagery is available.
# 
# - **Input Raster**: Imagery
# - **Output Classified Raster**: Superres_Inferencing_arcgis
# - **Model Definition**: model.emd
# - **padding**: The 'Input Raster' is tiled and the deep learning model runs on each individual tile separately before producing the final result. This may lead to unwanted artifacts along the edges of each tile as the model has little context to detect objects accurately. Padding as the name suggests allows us to supply some extra information along the tile edges, this helps the model to predict better.
# - **Cell size**: The model is very susceptible to the cell size. The cell size should be same as high resolution imagery we used to train the model which is 0.1 in our case.

# 

# `Classify Pixels Using Deep Learning` returns a raster layer. We have published the output from this sample here as a hosted [raster layer](https://www.arcgis.com/home/webmap/viewer.html?basemapUrl=https://tiles.arcgis.com/tiles/SMX5BErCXLM7eDtY/arcgis/rest/services/Inferenced_Superresolution_redlands/MapServer?cacheKey=a226e612302d3cdb).
# <br>
# <br>

# 

# # Conclusion <a class="anchor" id="13"></a>

# In this notebook, we demonstrated how to use SuperResolution model using ArcGIS API for Python in order to obtain high-resolution image from a low-resolution satellite imagery.

# ## References <a class="anchor" id="14"></a>

# [1] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for realtime style transfer and super-resolution”, 2016; [arXiv:1603.08155](https://arxiv.org/abs/1603.08155).  
# [2] Fast.ai [lesson 7](https://course.fast.ai/videos/?lesson=7).


# ====================
# information-extraction-from-madison-city-crime-incident-reports-using-deep-learning.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Information extraction from Madison city crime incident reports using Deep Learning

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc">
# <ul class="toc-item">
# <li><span><a href="#Introduction" data-toc-modified-id="Introduction-1">Introduction</a></span></li>
# <li><span><a href="#Prerequisites" data-toc-modified-id="Prerequisites-2">Prerequisites</a></span></li>
# <li><span><a href="#Imports" data-toc-modified-id="Imports-3">Imports</a></span></li>
# <li><span><a href="#Data-preparation" data-toc-modified-id="Data-preparation-4">Data preparation</a></span></li>
# <li><span><a href="#EntityRecognizer-model" data-toc-modified-id="EntityRecognizer-model-5">EntityRecognizer model</a></span></li>
# <ul class="toc-item">
# <li><span><a href="#Finding-optimum-learning-rate" data-toc-modified-id="Finding-optimum-learning-rate-5.1">Finding optimum learning rate</a></span>    
# <li><span><a href="#Model-training" data-toc-modified-id="Model-training-5.2">Model training</a></span>
# <li><span><a href="#Evaluate-model-performance" data-toc-modified-id="Evaluate-model-performance-5.3">Evaluate model performance</a></span>
# <li><span><a href="#Validate-results" data-toc-modified-id="Validate-results-5.4">Validate results</a></span></li>
# <li><span><a href="#Save-and-load-trained-models" data-toc-modified-id="Save-and-load-trained-models-5.5">Save and load trained models</a></span></li>
# </ul>
# <li><span><a href="#Model-inference" data-toc-modified-id="Model-inference-6">Model inference</a></span></li>
# <li><span><a href="#Publishing-the-results-as-feature-layer" data-toc-modified-id="Publishing-the-results-as-feature-layer-7">Publishing the results as feature layer</a></span></li>
# <li><span><a href="#Visualize-crime-incident-on-map" data-toc-modified-id="Visualize-crime-incident-on-map- 8">Visualize crime incident on map</a></span></li>
# <li><span><a href="#Create-a-hot-spot-map-of-crime-densities" data-toc-modified-id="Create-a-hot-spot-map-of-crime-densities-9">Create a hot spot map of crime densities</a></span></li>
# <li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-10">Conclusion</a></span></li>
# <li><span><a href="#References" data-toc-modified-id="References-11">References</a></span></li>
# </ul></div>

# # Introduction
# 
# Crime analysis is an essential part of efficient law enforcement for any city. It involves:
# -	Collecting data in a form that can be analyzed. 
# -	Identifying spatial/non-spatial patterns and trends in the data. 
# -	Informed decision making based on the analysis.
# 
# In order to start the analysis, the first and foremost requirement is analyzable data. A huge volume of data is present in the witness and police narratives of the crime incident. Few examples of such information are:
# -	Place of crime
# -	Nature of crime
# -	Date and time of crime
# -	Suspect
# -	Witness
# 
# Extracting such information from incident reports requires tedious work. Crime analysts have to sift through piles of police reports to gather and organize this information.
# 
# With recent advancements in Natural Language Processing and Deep learning, it's possible to devise an automated workflow to extract information from such unstructured text documents. In this notebook we will extract information from crime incident reports obtained from Madison police department [[1]](#References)using `arcgis.learn`'s **EntityRecognizer** class.

# # Prerequisites
# 
# - **Data preparation** and **model training workflows** using `arcgis.learn` is based on [spaCy](https://spacy.io/usage/linguistic-features#named-entities) & [Hugging Face Transformers](https://huggingface.co/transformers/v3.0.2/index.html) libraries. A user can choose an appropriate backbone / library to train his/her model.  
# - Refer to the section **"Install deep learning dependencies of arcgis.learn module"** [on this page](https://developers.arcgis.com/python/guide/install-and-set-up/#Install-deep-learning-dependencies) for detailed documentation on installation of the dependencies.
# - **Labelled data**: In order for `EntityRecognizer` to learn, it needs to see examples that have been labelled for all the custom categories that the model is expected to extract. Labelled data for this sample notebook is located at `data/EntityRecognizer/labelled_crime_reports.json`.
# - To learn how to use **Doccano**[[2]](#References) for labelling text, please see the guide on [Labeling text using Doccano](https://developers.arcgis.com/python/guide/labeling-text-using-doccano/).
# - Test documents to extract named entities are in a zipped file at `data/EntityRecognizer/reports.zip`.
# - To learn more on how `EntityRecognizer` works, please see the guide on [Named Entity Extraction Workflow with arcgis.learn](https://developers.arcgis.com/python/guide/how-named-entity-recognition-works/).

# # Necessary Imports

# In[3]:


import pandas as pd
import zipfile,unicodedata
from itertools import repeat
from pathlib import Path
from arcgis.gis import GIS
from arcgis.learn import prepare_textdata
from arcgis.learn.text import EntityRecognizer
from arcgis.geocoding import batch_geocode
import re
import os
import datetime


# In[2]:


gis = GIS('home')


# # Data preparation
# 
# Data preparation involves splitting the data into training and validation sets, creating the necessary data structures for loading data into the model and so on.  The `prepare_data()` function can directly read the training samples in one of the above specified formats and automate the entire process.

# In[3]:


training_data = gis.content.get('b2a1f479202244e798800fe43e0c3803')
training_data


# In[4]:


filepath = training_data.download(file_name=training_data.name)


# In[5]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[6]:


json_path = Path(os.path.join(os.path.splitext(filepath)[0] , 'labelled_crime_reports.json'))


# In[7]:


data = prepare_textdata(path= json_path, task="entity_recognition", dataset_type='ner_json', class_mapping={'address_tag':'Address'})


# The `show_batch()` method can be used to visualize the training samples, along with labels.

# In[8]:


data.show_batch()


# # EntityRecognizer model
# 
# `EntityRecognizer` model in `arcgis.learn` can be used with spaCy's [EntityRecognizer](https://spacy.io/api/entityrecognizer) backbone or with [Hugging Face Transformers](https://huggingface.co/transformers/v3.0.2/index.html) backbones
# 
# Run the command below to see what backbones are supported for the **entity recognition** task.

# In[9]:


print(EntityRecognizer.supported_backbones)


# Call the model's `available_backbone_models()` method with the **backbone** name to get the available models for that backbone. The call to **available_backbone_models** method will list out only few of the available models for each backbone. Visit [this](https://huggingface.co/transformers/pretrained_models.html) link to get a complete list of models for each of the transformer backbones. To know more choosing an appropriate **transformer** model for your dataset, visit [this](https://developers.arcgis.com/python/guide/how-named-entity-recognition-works/#How-to-choose-an-appropriate-model-for-your-dataset?) link
# 
# **Note** - Only a single model is available to train `EntityRecognizer` model with `spaCy` backbone

# In[10]:


print(EntityRecognizer.available_backbone_models("spacy"))


# First we will create model using the `EntityRecognizer()` constructor and passing it the `data` object.

# In[11]:


ner = EntityRecognizer(data, backbone="spacy")


# ## Finding optimum learning rate

# The learning rate[[3]](#References) is a tuning parameter that determines the step size at each iteration while moving toward a minimum of a loss function, it represents the speed at which a machine learning model **"learns"**. `arcgis.learn` includes learning rate finder, and is accessible through the model's `lr_find()` method, that can automatically select an optimum learning rate, without requiring repeated experiments.

# In[12]:


lr = ner.lr_find()


# ## Model training

# **Training the model** is an iterative process. We can train the model using its `fit()` method till the [F1 score](https://en.wikipedia.org/wiki/F1_score) (maximum possible value = 1) continues to improve with each training pass, also known as epoch. This is indicative of the model getting better at predicting the correct labels.

# In[13]:


ner.fit(epochs=30, lr=lr)


# ## Evaluate model performance

# Important metrics to look at while measuring the performance of the `EntityRecognizer` model are **Precision**, **Recall** & **F1-measures** [[4]](#References). 

# In[14]:


ner.precision_score()


# In[15]:


ner.recall_score()


# In[16]:


ner.f1_score()


# To find **precision**, **recall** & **f1** scores per label/class we will call the model's `metrics_per_label()` method.

# In[17]:


ner.metrics_per_label()


# ## Validate results
# 
# Now we have the trained model, let's look at how the model performs.

# In[18]:


ner.show_results()


# ## Save and load trained models
# 
# Once you are satisfied with the model, you can save it using the `save()` method. This creates an Esri Model Definition (EMD file) that can be used for inferencing on new data. 
# Saved models can also be loaded back using the `load()` method. `load()` method takes the path to the emd file as a required argument.

# In[19]:


ner.save('crime_model')


# # Model Inference
# 
# Now we can use the trained model to extract entities from new text documents using `extract_entities()` method. This method expects the folder path of where new text document are located, or a list of text documents.

# In[20]:


reports = os.path.join(filepath.split('.')[0] , 'reports')


# In[21]:


results = ner.extract_entities(reports) #extract_entities()also accepts path of the documents folder as an argument.


# In[22]:


results.head()


# # Publishing the results as a feature layer
# 
# The code below geocodes the extracted address and publishes the results as a feature layer.

# In[23]:


# This function generates x,y coordinates based on the extracted location from the model.

def geocode_locations(processed_df, city, region, address_col):
    #creating address with city and region
    add_miner = processed_df[address_col].apply(lambda x: x+f', {city} '+f', {region}') 
    chunk_size = 200
    chunks = len(processed_df[address_col])//chunk_size+1
    batch = list()
    for i in range(chunks):
        batch.extend(batch_geocode(list(add_miner.iloc[chunk_size*i:chunk_size*(i+1)])))
    batch_geo_codes = []
    for i,item in enumerate(batch):
        if isinstance(item,dict):
            if (item['score'] > 90 and 
                    item['address'] != f'{city}, {region}'
                    and item['attributes']['City'] == f'{city}'):
                batch_geo_codes.append(item['location'])
            else:
                batch_geo_codes.append('')    
        else:
            batch_geo_codes.append('') 
    processed_df['geo_codes'] = batch_geo_codes    
    return processed_df



# In[24]:


#This function converts the dataframe to a spatailly enabled dataframe.

def prepare_sdf(processed_df):
    processed_df['geo_codes_x'] = 'x'
    processed_df['geo_codes_y'] = 'y'
    for i,geo_code in processed_df['geo_codes'].iteritems():
        if geo_code == '': 
            processed_df.drop(i, inplace=True) #dropping rows with empty location
        else:
            processed_df['geo_codes_x'].loc[i] = geo_code.get('x')
            processed_df['geo_codes_y'].loc[i] = geo_code.get('y')
    
    sdf = processed_df.reset_index(drop=True)
    sdf['geo_x_y'] = sdf['geo_codes_x'].astype('str') + ',' +sdf['geo_codes_y'].astype('str')
    sdf = pd.DataFrame.spatial.from_df(sdf, address_column='geo_x_y') #adding geometry to the dataframe
    sdf.drop(['geo_codes_x','geo_codes_y','geo_x_y','geo_codes'], axis=1, inplace=True) #dropping redundant columns
    return sdf


# In[25]:


#This function will publish the spatical dataframe as a feature layer.

def publish_to_feature(df, gis, layer_title:str, tags:str, city:str, 
                       region:str, address_col:str):
    processed_df = geocode_locations(df, city, region, address_col)
    sdf = prepare_sdf(processed_df)
    try:        
        layer = sdf.spatial.to_featurelayer(layer_title, gis,tags) 
    except:
        layer = sdf.spatial.to_featurelayer(layer_title, gis, tags)

    return layer    


# In[26]:


# This will take few minutes to run
madison_crime_layer = publish_to_feature(results, gis, layer_title='Madison_Crime' + str(datetime.datetime.now().microsecond), 
                                         tags='nlp,madison,crime', city='Madison', 
                                         region='WI', address_col='Address')


# In[27]:


madison_crime_layer


# # Visualize crime incident on map

# In[28]:


result_map = gis.map('Madison, Wisconsin')
result_map.basemap = 'topographic'


# In[29]:


result_map


# In[30]:


result_map.add_layer(madison_crime_layer)


# # Create a hot spot map of crime densities
# 
# ArcGIS has a set of tools to help us identify, quantify and visualize spatial patterns in our data by identifying areas of statistically significant clusters.
# 
# The [`find_hot_spots`](https://developers.arcgis.com/rest/services-reference/find-hot-spots.htm) tool allows us to visualize areas having such clusters.

# In[31]:


from arcgis.features.analyze_patterns import find_hot_spots


# In[32]:


crime_hotspots_madison = find_hot_spots(madison_crime_layer, 
                                        context={"extent":
                                                 {"xmin":-10091700.007046243,"ymin":5225939.095608932,
                                                  "xmax":-9731528.729766665,"ymax":5422840.88047145,
                                                  "spatialReference":{"wkid":102100,"latestWkid":3857}}},
                                        output_name="crime_hotspots_madison" + str(datetime.datetime.now().microsecond))


# In[33]:


hotspot_map = gis.map('Madison, Wisconsin')
hotspot_map.basemap = 'terrain'


# In[34]:


hotspot_map


# In[35]:


hotspot_map.add_layer(crime_hotspots_madison)
hotspot_map.legend = True


# # Conclusion
# 
# This sample demonstrates how `EntityRecognizer()` from `arcgis.learn` can be used for information extraction from crime incident reports, which is an essential requirement for crime analysis. Then, we see how can this information be geocoded and visualized on a map for further analysis.

# # References
# 
# [1]: [Police Incident Reports(City of Madison)](https://www.cityofmadison.com/police/newsroom/incidentreports/)
# 
# [2]: [Doccano : text annotation tool for humans](https://github.com/doccano/doccano)
# 
# [3]: [Learning rate](https://en.wikipedia.org/wiki/Learning_rate)
# 
# [4]: [Precision, recall and F1-measures](https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-and-f-measures)


# ====================
# infrastructural_damage_due_to_blast_in_beirut.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Mapping Infrastructural Damage due to Beirut Blast

# ## Table of Contents
# 
# * [Introduction](#1)
# * [Necessary imports](#2)
# * [Connect to your GIS](#3)
# * [Get the data for analysis](#4)
# * [Methodology](#5)
# * [Prepare data for analysis](#6)
#     * [Create geometry of aoi ](#7)
#     * [Filter out sentinel-2 tiles](#8)
#     * [Extract bands](#9)
# * [Generate water bodies mask](#10)
#     * [Create normalized difference water index raster](#11)
#     * [Create binary raster](#12)
#     * [Extract water polygons](#13)
#     * [Masking out noise area](#14)
# * [Extract damaged area](#15)
#     * [Get Normalized difference building index raster](#16)
#     * [Create continous built-up raster](#17)
#     * [Create difference raster](#18)
#     * [Get the damaged area](#19)
#     * [Hotspots analysis](#20)
# * [Visualize results](#21)
#     * [Interpretation of results](#23)
# * [Conclusion](#24)
# * [Literature resources](#25)

# ## Introduction <a class="anchor" id="1"></a>

# On August 4, 2020, a fire started in Warehouse 12 near the grain silos at The port of Beirut located on the northern Mediterranean coast. The warehouse containing around 2700 tonnes of ammonium nitrate (highly explosive chemical used in manufacturing of fertilizer) exploded creating a mushroom cloud causing a huge explosion followed by few small explosions. Approximately 200 people died, more than 6,000 people were injured, and property worth US$15 billion was damaged making 300,000 people homeless.
# 
# The shockwaves were felt in parts of Europe, Israel, Palestine, Syria and Turkey. The blast was heard in Cyprus, approximately 250 km away from Beirut. The shockwaves of magnitude 3.3, caused due to explosion, were detected by the seismograph of United States Geological Survey. According to [reports](https://www.standard.co.uk/news/world/beirut-explosion-one-of-largest-blasts-history-a4517646.html), the Beirut explosion is considered as one of the most powerful non-nuclear explosions in history.
# 
# Performing field surveys after explosions can be risky and extremely difficult. A great alternative is to use satellite remote sensing data. Satellite data is able to cover the spatial extent of damage caused due to explosion.
# 
# This study uses Sentinel-2 multispectral data to map the extent of infrastructural damage. The spatial resolution of Sentinel-2 is 10m which covers infrastructural details better than Landsat with 30m spatial resolution.

# ## Neccessary Imports <a class="anchor" id="2"></a>

# In[1]:


import arcgis
from arcgis import *
from datetime import datetime
import pandas as pd
from arcgis.features import GeoAccessor, GeoSeriesAccessor
from arcgis.raster.analytics import convert_feature_to_raster, convert_raster_to_feature
from arcgis.raster.functions import greater_than, clip, apply
from arcgis.features.analysis import dissolve_boundaries
from ipywidgets import HBox, VBox, Label, Layout


# ## Connect to your GIS <a class="anchor" id="3"></a>

# In[2]:


from arcgis import GIS
agol_gis = GIS("home")
ent_gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# ## Get the data for analysis <a class="anchor" id="4"></a>

# [Sentinel-2 Views](https://www.arcgis.com/home/item.html?id=fd61b9e0c69c4e14bebd50a9a968348c) is used in the analysis: this multitemporal layer consists 13 bands with 10, 20, and 60m spatial resolution.. The imagery layer is rendered on-the-fly and available for visualization and analytics.  This imagery layer pulls directly from the Sentinel-2 on AWS collection and is updated daily with new imagery.

# In[3]:


s2 = agol_gis.content.get('fd61b9e0c69c4e14bebd50a9a968348c')
sentinel = s2.layers[0]
s2


# In[4]:


aoi1 =  agol_gis.content.search('title:beirut_aoi', 'Feature Layer Collection')[0]
aoi1


# ## Prepare data for analysis <a class="anchor" id="6"></a>

# `Sentinel-2 Views` imagery layers consists data for whole world. The first step is to filter out the before and after explosion data of the study area for this analysis.

# ### Create geometry of area of interest (AOI) <a class="anchor" id="7"></a>

# The geometry of AOI is created for filtering out the Sentinel-2 tiles for the study area.

# In[5]:


aoi_layer = aoi1.layers[0]
aoi_feature = aoi_layer.query()
aoi_geom = aoi_feature.features[0].geometry
aoi_geom['spatialReference'] = {'wkid':3857}


# ### Filter out sentinel-2 tiles  <a class="anchor" id="8"></a>

# In[6]:


m = gis2.map('Beirut', 14)
m


# 

# In[7]:


m.zoom_to_layer(aoi1)
m.basemap='satellite'


# #### Before blast

# Imagery of 2 days were filtered, Tile for July 24, 2020 represents the before explosion scenario.

# In[8]:


from datetime import datetime
selected = sentinel.filter_by(where="(Category = 1)",
                             time=[datetime(2020, 7, 20), datetime(2020, 8, 4)],
                             geometry=arcgis.geometry.filters.intersects(aoi_geom))
df = selected.query(out_fields="AcquisitionDate, GroupName, CloudCover, DayOfYear",
                   order_by_fields="AcquisitionDate").sdf

df['AcquisitionDate'] = pd.to_datetime(df['acquisitiondate'], unit='ms')
df


# In[9]:


s2_bb = apply(sentinel, 'Natural Color with DRA')
s2_bb.mosaic_by(lock_rasters=[11567718])
s2_bb.extent = m.extent
#s2_bb.save('s2_20200724_f', gis=gis2)
s2_bb = gis2.content.search('s2_20200724_f')[0]
s2_bb_lyr = s2_bb.layers[0]


# #### After blast
# Tile for August 08, 2020 represents the after blast scenerio of Beirut Port.

# In[10]:


selected = sentinel.filter_by(where="(Category = 1)",
                             time=[datetime(2020, 8, 5), datetime(2020, 8, 12)],
                             geometry=arcgis.geometry.filters.intersects(aoi_geom))
df = selected.query(out_fields="AcquisitionDate, GroupName, CloudCover, DayOfYear",
                   order_by_fields="AcquisitionDate").sdf

df['AcquisitionDate'] = pd.to_datetime(df['acquisitiondate'], unit='ms')
df


# In[11]:


s2_bb = apply(sentinel, 'Natural Color with DRA')
s2_bb.mosaic_by(lock_rasters=[11664437])
s2_bb.extent = m.extent
s2_bb.save('s2_20200808_f', gis=gis2)
s2_bb


# ## Generate water bodies mask<a class="anchor" id="10"></a>

# ### Create normalized difference water index raster<a class="anchor" id="11"></a>

# Normalized Difference Water Index (NDWI) is most suitable for mapping of water bodies, droughts, boundary evaluation. Water bodies have strong absorbability and low radiation within the visible to infrared spectral ranges and NDWI is based on this phenomenon. Default rendering function of  Normalized Difference Water Index (NDWI) is computed as Green(Band03)-NIR(Band08)/ Green(Band03)+NIR(Band08). To get the [NDWI Raw](https://www.arcgis.com/home/item.html?id=112db40d3640473aacb0d1f891462496) raster for the corresponding Sentinel-2 tile `apply` function was used.
# Rendering (or display) of band combinations and calculated indices is done on-the-fly from the source images via Raster Functions. 

# In[12]:


ndwi1 = apply(sentinel, 'NDWI Raw')
ndwi1.mosaic_by(lock_rasters=[11567718])
ndwi1.extent = m.extent
#ndwi_lyr = ndwi1.save('ndwi_lyr1')
ndwi1


# ### Create binary raster<a class="anchor" id="12"></a>

# The binary rasters were converted to feature layer for extracting the boundaries of water bodies. [greater_than](https://pro.arcgis.com/en/pro-app/help/data/imagery/greater-than-function.htm) function was used to create a binary raster where pixels with value greater than 0.08 were assigned as 1 and others pixels were assigned value of 0.

# In[13]:


water_mask = greater_than([ndwi1, 0.08],
                          extent_type='FirstOf', 
                          cellsize_type='FirstOf', 
                          astype='U16')
water_mask


# ### Extract water polygons<a class="anchor" id="13"></a>

# In the feature layer 'gridcode=0' represents non water class and 'gridcode=1' represents water class. Water polygons were selected using the `query` function from the dataframe of feature layer. A new feature layer was created using `gis.content.import` function which will only consist the water polygons.

# In[14]:


water_poly = convert_raster_to_feature(water_msk.layers[0], 
                                       field='Value', 
                                       output_type='Polygon', 
                                       simplify=True, 
                                       output_name=None, 
                                       gis=gis2)


# In[15]:


dfm=water_poly.layers[0].query('gridcode=0').sdf 
nwater_poly=gis2.content.import_data(dfm, title='nwpoly22212')


# ### Masking out noise area<a class="anchor" id="14"></a>
# 
# The first row of pixel along the coast has very high reflectance which is considered as noise and can manipulate the results. [create_buffer](https://developers.arcgis.com/python/api-reference/arcgis.features.analysis.html#create-buffers) function was used to remove the noise area. Sentinel-2 spatial resolution is 10 m, so negative buffer of 10m was created.

# In[16]:


buffer = arcgis.create_buffers(nwater_poly.layers[0], 
                               distances=[-10], 
                               field=None, 
                               units='Meters', 
                               dissolve_type='None', 
                               ring_type='Disks', 
                               side_type='Full', 
                               end_type='Round', 
                               output_name=None,
                               gis=gis2) 


# As the buffer polygons will be used for masking out the water pixels from the raster. [dissolve_boundaries](https://developers.arcgis.com/python/api-reference/arcgis.features.analysis.html#dissolve-boundaries) function was used to get a combined geometry of all polygons with `gridcode=0`.

# In[17]:


diss_f = dissolve_boundaries(buffer,
                             dissolve_fields=['gridcode'], 
                             output_name='dissolve_poly_f3', 
                             gis=gis2,  
                             multi_part_features=True)


# The geometry of `diss_f` was created for masking out the water pixels from the study area.

# In[18]:


aoi2_layer = diss_f.layers[0]
aoi2_feature = aoi2_layer.query(where='gridcode=0')
aoi2_geom = aoi2_feature.features[0].geometry
aoi2_geom['spatialReference'] = {'wkid':3857}


# ## Get the damaged area<a class="anchor" id="15"></a>

# ### Get Normalized Difference Built-Up Index rasters<a class="anchor" id="16"></a>
# The NDBI highlights urban areas with higher reflectance in the shortwave-infrared (SWIR) region, compared to the Near Infra-red (NIR) region. Applications include watershed runoff predictions, built-up mapping and land-use planning.
# The formula for computing Normalized Difference Built-Up Index is SWIR(Band11)-NIR(Band8)/ SWIR(Band11)+NIR(Band8). Sentinel-2 View `visual rendering` was used to get NDBI rasters. Rendering (or display) of band combinations and calculated indices is done on-the-fly from the source images via Raster Functions. To get the NDBI raster for the corresponding  Sentinel-2 tile `apply` function was used.

# In[19]:


ndbi1 = apply(sentinel, 'Normalized Difference Built-Up Index (NDBI)')
ndbi1.mosaic_by(lock_rasters=[11567718])
ndbi1.extent = m.extent
ndbi_r = ndbi1.save('s2_ndbi_20200724_f2', gis=gis2)
ndbi_lyr1 = ndbi_r.layers[0]


# In[20]:


ndbi2 = apply(sentinel, 'Normalized Difference Built-Up Index (NDBI)')
ndbi2.mosaic_by(lock_rasters=[11664437])
ndbi2.extent = m.extent
ndbi2_r = ndbi2.save('s2_ndbi_20200808_f2', gis=gis2)
ndbi_lyr2 = ndbi2_r.layers[0]


# In[21]:


ndbi_r = gis2.content.search('s2_ndbi_20200724_f')[0]
ndbi_lyr1 = ndbi_r.layers[0]
ndbi2_r = gis2.content.search('s2_ndbi_20200808_f')[0]
ndbi_lyr2 = ndbi2_r.layers[0]


# ## Create continous built-up raster<a class="anchor" id="17"></a>
# NDBI and NDWI rasters were used to extract the built-up areas using the following formula: `NDBI - NDWI`. In this raster the negative values will represent water pixels and positive values represent built-up pixels.

# In[22]:


## continous raster showing before blast scenerio
bb = ndbi_lyr1 - ndwi_lyr.layers[0]
bb_wwater = bb.save('bb_wwater')


# In[23]:


## continous raster showing after blast scenerio
ab = ndbi_lyr2 - ndwi_lyr.layers[0]
ab


# ### Create difference raster<a class="anchor" id="18"></a>

# The continuous rasters for before blast and after blast were subtracted to create a difference raster which shows the areas that have variation in NDBI value after the blast.

# In[24]:


difference_ras = bb - ab
diff_ras = difference_ras.save('diff_raster_f1', gis=gis2)
difference_ras


# As this analysis is focused on built-up damage, NDWI water mask was used to clip out the water pixels from the raster.

# In[25]:


clip_diff = clip(difference_ras, aoi2_geom)
clip_diff_ras = clip_diff.save("cl_diff_ras1", gis=gis2)
clip_diff_ras.layers[0]


# ### Get the damaged area<a class="anchor" id="19"></a>
# #### Get point feature layer
# 
# Hotspot analysis is necessary to get the damaged area polygon. [find_hot_spots](https://developers.arcgis.com/python/api-reference/arcgis.features.analysis.html#find-hot-spots) function requires point input layer. The raster was converted to point feature layer using [convert_raster_to_feature](https://enterprise.arcgis.com/en/portal/latest/use/geoanalytics-find-hot-spots.htm) function.

# In[26]:


b_pt = convert_raster_to_feature(clip_diff_ras.layers[0], 
                                        field='Value', 
                                        output_type='Point', 
                                        simplify=True, 
                                        output_name='contest_pt1', 
                                        gis=gis2)


# ### Hotspots analysis<a class="anchor" id="20"></a>
# [find_hot_spots](https://developers.arcgis.com/python/api-reference/arcgis.features.analysis.html#find-hot-spots) function was used to get the damaged area.

# In[27]:


hspot1 = arcgis.features.analysis.find_hot_spots(b_pt, 
                                                analysis_field='grid_code',
                                                output_name='wwater_hspots', 
                                                gis=gis2)


# The resulting point layer has both cold and hotspots. Points representing hotspots were filtered out using `query` on the data frame. Points with value greater than 1 in `gi_bin` column were taken and `gis.content.import` function was used to create a feature layer from the data frame. `gi_bin>1` represents Hot Spots and Negative values represents Cold Spots.

# In[28]:


dfm1 = hspot1.layers[0].query('gi_bin>1').sdf 
damg_pt = gis2.content.import_data(dfm1, title='hspots4')
dfm1


# The point layer was converted to raster using [convert_feature_to_raster](https://developers.arcgis.com/python/api-reference/arcgis.raster.analytics.html?highlight=convert_feature_to_raster#convert-feature-to-raster) function. Raster of 10m spatial resolution was created using the `gi_bin` column.

# In[29]:


dmg_ras = arcgis.raster.analytics.convert_feature_to_raster(damg_pt.layers[0],
                                                            {'distance':10,'units':'meters'}, 
                                                            value_field='gi_bin', 
                                                            output_name='damage_ras',  
                                                            gis=gis2)


# The `dmg_ras` was further converted to polygon for better vizualization of results. The `dmg_poly` layer was dissolved to get the extent layer which will show the built-ups which were affected by the explosion.

# In[30]:


dmg_ras = gis2.content.search('damage_ras')[0]
## Get the damaged polygon from raster
dmg_poly = convert_raster_to_feature(dmg_ras.layers[0], 
                                       field='Value', 
                                       output_type='Polygon', 
                                       simplify=True, 
                                       output_name=None, 
                                       gis=gis2)

## buffer was created of 10m
damg_poly = arcgis.create_buffers(dmg_poly, 
                               distances=[10], 
                               field=None, 
                               units='Meters', 
                               dissolve_type=None, 
                               ring_type='Disks', 
                               side_type='Full', 
                               end_type='Flat', 
                               output_name='damg_poly10',
                               gis=gis2)
## The polygons were dissolved to get the extent 
diss = arcgis.dissolve_boundaries(damg_poly,
                                  output_name='diss_poly7',
                                  gis=gis2,
                                  multi_part_features=True)


# ## Visualize results<a class="anchor" id="21"></a>

# #### Create map widget

# In[31]:


m1 = gis2.map('Beirut', 14)
m


# 

# #### Add data to map widget

# In[32]:


diss = gis2.content.search('diss_poly7')[0]
dfm4 = diss.layers[0].query().sdf 
dfm4


# In[33]:


dfm4.spatial.plot(map_widget=m1,
                   renderer_type='u',  
                   method='esriClassifyNaturalBreaks',  
                   class_count=2,  
                   col='objectid',  
                   cmap='Oranges',
                   outline_color=[0,0,0,0],
                   alpha=1
                   )


# ### Interpretation of results<a class="anchor" id="23"></a>

# The map widgets shows buildings which were damaged due to the explosion. Infrastructure on port was fully damaged due to the explosion and the buildings in the buffer area of 10 km were partially damaged. According to reports, approximately 7km infrastructure was damaged due to explosion.

# ## Conclusion<a class="anchor" id="24"></a>

# On August 4, 2020, one of history's most powerful non-nuclear explosion occurred at The Port of Beirut. The shockwaves were felt in parts of Europe, Israel, Palestine, Syria, Cyprus and USA.. In this study, Sentinel-2 multispectral data is used to map the extent of infrastructural damage caused due to the explosion. The results shows approximately 7 km area was affected by the blast. The same methodology can be used to estimate the extent of damage caused due to explosion for different regions with Sentinel-2 data using ArcGIS platform.

# ## Literature resources<a class="anchor" id="25"></a>

# |Literature | Source | Link |
# | -| - |-|
# | Article   | Beirut blast|https://www.bbc.com/news/world-middle-east-53656220|
# | Article   |Beirut Blast|https://www.standard.co.uk/news/world/beirut-explosion-one-of-largest-blasts-history-a4517646.html|


# ====================
# land_cover_classification_using_sparse_training_data.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Land cover classification using sparse training data

# ## Table of Contents
# * [Prerequisites](#Prerequisites)
# * [Introduction](#Introduction)
# * [Necessary imports](#Necessary-imports)
# * [Connect to your GIS](#Connect-to-your-GIS)
# * [Export training data](#Export-training-data)
# * [Train the model](#Train-the-model)
#  * [Prepare data](#Prepare-data)
#  * [Visualize training data](#Visualize-training-data)
#  * [Load model architecture](#Load-model-architecture)
#  * [Find an optimal learning rate](#Find-an-optimal-learning-rate)
#  * [Fit the model](#Fit-the-model)
#  * [Accuracy assessment](#Accuracy-assessment)
#  * [Visualize results in validation set](#Visualize-results-in-validation-set)
#  * [Save the model](#Save-the-model)
# * [Deployment and inference](#Deployment-and-inference)
# * [Conclusion](#Conclusion)

# ## Prerequisites 

# - Please refer to the prerequisites section in our [guide](https://developers.arcgis.com/python/guide/geospatial-deep-learning/) for more information. This sample demonstrates how to export training data and inference model using [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview). Alternatively, these can be done using [ArcGIS Image Server](https://www.esri.com/en-us/arcgis/products/arcgis-image-server) as well.
# - If you have already exported training samples using ArcGIS Pro, you can jump straight to the training section. The saved model can also be imported into ArcGIS Pro directly.
# - This notebook requires [ArcGIS API for Python](https://developers.arcgis.com/python/) version 1.8.1 or above.

# ## Introduction

# This notebook showcases an approach to performing land cover classification using sparse training data and multispectral imagery. We will demostrate the utility of methods including the `imagery_type` and `ignore_classes` available in `arcgis.learn` module to perform training.
# 
# 1) `imagery_type` parameter: The `prepare_data` function allows us to use imagery with any number of bands (4-band NAIP imagery in our case). This can be done by passing the imagery_type parameter. For more details, see [here](#Prepare-data).
# 
# 2) `ignore_classes` parameter: The Segmentation or Pixel Classification model allows us to ignore one/more classes from the training data while training the model. We will ignore the 'NoData' class in order to train our model on sparse data.
# 
# The image below shows a subset of our training data.

# 
# 

# Note: The training data does not cover the whole image (which is a requirement in older version (v<1.8.1)), hence it is called sparse training data.
# 
# This notebook demonstrates capabilities of ArcGIS API for Python to classify a 4-band NAIP imagery given sparse training data.

# ## Necessary imports

# In[1]:


import os
from pathlib import Path

from arcgis.gis import GIS
from arcgis.learn import prepare_data, UnetClassifier


# ## Connect to your GIS

# In[2]:


# Connect to GIS
gis = GIS('home')


# ## Export training data 

# We have labelled a few training polygons in the northern regions of Alabama state. The training data has the polygons labelled for six land cover classes namely 'buildings', 'roads and parking lots', 'water', 'harvested, open and bare lands', 'forest' and 'planted crops'. We need to classify NAIP imagery against these land cover classes. 
# 
# The feature class representing our labelled data contains an attribute field **classvalue** which represents the class of each labelled polygon. It is important to note here that all the pixels which are not labelled under any class will be mapped to 0, representing "NoData" class after exporting the data. 
# 
# This is how it looks:
# 

# Training data can be exported by using the `Export Training Data For Deep Learning` tool available in [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview) as well as [ArcGIS Image Server](https://www.esri.com/en-us/arcgis/products/arcgis-image-server).
# 
# - `Input Raster`: NAIP imagery
# - `Input Feature Class Or Classified Raster`: feature layer containing labeled polygons
# - `Class Value Field`: field containing class values against each class
# - `Tile Size X & Tile Size Y`: 256
# - `Stride X & Stride Y`: 64
# - `Meta Data Format`: 'Classified Tiles' as we are training an segmentation Model which in this case is `UnetClassifier`.
# - `Environments`: Set optimum `Cell Size`, `Processing Extent`.

# 
# 

# The labels to export data using [Export training data](#Export-training-data) tool are available as a layer package.

# In[3]:


labels = gis.content.get('eeec636c856740c6a3e2c185dda6e735')
labels


# ## Train the model

# We will be using U-Net, one of the well-recognized image segmentation algorithms, for our land cover classification. U-Net is designed like an auto-encoder. It has an encoding path (“contracting”) paired with a decoding path (“expanding”) which gives it the “U” shape. However, in contrast to the autoencoder, U-Net predicts a pixelwise segmentation map of the input image rather than classifying the input image as a whole. For each pixel in the original image, it asks the question: “To which class does this pixel belong?”. U-Net passes the feature maps from each level of the contracting path over to the analogous level in the expanding path. These are similar to residual connections in a `ResNet` type model, and allow the classifier to consider features at various scales and complexities to make its decision.

# ### Prepare data

# We will specify the path to our training data and a few hyperparameters.
# 
# - `path`: path of the folder containing training data.
# - `batch_size`: Number of images your model will train on each step inside an epoch, it directly depends on the memory of your graphic card. 8 worked for us on a 11GB GPU.
# - `imagery_type`: It is a mandatory input to enable a model for multispectral data processing. It can be "landsat8", "sentinel2", "naip", "ms" or "multispectral".

# In[4]:


training_data = gis.content.get('08ccd71f862940bda1d8350c8cc91a47')
training_data


# In[5]:


filepath = training_data.download(file_name=training_data.name)


# In[6]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[7]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# In[8]:


data = prepare_data(data_path, batch_size=8, imagery_type='naip')


# ### Visualize training data

# To get a sense of what the training data looks like, `arcgis.learn.show_batch()` method randomly picks a few training chips and visualizes them.
# - `rows`: Number of rows to visualize.

# In[9]:


data.show_batch(rows=3)


# ### Load model architecture

# As we now know, the 'NoData' class is mapped to '0' and we want to train our model on the six classes we had labeled, so we can put the 'NoData' class in `ignore_classes` parameter while creating a U-Net model. This parameter allows the model to skip all the pixels belonging to the mentioned class/classes, in other words model will not get trained on that class/classes.

# In[10]:


# Create U-Net Model
unet = UnetClassifier(data, backbone='resnet34', ignore_classes=[0])


# ### Find an optimal learning rate

# [Learning rate](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) is one of the most important hyperparameters in model training. ArcGIS API for Python provides a learning rate finder that automatically chooses the optimal learning rate for you.

# In[11]:


unet.lr_find()


# ### Fit the model 

# We will train the model for a few epochs with the learning rate we have found. For the sake of time, we can start with 10 epochs.

# In[12]:


unet.fit(10, 7.585775750291836e-05)


# Here, with only 10 epochs, we can see reasonable results — both training and validation losses have gone down considerably, indicating that the model is learning to classify land cover classes.

# ### Accuracy assessment

# As we have 6 different classes for this classification task, we need to do accuracy assessment for each of those. For that ArcGIS API for Python provides `per_class_metrics` function that will calculate precision and recall for each class.

# In[13]:


unet.per_class_metrics()


# Here, we can see the precision and recall values for each of the 6 classes are high with the model trained for just 10 epochs.

# ### Visualize results in validation set

# Its a good practice to see results of the model viz-a-viz ground truth. The code below picks random samples and shows us ground truth and model predictions, side by side. This enables us to preview the results of the model within the notebook.

# In[14]:


unet.show_results()


# ### Save the model

# We will save the model which we trained as a 'Deep Learning Package' ('.dlpk' format). Deep Learning package is the standard format used to deploy deep learning models on the ArcGIS platform.
# 
# We will use the `save()` method to save the trained model. By default, it will be saved to the 'models' sub-folder within our training data folder.

# In[15]:


unet.save('10e')


# ## Deployment and inference

# In this step, we will generate a classified raster using 'Classify Pixels Using Deep Learning' tool available in both ArcGIS Pro and ArcGIS Enterprise.
# 
# - `Input Raster`: The raster layer you want to classify.
# - `Model Definition`: It will be located inside the saved model in 'models' folder in '.emd' format.
# - `Padding`: The 'Input Raster' is tiled and the deep learning model classifies each individual tile separately before producing the final 'Output Classified Raster'. This may lead to unwanted artifacts along the edges of each tile as the model has little context to predict accurately. Padding as the name suggests allows us to supply some extra information along the tile edges, this helps the model to predict better.
# - `Cell Size`: Should be close to the size used to train the model. This was specified in the Export training data step.
# - `Processor Type`: This allows you to control whether the system's 'GPU' or 'CPU' will be used to classify pixels, by 'default GPU' will be used if available.

# 
# 

# Output of this tool will be a classified raster. Here below, you can see the NAIP image and its classified raster using the model we trained earlier.  

# 
# 

# ## Conclusion

# This sample showcases how pixel classifcation models like UnetClassifier can be trained using sparsely labeled data. Additionally, the sample shows how you can use multispectral imagery containing more than just the RGB bands to train the model. 


# ====================
# land_cover_classification_using_unet.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Land Cover Classification using Satellite Imagery and Deep Learning
# > * 🔬 Data Science
# * 🥠 Deep Learning and pixel-based classification

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Prerequisites" data-toc-modified-id="Prerequisites-1">Prerequisites</a></span></li><li><span><a href="#Introduction" data-toc-modified-id="Introduction-2">Introduction</a></span></li><li><span><a href="#Export-training-data-for-deep-learning" data-toc-modified-id="Export-training-data-for-deep-learning-3">Export training data for deep learning</a></span><ul class="toc-item"><li><span><a href="#Import-ArcGIS-API-for-Python-and-get-connected-to-your-GIS" data-toc-modified-id="Import-ArcGIS-API-for-Python-and-get-connected-to-your-GIS-3.1">Import ArcGIS API for Python and get connected to your GIS</a></span></li><li><span><a href="#Prepare-data-that-will-be-used-for-training-data-export" data-toc-modified-id="Prepare-data-that-will-be-used-for-training-data-export-3.2">Prepare data that will be used for training data export</a></span></li><li><span><a href="#Specify-a-folder-name-in-raster-store-that-will-be-used-to-store-our-training-data" data-toc-modified-id="Specify-a-folder-name-in-raster-store-that-will-be-used-to-store-our-training-data-3.3">Specify a folder name in raster store that will be used to store our training data</a></span></li><li><span><a href="#Export-training-data-using-arcgis.learn" data-toc-modified-id="Export-training-data-using-arcgis.learn-3.4">Export training data using <code>arcgis.learn</code></a></span></li></ul></li><li><span><a href="#Model-training" data-toc-modified-id="Model-training-4">Model training</a></span><ul class="toc-item"><li><span><a href="#Visualize-training-data" data-toc-modified-id="Visualize-training-data-4.1">Visualize training data</a></span></li><li><span><a href="#Load-model-architecture" data-toc-modified-id="Load-model-architecture-4.2">Load model architecture</a></span></li><li><span><a href="#Train-a-model-through-learning-rate-tuning-and-transfer-learning" data-toc-modified-id="Train-a-model-through-learning-rate-tuning-and-transfer-learning-4.3">Train a model through learning rate tuning and transfer learning</a></span></li><li><span><a href="#Visualize-classification-results-in-validation-set" data-toc-modified-id="Visualize-classification-results-in-validation-set-4.4">Visualize classification results in validation set</a></span></li></ul></li><li><span><a href="#Deployment-and-inference" data-toc-modified-id="Deployment-and-inference-5">Deployment and inference</a></span><ul class="toc-item"><li><span><a href="#Locate-model-package" data-toc-modified-id="Locate-model-package-5.1">Locate model package</a></span></li><li><span><a href="#Model-inference" data-toc-modified-id="Model-inference-5.2">Model inference</a></span></li></ul></li><li><span><a href="#Visualize-land-cover-classification-on-map" data-toc-modified-id="Visualize-land-cover-classification-on-map-6">Visualize land cover classification on map</a></span></li><li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-7">Conclusion</a></span></li><li><span><a href="#References" data-toc-modified-id="References-8">References</a></span></li></ul></div>

# ## Prerequisites
# 
# - Please refer to the prerequisites section in our [guide](https://developers.arcgis.com/python/guide/geospatial-deep-learning/) for more information. This sample demonstrates how to do export training data and model inference using [ArcGIS Image Server](https://www.esri.com/en-us/arcgis/products/arcgis-image-server). Alternatively, they can be done using [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview) as well.
# - If you have already exported training samples using ArcGIS Pro, you can jump straight to the training section. The saved model can also be imported into ArcGIS Pro directly.

# ## Introduction

# Land cover classification has been one of the most common tasks in remote sensing as it is the foundation for many global and environmental applications. Traditionally, people have been using algorithms like maximum likelihood classifier, SVM, random forest, and object-based classification. The recent success of AI brings new opportunity to this field. This notebook showcases an end-to-end to land cover classification workflow using ArcGIS API for Python. The workflow consists of three major steps: (1) extract training data, (2) train a deep learning **image segmentation** model, (3) deploy the model for inference and create maps. To better illustrate this process, we will use World Imagery and high-resolution labeled data provided by the [Chesapeake Conservancy land cover project](https://chesapeakeconservancy.org/conservation-innovation-center/high-resolution-data/land-cover-data-project/).
# <center> Figure 1. A subset of of the labeled data for Kent county, Delaware

# ## Export training data for deep learning

# ### Import ArcGIS API for Python and get connected to your GIS

# In[44]:


import os
from pathlib import Path

from arcgis import GIS
from arcgis.learn import UnetClassifier, prepare_data
from arcgis.raster import Raster


# In[67]:


gis = GIS('home')
ent_gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# ### Prepare data that will be used for training data export

# To export training data, we need a _labeled imagery layer_ that contains the class label for each location, and a _raster input_ that contains all the original pixels and band information. In this land cover classification case, we will be using a subset of the one-meter resolution Kent county, Delaware, dataset as the labeled imagery layer and World Imagery: Color Infrared as the raster input.

# In[3]:


label_layer = Raster("https://pythonapi.playground.esri.com/server/rest/services/KENT_10001_us8bit_cmap/ImageServer",
                     gis=ent_gis,
                     engine="image_server")


# Now let's retrieve the World Imagery layer.

# In[4]:


world_imagery_layer = Raster("https://pythonapi.playground.esri.com/server/rest/services/Imagery_AOI_NewYork/ImageServer",
                             gis=ent_gis,
                             engine="image_server")


# In[18]:


m = ent_gis.map("Kent county, Delaware", 15)
m


# In[6]:


m.add_layer(world_imagery_layer)


# In[19]:


m.add_layer(label_layer)


# ###  Specify a folder name in raster store that will be used to store our training data

# Make sure a raster store is ready on your raster analytics image server. This is where where the output subimages, also called chips, labels and metadata files are going to be stored.

# In[10]:


from arcgis.raster import analytics


# In[11]:


ds = analytics.get_datastores(gis=ent_gis)
ds


# In[12]:


ds.search(types="rasterStore")


# In[13]:


rasterstore = ds.get("/rasterStores/RasterDataStore")
rasterstore


# In[14]:


from datetime import datetime
samplefolder = "landcover_sample_world_imagery"+str(datetime.now().microsecond)
samplefolder


# In[15]:


output_folder = rasterstore.datapath + "/" + samplefolder
output_folder


# ### Export training data using `arcgis.learn`

# With the feature class and raster layer, we are now ready to export training data using the export_training_data() method in arcgis.learn module. In addtion to feature class, raster layer, and output folder, we also need to speficy a few other parameters such as tile_size (size of the image chips), strid_size (distance to move each time when creating the next image chip), chip_format (TIFF, PNG, or JPEG), metadata format (how we are going to store those training labels). More detail can be found [here](https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/export-training-data-for-deep-learning.htm). 
# 
# Depending on the size of your data, tile and stride size, and computing resources, this opertation can take 15mins~2hrs in our experiment. Also, do not re-run it if you already run it once unless you would like to update the setting.

# In[21]:


import arcgis
from arcgis import learn
arcgis.env.verbose = True


# In[26]:


ext = {'spatialReference': {'latestWkid': 3857, 'wkid': 102100},
       'xmin': -8449640.51886852,
       'ymin': 4685349.83980574,
       'xmax': -8352052.53352172,
       'ymax': 4782795.56861414}


# In[24]:


export = learn.export_training_data(input_raster=world_imagery_layer,
                                    output_location=output_folder,
                                    input_class_data=label_layer, 
                                    chip_format="TIFF", 
                                    tile_size={"x":400,"y":400}, 
                                    stride_size={"x":0,"y":0}, 
                                    metadata_format="Classified_Tiles",                                        
                                    context={"startIndex": 0, "exportAllTiles": False, "cellSize": 2, "extent":ext},
                                    gis = ent_gis)


# Now let's get into the raster store and look at what has been generated and exported.

# In[27]:


from arcgis.raster.analytics import list_datastore_content

samples = list_datastore_content(output_folder + "/images", filter = "*tif")
# print out the first five chips/subimages
samples[output_folder + "/images"][0:5]


# In[30]:


labels = list_datastore_content(output_folder + "/labels", filter = "*tif")
# print out the labels images for the first five chips
labels[output_folder + "/labels"][0:5]


# ## Model training

# If you've already done part 1, you should already have the training chips. Please change the path to your own export training data folder that contains "images" and "labels" folder.

# In[33]:


training_data = gis.content.get('e2d37d87c4f548b4b89b259b8373a904')
training_data


# In[34]:


filepath = training_data.download(file_name=training_data.name)


# In[35]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[36]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# In[38]:


data = prepare_data(data_path, batch_size=16)


# ### Visualize training data
# To get a sense of what the training data looks like, `arcgis.learn.show_batch()` method randomly picks a few training chips and visualize them.

# In[25]:


data.show_batch()


# ### Load model architecture
# We will be using U-net, one of the well-recogonized image segmentation algorithm, for our land cover classification. U-Net is designed like an auto-encoder. It has an encoding path (“contracting”) paired with a decoding path (“expanding”) which gives it the “U” shape. However, in contrast to the autoencoder, U-Net predicts a pixelwise segmentation map of the input image rather than classifying the input image as a whole. For each pixel in the original image, it asks the question: “To which class does this pixel belong?”. U-Net passes the feature maps from each level of the contracting path over to the analogous level in the expanding path.  These are similar to residual connections in a ResNet type model, and allow the classifier to consider features at various scales and complexities to make its decision.

# <center> </center>
# <center>Figure 2. Architecture of a Unet model [1]</center>

# In[26]:


model = UnetClassifier(data)


# ### Train a model through learning rate tuning and transfer learning
# [Learning rate](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) is one of the most important hyperparameters in model training. Here we explore a range of learning rate to guide us to choose the best one.

# In[51]:


lr = model.lr_find()


# Based on the learning rate plot above, we can see that the loss going down dramatically at 1e-4. Therefore, we set learning rate to be a range from 3e-5 to 1e-4, which means we will apply smaller rates to the first few layers and larger rates for the last few layers, and intermediate rates for middle layers, which is the idea of transfer learning. Let's start with 10 epochs for the sake of time.

# In[27]:


model.fit(10, lr=lr)


# ### Visualize classification results in validation set
# Now we have the model, let's look at how the model performs.

# In[28]:


model.show_results()


# As we can see, with only 10 epochs, we are already seeing reasonable results. Further improvment can be acheived through more sophisticated hyperparameter tuning. Let's save the model for further training or inference later. The model should be saved into a models folder in your folder. By default, it will be saved into your `data_path` that you specified in the very beginning of this notebook.

# ## Deployment and inference

# If you have finished trainiing the model in Part 2 of this notebook, you should have a model ready to be deployed. Using the `model.save()` function, you can save the model to the local disk. Additionally, if you pass `publish=True`, the model automatically gets published on the portal as a deep learning package. 

# In[ ]:


model.save('stage-1-10', publish=True, gis=ent_gis)


# ### Locate model package
# 
# The model package file includes a few files:
# 1. A model definition file with the extension .emd which includes information like model framework (e.g. tensorflow, pytorch). ArcGIS needs it to interpret your model.
# 2. A model file in binary format that we have trained in Part 2.

# In[85]:


classify_land_model_package = ent_gis.content.get('2e92f35da64e470ea326826fd6e463db')
classify_land_model_package


# Now we are ready to install the mode. Installation of the deep learning model item will unpack the model definition file, model file and the inference function script, and copy them to "trusted" location under the Raster Analytic Image Server site's system directory. 

# In[86]:


from arcgis.learn import Model, list_models


# In[87]:


land_cover_model = Model(classify_land_model_package)


# In[88]:


land_cover_model.install(gis=ent_gis)


# ### Model inference
# To test our model, let's get a new raster image by specifying a spatial extent.

# In[76]:


from arcgis.learn import classify_pixels

ext = {'spatialReference': {'latestWkid': 3857, 'wkid': 102100},
       'xmin': -8404328.36942389,
       'ymin': 4742037.04240543,
       'xmax': -8403971.57042703,
       'ymax': 4742293.68138143}

context = {'cellSize': 2,
           'processorType':'GPU',
           'extent': ext,
           'batch_size': 9}


# In[80]:


out_classify = classify_pixels(input_raster = ras_lyr, # make sure pass in the layer not url
                               model = land_cover_model,
                               model_arguments={"padding":100, "batch_size":16, "predict_background":True, "test_time_augmentation":True},
                               output_name = "land_cover_sample_inference_result"+str(datetime.now().microsecond),
                               context = context,
                               gis = ent_gis2)
out_classify


# ## Visualize land cover classification on map

# In[81]:


from arcgis.raster.functions import colormap

result_map = ent_gis.map('Kent County, Delaware')
result_map.basemap = 'satellite'


# In[82]:


# applying color map [value, red, green, blue]
land_cover_colormap=[[0, 0, 0, 0],
                     [1, 0, 197, 255],
                     [2, 0, 168, 132],
                     [3, 38, 115, 0],
                     [4, 76, 230, 0],
                     [5, 163, 255, 115],
                     [6, 255, 170, 0],
                     [7, 255, 0, 0],
                     [8, 156, 156, 156],
                     [9, 0, 0, 0],
                     [10, 115, 115, 0],
                     [11, 230, 230, 0],
                     [12, 255, 255, 115],
                     [13, 0, 0, 0],
                     [14, 0, 0, 0],
                     [15, 0, 0, 0]]


# In[31]:


result_map.add_layer(colormap(out_classify.layers[0], 
                              colormap = land_cover_colormap, 
                              astype='u8'),
                     {'opacity':0.2})
result_map


# ## Conclusion
# In this notebook, we have covered a lot of ground. In part 1, we discussed how to export training data for deep learning using ArcGIS python API and what the output looks like. In part 2, we demonstrated how to prepare the input data, train a pixel-based classification model, visualize the results, as well as apply the model to an unseen image. Then we covered how to install and publish this model and make it production-ready in part 3.
# 
# ## References
# [1] Olaf Ronneberger, Philipp Fischer, Thomas Brox: U-Net: Convolutional Networks for Biomedical Image Segmentation, 2015; <a href='https://arxiv.org/abs/1505.04597'>arXiv:1505.04597</a>.


# ====================
# land_parcel_extraction_using_edge_detection_deep_learning_model.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Land Parcel Extraction using Edge Detection model
# 
# > * 🔬 Data Science
# > * 🥠 Deep Learning and edge detection

# ## Table of Contents
# 
# * [Introduction](#Introduction)
# * [Necessary imports](#Necessary-imports)
# * [Connect to your GIS](#Connect-to-your-GIS)
# * [Export land parcel boundaries data](#Export-land-parcel-boundaries-data)
# * [Prepare data](#Prepare-data)
# * [Visualize a few samples from your training data](#Visualize-a-few-samples-from-your-training-data)
# * [Part 1 - Model training](#Part-1---Model-training)
#   * [Load BDCN or HED edge detector model architecture](#Load-BDCN-or-HED-edge-detector-model-architecture)
#   * [Tuning for optimal learning rate](#Tuning-for-optimal-learning-rate)
#   * [Fit the model on the data](#Fit-the-model-on-the-data)
#   * [Plot losses](#Plot-losses)
#   * [Visualize results](#Visualize-results)
#   * [Save the model](#Save-the-model)
#   * [Compute evaluation metrics](#Compute-evaluation-metrics)
# * [Part 2 - Deploying model, and extraction of parcels in imagery at scale](#Part-2---Deploying-model,-and-extraction-of-parcels-in-imagery-at-scale)
#   * [Generate a classified raster using classify pixels using deep learning tool](#Generate-a-classified-raster-using-classify-pixels-using-deep-learning-tool)
#   * [Post-processing workflow using modelbuilder](#Post-processing-workflow-using-modelbuilder)
#   * [Final output](#Final-output)
# * [References](#References)

# ## Introduction
# 
# High-resolution remote sensing images provide useful spatial information for plot delineation; however, manual processing is time-consuming.  Automatically extracting visible cadastral boundaries combined with (legal) adjudication and incorporation of local knowledge from human operators offers the potential to improve current cadastral mapping approaches in terms of time, cost, accuracy, and acceptance.
# 
# This sample shows how `ArcGIS API for Python` can be used to train a deep learning edge detection model to extract parcels from satellite imagery and thus more efficient approaches for cadastral mapping.
# 
# In this workflow we will basically have three steps.
# 
# - Export training data
# - Train a model
# - Deploy model and extract land parcels

# ## Necessary imports

# In[1]:


import os, zipfile
from pathlib import Path

import arcgis
from arcgis import GIS
from arcgis.learn import BDCNEdgeDetector, HEDEdgeDetector, prepare_data


# ## Connect to your GIS

# In[2]:


gis = GIS("home")
ent_gis = GIS('https://pythonapi.playground.esri.com/portal')


# ## Export land parcel boundaries data
# 
# Training data can be exported by using the `Export Training Data For Deep Learning` tool available in [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview) as well as [ArcGIS Image Server](https://www.esri.com/en-us/arcgis/products/arcgis-image-server). The training data consisted of polyline buffered feature class with a 'class' attribute. For this example, we prepared training data in `Classified Tiles` format using a `chip_size` of 400px and `cell_size` of 20cm in [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview).
# 
# - `Input Raster` : Esri World Imagery
# - `Input Feature Class or Classified Raster` : buffered polyline with class attribute
# - `Class Value Field` : field in the attributes containing class
# - `Tile Size X & Tile Size Y` : 400
# - `Stride X & Stride Y` : 128
# - `Reference System` : Map space
# - `Meta Data Format` : Classified tiles
# - `Environments` : Set optimum Cell Size, Processing Extent 

# Raster and parcels data used for exporting the training dataset are provided below

# In[3]:


training_area_raster = ent_gis.content.get('8202ffe4fcaf4ba9bfefe2154f98e7b8')
training_area_raster


# In[4]:


parcel_training_polygon = gis.content.get('ac0e639d6e9b43328605683efb37ff56')
parcel_training_polygon


# 

# `arcpy.ia.ExportTrainingDataForDeepLearning("Imagery", r"C:\sample\Data\Training Data 400px 20cm", "land_parcels_training_buffered.shp", "TIFF", 400, 400, 128, 128, "ONLY_TILES_WITH_FEATURES", "Classified_Tiles", 0, None, 0, 0, "MAP_SPACE", "NO_BLACKEN", "Fixed_Size")`

# This will create all the necessary files needed for the next step in the 'Output Folder', and we will now call it our training data.

# ## Prepare data

# Alternatively, we have provided a subset of training data containing a samples below and the parcel training polygon used for exporting data. You can use the data directly to run the experiments.

# In[32]:


training_data = gis.content.get('ab003694c99f4484a2d30df53f8b4d03')
training_data


# In[ ]:


filepath = training_data.download(file_name=training_data.name)


# In[ ]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[ ]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# We would specify the path to our training data and a few parameters.
# 
# - `path`: path of folder containing training data.
# - `chip_size`: Same as per specified while exporting training data
# - `batch_size`: No of images your model will train on each step inside an epoch, it directly depends on the memory of your graphic card.

# In[11]:


data = prepare_data(data_path, batch_size=2)


# ## Visualize a few samples from your training data

# The code below shows a few samples of our data with the same symbology as in ArcGIS Pro.
# 
# - `rows`: No of rows we want to see the results for.
# - `alpha`: controls the opacity of labels(Classified imagery) over the drone imagery

# In[10]:


data.show_batch(alpha=1)


# ## Part 1 - Model training

# ### Load BDCN or HED edge detector model architecture

# There are two available edge detection models in `arcgis.learn`, the `HEDEdgeDetector` and `BDCNEdgeDetector`. If backbone is not specified, by default this model will be loaded on a pretrained ResNet type backbone.

# In[12]:


#model = HEDEdgeDetector(data, backbone="vgg19")

#or

model = BDCNEdgeDetector(data, backbone="vgg19")


# List of supported backbones, that could be used during training.

# In[13]:


model.supported_backbones


# ### Tuning for optimal learning rate

# Optimization in deep learning is all about tuning 'hyperparameters'. In this step, we will find an 'optimum learning rate' for our model on the training data. Learning rate is a very important parameter, while training our model it will see the training data several times and adjust itself (the weights of the network). Too high learning rate will lead to the convergence of our model to a suboptimal solution and too low learning can slow down the convergence of our model. We can use the lr_find() method to find an optimum learning rate at which can train a robust model fast enough.

# In[17]:


# Find Learning Rate
lr = model.lr_find()


# In[16]:


lr


# ### Fit the model on the data

# To start with let us first train the model for 30 epochs. One epoch means the model will see the complete training set once and so on. If you feel the results are not satisfactory we can train it further for more number of epochs.

# In[18]:


model.fit(epochs=30, lr=lr)


# ### Plot losses

# In[19]:


model.plot_losses()


# ### Visualize results

# The code below will pick a few random samples and show us ground truth and respective model predictions side by side. This allows us to preview the results of your model in the notebook itself, once satisfied we can save the model and use it further in our workflow.
# 
# We have few parameters for visualization.
# - `alpha`: controls the opacity of predicted edges. Set to 1.
# - `thinning`: Its a post-processsing parameters, which thins or skeletonizes the predicted edges. We will be using our own pre-processing workflow build in ArcGIS pro hence, will set it to False. As per results, this could also be set to True/False during inferencing in pro.

# In[20]:


model.show_results(alpha=1,thinning=False)


# ### Save the model

# We would now save the model which we just trained as a `Deep Learning Package` or `.dlpk` format. Deep Learning package is the standard format used to deploy deep learning models on the ArcGIS platform. For this sample, we will be using this model in ArcGIS Pro to extract land parcels.
# 
# We will use the `save()` method to save the model and by default, it will be saved to a folder 'models' inside our training data folder.

# In[ ]:


model.save("edge_model_e30")


# We can observe some room for further improvement, so we trained the model for further 70 epochs. The deployment and results from the model trained for 100 epochs are shown upcoming section.

# ## Part 2 - Deploying model, and extraction of parcels in imagery at scale
# 
# The deep learning package saved in the previous step can be used to extract classified raster using the `Classify Pixels Using Deep Learning` tool. Further, the classified raster is regularised and finally converted to a vector Polygon layer. The post-processing steps use advanced ArcGIS geoprocessing tools to remove unwanted artifacts in the output. 
# 
# As the model has been trained on a limited amount of data, it is expected to only work well in nearby areas and similar geographies.
# 
# ### Generate a classified raster using classify pixels using deep learning tool
# 
# We will use the saved model to detect objects using the `Classify pixels Using Deep Learning` tool available in [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview) as well as [ArcGIS Image Server](https://www.esri.com/en-us/arcgis/products/arcgis-image-server). For this sample, we will use world imagery to delineate boundaries.

# Raster used for testing the model is provided below

# In[4]:


test_area_raster = ent_gis.content.get('481f2ba520274f0fabfe470d2c8def39')
test_area_raster


# 

# `out_classified_raster = arcpy.ia.ClassifyPixelsUsingDeepLearning("Imagery", r"C:\Esri_project\edge_detection_ready_to_use_model\models\edge_model_e30\edge_model_e100.emd", "padding 56;batch_size 4;thinning False"); 
# out_classified_raster.save(r"C:\sample\sample.gdb\edge_detected")`

# Output of this tool will be in form of a 'classified raster' containing both background and edges. For better visualization, the predicted raster is provided as a web map https://pythonapi.playground.esri.com/portal/home/webmap/viewer.html?webmap=35560c83171f4742847cdeffb813bad9&amp;extent=-77.5235,39.0508,-77.4729,39.0712

# 

# <center> A subset of predictions by trained model <center>

# ### Post-processing workflow using modelbuilder
# 
# As postprocessing workflow below involves geoprocessing tools and the parameters set in these tools need to be experimented with to get optimum results. We would use model builder to build this workflow for us and which will enable us to iteratively change the parameters in this workflow. The `Input raster` should be a predicted reclassified raster with 0-255 range pixel value.

# Tools used and important parameters:
# 
# - `Input raster`: Predicted reclassified raster with 0-255 range pixel value. where 0:background and 255:edge.
# - `Expand` : Expands specified zones of a raster by a specified number of cells. default value set for number of cells to 3.
# - `Raster to polygon` : Vectorizes raster to polygon based on cell values, we will keep the 'Simplify Polygons' and 'Create Multipart Features' options unchecked.
# - `Select by attribute` : Adds, updates, or removes a selection based on an attribute query. We try to filter out polygons with smaller area.
# - `Simplify polygon` : Simplifies polygon outlines by removing relatively extraneous vertices while preserving essential shape. Simplification tolerance was set to 0.1.
# - `Eliminate polygon parts` : Creates a new output feature class containing the features from the input polygons with some parts or holes of a specified size deleted. 'Condition' was set to 'area'.
# - `Polygon to centerline` : Creates centerlines from polygon features. 
# - `Simplify line` : Simplifies lines by removing relatively extraneous vertices while preserving essential shape. Simplification tolerance was set to 4.
# - `Trim line` : Removes portions of a line that extend a specified distance past a line intersection (dangles). Dangle length was set to 100 and delete short features.
# - `Feature to polygon` : Creates a feature class containing polygons generated from areas enclosed by input line or polygon features. 
# - `Regularize building footprints` : This tool will produce the final finished results by shaping them to how actual buildings look. 'Method' for regularisation would be 'Right Angles and Diagonal' and the parameters 'tolerance', 'Densification' and 'Precision' are '1.5', '1', '0.25' and '2' respectively.
# - `Simplify building` : Simplifies the boundary or footprint of building polygons while maintaining their essential shape and size.

# 

# Attached below is the toolbox:

# In[6]:


gis.content.get('9c8939622d954528916e0d6ffda4065d')


# 

# <center> Post-processing workflow in model builder <center>

# ### Final output

# The final output will be in form of a feature class. A link to the web map is provided for better visualization of extracted parcels. Provided is a map for better visualization https://arcg.is/1u4LSX0.

# In[3]:


gis.content.get('f79515de95254eda83a0d3bc331bdcb7')


# 

# <center> A few subsets of results overlayed on the imagery <center>

# ## References

# [1] He, Jianzhong, Shiliang Zhang, Ming Yang, Yanhu Shan, and Tiejun Huang. "Bi-directional cascade network for perceptual edge detection.", 2019; [https://arxiv.org/abs/1902.10903 arXiv:1902.10903v1].


# ====================
# landcover_classification_using_hyperspectral_imagery_and_deep_learning.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Landcover mapping using hyperspectral imagery and deep learning

# ## Table of Contents
# * [Introduction](#1)
# * [Hyperion data preparation](#19)
#     * [Download hyperion imagery](#2)
#     * [Pre-processing of hyperion imagery](#3)
#     * [Export training data for deep learning model](#5) 
# * [Model training](#7)
#     * [Necessary Imports](#6) 
#     * [Get training data](#8)
#     * [Visualize training data](#9)
#     * [Load model architecture](#10)
#     * [Train the model](#11)
#     * [Visualize classification results in validation set](#12)
#     * [Evaluate model performance](#13) 
#     * [Save model](#14)
# * [Model inference](#15)
# * [Results visualization](#18)
# * [Limitation](#21)
# * [Conclusion](#16)
# * [References](#17)

# ## Introduction<a class="anchor" id="1"></a>

# Generally, multispectral imagery is preferred for Landuse Landcover (LULC) classification, due to its high temporal resolution and high spatial coverage. With the advances in remote sensing technologies, hyperspectral images are now also a good option for LULC classification, due to their high spectral resolution. The main difference between multispectral and hyperspectral imagery, is the number of bands and how narrow those bands are. One of the advantages hyperspectral sensors have over multispectral sensors is the ability to differentiate within classes. For instance, due to the high spectral information content that creates a unique spectral curve for each class, hyperspectral sensors can distinguish by tree or crop species.

# <center> Multispectral imagery<centre>

# <center> Hyperspectral imagery<centre>

# In this notebook, we will use hyperspectral data to train a deep learning model and will see if the model can extract subclasses of two LULC classes: developed areas and forests.
# Hyperion imagery is used in the current analysis to classify the types of forests and developed areas. The data can be downloaded from USGS [earth explorer](https://earthexplorer.usgs.gov/). 

# ## Hyperion data preparation<a class="anchor" id="19"></a>

# The [Earth Observing-1 (EO-1) satellite](https://www.usgs.gov/centers/eros/science/usgs-eros-archive-earth-observing-one-eo-1-hyperion?qt-science_center_objects=0#qt-science_center_objects) was launched November 21, 2000 as a one-year technology demonstration/validation mission. After the initial technology mission was completed, NASA and the USGS agreed to the continuation of the EO-1 program as an Extended Mission. The EO-1 Extended Mission is chartered to collect and distribute Hyperion hyperspectral and Advanced Land Imager (ALI) multispectral products according to customer tasking requests.
# 
# Hyperion collects 220 unique spectral channels ranging from 0.357 to 2.576 micrometers with a 10-nm bandwidth. The instrument operates in a pushbroom fashion, with a spatial resolution of 30 meters for all bands. The standard scene width is 7.7 kilometers.

# ### Download hyperion imagery<a class="anchor" id="2"></a>

# Login to the earth explorer using the USGS credentials. Then, select the Address/Place option in the Geocoding Method dropdown and write the name or address of the area of interest.

# 

# Draw a polygon over the area of interest and change the Cloud Cover Range to 0% - 10%. Click on Results in the bottom left.

# 

# Next, expand EO-1, select EO-1 Hyperion, and click on Results.

# 

# Download the data from the product list.

# 

# ### Pre processing of hyperion imagery<a class="anchor" id="3"></a>

# #### Remove bad bands

# Not all bands are useful for analysis. Bad bands are primarily water vapor bands consists inforation about atmosphere, that cause spikes in the reflectance curve. List of bad bands of the Hyperion sensor, L1R product are:
# 1 to 7 - Not illuminated
# 
# 58 to 78 - Overlap region
# 
# 120 to 132 - Water vapor absorption band
# 
# 165 to 182 - Water vapor absorption band
# 
# 185 to 187 - Identified by Hyperion bad band list
# 
# 221 to 224 - Water vapor absorption band
# 
# 225 to 242 - Not illuminated
# 
# Bands with vertical stripping - (8-9, 56-57, 78-82, 97-99, 133-134, 152-153, 188, 213-216, 219-220) 

# #### Create composite rasters

# Two sets of composite rasters were created using [Composite bands function](https://pro.arcgis.com/en/pro-app/help/data/imagery/composite-bands-function.htm). The band range of two raster composites are as follows: Two sets of composite rasters were created using the [Composite bands function](https://pro.arcgis.com/en/pro-app/help/data/imagery/composite-bands-function.htm) function. The band range of the two raster composites are as follows:
# 
# NIR & visible composite - 9 to 55
# 
# SWIR composite - 82-96, 100-119, 135-164, 183-184, 189-212, 217-218
# 
# Click on Imagery tab -> click on Raster Functions -> Search "Composite Bands" -> Click on browse button and select the bands -> After selecting the bands, click on Create new layer.

# 

# #### Convert DN values to at-sensor radiance

# Scale factors will be used to convert the pixel DN value to at-sensor radiance. A scale factor of 40 will be used for the VNIR composite raster and a scale factor of 80 will be used for the SWIR composite. The rasters will be divided by the scale factor using the [Raster Calculator](https://desktop.arcgis.com/en/arcmap/10.3/tools/spatial-analyst-toolbox/raster-calculator.htm) tool in ArcGIS Pro.

# 

# #### Create combined composite raster

# Combine both the VNIR and SWIR radiance composite rasters using the "Composite Band" function.

# 

# #### Eliminate NoData pixels

# ##### Create null raster

# The output raster has a 0 pixel value for NoData that is shown as a black pixel. The NoData pixels can be eliminated using the [Set Null](https://pro.arcgis.com/en/pro-app/tool-reference/spatial-analyst/set-null.htm) tool.
# 
# Use the following parameters:
# 
# Raster: combined_composite
# 
# False raster = 0
# 
# Cellsize Type = Max Of
# 
# Extent Type = Intersection Of

# 

# ##### Clip

# The [Clip](https://pro.arcgis.com/en/pro-app/help/data/imagery/clip-function.htm) tool will be used to eliminate instances of NoData from the combined composite raster using the null raster that was created in the previous step.
# 
# The input parameters are as follows:
# 
# Raster: combined_composite
# 
# Clipping Type: Outside
# 
# Clipping Geometry/Raster: null_raster

# 

# #### Principal Component Analysis

# Principle component analysis can be used to determine the comparatively higher information bands of Hyperion's 224 total bands. [Principal Components](https://pro.arcgis.com/en/pro-app/tool-reference/spatial-analyst/principal-components.htm)  performs Principal Component Analysis (PCA) on a set of raster bands and generates a single multiband raster as its output. The value specified for the number of principal components determines the number of principal component bands in the output multiband raster. The number must not be larger than the total number of raster bands in the input.
# 
# The input parameters are as follows:
# 
# Input raster bands: Clip_combined_composite
# 
# Number of Principal components: 12 (12 Principal component were chosen because they cover more than 98% of the total variance)
# 
# The output raster will have 12 bands representing 12 Principal components.

# 

# ### Export training data for deep learning model<a class="anchor" id="5"></a>

# In[1]:


from arcgis.gis import GIS
gis = GIS('home')
ent_gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# The raster generated from the Hyperion imagery through the `Principal Components` tool will be used as the `input raster` for the training data.

# In[2]:


pca_raster = ent_gis.content.get('1bb227a836094127b530cf8db3a7fa4d')
pca_raster


# The following feature layer will be used as label for training the model.

# In[3]:


lulc_polygons = ent_gis.content.get('f69011a01c39494bade9b30708bce064')
lulc_polygons


# In[4]:


m = gis.map('USA', 4)
m.add_layer(lulc_polygons)
m.legend=True
m


# 

# The above layer consists of 7 classes. Developed areas have three sub classes: High, Medium, and Low density areas and Forested areas have 3 types of forests: Evergreen, Mixed, and Deciduous forest. `No DATA` represents all other LULC classes.

# In[5]:


m.zoom_to_layer(lulc_polygons)


# A feature layer will be used as a `input mask polygon` while exporting the training data to define the extent of the study area.

# In[8]:


mask_poly = ent_gis.content.get('bfe7bb7ea8454d9384cb8005a5c0f855')
mask_poly


# The [Export training data for deep learning](https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/export-training-data-for-deep-learning.htm) tool is used to prepare training data for training a deep learning model. The tool is available in both `ArcGIS Pro` and `ArcGIS Enterprise`.

# 

# ## Model training<a class="anchor" id="7"></a>

# This step will utilize Jupyter Notebooks, and documentation on how to install and setup the environment is available [here](https://developers.arcgis.com/python/guide/install-and-set-up/). 

# ### Necessary Imports<a class="anchor" id="6"></a>

# In[6]:


import os
from pathlib import Path

import arcgis
from arcgis.learn import prepare_data, UnetClassifier


# ### Get training data<a class="anchor" id="8"></a>

# We have already exported the data, and it can be directly downloaded using the following steps:

# In[7]:


training_data = gis.content.get('0a8ff4ce9f734bb5bb72104aea8526af')
training_data


# In[8]:


filepath = training_data.download(file_name=training_data.name)


# In[9]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[10]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# ### Visualize training data<a class="anchor" id="9"></a>

# The `prepare_data` function takes a training data path as input and creates a fast.ai databunch with specified transformation, batch size, split percentage, etc.

# In[11]:


data = prepare_data(data_path,  
                    batch_size=8)


# To get a sense of what the training data looks like, use the `show_batch()` method to randomly pick a few training chips and visualize them. The chips are overlaid with masks representing the building footprints in each image chip.

# In[12]:


data.show_batch(alpha=1)


# ### Load model architecture<a class="anchor" id="10"></a>

# `arcgis.learn` provides the UnetClassifier model for per pixel classification that is based on a pretrained convnet, like ResNet, that acts as the 'backbone'. More details about UnetClassifier can be found [here](https://developers.arcgis.com/python/api-reference/arcgis.learn.toc.html#unetclassifier).

# In[13]:


model = UnetClassifier(data, pointrend=True)


# ### Train the model<a class="anchor" id="11"></a>
# 

# Learning rate is one of the most important hyperparameters in model training. We will use the `lr_find()` method to find an optimum learning rate at which we can train a robust model.

# In[14]:


lr = model.lr_find()


# We are using the suggested learning rate above to train the model for 2000 epochs.

# In[15]:


model.fit(100, lr=lr)


# We have trained the model for a further 1900 epochs to improve model performance. For the sake of time, the cell below is commented out.

# In[16]:


# model.fit(1900)


# ### Visualize classification results in validation set<a class="anchor" id="12"></a>

# It's a good practice to see results of the model vis-a-vis ground truth. The code below picks random samples and shows us ground truths and model predictions, side by side. This enables us to preview the results of the model within the notebook.

# In[17]:


model.show_results()


# ### Evaluate model performance<a class="anchor" id="13"></a>

# As we have 7 classes for this segmentation task, we need to perform an accuracy assessment for each class. To achieve this, ArcGIS API for Python provides the per_class_metrics function that calculates a precision, recall, and f1 score for each class.

# In[18]:


model.per_class_metrics()


# ### Save model<a class="anchor" id="14"></a>

# We will save the model that we trained as a 'Deep Learning Package' ('.dlpk' format). A Deep Learning Package is the standard format used to deploy deep learning models on the ArcGIS platform.
# 
# We will use the `save()` method to save the trained model. By default, it will be saved to the 'models' sub-folder within our training data folder.

# In[19]:


model.save('unet_2000e', publish=True, gis=gis)


# ## Model inference<a class="anchor" id="15"></a>

# Using ArcGIS Pro, we can use the trained model on a test image/area to classify different types of built-up areas and forests in the hyperspectral satellite image.
# 
# After we have trained the `UnetClassifier` model and saved the weights for classifying images, we can use the  [Classify Pixels Using Deep Learning](https://pro.arcgis.com/en/pro-app/latest/tool-reference/image-analyst/classify-pixels-using-deep-learning.htm) tool, available in both ArcGIS pro and ArcGIS Enterprise, for inferencing at scale.

# In[20]:


inference_raster = ent_gis.content.get('df7781fe3d904afa914ef8804b08c802')
inference_raster


# 

# `with arcpy.EnvManager(processorType="GPU"):
# out_classified_raster = arcpy.ia.ClassifyPixelsUsingDeepLearning("pca_raster_for_inferencing",  
#                                                                 r"C:\path\to\model.dlpk", 
#                                                                  "padding 32;batch_size 2;predict_background True;
#                                                                   tile_size 128", 
#                                                                  "PROCESS_AS_MOSAICKED_IMAGE", 
#                                                                   None); 
# out_classified_raster.save(r"C:\sample\sample.gdb\inferenced_results")`

# ## Results visualization<a class="anchor" id="18"></a>

# The classified output raster is generated using ArcGIS Pro. The output raster is published on the portal for visualization. 

# In[21]:


inferenced_results = ent_gis.content.get('29a95588665245359d3787e48fc501cc')
inferenced_results


# In[22]:


rgb_imagery = ent_gis.content.get('94d505a8fd084e7f896ba46247b12739')
rgb_imagery


# ### Create map widgets
# Two map widgets are created showing the RGB imagery and Inferenced results.

# In[23]:


m1 = gis.map()
m1.add_layer(rgb_imagery)
#m1.basemap = 'satellite'
m2 = gis.map()
m2.add_layer(inferenced_results)


# ### Synchronize web maps
# The maps are synchronized with each other using [MapView.sync_navigation](https://developers.arcgis.com/python/api-reference/arcgis.widgets.html#arcgis.widgets.MapView.sync_navigation) functionality. Map view synchronization helps in comparing the inferenced results with the RGB imagery. A detailed description about advanced map widget options can be referenced [here](https://developers.arcgis.com/python/guide/advanced-map-widget-usage/).

# In[24]:


m1.sync_navigation(m2)


# ### Set the map layout

# In[25]:


from ipywidgets import HBox, VBox, Label, Layout


# [Hbox and Vbox](https://developers.arcgis.com/python/api-reference/arcgis.widgets.html?highlight=hbox) were used to set the layout of map widgets.

# In[26]:


hbox_layout = Layout()
hbox_layout.justify_content = 'space-around'

hb1=HBox([Label('FCC imagery'),Label('Results')])
hb1.layout=hbox_layout


# ### Results

# The resulting predictions are provided as a map for better visualization.

# In[27]:


VBox([hb1,HBox([m1,m2])])


# 

#  #### <p style='text-align: right;'> Inferenced results legend </p>

# In[28]:


m2.zoom_to_layer(inferenced_results)


# In the map widgets, it can be seen that the model is able to differentiate between forests and open spaces (gardens, parks, etc). The different types of developed areas are also accurately classified by the model, with the bright blue-gray patches representing high density developed areas, for instance. The forested areas have also been accurately classified by the model.

# ## Limitation<a class="anchor" id="21"></a>

# One of the major limitations in working with hyperspectral data is its lack of availability for many parts of the world. In comparison to multispectral imagery tiles, like those from Landsat-8, one tile of hyperion imagery covers a relatively small area. If a training dataset is prepared with multiple tiles, a model can be trained and more accurate results can be generated.

# ## Conclusion<a class="anchor" id="16"></a>

# In this notebook, we have covered a lot of ground. In Part 1, we covered how Hyperion data can be downloaded using USGS Earth Explorer, the pre-processing steps of Hyperion, how to create a Principal Component raster, and how to export training data for deep learning using ArcGIS Pro. In Part 2, we demonstrated the steps to prepare the input data, train a pixel-based classification model, visualize the results, and generate the accuracy metrics. Finally, in Part 3, we demonstrated the process of inferencing results on a test raster/area. The same workflow can be used to identify minerals/rocks, plant species, oil spills, etc. using hyperspectral imageries and arcgis.learn models.

# ## References<a class="anchor" id="17"></a>

# * https://developers.arcgis.com/python/guide/how-unet-works/


# ====================
# landsat8_to_sentinel2_pix2pix.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Landsat 8 to Sentinel-2 using Pix2Pix

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Necessary imports](#Necessary-imports)
# * [Export training data](#Export-training-data)
# * [Train the model](#Train-the-model)
#  * [Prepare the data](#Prepare-the-data)
#  * [Visualize the training data](#Visualize-the-training-data)
#  * [Load the model architecture](#Load-the-model-architecture)
#  * [Find an optimal learning rate](#Find-an-optimal-learning-rate)
#  * [Fit the model](#Fit-the-model)
#  * [Visualize the results in validation set](#Visualize-the-results-in-validation-set)
#  * [Save the model](#Save-the-model)
# * [Model inference](#Model-inference)
# * [Results](#Results)
# * [Conclusion](#Conclusion)
# * [References](#References)

# ## Introduction

# The Landsat program is the longest running mission for the acquisition of satellite imagery of the earth. The images acquired have opened multiple doors for studies observing the earth. In 2015, Sentinel-2, with 10 m resolution imagery, further enhanced our ability to study and observe the earth. Now, with the advancements in deep learning techniques, there is a need to revisit historic images from the Landsat program and increase their resolution to perform studies at a higher resolution.  
# 
# In this sample notebook, we will see how we can make use of the Pix2Pix model to convert 30 meter resolution Landsat 8 imagery to 10 meter resolution Sentinel-2 imagery, thus allowing us to use Landsat 8 imagery for processes like precision agriculture.

# ## Necessary imports

# In[1]:


import os, zipfile
from pathlib import Path

from arcgis.gis import GIS
from arcgis.learn import prepare_data, Pix2Pix


# In[10]:


gis = GIS(profile = "your_online_profile")


# ## Export training data 

# For this example, we will be using both Landsat 8 and Sentinel-2 imagery. We have exported this data in a “Export_Tiles” metadata format, available in the [`Export Training Data For Deep Learning`](https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/export-training-data-for-deep-learning.htm) tool. This `Export Training Data For Deep Learning` tool is available in [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview) and [ArcGIS Image Server](https://www.esri.com/en-us/arcgis/products/arcgis-image-server).
# 
# - `Input Raster`: Landsat 8 imagery
# - `Additional Raster`: Sentinel-2 imagery 
# - `Tile Size X & Tile Size Y`: 256
# - `Stride X & Stride Y`: 128
# - `Meta Data Format`: 'Export_Tiles', as we are training a `Pix2Pix` model.
# - `Environments`: Set `Cell Size` as 10 to convert Landsat 8 imagery to Sentinel-2 imagery with a 10m resolution.

# 
# 

# Here is the [link to the sample data](https://geosaurus.maps.arcgis.com/home/item.html?id=40b1bae898c441c9832d9dd15a73ab70) exported by following the steps mentioned above.

# In[27]:


output_l1 = r"./Landsat8 to Sentinel2"
if not os.path.exists(output_l1):
    os.mkdir(output_l1)


# In[28]:


output_l2 = r"./Landsat8 to Sentinel2/L_to_S_data_export_larger_extent"
if not os.path.exists(output_l2):
    os.mkdir(output_l2)


# In[13]:


sample_data = gis.content.get("40b1bae898c441c9832d9dd15a73ab70")
sample_data.download(save_path=output_l2)


# In[17]:


zf = zipfile.ZipFile(os.path.join(output_l2, 'Exported_Data_Landsat_to_Sentinel_10.zip'))
zf.extractall(path = os.path.join(output_l2, ''))


# ## Train the model

# ### Prepare the data

# Here, we will specify the path to our training data and a few hyperparameters.
# 
# - `path`: path of the folder containing the training data.
# - `batch_size`: The number of images your model will train on each step inside an epoch. This directly depends on the memory of your graphic card. 64 worked for us on a 32GB GPU.

# In[21]:


output_path = r"./Landsat8 to Sentinel2/L_to_S_data_export_larger_extent/Exported_Data_Landsat_to_Sentinel_10"
data = prepare_data(output_path, batch_size=64)


# ### Visualize the training data

# To get a sense of what the training data looks like, the arcgis.learn.show_batch() method will randomly select a few training chips and visualize them.
# 
# - `rows`: The number of rows to visualize

# In[22]:


data.show_batch(rows=2)


# ### Load the model architecture

# In[23]:


model = Pix2Pix(data)


# ### Find an optimal learning rate

# [Learning rate](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) is one of the most important hyperparameters in model training. The `ArcGIS API for Python` provides a learning rate finder that will automatically select the optimal learning rate for you.

# In[9]:


lr = model.lr_find()


# ### Fit the model 

# Next, we will train the model for a few epochs with the learning rate found above.

# In[10]:


model.fit(100, lr=lr)


# Here, with 100 epochs, we can see reasonable results, as both the training and validation losses have gone down considerably. This indicates that the model is learning to translate Landsat 8 imagery to Sentinel-2.

# ### Visualize the results in validation set

# It is a good practice to see results of the model viz-a-viz the ground truth. The code below picks random samples and displays the ground truth and model predictions side by side. This enables us to preview the results of the model within the notebook.

# In[11]:


model.show_results(2)


# Generative Adversarial Network (GAN) models require a significant amount time to train, and even with the initial 100 epochs trained so far, there is still room for more training. As such, we trained this model for an additional 400 epochs (500 in total), to achieve good results. With the additional training, the training and validation losses dropped to 5.45 and 52.6 respectively and the `D_loss` stabilized at 0.54.

# Below is the loss curve, which represents the training and validation losses during the training process.

# In[18]:


model.plot_losses()


# ### Save the model

# Next, we will save the trained model as a 'Deep Learning Package' ('.dlpk' format). Deep Learning package is the standard format used to deploy deep learning models on the ArcGIS platform.
# 
# We will use the `save()` method to save the trained model. By default, it will be saved to the 'models' sub-folder within our training data folder.

# In[24]:


model.save("Landsat_to_Sentinel_LE_500e")


# ## Model inference

# In this step, we will generate a classified raster using the 'Classify Pixels Using Deep Learning' tool available in both `ArcGIS Pro` and `ArcGIS Enterprise`.
# 
# - `Input Raster`: The raster layer you want to classify.
# - `Model Definition`: Located inside the saved model in the 'models' folder in '.emd' format.
# - `Padding`: The 'Input Raster' is tiled, and the deep learning model classifies each individual tile separately before producing the final 'Output Classified Raster'. This may lead to unwanted artifacts along the edges of each tile, as the model has little context to predict accurately. Padding allows us to supply extra information along the tile edges, thus helping the model to make better predictions.
# - `Cell Size`: Should be close to the size used to train the model.
# - `Processor Type`: Allows you to control whether the system's 'GPU' or 'CPU' will be used to classify pixels. By default, 'GPU' will be used if available.

# 
# 

# It is advised to zoom in to the right extent of the area of interest in order to avoid/reduce noise from the results as the model is not trained to be generalized to work across the globe.

# ## Results 

# The gif below was achieved with the model trained in this notebook. The model converted a Landsat 8 image with a 30m resolution to an image with a 10m resolution and 15 bands, similar to Sentinel-2 images.
# 
# <p align="center"></p>

# Below is the same generated image viewed through the agriculture band combination of SWIR-1 (B11), near-infrared (B8), and blue (B2). This band combination can be used to monitor the health of crops.

# 
# 

# Here, the dark green patches highlight dense vegetation and healthy crops.

# ## Conclusion 

# In this notebook, we have demonstrated how to use the `Pix2Pix` model, available in the `ArcGIS API for Python`, to translate Landsat 8 imagery to Sentinel-2 imagery.

# ## References 

# [1] How Pix2Pix works ?, https://developers.arcgis.com/python/guide/how-pix2pix-works/.


# ====================
# locating_a_new_retirement_community.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Locating a new retirement community
# 

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <br>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Introduction" data-toc-modified-id="Introduction-1">Introduction</a></span></li><li><span><a href="#Workflow" data-toc-modified-id="Workflow-2">Workflow</a></span></li><li><span><a href="#Necessary-Imports" data-toc-modified-id="Necessary-Imports-3">Necessary Imports</a></span></li><li><span><a href="#Get-the-data-for-your-analysis" data-toc-modified-id="Get-the-data-for-your-analysis-4">Get the data for your analysis</a></span></li><li><span><a href="#Create-a-5-mile-drive-time-polygon-around-the-best-performing-community." data-toc-modified-id="Create-a-5-mile-drive-time-polygon-around-the-best-performing-community-5">Create a 5-mile drive time polygon around the best performing community.</a></span></li><li><span><a href="#Proximity-analysis" data-toc-modified-id="Proximity-analysis-6">Proximity analysis</a></span></li><li><span><a href="#Determine-the-top-tapestry-segments." data-toc-modified-id="Determine-the-top-tapestry-segments.-7">Determine the top tapestry segments.</a></span></li><li><span><a href="#Enriching-study-areas" data-toc-modified-id="Enriching-study-areas-8">Enriching study areas</a></span></li><li><span><a href="#Convert-the-top-four-target-area-tapestry-counts-to-percentages." data-toc-modified-id="Convert-the-top-four-target-area-tapestry-counts-to-percentages-9">Convert the top four target area tapestry counts to percentages</a></span></li><li><span><a href="#Obtain-the-same-data-for-the-candidate-ZIP-Codes." data-toc-modified-id="Obtain-the-same-data-for-the-candidate-ZIP-Codes-10">Obtain the same data for the candidate ZIP Codes.</a></span></li><li><span><a href="#Rank-the-candidate-ZIP-Codes-by-their-similarity-to-the-target-area." data-toc-modified-id="Rank-the-candidate-ZIP-Codes-by-their-similarity-to-the-target-area.-11">Rank the candidate ZIP Codes by their similarity to the target area.</a></span></li><li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-12">Conclusion</a></span></li><li><span><a href="#References" data-toc-modified-id="References-13">References</a></span></li><li><span><a href="#Summary-of-tools" data-toc-modified-id="Summary-of-tools-12">Summary-of-tools</a></span></li></ul></div>

# ### Introduction

# In the past, retirement communities in the United states were built in the suburbs, in warmest parts of the country. These days, however, people approaching retirement are not willing to relocate. They want to be connected to their friends and family, remain close to existng doctors and enjoy cultural and educational oportunites. 
# 
# This sample demonstrates the utility of ArcGIS API for Python to identify some great locations for a new retirement community, which will satisfy these needs of senior citizens.  It will demostrate the use tools such as 
# `create_drive_time_areas`, `enrich_layer`, `find_similar_locations`, and editing field definitions and layer data.
# 
# First, we will look for locations that have large number of senior citizens but very few existing retirement communities. We will then rank these locations by comparing it to the most current successsful retirement community.
# 
# The LocatingRetirementCommunity Feature Layer Collection includes two layers. The first layer, called **Target Community**, contains the current best performing retirement community near Knoxville, Tennessee. The second layer, called **Candidates**, contains the 898 ZIP Codes in the continental USA associated with statistically significant hot or cold spot areas for all of these criteria:
# 
# * High demand for retirement housing opportunities
# * Low supply of retirement housing
# * Low housing unit vacancy rates
# * Large projected age of 55 and older populations
# 
# Hot spot analysis to identify these candidate ZIP Codes was done using ArcMap. This analysis is included in both the [ArcMap and ArcGIS Pro workflows]( http://desktop.arcgis.com/en/analytics/case-studies/locating-a-new-retirement-community.htm).
# 
# 
# In the workflow below, we will be using ArcGIS API for Python to create a 5-mile drive distance around the best performing community and obtaining [tapestry](http://www.esri.com/landing-pages/tapestry) and demographic data for the area. We will then obtain the same data for the candidate ZIP Codes. Finally, we will use the Find Similar Locations tool to identify the top four high demand, low vacancy, large projected age of 55+ population ZIP Codes that are most similar to the area surrounding the best performing community.

# ### Workflow

# 

# ### Necessary Imports

# In[1]:


import pandas as pd
from datetime import datetime as dt

from arcgis.gis import GIS
from arcgis.geoenrichment import *
from arcgis.features.use_proximity import create_drive_time_areas
from arcgis.features.enrich_data import enrich_layer
from arcgis.features import FeatureLayerCollection
from arcgis.features.find_locations import find_similar_locations


# ### Get the data for your analysis

# In[2]:


gis = GIS('home')


# Search for the **LocatingRetirementCommunity** layer. You can specify the owner's name to get more specific results. To search for content from the Living Atlas, or content shared by other users on ArcGIS Online, set `outside_org=True`.

# In[3]:


items = gis.content.search('title: LocatingRetirementCommunity owner:api_data_owner',
                           'Feature layer',
                           outside_org=True)


# Display the list of results.

# In[4]:


from IPython.display import display

for item in items:
    display(item)


# Since the first item is a Feature Layer Collection, accessing the layers property will give us a list of FeatureLayer objects.

# In[5]:


lyrs = items[0].layers


# In[6]:


for lyr in lyrs:
    print(lyr.properties.name)


# In[7]:


target_community = lyrs[0]
candidates = lyrs[1]


# In[8]:


m1 = gis.map('Knoxville')
m1


# In[9]:


m1.add_layer(target_community)


# ### Create a 5-mile drive time polygon around the best performing community.

# ### Proximity analysis
# 
# 
# Proximity analysis tools help us answer one of the most common questions posed in spatial analysis: What is near what?
# 
# Proximity tools are available in the `features.use_proximity` module of ArcGIS API for Python. We can use the `create_drive_time_areas` tool to create a 5-mile drive distance buffer around the best performing community.

# In[10]:


target_area = create_drive_time_areas(target_community,
                                      break_values=[5],
                                      break_units='Miles',
                                      overlap_policy='Overlap',
                                      travel_mode='Driving Distance',
                                      output_name='DriveTimeAreasOfTargetCommunities' + str(dt.now().microsecond))


# In[11]:


target_area


# In[12]:


target_area_map = gis.map('Knoxville')
target_area_map


# In[13]:


target_area_map.add_layer(target_area)
target_area_map.add_layer(target_community)


# ### Determine the top tapestry segments.
# We will be looking for ZIP Codes that are similar to the area surrounding the best performing retirement community. We will take advantage of [tapestry variables](https://www.esri.com/en-us/arcgis/products/tapestry-segmentation/overview) because they summarize many aspects of a population, such as age, income, home value, occupation, education, and consumer spending behaviors. To identify the top tapestries within the 5-mile drive distance area, we will obtain and compare all 68 tapestry segments. We will also obtain the tapestry base variable so you can calculate percentages.

# In[14]:


countries = get_countries()
usa = Country.get('US')
type(usa)
df = usa.data_collections
th_var = list(df.loc['tapestryhouseholdsNEW']['analysisVariable'].unique())
other_var = ["KeyUSFacts.POPGRWCYFY","AtRisk.TOTPOP_CY","industry.UNEMPRT_CY"]


# In[15]:


analysis_var = th_var + other_var


# In[16]:


tot_var= [th_var[x] for x in range(len(th_var)) if not th_var[x].split('.')[1].startswith('THHG')]


# In[17]:


tot_var


# In[18]:


result = [ x.split('.')[1] for x in tot_var]


# In[19]:


result


# The 68 distinct markets of Tapestry detail the diversity of the American population.There are 14 LifeMode groups and 6 Urbanization groups which summarize markets that share similar traits.We will not use this for our case study. See
# <a href="https://doc.arcgis.com/en/esri-demographics/data/tapestry-segmentation.htm">Tapestry Segmentation</a> for a detailed overview.

# ### Enriching study areas
# 
# 
# The `enrich_layer` tool gives us demographic and landascape data for the people, places, and businesses in a specific area, or within a selected travel time or distance from a location.
# 
# To obtain the tapestry and demographic data for the area, we will use `enrich_layer` tool from the `arcgis.features.enrich_data` module.

# In[20]:


target_area_data = enrich_layer(target_area,
                                analysis_variables=analysis_var,
                                output_name="GetEnrichedHouseholdTapestry" + str(dt.now().microsecond))


# In[21]:


target_area_data


# In[22]:


data_lyr = target_area_data.layers[0]


# We will convert the layer into spatially enabled dataframe to analyze top 4 tapestries.

# In[23]:


sdf = pd.DataFrame.spatial.from_layer(data_lyr)
sdf.head()


# In[24]:


df = sdf[result]
df.select_dtypes(include=['float64','int64']).T.sort_values(by=0, ascending=False).head().index


# THHBASE contains the total and the highest four tapestry segments in the target community are 'THHBASE', 'THH17', 'THH35', 'THH02', 'THH05'. We will normalize them by the base value.

# ### Convert the top four target area tapestry counts to percentages.

# Rather than counts, we want to compare tapestry percentages between each candidate ZIP Code and the target area.

# Let's create four new fields to hold the tapestry percentages. We can create a feature layer instance to add new fields into your layer.

# A feature service serves a collection of feature layers and tables, with the associated relationships among the entities. It is represented by arcgis.features.FeatureLayerCollection in the ArcGIS Python API.
# 
# Instances of FeatureLayerCollection can be constructed using a feature service, as shown below:

# In[25]:


target_data = FeatureLayerCollection.fromitem(target_area_data)


# The collection of layers and tables in a FeatureLayerCollection can be accessed using the layers and tables properties respectively:

# In[26]:


data_lyr = target_data.layers[0]


# In[27]:


data_lyr.manager.add_to_definition({"fields":[{"name":"THH17PERC",
                                                 "type":"esriFieldTypeDouble",
                                                 "alias":"In Style (5B) PERC",
                                                 "nullable":True,"editable":True,"length":256}]})


# In[28]:


data_lyr.manager.add_to_definition({"fields":[{"name":"THH35PERC",
                                         "type":"esriFieldTypeDouble",
                                         "alias":"Bright Young Professionals (8C) PERC",
                                         "nullable":True,"editable":True,"length":256}]})


# In[29]:


data_lyr.manager.add_to_definition({"fields":[{"name":"THH02PERC",
                                         "type":"esriFieldTypeDouble",
                                         "alias":"Professional Pride (1B) PERC",
                                         "nullable":True,"editable":True,"length":256}]})


# In[30]:


data_lyr.manager.add_to_definition({"fields":[{"name":"THH05PERC",
                                         "type":"esriFieldTypeDouble",
                                         "alias":"Exurbanites",
                                         "nullable":True,"editable":True,"length":256}]})


# Refresh to update the fields in our layer.

# In[31]:


target_data.manager.refresh()


# In[32]:


df = pd.DataFrame.spatial.from_layer(data_lyr)


# In[33]:


df[['THH17PERC','THH35PERC','THH02PERC','THH05PERC']]


# Next we calculate percentage and add it to the fields.

# In[34]:


data_layer = target_data.layers[0]


# In[35]:


data_layer.calculate('1=1', 
                     calc_expression=[{"field":"THH17PERC","sqlExpression":"THH17 / THHBASE"}])


# In[36]:


data_layer.calculate('1=1', 
                     calc_expression=[{"field":"THH35PERC","sqlExpression":"THH35 / THHBASE"}])


# In[37]:


data_layer.calculate('1=1', 
                     calc_expression=[{"field":"THH02PERC","sqlExpression":"THH02 / THHBASE"}])


# In[38]:


data_layer.calculate('1=1', 
                     calc_expression=[{"field":"THH05PERC","sqlExpression":"THH05 / THHBASE"}])


# In[39]:


sf = pd.DataFrame.spatial.from_layer(data_lyr)


# In[40]:


sf[['THH17PERC', 'THH35PERC', 'THH02PERC', 'THH05PERC']]


# In[41]:


sf[['THH17PERC', 'THH35PERC', 'THH02PERC', 'THH05PERC']]*100


# ### Obtain the same data for the candidate ZIP Codes.
# 

# In[42]:


candidates_data = enrich_layer(candidates,
                               analysis_variables=analysis_var,
                               output_name="EnrichCandidatesWithHouseholdTapestry" + str(dt.now().microsecond))


# In[43]:


cand_data = FeatureLayerCollection.fromitem(candidates_data)


# In[44]:


cand_layer = cand_data.layers[0]


# In[45]:


cand_layer.manager.add_to_definition({"fields":[{"name":"THH17PERC",
                                                 "type":"esriFieldTypeDouble",
                                                 "alias":"In Style (5B) PERC",
                                                 "nullable":True,"editable":True,"length":256}]})


# In[46]:


cand_layer.manager.add_to_definition({"fields":[{"name":"THH35PERC",
                                         "type":"esriFieldTypeDouble",
                                         "alias":"Bright Young Professionals (8C) PERC",
                                         "nullable":True,"editable":True,"length":256}]})


# In[47]:


cand_layer.manager.add_to_definition({"fields":[{"name":"THH02PERC",
                                         "type":"esriFieldTypeDouble",
                                         "alias":"Professional Pride (1B) PERC",
                                         "nullable":True,"editable":True,"length":256}]})


# In[48]:


cand_layer.manager.add_to_definition({"fields":[{"name":"THH05PERC",
                                         "type":"esriFieldTypeDouble",
                                         "alias":"Exurbanites",
                                         "nullable":True,"editable":True,"length":256}]})


# In[49]:


rf = pd.DataFrame.spatial.from_layer(cand_layer)


# In[50]:


rf.THHBASE.sort_values().head()


# Notice that some of the base counts are zero. When calculating percentations with zero values, we will get a division by zero error. To avoid this, we will filter these zero (or very small) population ZIP Codes and exclude them from further analysis.

# In[51]:


cand_layer.calculate('THHBASE > 0', 
                     calc_expression=[{"field":"THH17PERC","sqlExpression":"THH17 / THHBASE"}])


# In[52]:


cand_layer.calculate('THHBASE > 0', 
                     calc_expression=[{"field":"THH35PERC","sqlExpression":"THH35 / THHBASE"}])


# In[53]:


cand_layer.calculate('THHBASE > 0', 
                     calc_expression=[{"field":"THH02PERC","sqlExpression":"THH02 / THHBASE"}])


# In[54]:


cand_layer.calculate('THHBASE > 0', 
                     calc_expression=[{"field":"THH05PERC","sqlExpression":"THH05 / THHBASE"}])


# In[55]:


cf = pd.DataFrame.spatial.from_layer(cand_layer)


# In[56]:


cf[['THH17PERC', 'THH35PERC', 'THH02PERC', 'THH05PERC']]


# ### Rank the candidate ZIP Codes by their similarity to the target area.

# In[57]:


top_4_most_similar_results = find_similar_locations(data_layer,
                                                    cand_layer,
                                                    analysis_fields=['THH17','THH35','THH02','THH05','POPDENS14','FAMGRW10_14','UNEMPRT_CY'],
                                                    output_name = "Top4SimilarLocations" + str(dt.now().microsecond),
                                                    number_of_results=4)


# In[58]:


top_4_most_similar_results = top_4_most_similar_results['similar_result_layer']
top_4_most_similar_results


# In[59]:


map1 = gis.map('Atlanta')
map1.add_layer(top_4_most_similar_results)


# In[60]:


map2 = gis.map('Houston')
map2.add_layer(top_4_most_similar_results)


# In[61]:


from ipywidgets import *

map1.layout=Layout(flex='1 1', padding='10px', height='420px')
map2.layout=Layout(flex='1 1', padding='10px', height='420px')

box = HBox([map1, map2])
box


# One of the top ZIP Codes is located near Houston and three are located near Atlanta.

# ### Conclusion

# We have analysed great locations to start a new retiremnent community project.

# ### References
# - Brennan, Morgan. 2012. "America's Friendliest Towns." Forbes, December 19, 2012. http://www.forbes.com/sites/morganbrennan/2012/12/19/americas-friendliest-towns/
# 
# - City of Roswell, Georgia. "City Awards and Achievements." http://www.roswellgov.com/discover-us/city-awards-achievements
# 
# - City of Taylor Lake Village, Texas. http://www.taylorlakevillage.us
# 
# - Grunewald, Will. 2014. "How Baby Boomers Are Changing Retirement Living." Washingtonian, March 13, 2014. http://www.washingtonian.com/articles/people/how-baby-boomers-are-changing-retirement-living/
# 
# - Heneghan, Carolyn. 2014. "The 50 Safest Cities in Georgia." The SafeWise Report, Safewise.com, February 17, 2014. http://www.safewise.com/blog/50-safest-cities-georgia/
# 
# - Holley, Peter; Lomax, John; and Shilcutt, Katharine. 2014. "Where to Live Now: The 25 Hottest Neighborhoods of 2014." Houstonia, April 3, 2014. http://www.houstoniamag.com/articles/2014/4/3/where-to-live-now-hottest-neighborhoods-april-2014
# 
# - Kilborn, Peter T. 2009. "In Depth: America's 25 Best Places to Move." Forbes, July 7, 2009. http://www.forbes.com/2009/07/07/relocate-relocation-cities-lifestyle-real-estate-affordable-moving_slide.html
# 
# - Northern, Amanda. 2015. "Here Are The Best Places To Live In Georgia… And Why." OnlyInYourState.com, June 19, 2015. http://www.onlyinyourstate.com/georgia/best-places-to-live-in-ga/
# 

# ### Summary of tools

# <table>
#   <tr>
#     <th>Method</th>
#     <th>Generic Question</th>
#     <th>Examples</th>
#   </tr>
#   <tr>
#     <td>Attribute Query</td>
#     <td>Which features have the characteristics I'm interested in?</td>
#     <td>Which features have more than 50,000 people and median annual incomes larger than $50,000?<br>Which hospitals have readmission rates larger than 10 percent?</td>
#   </tr>
#   <tr>
#     <td>Find Similar Locations</td>
#     <td>Which features are most like my target feature?</td>
#     <td>Which stores are similar to my best performing store?<br>Which crimes in the database are most like the current one I want to solve?</td>
#   </tr>
#   <tr>
#     <td>Identify drive time or drive distance areas</td>
#     <td>What are the dimensions, based on time or distance, of the area surrounding a location?</td>
#     <td>What does a 5-mile or 5-minute walking, biking, or driving distance around the hospital look like?<br>What can I visit within a 2KM walk from my hotel?</td>
#   </tr>
#   <tr>
#     <td>Enrich layer</td>
#     <td>Want to know about the tapestry and demographic data for the area?</td>
#     <td>Which zip codes have higher population of +55 age and above?</td>
#   </tr>
# </table>


# ====================
# lunar_craters_detection_from_dem_using_deep_learning.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Lunar Craters Detection using Deep Learning
# > * 🔬 Data Science
# > * 🥠 Deep Learning

# ## Table of Contents
# 
# * [Introduction](#Introduction)
# * [Necessary imports](#Necessary-imports)
# * [Connect to your GIS](#Connect-to-your-GIS)
# * [Methodology](#Methodology)
# * [Export training data](#Export-training-data)
# * [Model training](#Model-training)
#   * [Prepare data](#Prepare-data)
#   * [Load MaskRCNN model architecture](#Load-Pix2Pix-model-architecture)
#   * [Tuning for optimal learning rate](#Tuning-for-optimal-learning-rate)
#   * [Fit the model](#Fit-the-model)
#   * [Visualize results in validation set](#Visualize-results-in-validation-set)
#   * [Save the model](#Save-the-model)
#   * [Accuracy assessment](#Accuracy-assessment)
# * [Model inferencing](#Model-inferencing)
# * [Results visualization](#1)
# * [Conclusion](#Conclusion)
# * [References](#References)

# ## Introduction 

# When ground data is not available, crater detection and counting play a vital role in predicting the moons surface age. Further, crater detection is necessary for identifying viable landing sites for lunar landers, as well as for establishing landmarks for navigation on the moon. Traditionally, craters have manually been digitized using visual interpretation, a very time-consuming and inefficient method. However, with the proliferation of deep learning, the process of crater detection is now able to be bolstered by automated detection. This notebook will demonstrate how the [ArcGIS API for Python](https://developers.arcgis.com/python/) can be used to train a deep learning crater detection model using a Digital Elevation Model (DEM), which can then be deployed in [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview) or [ArcGIS Enterprise](https://www.esri.com/en-us/arcgis/products/arcgis-enterprise/overview).

# ## Necessary imports

# In[1]:


import os
from pathlib import Path

from arcgis import GIS
from arcgis.learn import MaskRCNN, prepare_data


#  ## Connect to your GIS

# In[2]:


gis = GIS('home')


# ## Export training data

# The DEM of the moon will be used as `input_raster` for the training data and has a spatial resolution of 118m. It can be downloaded from the [USGS](https://astrogeology.usgs.gov/search/map/Moon/LRO/LOLA/Lunar_LRO_LOLAKaguya_DEMmerge_60N60S_512ppd) website.

# In[3]:


lunar_dem = gis.content.get('beae2f7947704c938eb957c6deb2fa2b')
lunar_dem


# The crater feature layer will be used as the `Input Feature Class` for the training data. 

# Feature layer of craters 5-20 km will be used to export training data at 500 and 5000 cell size. 

# In[5]:


craters_5_20km = gis.content.get('af240e45a3ca445b88d6ec19209d3bb5')
craters_5_20km


# Feature layer of craters more than 20 km will be used to export training data at 10000 cell size. 

# In[4]:


craters_more_than_20km = gis.content.get('36272e93ec4547abba495c797a4fb921')
craters_more_than_20km


# ## Methodology

# 

# The data will be exported in the “RCNN Masks” metadata format, which is available in the [Export Training Data For Deep Learning tool](https://pro.arcgis.com/en/pro-app/latest/tool-reference/image-analyst/export-training-data-for-deep-learning.htm). This tool is available in both [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview) and [ArcGIS Image Server](https://www.esri.com/en-us/arcgis/products/arcgis-image-server). The various inputs required by the tool are described below:
# 
# `Input Raster`: Lunar DEM
# 
# `Input Feature Class Or Classified Raster Or Table`: The craters feature layer
# 
# `Tile Size X & Tile Size Y`: 256
# 
# `Stride X & Stride Y`: 128
# 
# `Meta Data Format`: 'RCNN Masks', as we are training a MaskRCNN model.
# 
# *`Environments`*:
# - `Cell Size`: The data is exported to 3 different cell sizes so that it can learn to detect craters of different diameters. The data is exported at 500, 5000, and 10000.  
# 
# - `Processing Extent`: Default

# 

# Note: The training data must be exported in the same output folder for all three cell sizes.

# Inside the exported data folder, the 'images' folder contains all of the chips of the DEM, and the 'labels' folder contains the masks of the craters.

# ## Model training

# Alternatively, we have provided a subset of training data containing a few samples that follow the same directory structure mentioned above, as well as the raster and crater feature layers used for exporting the training dataset. The provided data can be used directly to run the experiments.

# In[6]:


training_data = gis.content.get('0a4e2d4ad7bf41f6973c7e3434faf7d4')
training_data


# In[7]:


filepath = training_data.download(file_name=training_data.name)


# In[8]:


#Extract the data from the zipped image collection
import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# ### Prepare data

# In[9]:


output_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# In[10]:


data = prepare_data(output_path, batch_size=8)


# ### Visualize a few samples from your training data

# To get a sense of what the training data looks like, the `arcgis.learn.show_batch()` method will randomly select a few training chips and visualize them.

# In[12]:


data.show_batch(rows=2)


# ### Load model architecture

# The `arcgis.learn` module provides the `MaskRCNN` model for instance segmentation tasks. `MaskRCNN` is based on a pretrained convolutional neural network, like `ResNet`, that acts as the 'backbone'. More details about `MaskRCNN` can be found [here](https://developers.arcgis.com/python/guide/how-maskrcnn-works/).

# In[13]:


model = MaskRCNN(data)


# ### Tuning for optimal learning rate

# Learning rate is one of the most important hyperparameters in model training. ArcGIS API for Python provides a learning rate finder that automatically chooses the optimal learning rate for you.

# In[14]:


lr = model.lr_find()
lr


# ### Fit the model

# Next, the model is trained for a few epochs, with `early_stopping=True` and the learning rate recommended above.

# In[15]:


model.fit(100, lr, early_stopping=True)


# Here, the model training automatically stopped at the 56th epoch. We can see reasonable results, as both the training and validation losses decreased considerably, indicating that the model is learning to translate between different domains of imagery.

# ### Save the model

# Here, we will save the trained model as a 'Deep Learning Package' ('.dlpk'). The Deep Learning Package format is the standard format used when deploying deep learning models on the ArcGIS platform.

# In[16]:


model.save("moon_mrcnn_2", publish=True)


# ### Visualize results in validation set

# It is a good practice to view the results of the model viz-a-viz the ground truth. The `model.show_results()` method can be used to display the detected craters. Each detection is visualized as a mask by default.

# In[17]:


model.show_results(rows=2, mask_threshold=0.7)


# 

# ### Accuracy assessment

# For the accuracy assessment, we can compute the average precision score for the trained model. Average precision will compute the average precision on the validation set for each class. We can compute the average precision score by calling `model.average_precision_score`. It takes the following parameters:
# 
# - `detect_thresh`: The probability above which a detection will be considered for computing the average precision.
# - `iou_thresh`: The intersection over union threshold with the ground truth labels, above which a predicted bounding box will be considered a true positive.
# - `mean`: If set to `False`, will return the class-wise average precision. Otherwise will return the mean average precision.

# In[18]:


model.average_precision_score()


# ## Model inferencing

# After training the `MaskRCNN` model and saving the weights of the detected crater masks, we can use the [Detect Objects Using Deep Learning](https://pro.arcgis.com/en/pro-app/latest/tool-reference/image-analyst/detect-objects-using-deep-learning.htm) tool available in [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview) and [ArcGIS Image Server](https://www.esri.com/en-us/arcgis/products/arcgis-image/options/arcgis-image-server?rsource=https%3A%2F%2Fwww.esri.com%2Fen-us%2Farcgis%2Fproducts%2Farcgis-image-server).

# 

# `arcpy.ia.DetectObjectsUsingDeepLearning(in_raster="dem_for_inference",
# out_detected_objects=r"\\detected_craters",
# in_model_definition=r"\\models\moon_mrcnn_2\moon_mrcnn_2.dlpk",
# model_arguments ="padding 56;batch_size 4;threshold 0.6;return_bboxes False",
# run_nms="NMS",
# confidence_score_field="Confidence",
# class_value_field="Class",
# max_overlap_ratio=0,
# processing_mode="PROCESS_AS_MOSAICKED_IMAGE")`

# The inferencing above was done on the following cell sizes: 500m (small craters), 5000m (medium-sized craters), and 10,000m (large craters). All of the inferenced outputs were then merged together to get the final feature layer, which consists of all the detected craters.

# ## Results visualization<a class="anchor" id="1"></a>

# Finally, after we have used ArcGIS Pro to detect craters in two different areas, we will publish the results to our portal and visualize them as maps.

# In[19]:


from arcgis.mapping import WebMap
## Area1 Craters
LunarArea1 = gis.content.get('1a76ed548cfc4e159d860ae253e5ecc3')
map1 = WebMap(LunarArea1)
map1


# 

# In[20]:


## Area2 Craters
LunarArea2 = gis.content.get('2ce5ad91fab649c7982dde750cce3390')
map2 = WebMap(LunarArea2)
map2


# 

# In the maps above, we can see that the trained `MaskRCNN` model was able to accurately detect craters of varying sizes. While the training data that was provided to the model contained craters digitized as circle features, the model was even able to learn to detect more accurate, non-circular boundaries of the craters.

# ## Conclusion

# This notebook showcased how instance segmentation models, like `MaskRCNN`, can be used to automatically detect lunar craters using a DEM. We also demonstrated how custom transformations based on the data, irrespective of already present standard transformations, can be added during data preparation to achieve better performance.


# ====================
# measure_shrinking_lakes_using_deep_learning.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Classify land cover to measure shrinking lakes
# 
# #### Compare imagery to calculate area change in Lake Aculeo Lagoon, Chile.

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Necessary imports](#Necessary-imports)
# * [Get data and model for classification](#Get-data-and-model-for-classification)
# * [Visually compare Lake Aculeo Lagoon over time](#Visually-compare-Lake-Aculeo-Lagoon-over-time)
# * [Classify land cover in 2016](#Classify-land-cover-in-206)
# * [Classify land cover in 2017](#Classify-land-cover-in-2017)
# * [Clean up the classification](#Clean-up-the-classification)
# * [Calculate area over time](#Calculate-area-over-time)
# * [Conclusion](#Conclusion)
# * [References](#References)

# ## Introduction 

# One of the important freshwater waterbodies of central Chili, Lake Aculeo Lagoon shrank over time due to climatic drought, drying of surface tributaries and streams that provided a continuous ﬂow of water and high existing demand of ground water [[1]](https://www.researchgate.net/publication/338731020_The_First_Drying_Lake_in_Chile_Causes_and_Recovery_Options). The lagoon dried completely in May of 2018. Those whose livelihoods depend on the lake are alarmed, as the shrinking lake changes the land cover of the area and impacts the economy. In order to help them identify and save their waterbodies, we'll generate land cover and compare imagery between 2016 and 2017 to quantify the surface area of the lake and show changes over time.
# 
# Manually extracting features from raw data, such as generating land cover maps, is time consuming. Deep learning automates the process and minimizes the manual interaction necessary to complete these tasks. To quantify the change in lake surface area from 2016 to 2017, we'll classify the land cover in both images, identifying the areas covered with water and distinguishing them from other land cover, such as vegetation or urban areas.
# 
# This sample aims to demonstrate how ArcGIS pretrained models can be used to generate landcover from imageries of different time periods for further analysis. Once the land covers are created, we will use arcgis.raster submodule of ArcGIS API for Python for calculating surface area reduced from 2016 to 2017.

# Note: This sample is supported with ArcGIS Image for ArcGIS Online. For more details read [here](https://www.esri.com/arcgis-blog/products/arcgis-online/imagery/introducing-arcgis-image-for-arcgis-online/).

# ## Necessary imports

# In[1]:


import arcgis
from arcgis.gis import GIS
from arcgis.learn import classify_pixels
from arcgis.raster.functions import equal_to
from arcgis.raster.functions import extract_band


import pandas as pd
from datetime import datetime as dt
from ipywidgets import HBox, VBox, Label, Layout


# In[2]:


gis = GIS("home")


# ## Get data and model for classification

# Search for  Multispectral Sentinel-2 Level 1-C imagery for year 2016 and 2017 on the [Copernicus Open Access Hub](https://scihub.copernicus.eu/dhus/#/home).  Download imagey of the desired time period and publish them in ArcGIS Online. The code below uses the `get` method to get items.

# In[3]:


sentinel2016 = gis.content.get("a264d1eaa8bd4ff2a0a1e27b59daa7d3")
sentinel2017 = gis.content.get("ea192d03d1324c62a2cb91e26e3a0ece")


# Search for the <b>Land Cover Classification (Sentinel-2)</b> deep learning package in ArcGIS Online. 

# In[4]:


model = gis.content.get('afd124844ba84da69c2c533d4af10a58')
model


# This model is trained on [Corine Land Cover (CLC) 2018](https://land.copernicus.eu/pan-european/corine-land-cover) data that contains 16 classes. The screenshot below shows the color code for each land cover class.

# 

# ## Visually compare Lake Aculeo Lagoon over time

# As all the bands cannot be depicted at the same time, we usually pick a combination of three bands that we display through the color channels red, green, and blue, which can be seen by the human eye. 

# In[5]:


sentinel2016_rgb = extract_band(sentinel2016.layers[0], [4, 3, 2])
sentinel2017_rgb = extract_band(sentinel2017.layers[0], [4, 3, 2])


# Let's compare the imagery visually to get a sense of how the shape of the lake has evolved over time.

# In[6]:


map1 = gis.map()
map1.add_layer(sentinel2016_rgb)
map2 = gis.map()
map2.add_layer(sentinel2017_rgb)


# In[7]:


map1.sync_navigation(map2)


# In[8]:


hbox_layout = Layout()
hbox_layout.justify_content = "space-around"
hb1 = HBox([Label("Sentinel-2016"), Label("Sentinel-2017")])
hb1.layout = hbox_layout
VBox([hb1, HBox([map1, map2])])


# The imagery shows a bright and clear distinction between the blue lake and the green vegetation nearby. 

# In[9]:


map1.zoom_to_layer(sentinel2016_rgb)


# ## Classify land cover in 2016

# To quantify the change in lake surface area from 2016 to 2017, we'll classify the land cover in both images, identifying the areas covered with water and distinguishing them from other land cover, such as vegetation or urban areas. In multispectral imagery, such as Sentinel-2, every individual pixel (or cell) in the image has a value for every spectral band. As we can see from the vibrant imagery of Lake Poyang, there are many possible color values for all varieties of shades and hues. However, all the pixels representing the same land cover tend to have somewhat similar spectral values. By classifying the image, we'll identify the pixels that are similar in value and group them together to represent a small number of classes, such as water, vegetation, or urban areas.

# In[10]:


ext = map1.extent  # desired extent for generating land cover


# In[11]:


classified_out2016 = classify_pixels(
    input_raster=sentinel2016.layers[0],
    model=model,
    model_arguments={
        "padding": 56,
        "batch_size": 64,
        "predict_background": True,
        "tile_size": 224,
        "test_time_augmentation": True,
        "merge_classes": True,
        "sentinel_imagery_level": 1,
    },
    output_name="classified_output_2016" + str(dt.now().microsecond),
    context={"extent": ext, "processorType": "GPU", "cellSize": 10},
    tiles_only=False,
)
classified_out2016


# Let's visualize the 2016 raster and its land cover on the map.

# In[12]:


map3 = gis.map()
map3.add_layer(sentinel2016_rgb)
map4 = gis.map()
map4.add_layer(classified_out2016)


# In[13]:


map3.sync_navigation(map4)


# In[14]:


hbox_layout = Layout()
hbox_layout.justify_content = "space-around"
hb1 = HBox([Label("Sentinel-2016"), Label("Classified Raster 2016")])
hb1.layout = hbox_layout
VBox([hb1, HBox([map3, map4])])


# In[15]:


map3.zoom_to_layer(sentinel2016_rgb)


# ## Classify land cover in 2017

# Next, We will follow the same process to generate land cover for the 2017 raster.

# In[16]:


classified_out2017 = classify_pixels(
    input_raster=sentinel2017.layers[0],
    model=model,
    model_arguments={
        "padding": 56,
        "batch_size": 64,
        "predict_background": True,
        "tile_size": 224,
        "test_time_augmentation": True,
        "merge_classes": True,
        "sentinel_imagery_level": 1,
    },
    output_name="classified_output_2017" + str(dt.now().microsecond),
    context={"extent": ext, "processorType": "GPU", "cellSize": 10},
    tiles_only=False,
)
classified_out2017


# In[17]:


map5 = gis.map()
map5.add_layer(sentinel2017_rgb)
map6 = gis.map()
map6.add_layer(classified_out2017)


# In[18]:


map5.sync_navigation(map6)


# In[18]:


hbox_layout = Layout()
hbox_layout.justify_content = "space-around"
hb1 = HBox([Label("Sentinel-2017"), Label("Classified Raster 2017")])
hb1.layout = hbox_layout
VBox([hb1, HBox([map5, map6])])


# In[19]:


map5.zoom_to_layer(sentinel2017_rgb)


# ## Clean up the classification

# We'll now clean up the classified images with generalization analysis tools to remove minor water bodies around the lake. We'll also smooth the lake's boundaries.
# 
# We are only interested in pixels that have been clasified as water, so we will run an `equal_to` tool to produce a raster that contains pixels classified as water.  

# In[20]:


water_mask2016 = equal_to(
    rasters=[
        classified_out2016.layers[0],
        51,
    ],  # 51 is the code value for Inland Waters class.
    astype="U16",
).save("water_mask2016" + str(dt.now().microsecond))
water_mask2017 = equal_to(
    rasters=[classified_out2017.layers[0], 51], astype="U16"
).save("water_mask2017" + str(dt.now().microsecond))


# Next, we will convert the raster into polygon features in order to calculate area of desired lake polygon.

# In[21]:


lake_poly_2016 = water_mask2016.layers[0].to_features()
lake_poly_2017 = water_mask2017.layers[0].to_features()


# All features with gridcode 0 have nodata values, while gridcode 1 has features with water polygons.

# In[22]:


df_2016 = lake_poly_2016.layers[0].query(as_df=True, where="gridcode= 1")
df_2017 = lake_poly_2017.layers[0].query(as_df=True, where="gridcode= 1")


# The code below calculates the area of all of the features and then returns the area of largest water polygon, the lake.

# ## Calculate area over time

# In[23]:


area_lost = (
    df_2016.SHAPE.geom.area.max() - df_2017.SHAPE.geom.area.max()
)  # area in square meters
area_lost


# In[24]:


area_lost/1000000 #area in square kilometers


# ## Conclusion

# In this sample, we compared visually and classified Sentinel-2 imagery of Lake Aculeo Lagoon to understand how much the lake's area has changed over time. These findings indicate a severe problem: the lake has lost around 1.3 square kilometers in only 1 year. These calculations don't reveal the causes of Lake Aculeo Lagoon's reduction, however, they do provide factual evidence of a serious problem and provide a starting point for environmental scientists, and others, to conduct further research.

# ## References

# - https://learn.arcgis.com/en/projects/classify-land-cover-to-measure-shrinking-lakes/#calculate-area-over-time
# - https://earthengine.google.com/timelapse/
# - https://livingatlas.arcgis.com/en/browse/?q=dlpk#d=2&q=dlpk


# ====================
# model_explainability_using_shap_for_tabular_data.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Model explainability for ML Models

# ## Table of Contents
# * [Introduction](#Introduction)
# * [What is SHAP](#SHAP)
# * [Prepare Data](#prepare)
# * [Load a Trained ML model](#LoadModel)
# * [Local Prediction Interpretation](#Local)
# * [Global Interpretation](#Global)

# ## Introduction 
# <a id='Introduction'></a> 

# Machine learning is growing at a fast pace. Researchers are coming up with new models and architectures that are taking the predictive power of these models to new heights everyday. Amidst this growing enthusiasm about the improving model performance, there is certain growing hesitancy among the users. This hesitancy is due to the "black box" nature of a lot of these models. A lack of transparency is definitely seen and the users are even now ready to sacrifice a bit of model performance in favour of using a model which is more "explainable". Tree based models have always been explainable due to their inherent nature. 
# 
# To support the growing need to make models more explainable, `arcgis.learn` has now added explainability feature to all of its models that work with tabular data. This includes all the MLModels and the fully connected networks. `arcgis.learn` is now integrated with the model explainability that [SHAP](https://github.com/slundberg/shap) offers. 

# ## What is SHAP? 
# <a id='SHAP'></a> 

# SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions. More details about SHAP and its implementation can be found here. https://github.com/slundberg/shap

# ## Prepare Data
# <a id='prepare'></a> 

# In this guide, we will use a pretrained model to get the predictions of energy generation for Solar Photovoltaic power plants using weather variables. We will then see, how using model explainability we can get the explanations for individual predictions. Finally, we will also see global model interpretibility. To get more details of the use case and how to train such a model please refer https://developers.arcgis.com/python/sample-notebooks/solar-energy-prediction-using-weather-variables/

# In[1]:


import shap
import arcgis
from arcgis.gis import GIS
from arcgis.learn import FullyConnectedNetwork, MLModel, prepare_tabulardata
from sklearn.preprocessing import MinMaxScaler,RobustScaler
import pandas as pd


# In[2]:


gis=GIS('home')


# In[3]:


calgary_no_southland_solar = gis.content.search('calgary_no_southland_solar owner:api_data_owner', 'feature layer')[0]
calgary_no_southland_solar_layer = calgary_no_southland_solar.layers[0]
m1 = gis.map('calgary', zoomlevel=10)
m1.add_layer(calgary_no_southland_solar_layer)
m1


# ![Solar.PNG](attachment:Solar.PNG)

# In[ ]:


calgary_no_southland_solar_layer_sdf = calgary_no_southland_solar_layer.query().sdf
calgary_no_southland_solar_layer_sdf=calgary_no_southland_solar_layer_sdf[['FID','date','ID','solar_plan','altitude_m',
                                                                           'latitude','longitude','wind_speed','dayl__s_',
                                                                           'prcp__mm_d','srad__W_m_','swe__kg_m_', 'tmax__deg',
                                                                           'tmin__deg','vp__Pa_','kWh_filled','capacity_f',
                                                                           'SHAPE']]
X = ['altitude_m', 'wind_speed', 'dayl__s_', 'prcp__mm_d','srad__W_m_','swe__kg_m_','tmax__deg','tmin__deg','vp__Pa_']
preprocessors =  [('altitude_m', 'wind_speed', 'dayl__s_', 'prcp__mm_d','srad__W_m_','swe__kg_m_','tmax__deg',
                   'tmin__deg','vp__Pa_', RobustScaler())]


# In[4]:


data = prepare_tabulardata(calgary_no_southland_solar_layer,
                           'capacity_f',
                           explanatory_variables=X,                           
                           preprocessors=preprocessors)


# ## Load a Trained ML Model 
# <a id='LoadModel'></a> 

# Here we load a trained model which can predict the efficiency of the solar panels based on attributes like length of the day, weather , temperature etc.

# In[15]:


model=MLModel.from_model('C:/Users/Karthik/Desktop/Base/MLModel/ML_Model_RF1/ML_Model_RF1.emd',data)


# In[ ]:


valid=data._dataframe.iloc[data._validation_indexes,:]
train=data._dataframe.iloc[data._training_indexes,:]


# ## Local Prediction interpretation
# <a id='Local'></a>

# Now we make a prediction on a sample row. The predict method in `arcgis.learn` for MLModel and Fully connected network has been modified to accept two new parameters 'explain' and 'explain_index'. 
# 
# - The parameter `explain` is boolean and setting this parameter to True allows the user to get the explanation of the prediction. 
# 
# - The parameter `explain_index` is an int and it is the index of the row of the dataframe for which the user would like to get the explanation for. It is only possible to get explanation for one row/index/sample at a time. 

# In[16]:


out=model.predict(model._data._dataframe[X],prediction_type='dataframe',explain=True,explain_index=0 )
out.head(10)


# Running the predictions with explain set to true will generate a plot which is shown above, along with the predictions for all the samples passed into the predict function. Note, that the prediction explanation is generated only for the first observation (With row 0) since 0 was passed as the value for the parameter `explain_index`

# The plot says that the mean value of the entire dataset (denoted as base value in the plot) is 0.12 and the prediction for this specific observation is -0.01.
# Apart from that, plot also says that the parameter srad_W_m has had the greatest influence on the model in getting the prediction of this specific record. This is represented by the length of the block srad_W_m_. 
# All the parameters in Blue are making the model move away towards the left from the base prediction and the parameters in Red (In this case - None) are influencing the model positively. 
# 
# For more details , refer to https://github.com/slundberg/shap

# ## Global Interpretation
# <a id='Global'></a>

# We can now also visualize the global interpretation (similar to [feature importance](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html) in sklearn). Unlike sklearn this plot can be generated for non tree models of sklearn as well.This plot below, tells the user the impact that each feature in the dataset has on the model as a whole. We can infer from the plot that the feature srad_W_m_ has the highest impact on the model decision making and the feature altitude_m has the least impact.

# In[10]:


model_lin.feature_importances_



# ====================
# multi_class_change_detection_using_segmentation_deep_learning_models.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # multi-class change detection using image segmentation deep learning models
# 
# > * 🔬 Data Science
# > * 🥠 Deep Learning and image segmentation

# ## Table of Contents
# 
# * [Introduction](#Introduction)
# * [Necessary imports](#Necessary-imports)
# * [Connect to your GIS](#Connect-to-your-GIS)
# * [Export training data](#Export-training-data)
# * [Prepare the data](#Prepare-the-data)
# * [Train change detection model](#Train-change-detection-model)
#   * [Load UNet model architecture](#Load-UNet-model-architecture)
#   * [Tune for optimal learning rate](#Tune-for-optimal-learning-rate)
#   * [Fit the model](#Fit-the-model)
#   * [Visualize results](#Visualize-results)
#   * [Save the model](#Save-the-model)
# * [Model inferencing](#Model-inferencing)
#   * [Generate a change raster utilizing the Classify Pixels Using Deep Learning tool](#Generate-a-change-raster-utilizing-the-Classify-Pixels-Using-Deep-Learning-tool)
#   * [Visualize change map](#Visualize-change-map)
# * [Conclusion](#Conclusion)
# * [References](#References)

# # Introduction

# Change detection is a process used in global remote sensing to find changes in landcover over a period of time, either by natural or man-made activities, over large areas. This process is used in many applications, including in environmental monitoring, disaster evaluation, and urban expansion studies. Current change detection methods typically follow one of two approaches, utilizing either post-classification analysis or difference image analysis. These methods are often resource-heavy and time intensive.
# 
# In this notebook, we will show a novel way to detect and classify change using semantic segmentation models available in `arcgis.learn`. For more details about the image segmentation model and its workings, refer to [How U-net works?](https://developers.arcgis.com/python/guide/how-unet-works/) in the guide section.

# ## Necessary imports

# In[1]:


import os, zipfile
from pathlib import Path
from os import listdir
from os.path import isfile, join

from arcgis import GIS
from arcgis.learn import prepare_data, PSPNetClassifier, UnetClassifier, DeepLab


# ## Connect to your GIS

# In[4]:


gis = GIS(profile='your_online_profile')
ent_gis = GIS(profile='your_enterprise_profile')


# ## Export training data

# For this scenario, we have landsat-7 imagery and a classified change map with class information between two time periods (2001-2016) collected over Egypt, with a spatial resolution of 30 m. We will export this data in the “Classified Tiles” metadata format available in the `Export Training Data For Deep Learning` tool. The tool is available in [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview) and [ArcGIS Image Server](https://www.esri.com/en-us/arcgis/products/arcgis-image-server).
# 
# For the model to identify changes between imagery, we will build composite imagery from two different time periods using the [composite](https://pro.arcgis.com/en/pro-app/latest/tool-reference/data-management/composite-bands.htm) tool in [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview). It will be provided as an `Input Raster` to the model, and the change map will be provided as labels. The change map was produced by providing the `change detection wizard` available in [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview), with land use and land cover classification maps from two time periods (2011, 2016) or the from and to raster. The 'to class' or the class to which it has changed to, is used as change labels for the model training. The `change detection wizard` tool was used in order to create a change dataset. If change raster is already avialable it can be used directly to export and train the model.
# 
# - `Input Raster`: Composite_imagery_2001_2016
# - `Input feature class or classified raster`: change_map_2001_2016
# - `Tile Size X & Tile Size Y`: 256
# - `Stride X & Stride Y`: 128
# - `Meta Data Format`: 'Classified Tiles' as we are training a segmentation model.
# - `Environments`: Set optimum `Cell Size`, `Processing Extent`.

# The rasters used for exporting the training dataset are provided below:

# In[5]:


landsat_composite_2001_16 = ent_gis.content.get('dbe2221e4e1240e986586f0eb3a1479b')
landsat_composite_2001_16


# In[6]:


landsat_changemap_2001_16 = ent_gis.content.get('8eef999714d64e9f9494030b9c5f76c2')
landsat_changemap_2001_16


# <div style="align: center; text-align:center; line-height: 5em"> 
#     <div class="caption"> Export Training Data for Deep Learning tool</div>
# </div>

# ## Prepare the data

# Alternatively, we have also provided a subset of training data containing a few samples with the rasters used for exporting the training dataset. You may use the data directly to carry out the experiments.

# In[7]:


training_data = gis.content.get('3aedfa7e790541a79730eb1db47ed419')
training_data


# In[8]:


filepath = training_data.download(file_name=training_data.name)


# In[5]:


#Extract the data from the zipped image collection
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[6]:


data=prepare_data(path=Path(filepath).parent, batch_size=2, 
                  class_mapping={1:'to forest',2:'to savannas',3:'to grasslands',4:'to wetlands',5:'to croplands',
                                 6:'to urban',7:'to tundra',8:'to barren',9:'to water',10:'to open shrublands',
                                 11:'No change'})


# Displayed below are the various change class values with names with which pixels of the change raster are assigned 

# In[12]:


data.class_mapping


# ### Visualize a few samples from your training data

# To get a better sense of the training data, we will use the `show_batch()` method in `arcgis.learn`. This method randomly picks a few training chips, overlayed by their respective labels, and visualizes them.
# 
# - `alpha`: Opacity of overlayed labels over the training chips

# In[9]:


data.show_batch(alpha=0.5)


# ## Train change detection model

# ### Load UNet model architecture

# In[4]:


model = UnetClassifier(data)

#or

#model = PSPNetClassifier(data)

#or

#model = DeepLab(data)


# ### Tune for optimal learning rate

# In[5]:


lr = model.lr_find()


# ### Fit the model

# To train the model, we use the `fit()` method. To start, we will train our model for 10 epochs. The number of epochs defines how many times a model will be exposed to the entire training set.
# 
# - `epochs`: Number of cycles of training on the data.
# - `lr`: Learning rate to be used for training the model.

# In[6]:


model.fit(10, lr)


# Here, with 10 epochs, we see that both the training and validation losses have decreased significantly, indicating that the model is learning to detect changes.

# ### Visualize results

# It is a good practice to visualize the results obtained from the model with the actual ground truth data. The `show_results` function picks random samples, and visualizes the ground truth and model predictions side by side. This enables us to preview the results of the model within the notebook.

# In[7]:


model.show_results()


# ### Save the model

# We will save the model that we trained as a 'Deep Learning Package' ('.dlpk' format). A Deep Learning package is the standard format used to deploy deep learning models on the ArcGIS platform.
# 
# We will use the `save()` method to save the trained model. By default, it will be saved to a sub-folder inside the training data folder with the name "models".

# In[ ]:


model.save(r'unet_change_model_e10')


# ### Compute model metrics

# In[19]:


model.accuracy()


# ## Model inferencing

# ### Generate a change raster utilizing the Classify Pixels Using Deep Learning tool

# After training the change detection model and saving the weights for detecting changes, we can use the `Classify Pixels Using Deep Learning` tool, available in both [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview) and [ArcGIS Enterprise](https://www.esri.com/en-us/arcgis/products/arcgis-enterprise/overview), for inferencing at scale. The test raster is a composite of 2001 and 2016 rasters, for another geographically identical location.

# In[29]:


test_data = ent_gis.content.get('8e0ca6e7cefb45289eccc0e7ae2c9bfd')
test_data


# <div style="align: left; text-align:center; line-height: 5em">
#     <div class="caption">Classify Pixels Using Deep Learning tool</div>
# </div>

# `with arcpy.EnvManager(cellSize=0.0002694460214, processorType='GPU', gpuId = '0'):`
# 
# 
#     `out_classified_raster = arcpy.ia.ClassifyPixelsUsingDeepLearning("test_area_change_landsat.tif", r"C:\Esri\unet_multiclass_change\models\model.emd", "padding 56;batch_size 4;predict_background True;tile_size 224")`
# 
#     `out_classified_raster.save(r"C:\Esri\unet_multiclass_change\test_area_change_predicted.tif")`

# ### Visualize change map

# The image below displays the final test areas used for model predictions for 2011 and 2016, as well as the final predicted areas of change from 2011 to 2016.

# <div style="align: left; text-align:center;line-height: 5em">
# <div class="caption"> (Left) 2011, (Mid) 2016, and (Right) predicted change from 2011 to 2016 </div>
# </div>

# ## Conclusion

# In this notebook, we demonstrated how to use image segmentation models, i.e. U-Net, DeepLab, PSPNet, available in `arcgis.learn`, to detect multi-class changes between imagery of two time-periods.

# ## References

# [1] Olaf Ronneberger, Philipp Fischer, Thomas Brox: U-Net: Convolutional Networks for Biomedical Image Segmentation, 2015;[arXiv:1505.04597](https://arxiv.org/abs/1505.04597) 


# ====================
# part1_prepare_hurricane_data.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Data Preparation - Hurricane analysis, part 1/3

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Download hurricane data from NCEI FTP portal](#Download-hurricane-data-from-NCEI-FTP-portal)
#     * [Download each file into the hurricanes_raw directory](#Download-each-file-into-the-hurricanes_raw-directory)
# * [Process CSV files by removing header rows](#Process-CSV-files-by-removing-header-rows)
#     * [Automate across all files](#Automate-across-all-files)
# * [Cleaning hurricane observations with Dask](#Cleaning-hurricane-observations-with-Dask)
#     * [Read input CSV data](#Read-input-CSV-data)
#     * [Merge all location columns](#Merge-all-location-columns)
#     * [Merge similar columns](#Merge-similar-columns)
#         * [Merge wind columns](#Merge-wind-columns)
#             * [Merge pressure columns](#Merge-pressure-columns)
#             * [Merge grade columns](#Merge-grade-columns)
#             * [Merge eye diameter columns](#Merge-eye-diameter-columns)
#         * [Identify remaining redundant columns](#Identify-remaining-redundant-columns)
#         * [Drop all redundant columns](#Drop-all-redundant-columns)
#     * [Perform delayed computation](#Perform-delayed-computation)
#         * [Preview results](#Preview-results)
# * [Creating hurricane tracks using Geoanalytics](#Creating-hurricane-tracks-using-Geoanalytics)
#     * [Create a data store](#Create-a-data-store)
#     * [Perform data aggregation using reconstruct tracks tool](#Perform-data-aggregation-using-reconstruct-tracks-tool)
#         * [Execute reconstruct tracks tool](#Execute-reconstruct-tracks-tool)
#         * [Analyze the result of aggregation](#Analyze-the-result-of-aggregation)
# * [Conclusion](#Conclusion)

# ## Introduction

# 
# Hurricanes are large swirling storms that produce winds of speeds `74` miles per hour (`119` kmph) or higher. When hurricanes make a landfall, they produce heavy rainfall, cause storm surges and intense flooding. Often hurricanes strike places that are dense in population, causing devastating amounts of death and destruction throughout the world.

# 

# Since the recent past, agencies such as the [National Hurricane Center](https://www.nhc.noaa.gov/aboutintro.shtml) have been collecting quantitative data about hurricanes. In this study we use meteorological data of hurricanes recorded in the past `169` years to analyze their location, intensity and investigate if there are any statistically significant trends. We also analyze the places most affected by hurricanes and what their demographic make up is. We conclude by citing relevant articles that draw similar conclusions.

# This notebook covers part 1 of this study. In this notebook, we:
# 
#   - download data from NCEI portal
#   - do extensive pre-processing in the form of clearing headers, merging redundant columns
#   - aggregate the observations into hurricane tracks.

# **Note**: To run this sample, you need a few extra libraries in your conda environment. If you don't have the libraries, install them by running the following commands from cmd.exe or your shell.

# In[ ]:


pip install dask==2.14.0
pip install toolz
pip install fsspec==0.3.1


# ## Download hurricane data from NCEI FTP portal
# The [National Centers for Environmental Information](https://www.ncdc.noaa.gov/), formerly [National Climatic Data Center](https://www.ncdc.noaa.gov/) shares the historic hurricane track datasets at [ftp://eclipse.ncdc.noaa.gov/pub/ibtracs/v03r09/all/csv/](ftp://eclipse.ncdc.noaa.gov/pub/ibtracs/v03r09/all/csv/). We use the `ftplib` Python library to login in and download these datasets.

# In[1]:


# imports for downloading data from FTP site
import os
from ftplib import FTP

# imports to process data using DASK
from dask import delayed
import dask.dataframe as ddf

# imports for data analysis and visualization
import pandas as pd
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

# imports to perform spatial aggregation using ArcGIS GeoAnalytics server
from arcgis.gis import GIS
from arcgis.geoanalytics import get_datastores
from arcgis.geoanalytics.summarize_data import reconstruct_tracks
import arcgis

# miscellaneous imports
from pprint import pprint
from copy import deepcopy


# Establish an anonymous connection to FTP site.

# In[4]:


conn = FTP(host='eclipse.ncdc.noaa.gov')
conn.login()


# Change directory to folder containing the hurricane files. List the files.

# In[5]:


conn.cwd('/pub/ibtracs/v03r10/all/csv/year/')
file_list = conn.nlst()
len(file_list)


# Print the top 10 items.

# In[6]:


file_list[:10]


# ### Download each file into the `hurricanes_raw` directory

# In[1]:


data_dir = r'data/hurricanes_data/'


# In[31]:


if 'hurricanes_raw' not in os.listdir(data_dir):
    os.mkdir(os.path.join(data_dir,'hurricanes_raw'))

hurricane_raw_dir = os.path.join(data_dir,'hurricanes_raw')
os.listdir(data_dir)


# Now we are going to download data from 1842-2017, whih might take around 15 mins.

# In[11]:


file_path = hurricane_raw_dir
for file in file_list:
    with open(os.path.join(file_path, file), 'wb') as file_handle:
        try:
            conn.retrbinary('RETR ' + file, file_handle.write, 1024)
            print(f'Downloaded {file}')
        
        except Exception as download_ex:
            print(f'Error downloading {file} + {str(download_ex)}')


# ## Process CSV files by removing header rows
# The CSV files have multiple header rows. Let us start by processing one of the files as an example.

# In[2]:


csv_path = os.path.join(hurricane_raw_dir,'Year.2017.ibtracs_all.v03r10.csv')


# In[18]:


df = pd.read_csv(csv_path)
df.head()


# The input looks mangled. This is because the file's row `1` has a header that pandas fails to read. So let us skip that row.

# In[19]:


df = pd.read_csv(csv_path, skiprows=1)
df.head()


# A little better. But the file's 3rd row is also a header. Let us drop that row.

# In[20]:


df.drop(labels=0, axis=0, inplace=True)
df.head()


# ### Automate across all files
# Now we need to repeat the above cleaning steps across all CSV files. In the steps below, we will read all CSV files, drop the headers, and write to disk. This step is necessary as it will ease subsequent processing using the DASK library.

# In[9]:


file_path = hurricane_raw_dir
num_records = {}
for file in file_list:
    df = pd.read_csv(os.path.join(file_path, file), skiprows=1)
    num_records[str(file.split('.')[1])] = df.shape[0]
    
    df.drop(labels=0, axis=0, inplace=True)
    df.to_csv(os.path.join(file_path, file))
    print(f'Processed {file}')


# # Cleaning hurricane observations with Dask
# The data collected from NOAA NCDC source is just too large to clean with Pandas or Excel. With `350,000 x 200` in dense matrix, this data is larger than memory for a normal computer. Hence traditional packages such as Pandas cannot be used as they expect data to fit fully in memory.
# 
# Thus, in this part of the study, we use **[Dask](https://dask.org/)**, a distributed data analysis library. Functionally, Dask provides a `DataFrame` object that behaves similar to a traditional pandas `DataFrame` object. You can perform slicing, dicing, exploration on them. However transformative operations on the `DataFrame` get queued and are operated only when necessary. When executed, Dask will read data in chunks, distribute it to workers (be it cores on a single machine or multiple machines in a cluster set up) and collect the data back for you. Thus, DASK allows you to work with any larger than memory dataset as it performs operations on chunks of it, in a distributed manner.

# ## Read input CSV data
# As mentioned earlier, DASK allows you to work with larger than memory datasets. These datasets can reside as one large file or as multiple files in a folder. For the latter, DASK allows you to just specify the folder containing the datasets as input. In turn, it provides you a single `DataFrame` object that represents all your datasets combined together. The operations you perform on this `DataFrame` get queued and executed only when necessary.

# In[3]:


fld_path = hurricane_raw_dir
csv_path = os.path.join(fld_path,'*.csv')


# Preemptively, specify the assortment of values that should be treated as null values.

# In[36]:


table_na_values=['-999.','-999','-999.000', '-1', '-1.0','0','0.0']
full_df = ddf.read_csv(csv_path, na_values=table_na_values, dtype={'Center': 'object'})


# You can query the top few (or bottom few) records as you do on a regular Pandas `DataFrame` object.

# In[37]:


full_df.head()


# Drop the first duplicate index column.

# In[38]:


full_df = full_df.drop(labels=['Unnamed: 0'], axis=1)


# In[39]:


all_columns=list(full_df.columns)
len(all_columns)


# This dataset has `200` columns. Not all are unique, as you can see from the print out below:

# In[40]:


pprint(all_columns, compact=True, width=100)


# Reading the metadata from NOAA NCDC site, we find sensor measurements get unique columns if they are collected by a different agency. Thus we find multiple **pressure**, **wind speed**, **latitude**, **longitude**, etc. columns with different suffixes and prefixes. Data is sparse as it gets distributed between these columns. For our geospatial analysis, it suffices if we can merge these columns together and get location information from the coordinates.

# ## Merge all location columns
# Below we prototype merging location columns. If this succeeds, we will proceed to merge all remaining columns.

# In[41]:


lat_columns = [x for x in all_columns if 'lat' in x.lower()]
lon_columns = [x for x in all_columns if 'lon' in x.lower()]
for x in zip(lat_columns, lon_columns):
    print(x)


# In this dataset, if data is collected by 1 agency, the corresponding duplicate columns from other agencies are empty. However there may be exceptions. Hence we define a custom function that will pick median value for a row, from a given list of columns. This way, we can consolidate latitude / longitude information from all the agencies.

# In[42]:


def pick_median_value(row, col_list):
    return row[col_list].median()


# In[43]:


full_df['latitude_merged'] = full_df.apply(pick_median_value, axis=1,
                                          col_list = lat_columns)


# In[44]:


full_df['longitude_merged'] = full_df.apply(pick_median_value, axis=1,
                                          col_list = lon_columns)


# With `dask`, the above operation was delayed and stored in a queue. It has not been evaluated yet. Next, let us evaluate for `5` records and print output. If results look good, we will merge all remaining related columns together.

# In[45]:


full_df.head(5)


# The results look good. Two additional columns (`latitude_merged`, `longitude_merged`) have been added. By merging related columns, the redundant sparse columns can be removed, thereby simplifying the dimension of the input dataset. 
# 
# Now that this prototype looks good, we will proceed by identifying the lists of remaining columns that are redundant and can be merged.

# ## Merge similar columns
# To keep track of which columns have been accounted for, we will duplicate the `all_columns` list and remove ones that we have identified.

# In[46]:


columns_tracker = deepcopy(all_columns)
len(columns_tracker)


# From the `columns_tracker` list, let us remove the redundant columns we already identified for location columns.

# In[47]:


columns_tracker = [x for x in columns_tracker if x not in lat_columns]
columns_tracker = [x for x in columns_tracker if x not in lon_columns]
len(columns_tracker)


# Thus, we have reduced the number of columns from `200` to `142`. We will progressively reduce this while retaining key information. 
# 
# ### Merge wind columns
# Wind, pressure, grade are some of the meteorological observations this dataset contains. To start off, let us identify the wind columns:

# In[48]:


# pick all columns that have 'wind' in name
wind_columns = [x for x in columns_tracker if 'wind' in x.lower()]

# based on metadata doc, we decide to eliminate percentile and wind distance columns
columns_to_eliminate = [x for x in wind_columns if 'radii' in x or 'percentile' in x.lower()]

# trim wind_columns by removing the ones we need to eliminate
wind_columns = [x for x in wind_columns if x not in columns_to_eliminate]
wind_columns


# In[49]:


full_df['wind_merged'] = full_df.apply(pick_median_value, axis=1,
                                          col_list = wind_columns)


# #### Merge pressure columns
# We proceed to identify all `pressure` columns. But before that, we update the `columns_tracker` list by removing those we identified for wind:

# In[50]:


columns_tracker = [x for x in columns_tracker if x not in wind_columns]
columns_tracker = [x for x in columns_tracker if x not in columns_to_eliminate]
len(columns_tracker)


# In[51]:


# pick all columns that have 'pres' in name
pressure_columns = [x for x in columns_tracker if 'pres' in x.lower()]

# from metadata, we eliminate percentile and pres distance columns
if columns_to_eliminate:
    columns_to_eliminate.extend([x for x in pressure_columns if 'radii' in x or 'percentile' in x.lower()])
else:
    columns_to_eliminate = [x for x in pressure_columns if 'radii' in x or 'percentile' in x.lower()]

# trim wind_columns by removing the ones we need to eliminate
pressure_columns = [x for x in pressure_columns if x not in columns_to_eliminate]
pressure_columns


# In[52]:


full_df['pressure_merged'] = full_df.apply(pick_median_value, axis=1,
                                          col_list = pressure_columns)


# #### Merge grade columns

# In[53]:


columns_tracker = [x for x in columns_tracker if x not in pressure_columns]
columns_tracker = [x for x in columns_tracker if x not in columns_to_eliminate]
len(columns_tracker)


# Notice the length of `columns_tracker` is reducing progressively as we identify redundant columns.

# In[54]:


# pick all columns that have 'grade' in name
grade_columns = [x for x in columns_tracker if 'grade' in x.lower()]
grade_columns


# In[55]:


full_df['grade_merged'] = full_df.apply(pick_median_value, axis=1,
                                          col_list = grade_columns)


# #### Merge eye diameter columns

# In[56]:


columns_tracker = [x for x in columns_tracker if x not in grade_columns]
len(columns_tracker)


# In[57]:


# pick all columns that have 'eye' in name
eye_dia_columns = [x for x in columns_tracker if 'eye' in x.lower()]
eye_dia_columns


# In[58]:


full_df['eye_dia_merged'] = full_df.apply(pick_median_value, axis=1,
                                          col_list = eye_dia_columns)


# ### Identify remaining redundant columns

# In[59]:


columns_tracker = [x for x in columns_tracker if x not in eye_dia_columns]
len(columns_tracker)


# We are down to `49` columns, let us visualize what those look like.

# In[60]:


pprint(columns_tracker, width=119, compact=True)


# Based on metadata shared by data provider, we choose to retain only the first `11` columns. We add the rest to the list `columns_to_eliminate`.

# In[61]:


columns_to_eliminate.extend(columns_tracker[11:])
pprint(columns_to_eliminate, width=119, compact=True)


# ### Drop all redundant columns
# So far, we have merged similar columns together and collected the lists of redundant columns to drop. Below we compile them into a single list.

# In[62]:


len(full_df.columns)


# In[63]:


columns_to_drop = lat_columns + lon_columns + wind_columns + pressure_columns + \
                    grade_columns + eye_dia_columns+columns_to_eliminate
len(columns_to_drop)


# ## Perform delayed computation
# In Dask, all computations are delayed and queued. The `apply()` functions called earlier are not executed yet, however respective columns have been created as you can see from the DataFrame display above. In the cells below, we will call `save()` to make Dask compute on this larger than memory dataset.
# 
# Calling `visualize()` on the delayed compute operation or the `DataFrame` object will plot the dask task queue as shown below. The graphic below provides a glimpse on how Dask distributes its tasks and how it reads this 'larger than memory dataset' in chunks and operates on them.

#  Drawing dask graphs requires the `graphviz` python library and the `graphviz` system library to be installed.

# In[ ]:


get_ipython().system('conda install --yes -c anaconda graphviz')
get_ipython().system('conda install --yes -c conda-forge python-graphviz')


# In[34]:


full_df.visualize()


# Below we execute all the column merge and the column drop operations that we have queued so far. We store the resulting `DataFrame` in a new variable.

# In[ ]:


if 'hurricanes_merged' not in os.listdir(data_dir):
    os.mkdir(os.path.join(data_dir,'hurricanes_merged'))

merged_csv_path = os.path.join(data_dir, 'hurricanes_merged')


# Please note that the merge operation below might take about one hour. If you do not want to run the cell below, preprocessed data has been provided in sample data.

# In[64]:


merged_df = full_df.drop(columns_to_drop, axis=1)
merged_df.to_csv(os.path.join(merged_csv_path, 'hurr_dask_*.csv'))


# The `save()` operation spawns several workers that compute in parallel. Notice the ouput file name contains a wildcard (`*`). This allows Dask to both read data in chunks and write outputs in chunks. The save operation will result in creating a number of processed CSV files whose names are prefixed with `hurr_dask` and suffixed with a number.

# ### Preview results
# Once Dask realizes a delayed computation, it returns the result as an in-memory Pandas DataFrame object. Thus, the `merged_df` variable represents a Pandas `DataFrame` object with `348,703` records and `17` columns.

# In[65]:


merged_df.shape


# # Creating hurricane tracks using Geoanalytics
# 
# The data collected so far are a set of Point observations representing all hurricanes recorded in the last `169` years. To make sense of these points, we need to connect them together to create a track for each hurricane. This part of the notebook uses ArcGIS GeoAnalytics server to reconstruct such hurricane tracks. The GeoAnalytics server is capable of processing on massive datasets in a scalable and distributed fashion.
# 
# The DASK process merged redundant columns together and ouput a folder full of CSV files. The GeoAnalytics server is also capabale of accepting a folder of datasets as 1 dataset and working on them. Thus in this part, we register that folder as a **datastore** and on the GeoAnalytics server for processing.
# 
# **Reconstruct tracks**:
# Reconstruct tracks is a type of data aggregation tool available in the `arcgis.geoanalytics` module. This tool works with a layer of point features or polygon features that are time enabled. It first determines which points belong to a track using an identification number or identification string. Using the time at each location, the tracks are ordered sequentially and transformed into a line representing the path of movement. The map below shows a subset of the point datasets.

# 

# ## Create a data store
# For the GeoAnalytics server to process your big data, it needs the data to be registered as a data store. In our case, the data is in multiple CSV files and we will register the folder containing the files as a data store of type `bigDataFileShare`.
# 
# Let us connect to our Organization.

# In[7]:


gis = GIS(url='https://pythonapi.playground.esri.com/portal', username='arcgis_python', password='amazing_arcgis_123')


# Get the geoanalytics datastores and search for the registered datasets:

# In[20]:


# Query the data stores available
datastores = get_datastores()
bigdata_fileshares = datastores.search()
bigdata_fileshares


# The dataset `hurricanes_dask_csv` data is registered as a big data file share with the Geoanalytics datastore, so we can reference it:

# In[21]:


data_item = bigdata_fileshares[1]


# If there is no big data file share for hurricane track data registered on the server, we can register one that points to the shared folder containing the CSV files.

# In[ ]:


data_item = datastores.add_bigdata("Hurricane_tracks", r"\\path_to_hurricane_data")


# Once a big data file share is registered, the GeoAnalytics server processes all the valid file types to discern the schema of the data, including information about the geometry in a dataset. If the dataset is time-enabled, as is required to use some GeoAnalytics Tools, the manifest reports the necessary metadata about how time information is stored as well.
# 
# This process can take a few minutes depending on the size of your data. Once processed, querying the manifest property will return a schema. As you can see from below, the schema contains the columns we merged using DASK previously.

# In[22]:


datasets = data_item.manifest['datasets']
len(datasets)


# In[23]:


[dataset['name'] for dataset in datasets]


# In[24]:


datasets[0]


# ## Perform data aggregation using reconstruct tracks tool
# 
# When you add a big data file share, a corresponding item gets created in your GIS. You can search for it like a regular item and query its layers.

# In[25]:


search_result = gis.content.search("bigDataFileShares_hurricanes_dask_csv", item_type = "big data file share")
search_result


# In[26]:


data_item = search_result[0]
cleaned_csv = data_item.layers[0]
cleaned_csv


# ### Execute reconstruct tracks tool
# 
# The `reconstruct_tracks()` function is available in the `arcgis.geoanalytics.summarize_data` module. In this example, we are using this tool to aggregate the numerous points into line segments showing the tracks followed by the hurricanes. The tool creates a feature layer item as an output which can be accessed once the processing is complete.

# In[27]:


agg_result = reconstruct_tracks(cleaned_csv, 
                                track_fields='Serial_Num',   # the Hurricane id number
                                method='GEODESIC', output_name='hurricane_tracks_aggregated_ga')


# ### Analyze the result of aggregation
# The reconstruct tracks produces summary statistics such as `MIN`, `MAX`, `MEAN`, `MEDIAN`, `RANGE`, `SD`, `VAR`, `SUM`, for numeric columns and `COUNT` for ordinal columns during the aggregation process. Let us list the fields in this dataset to view them.

# In[28]:


agg_tracks_layer = agg_result.layers[0]
agg_fields = [f.name for f in agg_tracks_layer.properties.fields]
pprint(agg_fields, compact=True)


# To get the number of hurricanes the reconstruct tracks tool identified, we run a query on the aggregation layer and get just the number of records.

# In[7]:


agg_tracks_layer.query(return_count_only=True)


# # Conclusion
# In this notebook, we observed how to download meteorological data over FTP from NEIC website. The data came in `169` CSV files for the past `169` years. We sanitized it initially using Pandas to remove bad header rows. We then used DASK library to read all `169` files as a single file and merged data from redundant columns. This pre-processing resulted in multiple output CSV files covering a total of 348k records and 17 columns.
# 
# This data was fed to the ArcGIS GeoAnalytics server for aggregation. The reconstruct tracks tool on GeoAnalytics server reduced this point dataset into hurricane tracks (lines) and during this aggregation, it calculated summary statistics for the numerical columns. The tool identified **`12,362`** individual hurricanes from the past `169` years.
# 
# In Part 2 of this study, we will visualize and explore this dataset to understand the prevelance, duration of hurricanes and the communities affected by hurricanes worldwide.
# 
# In Part 3, we will analyze this aggregated result comprehensively, answer important questions such as, does the intensity of hurricanes increase over time and draw conclusions.


# ====================
# part2_explore_hurricane_tracks.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Exploratory Statistics - Hurricane analysis, part 2/3

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Access-aggregated-hurricane-data" data-toc-modified-id="Access-aggregated-hurricane-data-1.1">Access aggregated hurricane data</a></span><ul class="toc-item"><li><span><a href="#Query-hurricane-tracks-into-a-Spatially-Enabled-DataFrame" data-toc-modified-id="Query-hurricane-tracks-into-a-Spatially-Enabled-DataFrame-1.1.1">Query hurricane tracks into a Spatially Enabled <code>DataFrame</code></a></span></li></ul></li><li><span><a href="#Exploratory-data-analysis" data-toc-modified-id="Exploratory-data-analysis-1.2">Exploratory data analysis</a></span><ul class="toc-item"><li><span><a href="#Does-the-number-of-hurricanes-increase-with-time?" data-toc-modified-id="Does-the-number-of-hurricanes-increase-with-time?-1.2.1">Does the number of hurricanes increase with time?</a></span></li><li><span><a href="#How-many-hurricanes-occure-per-basin-and-sub-basin?" data-toc-modified-id="How-many-hurricanes-occure-per-basin-and-sub-basin?-1.2.2">How many hurricanes occure per basin and sub basin?</a></span></li><li><span><a href="#Are-certain-hurricane-names-more-popular?" data-toc-modified-id="Are-certain-hurricane-names-more-popular?-1.2.3">Are certain hurricane names more popular?</a></span></li><li><span><a href="#Is-there-a-seasonality-in-the-occurrence-of-hurricanes?" data-toc-modified-id="Is-there-a-seasonality-in-the-occurrence-of-hurricanes?-1.2.4">Is there a seasonality in the occurrence of hurricanes?</a></span></li><li><span><a href="#What-percent-of-hurricanes-make-landfall?" data-toc-modified-id="What-percent-of-hurricanes-make-landfall?-1.2.5">What percent of hurricanes make landfall?</a></span></li><li><span><a href="#How-far-do-hurricanes-travel-inland-after-landfall?" data-toc-modified-id="How-far-do-hurricanes-travel-inland-after-landfall?-1.2.6">How far do hurricanes travel inland after landfall?</a></span></li><li><span><a href="#Where-do-hurricanes-make-landfall?" data-toc-modified-id="Where-do-hurricanes-make-landfall?-1.2.7">Where do hurricanes make landfall?</a></span><ul class="toc-item"><li><span><a href="#Perform-density-analysis-on-hurricane-landfall-locations" data-toc-modified-id="Perform-density-analysis-on-hurricane-landfall-locations-1.2.7.1">Perform density analysis on hurricane landfall locations</a></span></li></ul></li><li><span><a href="#What-are-the-demographics-of-places-with-highest-density-of-landfalls?" data-toc-modified-id="What-are-the-demographics-of-places-with-highest-density-of-landfalls?-1.2.8">What are the demographics of places with highest density of landfalls?</a></span></li></ul></li><li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-1.3">Conclusion</a></span></li></ul></li></ul></div>

# This is the second part to a three part set of notebooks that process and analyze historic hurricane tracks. In the previous notebook we saw:
#  1. downloading historic hurricane datasets using Python
#  2. cleaning and merging hurricane observations using DASK
#  3. aggregating point observations into hurricane tracks using ArcGIS GeoAnalytics server
# 
# In this notebook you will analyze the aggregated tracks to investigate the communities that are most affected by hurricanes, as well as as answer important questions about the prevalance of hurricanes, their seasonality, their density, and places where they make landfall.

# **Note**: To run this sample, you need a few extra libraries in your conda environment. If you don't have the libraries, install them by running the following commands from cmd.exe or your shell.

# In[ ]:


pip install scipy
pip install seaborn


# Import the libraries necessary for this notebook.

# In[1]:


# import ArcGIS Libraries
from arcgis.gis import GIS
from arcgis.geometry import filters
from arcgis.geocoding import geocode
from arcgis.features.manage_data import overlay_layers
from arcgis.geoenrichment import enrich

# import Pandas for data exploration
import pandas as pd
import numpy as np
from scipy import stats

# import plotting libraries
import matplotlib.pyplot as plt
import seaborn as sns
get_ipython().run_line_magic('matplotlib', 'inline')

# import display tools
from pprint import pprint
from IPython.display import display

# import system libs
from sys import getsizeof

# Miscellaneous imports
import warnings
warnings.filterwarnings('ignore')


# In[3]:


gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# ## Access aggregated hurricane data
# Below, we access the tracks aggregated using GeoAnalytics in the previous notebook.

# In[4]:


hurricane_tracks_item = gis.content.search('title:hurricane_tracks_aggregated_ga', item_type="feature layer")[0]
hurricane_fl = hurricane_tracks_item.layers[0]


# The **GeoAnalytics** step calculated summary statistics of all numeric fields. However, only a few of the columns are of interest to us.

# In[5]:


pprint([f['name'] for f in hurricane_fl.properties.fields], compact=True, width=80)


# Below we select the following fields for the rest of this analysis.

# In[9]:


fields_to_query = ['objectid', 'count', 'min_season', 'any_basin', 'any_sub_basin',
                   'any_name', 'mean_latitude_merged', 'mean_longitude_merged',
                   'max_wind_merged', 'range_wind_merged', 'min_pressure_merged',
                   'range_pressure_merged', 'max_eye_dia_merged', 'track_duration',
                   'end_datetime', 'start_datetime']


# ### Query hurricane tracks into a Spatially Enabled `DataFrame`

# In[10]:


all_hurricanes_df = hurricane_fl.query(out_fields=','.join(fields_to_query), as_df=True)


# In[11]:


all_hurricanes_df.shape


# There are **`12,362`** hurricanes identified by GeoAnalytics aggregate tracks tool. To get an idea about this aggregated dataset, call the `head()` method.

# In[12]:


all_hurricanes_df.head()


# To better analyze this data set, the date columns need to be changed to a format that Pandas understands better. This is accomplished by calling the `to_datetime()` method and passing the appropriate time columns.

# In[14]:


all_hurricanes_df['start_datetime'] = pd.to_datetime(all_hurricanes_df['start_datetime'])
all_hurricanes_df['end_datetime'] = pd.to_datetime(all_hurricanes_df['end_datetime'])
all_hurricanes_df.index = all_hurricanes_df['start_datetime']
all_hurricanes_df.head()


# The track duration and length columns need to be projected to units (days, hours, miles) that are meaningful for analysis.

# In[15]:


all_hurricanes_df['track_duration_hrs'] = all_hurricanes_df['track_duration'] / 3600000
all_hurricanes_df['track_duration_days'] = all_hurricanes_df['track_duration'] / (3600000*24)


# ## Exploratory data analysis
# In this section we perform exploratory analysis of the dataset and answer some interesting questions.

# In[21]:


map1 = gis.map('USA')
map1


# In[17]:


all_hurricanes_df.sample(n=500, random_state=2).spatial.plot(map1, 
                                                             renderer_type='u',
                                                             col='any_basin',
                                                            cmap='prism')


# The map above draws a set of `500` hurricanes chosen at random. You can visualize the Spatially Enabled DataFrame object with different types of renderers. In the example above a unique value renderer is applied on the **basin** column. You can switch the map to 3D mode and view the same on a globe.

# In[22]:


map2 = gis.map()
map2.mode= '3D'
map2


# In[27]:


all_hurricanes_df.sample(n=500, random_state=2).spatial.plot(map2, 
                                                             renderer_type='u',
                                                             col='any_basin',
                                                            cmap='prism')


# ### Does the number of hurricanes increase with time?
# To understand if number of hurricanes have increased over time, we will plot a histogram of the `MIN_Season` column.

# In[18]:


ax = sns.distplot(all_hurricanes_df['min_season'], kde=False, bins=50)
ax.set_title('Number of hurricanes recorded over time')


# The number of hurricanes recorded increases steadily until `1970`. This could be due to advances in geospatial technologies allowing scientists to better monitor hurricanes. However, after `1970` we notice a reduction in the number of hurricanes. This is in line with what [scientists observe and predict](https://www.nytimes.com/2018/10/10/climate/hurricane-michael-climate-change.html).

# ### How many hurricanes occure per basin and sub basin?
# Climate scientists have organized global hurricanes into `7` basins and a number of sub basins. The snippet below groups the data by basin and sub basin, counts the occurrences, and plots the frequency in bar charts.

# In[19]:


fig1, ax1 = plt.subplots(1,2, figsize=(12,5))

basin_ax = all_hurricanes_df['any_basin'].value_counts().plot(kind='bar', ax=ax1[0])
basin_ax.set_title('Number of hurricanes per basin')
basin_ax.set_xticklabels(['Western Pacific', 'South Indian', 'North Atlantic',
                          'Eastern Pacicifc', 'North Indian','Southern Pacific',
                          'South Atlantic'])

sub_basin_ax = all_hurricanes_df['any_sub_basin'].value_counts().plot(kind='bar', ax=ax1[1])
sub_basin_ax.set_title('Number of hurricanes per sub basin')
sub_basin_ax.set_xticklabels(['MM','North Atlantic','Bay of Bengal','Western Australia',
                             'Eastern Australia', 'Carribean Sea', 'Gulf of Mexico',
                             'Arabian Sea', 'Central Pacific'])
sub_basin_ax.tick_params()


# Thus, most hurricanes occur in the **Western Pacific** basin. This is the region that is east of China, Phillipines and rest of South East Asia. This is followed by the **South Indian** basin which spans from west of Australia to east of Southern Africa. The **North Atlantic** basin which is the source of hurricanes in the continental United States ranks as the third busiest hurricane basin.

# ### Are certain hurricane names more popular?
# 

# Pandas provides a handy function called `value_counts()` to count unique occurrences. We use that below to count the number of times each hurricane name has been used. We then print the top `25` most frequently used names.

# In[20]:


# Get the number of occurrences of top 25 hurricane names
all_hurricanes_df['any_name'].value_counts()[:25]


# Names like `FLORENCE`, `IRMA`, `OLGA`.. appear to be more popular. Interestingly, all are of female gender. We can take this further to explore at what time periods have the name `FLORENCE` been used?

# In[21]:


all_hurricanes_df[all_hurricanes_df['any_name']=='FLORENCE'].index


# The name `FLORENCE` had been used consistently in since the 1950s, reaching a peak in popularity during the 60s.

# ### Is there a seasonality in the occurrence of hurricanes?

# Hurricanes happen when water temperatures (`Sea Surface Temperature` SST) are warm. Solar incidence is one of the key factors affecting SST and this typically happens during summer months. However, summer happens during different months in northern and southern hemispheres. To visualize this seasonality, we need to group our data by month as well as basin. The snippet below creates a multilevel index grouper in `pandas`.

# In[22]:


# Create a grouper object
grouper = all_hurricanes_df.start_datetime.dt.month_name()

# use grouper along with basin name to create a multilevel groupby object
hurr_by_basin = all_hurricanes_df.groupby([grouper,'any_basin'], as_index=True)
hurr_by_basin_month = hurr_by_basin.count()[['count', 'min_pressure_merged']]
hurr_by_basin_month.head()


# Now, we turn the index into columns for further processing.

# In[23]:


# turn index into columns
hurr_by_basin_month.reset_index(inplace=True)
hurr_by_basin_month.drop('min_pressure_merged', axis=1, inplace=True)
hurr_by_basin_month.columns = ['month', 'basin', 'count']
hurr_by_basin_month.head()


# We add the month column back, but this time we will help Pandas understand how to sort months in the correct chronological order.

# In[64]:


fig, ax = plt.subplots(1,1, figsize=(15,7))
month_order = ['January','February', 'March','April','May','June',
              'July','August','September','October','November','December']

sns.barplot(x='month', y='count', hue='basin', data=hurr_by_basin_month, ax=ax,
            order=month_order)


# The bars in **red** represent the number of hurricanes in the **Sounth Indian** ocean, which spans from west of Australia to east of southern Africa. The **brown** bars are for the **Western Pacific** ocean, which spans east of China. The **orange** bars are for the **North Atlantic** ocean. The sinusoidal nature of these bars show the charateristic offset in summer between northern and southern hemispheres. The **green** bars represent **North Indian** hurricanes, which is dominated by the monsoon effect, and is seen to be prevalant for most part of the year.

# ### What percent of hurricanes make landfall?
# While exploring the hurricane data on maps, we noticed their geographic distribution and that they travel long distances over the oceans. Thus, do all hurricanes eventually make landfall? If not, what percent of them do? This is an important question to answer as the threat to human life decreases dramatically when a hurricane does not make a landfall.
# 
# We will answer this question by performing **overlay analysis**. For this, we need to intersect the hurricane tracks with world boundary dataset. We will make an anonymous connection to ArcGIS Online to look for a layer published by Esri in the Living Atlas.

# In[70]:


agol_gis = GIS(set_active=False)


# In[107]:


world_boundaries_item = agol_gis.content.search('World Continents owner:esri_dm', 'feature layer', outside_org=True)[0]
world_boundaries_item


# We will import the `overlay_layers` tool from the `manage_data` toolset to perform the overlay analysis.

# In[108]:


boundary_fl = world_boundaries_item.layers[0]
from arcgis.features.manage_data import overlay_layers


# The overlay operation below migth take 1~2 hours in a regular hosted notebook environment.

# In[96]:


inland_tracks = overlay_layers(input_layer=hurricane_fl, overlay_layer = boundary_fl, 
                               overlay_type='INTERSECT', output_type='INPUT', 
                               output_name='hurricane_landfall_tracks', gis=gis)


# As part of the intersect operation, the output type is specified as `Input`. Since the input layer is hurricane tracks (a line layer), the result will continue to be a line layer. We can draw this layer on a map to view those hurricanes that have made a landfall and traveled inland.

# In[24]:


landfall_tracks_map = gis.map("USA")
landfall_tracks_map


# In[98]:


landfall_tracks_map.add_layer(inland_tracks)


# We query the landfall tracks layer into a DataFrame. We will then plot a bar chart showing what percent of hurricanes in each basin make a landfall.

# In[27]:


fields_to_query = ['min_season', 'any_basin','any_name', 'max_wind_merged', 
                   'min_pressure_merged', 'track_duration','end_datetime', 
                   'start_datetime', 'analysislength']

landfall_tracks_fl = inland_tracks.layers[0]


# In[28]:


landfall_tracks_df = landfall_tracks_fl.query(out_fields=fields_to_query).df
landfall_tracks_df.head(3)


# In[30]:


fig1, ax1 = plt.subplots(1,1, figsize=(12,5))

basin_ax = all_hurricanes_df['any_basin'].value_counts().plot(kind='bar', ax=ax1)
basin_ax = landfall_tracks_df['any_basin'].value_counts().plot(kind='bar', 
                                                               ax=ax1, 
                                                               cmap='viridis', 
                                                               alpha=0.5)
basin_ax.set_title('Number of hurricanes per basin that make landfall')
basin_ax.tick_params(axis='x', labelrotation=65)
basin_ax.set_xticklabels(['Western Pacific', 'South Indian', 'North Atlantic',
                          'Eastern Pacicifc', 'North Indian','Southern Pacific',
                          'South Atlantic'])
basin_ax.tick_params()


# The bar chart above plots the number of hurricanes (per basin) that made landfall over another bar chart of the total number of hurricanes per basin. From the chart, most hurricanes in the **Western Pacific**, **South Indian**, **North Atlantic** ocean make landfall. Hurricanes in **Southern Pacific** ocean rarely make landfall. This helps us guage the severity of hurricanes in different geographic basins.

# ### How far do hurricanes travel inland after landfall?
# Hurricanes in general lose velocity and intensity after they make a landfall. Thus they can only travel a short distance inland. As a result of the overlay analysis, an `analysislength` column is created. We can plot the histogram of that column to understand how far hurricanes have traveled inland after landfall.

# In[43]:


landfall_tracks_df['analysislength'].plot(kind='hist', bins=100,
                                          title='Histogram of distance traveled inland',
                                         figsize=(12,7), xlim=[-100,2500])
plt.xlabel('Distance in miles')


# Thus, majority of hurricanes travel less than `500` miles after making a landfall. We can query which are the top `50` hurricanes that have traveled longest inland.

# In[54]:


# filter the top 50 longest hurricanes (distance traveled inland)
top_50_longest = landfall_tracks_df.sort_values(by=['analysislength'], axis=0, ascending=False).head(50)
top_50_longest['any_basin'].value_counts().plot(kind='bar')


# **Southern Pacific** basin, followed by **South Indian** basin contains the hurricanes that have traveled longest inland.

# In[59]:


inland_map = gis.map()
inland_map


# In[58]:


top_50_longest.spatial.plot(inland_map, renderer_type='u', col='any_basin',cmap='prism')


# Plotting this on the map, we notice hurricanes have traveled longest inland over the east coast of North America, China and Australia. Interestingly, Australia bears landfall of hurricanes from both **South Indian** and **Western Pacific** basins.

# ### Where do hurricanes make landfall?
# Of equal interest is finding where hurricanes make landfall. From experience we know certain regions are prone to hurricane damage more than the rest. Using spatial data science, we can empirically derive those regions that have statistically more hurricane landfalls compared to the rest.
# 
# For this, we will repeat the **overlay analysis**. However, this time, we will change the `output_type` to `POINT`. The tool will return the points along the coast lines where hurricanes make landfall.

# In[109]:


landfall_item = overlay_layers(input_layer=hurricane_fl, overlay_layer = boundary_fl, 
                               overlay_type='INTERSECT', output_type='POINT', 
                               output_name='hurricane_landfall_locations', gis=gis)


# In[110]:


landfall_points_fl = landfall_item.layers[0]


# In[25]:


landfall_map = gis.map()
landfall_map


# In[50]:


landfall_map.add_layer(landfall_item)


# #### Perform density analysis on hurricane landfall locations
# The map above shows hundreds of thousands of points spread around the world. Do all these places have equal probability of being hit by a hurricane? To answer this, we will perform **density analysis**. The `calculate_density` tool available under the `analyze_patterns` toolset is used for this.

# In[112]:


from arcgis.features.analyze_patterns import calculate_density
landfall_density_item = calculate_density(input_layer=landfall_points_fl, radius=30,
                                          radius_units='Miles', area_units='SquareMiles',
                                          classification_type='NaturalBreaks', num_classes=7,
                                          output_name='landfall_density', gis=gis)


# In[129]:


landfall_map = gis.map('Florida, USA')
landfall_map


# In[128]:


landfall_map.add_layer(landfall_density_item)


# The map here computes the kernel density of landfall locations. It does this by summing the number of landfalls within a radius of `30` miles and dividing it by the area of this radius. Thus it spreads the number of landfalls over a smooth surface, then classifies this surface into `7` classes. By performing density anlaysis, we are able to wade through large clouds of landfall points and identify locations that have more landfalls compared to the rest of the world.
# 
# Let us visualize the density analysis results on a table and as a chart.

# In[130]:


landfall_density_sdf = landfall_density_item.layers[0].query(as_df=True)
landfall_density_sdf.head()


# In[133]:


ax = landfall_density_sdf['class'].hist()
ax.set_title('Histogram of hurricane landfall densities')


# From the histogram above, we notice there are only a very few places that can be classified as having a high density of hurricane landfalls. Let us analyze these places a bit further.

# In[134]:


high_density_landfalls = landfall_density_sdf[(landfall_density_sdf['class']==6) | 
                                              (landfall_density_sdf['class']==7)]

high_density_landfalls.shape


# We have identified `40` sites worldwide that have a high density of hurricane landfalls based on the anlaysis of data spanning the last `169` years. Below, we plot them on a map.

# In[146]:


high_density_landfall_map1 = gis.map('North Carolina')
high_density_landfall_map2 = gis.map('China')
display(high_density_landfall_map1)
display(high_density_landfall_map2)


# In[145]:


high_density_landfalls.spatial.plot(map_widget=high_density_landfall_map1, line_width=0, outline_color=0)
high_density_landfalls.spatial.plot(map_widget=high_density_landfall_map2, line_width=0, outline_color=0)


# The places that turn up are not much of a surprise. We notice the coast of **Carolinas** in the United States, the states of **West Bengal**, **Orissa** in India, several places along the East coast of **China**, southern tip of **Japan**, and most of the island of **Philippines** are the places that are most affected on a repeat basis.

# ### What are the demographics of places with highest density of landfalls?
# Now that we have found the places that have the highest desnity of landfalls, we are faced with the question, who lives there? What is the impact of repeat natural calamities on these places? We can answer those questions by **geoenriching** those polygons with demographic and socio-economic attributes.
# 
# <blockquote>
#     Note: Geoenrichment costs ArcGIS Online credits and requires this functionality to be configured in your Organization
# </blockquote>
# 
# Below we use the `enrich()` function to add socio-economic columns to the landfall density `DataFrame`. For a data collection, we pick `keyGlobalFacts` which is available for most part of the world.

# In[210]:


landfalls_enriched = enrich(high_density_landfalls, data_collections='keyGlobalFacts')
landfalls_enriched.head()


# The `enrich()` operation accepts the Spatially Enabled `DataFrame`, performs spatial aggregation and returns another `DataFrame` with socio-economic and demographic columns added to it. The `data_collections` parameter decides which additional columns get added.
# 
# Let us visualize the population that is affected by country. For this, we group by the `sourceCountry` column and sum up the results. The cell below plots the total number of men, women and households that live within the high density polygons.

# In[216]:


fig, ax = plt.subplots(1,2, figsize=(15,5))
# plot bar chart 1
grouper1 = landfalls_enriched[['TOTFEMALES','TOTMALES','sourceCountry']].groupby(by='sourceCountry')
grouper1.sum().plot(kind='bar', stacked=True, ax=ax[0], 
                    title='Population living within high density areas')

# plot bar chart 2
grouper2 = landfalls_enriched[['TOTHH','sourceCountry']].groupby(by='sourceCountry')
grouper2.sum().plot(kind='bar', ax=ax[1], 
                    title='Total number of households within high density areas')

plt.tight_layout()


# Hurricanes make landfalls on coasts, which invariably are places of dense population. By geo enriching the density maps, we are able to determine that the coasts of **China** (7 million), **Hongkong** (7M), **India** (6M) and **Philippines** (3M) are at high risk as they suffer repeat landfalls and also support large populations.
# 
# This information can be used by city planners to better zone the coasts and avoid development along hurricane prone areas.

# ## Conclusion
# In this notebook, we accessed the aggregated hurricane track data from Part 1 as Spatially Enabled `DataFrame` objects. We visualized these DataFrames both spatially and as charts to reveal interesting information about the global hurricane dataset.
# 
# We learned that global hurricanes are classified into `7` basins, with the most hurricanes occurring over **Western Pacific**. The **North Atlantic** basin, which affects the continental United States, ranks as third busiest basin. The number of hurricanes recorded worldwide has been steadily climbing as technology improves. However, after the `1970`s, we notice a year-over-year reduction in the number of hurricanes. We analyze more of this phenomena in the next part of this study.
# 
# As for hurricane names, the top spot is a tie between **Irma** and **Florence**, with `15` occurrences for each so far. By turning this DataFrame into a timeseries, we were able to observe a sinusoidal seasonality. The peaks between hurricanes in northern and cyclones in southern hemisphere were offest by about `6` months, matching the time when summer occurs in these hemispheres. We also noticed that hurricanes over the **North Indian** basin occur throughout the year as they are influenced by a monsoon phenomena.
# 
# We then performed **overlay analysis** to understand where hurricanes make landfall and the path they take once they make landfall. We noticed that the majority of hurricanes make landfall. Once they make a landfall, the majority travel under `100` miles inland. 
# 
# We extended the overlay analysis to calculate the exact points where landfall occurs. After performing a density analysis on the landfall locations, we found places along the coasts of the **Carolinas** in the United States, **Orissa, West Bengal** in India, several places along the east coast of **China**, southern tip of **Japan** and most of **Phillippines** are affected from a repeat landfall / historical perspective. 
# 
# By geo-enriching the high density places, we were able to understand the number of people that live in these places. **China** tops the list with most people affected.
# 
# In the next notebook, we extend this analysis to answer the important question: Does the intensity of hurricanes increase over time?


# ====================
# part3_analyze_hurricane_tracks.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Correlation - Hurricane analysis, part 3/3

# ## Table of Contents
# * [Access aggregated hurricane data](##Access-aggregated-hurricane-data)
#     * [Query hurricane tracks into a Spatially Enabled <code>DataFrame</code>](#Query-hurricane-tracks-into-a-Spatially-Enabled-DataFrame)
#     * [Query landfall tracks layer into a Spatially Enabled <code>DataFrame</code>](#Query-landfall-tracks-layer-into-a-Spatially-Enabled-DataFrame)
# * [Manage missing sensor data](#Manage-missing-sensor-data)
#     * [Visualize missing records](#Visualize-missing-records)
#     * [Missing value imputation](#Missing-value-imputation)
# * [Does intensity of hurricanes increase over time?](#Does-intensity-of-hurricanes-increase-over-time?)
# * [Does the number of hurricanes increase over time?](#Does-the-number-of-hurricanes-increase-over-time?)
# * [Does hurricane wind speed increase over time?](#Does-hurricane-wind-speed-increase-over-time)
# * [Analyzing hurricane category over time by basin](#Analyzing-hurricane-category-over-time-by-basin)
# * [Analyzing hurricane wind speed over time by basin](#Analyzing-hurricane-wind-speed-over-time-by-basin)
# * [Does eye pressure decrease over time?](#Does-eye-pressure-decrease-over-time?)
# * [Do hurricanes linger longer over time?](#Do-hurricanes-linger-longer-over-time?)
# * [Do hurricanes travel longer inland over time?](#Do-hurricanes-travel-longer-inland-over-time?)
# * [Correlate observations over time](#Correlate-observations-over-time)
# * [Conclusion](#Conclusion)
# * [References](#References)

# 
# This is the third part to a three part set of notebooks that process and analyze historic hurricane tracks. In the previous notebooks we saw:
# 
# **Part 1**
#  1. Downloading historic hurricane datasets using Python
#  2. Cleaning and merging hurricane observations using DASK
#  3. Aggregating point observations into hurricane tracks using ArcGIS GeoAnalytics server
# 
# **Part 2**
#  1. Visualize the locations of hurricane tracks
#  2. Different basins and the number of hurricanes per basin
#  3. Number of hurricanes over time
#  4. Seasonality in occurrence of hurricanes
#  5. Places where hurricanes make landfall and the people are affected
# 
# In this notebook you will analyze the aggregated tracks to answer important questions about hurricane severity and how they correlate over time.

# Import the libraries necessary for this notebook.

# In[1]:


# import ArcGIS Libraries
from arcgis.gis import GIS
from arcgis.geometry import filters
from arcgis.geocoding import geocode
from arcgis.features.manage_data import overlay_layers
from arcgis.geoenrichment import enrich

# import Pandas for data exploration
import pandas as pd
import numpy as np
from scipy import stats

# import plotting libraries
import matplotlib.pyplot as plt
import seaborn as sns
get_ipython().run_line_magic('matplotlib', 'inline')

# import display tools
from pprint import pprint
from IPython.display import display

# import system libs
from sys import getsizeof

# Miscellaneous imports
import warnings
warnings.filterwarnings('ignore')


# In[2]:


gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# ## Access aggregated hurricane data
# Below, we access the tracks aggregated using GeoAnalytics in the previous notebook.

# In[4]:


hurricane_tracks_item = gis.content.search('title:hurricane_tracks_aggregated_ga', item_type="feature layer")[0]
hurricane_fl = hurricane_tracks_item.layers[0]


# The **GeoAnalytics** step calculated summary statistics of all numeric fields. However, only a few of the columns are of interest to us.

# In[5]:


pprint([f['name'] for f in hurricane_fl.properties.fields], compact=True, width=80)


# Below we select the following fields for the rest of this analysis.

# In[9]:


fields_to_query = ['objectid', 'count', 'min_season', 'any_basin', 'any_sub_basin',
                   'any_name', 'mean_latitude_merged', 'mean_longitude_merged',
                   'max_wind_merged', 'range_wind_merged', 'min_pressure_merged',
                   'range_pressure_merged', 'max_eye_dia_merged', 'track_duration',
                   'end_datetime', 'start_datetime']


# ### Query hurricane tracks into a Spatially Enabled `DataFrame`

# In[10]:


all_hurricanes_df = hurricane_fl.query(out_fields=','.join(fields_to_query), as_df=True)


# In[11]:


all_hurricanes_df.shape


# There are **`12,362`** hurricanes identified by GeoAnalytics aggregate tracks tool. To get an idea about this aggregated dataset, call the `head()` method.

# In[12]:


all_hurricanes_df.head()


# To better analyze this data set, the date columns need to be changed to a format that Pandas understands better. This is accomplished by calling `to_datetime()` method and passing the appropriate time columns.

# In[14]:


all_hurricanes_df['start_datetime'] = pd.to_datetime(all_hurricanes_df['start_datetime'])
all_hurricanes_df['end_datetime'] = pd.to_datetime(all_hurricanes_df['end_datetime'])
all_hurricanes_df.index = all_hurricanes_df['start_datetime']
all_hurricanes_df.head()


# The track duration and length columns need to be projected to units (days, hours, miles) that are meaningful for analysis.

# In[15]:


all_hurricanes_df['track_duration_hrs'] = all_hurricanes_df['track_duration'] / 3600000
all_hurricanes_df['track_duration_days'] = all_hurricanes_df['track_duration'] / (3600000*24)


# ### Query landfall tracks layer into a Spatially Enabled `DataFrame`
# We query the landfall tracks layer created in the pervious notebook into a DataFrame.

# In[27]:


inland_tracks = gis.content.search('hurricane_landfall_tracks')[0]

fields_to_query = ['min_season', 'any_basin','any_name', 'max_wind_merged', 
                   'min_pressure_merged', 'track_duration','end_datetime', 
                   'start_datetime', 'analysislength']

landfall_tracks_fl = inland_tracks.layers[0]


# In[28]:


landfall_tracks_df = landfall_tracks_fl.query(out_fields=fields_to_query).df
landfall_tracks_df.head(3)


# ## Manage missing sensor data
# Before we can analyze if hurricanes intensify over time, we need to identify and account for missing values in our data. Sensor measurements such as **wind speed**, **atmospheric pressure**, **eye diameter**, generally suffer from missing values and outliers. The reconstruct tracks tool has identified `12,362` individual hurricanes that occurred during the past `169` years.

# In[122]:


all_hurricanes_df.shape


# ### Visualize missing records
# An easy way to visualize missing records is to hack the `heatmap` of `seaborn` library to display missing records. The snippet below shows missing records in **yellow** color.

# In[140]:


missing_data_viz = all_hurricanes_df.replace(0,np.NaN)
missing_data_viz = missing_data_viz.replace(-9999.0,np.NaN)
missing_data_viz['min_pressure_merged'] = missing_data_viz['min_pressure_merged'].replace(100.0,np.NaN)

plt.figure(figsize=(10,10))
missing_data_ax = sns.heatmap(missing_data_viz[['max_wind_merged', 'min_pressure_merged',
                                                'max_eye_dia_merged', 'track_duration']].isnull(),
                              cbar=False, cmap='viridis')
missing_data_ax.set_ylabel('Years')
missing_data_ax.set_title('Missing values (yellow) visualized as a heatmap')


# All three observation columns - wind speed, atmospheric pressure and eye diameter, suffer from missing values. In general as technology improved over time, we were able to collect better data with fewer missing observations. In the sections below, we attempt to fill these values using different techniques. We will compare how they fare and pick one of them for rest of the analysis.

# ### Missing value imputation

# **Technique 1: Drop missing values**:
# An easy way to deal with missing values is to drop those record from analysis. If we were to do that, we lose over a third of the hurricanes.

# In[141]:


hurricanes_nona = missing_data_viz.dropna(subset=['max_wind_merged','min_pressure_merged'])
hurricanes_nona.shape


# **Technique 2: Fill using median value**:
# A common technique is to fill using median value (or a different measure of centrality). This technique computes the median of the entire column and applies that to all the missing values.

# In[142]:


fill_values = {'max_wind_merged': missing_data_viz['max_wind_merged'].median(),
                'min_pressure_merged': missing_data_viz['min_pressure_merged'].median(),
              'track_duration_hrs': missing_data_viz['track_duration_hrs'].median()}
hurricanes_fillna = missing_data_viz.fillna(value=fill_values)


# **Technique 3: Fill by interpolating between existing values**: A sophisticated approach is to interploate a missing value based on two of its closest observations.

# In[143]:


hurricanes_ipl = missing_data_viz
hurricanes_ipl['max_wind_merged'] = hurricanes_ipl['max_wind_merged'].interpolate()
hurricanes_ipl['min_pressure_merged'] = hurricanes_ipl['min_pressure_merged'].interpolate()
hurricanes_ipl['track_duration_hrs'] = hurricanes_ipl['track_duration_hrs'].interpolate()


# **Visualize all 3 techniques**
# 
# To compare how each of these techniques fared, we will plot the histogram of **wind speed** column after managing for missing values.

# In[144]:


fig, ax = plt.subplots(1,3, sharex=True, figsize=(15,5))
fig.suptitle('Comparing effects of missing value imputations on Wind speed column', 
             fontsize=15)

hurricanes_nona['max_wind_merged'].plot(kind='hist', ax=ax[0], bins=35, title='Drop null values')
hurricanes_fillna['max_wind_merged'].plot(kind='hist', ax=ax[1], bins=35, title='Impute with median')
hurricanes_ipl['max_wind_merged'].plot(kind='hist', ax=ax[2], bins=35, title='Impute via interpolation')
for a in ax:
    a.set_xlabel('Wind Speed')


# Next, we will plot the histogram of **atmospheric pressure** column after managing for missing values.

# In[145]:


fig, ax = plt.subplots(1,3, sharex=True, figsize=(15,5))
fig.suptitle('Comparing effects of missing value imputations on Pressure column', 
             fontsize=15)

hurricanes_nona['min_pressure_merged'].plot(kind='hist', ax=ax[0], title='Drop null values')
hurricanes_fillna['min_pressure_merged'].plot(kind='hist', ax=ax[1], title='Impute with median')
hurricanes_ipl['min_pressure_merged'].plot(kind='hist', ax=ax[2], title='Impute via interpolation')
for a in ax:
    a.set_xlabel('Atmospheric Pressure')


# **Fill using interpolation preserves shape** of the original distribution. So it will be used for further anlaysis.

# # Does intensity of hurricanes increase over time?
# This last part of this study analyzes if there a temporal trend in the intensity of hurricanes. A number of studies have concluded that anthropogenic influences in the form of global climate change make hurricanes worse and dangerous. We analyze if such patterns can be noticed from an empirical standpoint.

# ## Does the number of hurricanes increase over time?

# In[363]:


ax = all_hurricanes_df['min_season'].hist(bins=50)
ax.set_title('Number of hurricanes per season')


# From the previous notebook, we noticed the number of hurricanes recorded has been steadily increasing, partly due to advancements in technology. We notice a reduction in number of hurricanes after 1970s. Let us split this up by basin and observe if the trend is similar.

# In[378]:


fgrid = sns.FacetGrid(data=all_hurricanes_df, col='any_basin', col_wrap=3,
                     sharex=False, sharey=False)
fgrid.map(plt.hist, 'min_season', bins=50)
fgrid.set_axis_labels(x_var='Seasons', y_var='Frequency of hurricanes')


# Plotting the frequency of hurricanes by basin shows a similar trend with the number of hurricanes reducing globally after 1970s. [This is consistent with certain studies (1)](https://link.springer.com/epdf/10.1007/s00382-013-1713-0?shared_access_token=cHh_-kXbFWgMacxvFkMyWPe4RwlQNchNByi7wbcMAY65lj3v5VSTqd8rbUj9HyyFUZS9H0Z0WdJn6EgG-Onhfk9U48b6jvTjrJTuMH8OuGbT6R1Y54MGaHwYq2EEp6ppcoACaJgdWo3v3VtPDcq9Whm0MUw98Z4wCFy7jbwwcY4%3D). However, this is only one part of the story. Below, we continue to analyze if the nature of hurricanes itself is changing, while the total number may reduce.

# ## Does hurricane wind speed increase over time?
# To understand if wind speed increases over time, we create a scatter plot of `min_season` against the `max_wind_merged` column. The `seaborn` plotting library can enhance this plot with a **correlation coefficient** and its level of signifance (**p value**).

# In[266]:


jgrid = sns.jointplot(x='min_season', y='max_wind_merged', data=hurricanes_ipl,
             kind='reg', joint_kws={'line_kws':{'color':'green'}}, height=7, space=0.5)
j = jgrid.annotate(stats.pearsonr)
j = jgrid.ax_joint.set_title('Does hurricane wind speed increase over time?')


# From the plot above, we notice a small positive correlation. Wind speeds are observed to increase with time. The small `p-value` suggests this correlation (albeit small) is **statistically significant**. The plot above considers hurricanes across all the basins and regresses that against time. To get a finer picture, we need to **split the data by basins** and observe the correlation.

# In[329]:


# since there are not many hurricanes observed over South Atlantic basin (SA), 
# we drop it from analysis
hurricanes_major_basins_df = hurricanes_ipl[hurricanes_ipl['any_basin'].isin(
                                            ['WP','SI','NA','EP','NI','SP'])]


# Define a function that can compute `pearson-r` correlation coefficient for any two columns across all basins.

# In[322]:


def correlate_by_basin(column_a, column_b, df=hurricanes_major_basins_df, 
                       category_column = 'any_basin'):
    # clean data by dropping any NaN values
    df_nona = df.dropna(subset=[column_a, column_b])
    
    # loop through the basins
    basins = list(df[category_column].unique())
    return_dict = {}
    for basin in basins:
        subset_df = df_nona[df_nona[category_column] == basin]
        r, p = stats.pearsonr(list(subset_df[column_a]), list(subset_df[column_b]))
        
        return_dict[basin] = [round(r,4), round(p,4)]
    
    # return correlation coefficient and p-value for each basin as a DataFrame
    return_df = pd.DataFrame(return_dict).T
    return_df.columns=['pearson-r','p-value']
    return return_df


# ### Analyzing hurricane wind speed over time by basin
# Below we plot a grid of scatter plots with linear regression plots overlaid over them. The `seaborn` library's `lmplot()` function makes it trivial to accomplish this in a single command.

# In[342]:


fgrid = sns.lmplot('min_season', 'max_wind_merged', col='any_basin', 
                   data=hurricanes_major_basins_df, col_wrap=3,
                   sharex=False, sharey=False, line_kws={'color':'green'})


# From the scatter plots above, we notice the wind speeds in most basins show a slight positive trend, with **North Atlantic** being an exception. To explore this further, we compute the correlation coefficient and its p-value below.

# In[328]:


wind_vs_season = correlate_by_basin('min_season','max_wind_merged')
print('Correlation coefficients for min_season vs max_wind_merged')
wind_vs_season


# The table above displays the correlation coefficient of hurricane wind speed over time. Hurricanes over the **Southern Pacific** basin exhibit a positive trend of increasing wind speeds. The `r` value over the **North Atlantic** shows a weak negative trend. Since all the `p-value`s are less than `0.05`, these correlations are statistically significant.

# ### Analyzing hurricane category over time by basin
# Hurricanes are classified on a [Saffir-Simpson](https://www.nhc.noaa.gov/aboutsshws.php) scale of `1-5` based on their wind speed. Let us compute this column on the dataset and observe if there are temporal aspects to it.

# In[414]:


def categorize_hurricanes(row, wind_speed_column='max_wind_merged'):
    wind_speed = row[wind_speed_column] * 1.152  # knots to mph
    if 74 <= wind_speed <= 95:
        return '1'
    elif 96 <= wind_speed <= 110:
        return '2'
    elif 111 <= wind_speed <= 129:
        return '3'
    elif 130 <= wind_speed <= 156:
        return '4'
    elif 157 <= wind_speed <= 500:
        return '5'


# In[435]:


hurricanes_major_basins_df['category_str'] = hurricanes_major_basins_df.apply(categorize_hurricanes, 
                                                                              axis=1)
hurricanes_major_basins_df['category'] = pd.to_numeric(arg=hurricanes_major_basins_df['category_str'],
                                                      errors='coerce', downcast='integer')

hurricanes_major_basins_df.head(2)


# We will create violin and bar plots to visualize the number of hurricane categories over different basins.

# In[429]:


fig, ax = plt.subplots(1,2, figsize=(15,6))
vplot = sns.violinplot(x='any_basin', y='category', data=hurricanes_major_basins_df, ax=ax[0])
vplot.set_title('Number of hurricanes per category in each basin')

cplot = sns.countplot(x='any_basin', hue='category_str', data=hurricanes_major_basins_df,
             hue_order=['1','2','3','4','5'], ax=ax[1])
cplot.set_title('Number of hurricanes per category in each basin')


# We notice all basins are capable of generating major hurricanes (over 3). The **Eastern Pacific** basin appears to have a larger than the proportional number of major hurricanes. Below, we will regress the hurricane category against time to observe if there is a positive trend.

# In[440]:


kde_regplot = sns.jointplot(x='min_season', y='category', 
                            data=hurricanes_major_basins_df, kind='kde', 
                            stat_func=stats.pearsonr).plot_joint(sns.regplot, 
                                                                 scatter=False)
kde_regplot.ax_joint.set_title('Scatter plot of hurricane categories over seasons')


# Even at a global level, we notice a strong positive correlation between hurricane category and seasons. Below, we will split this across basins to observe if the trend holds well.

# In[419]:


wgrid = sns.lmplot('min_season', 'category', col='any_basin', 
                   data=hurricanes_major_basins_df, col_wrap=3,
                   sharex=False, sharey=False, line_kws={'color':'green'})


# In[442]:


category_corr_df = correlate_by_basin('min_season','category', df=hurricanes_major_basins_df)
print('Correlation coefficients for min_season vs hurricane category')
category_corr_df


# Thus, at both global and basin scales, we notice a **positive trend in the number of hurricanes of category `4` and higher**, while there is a general reduction in the quantity of hurricanes per season. This is along the lines of several studies [[1]](https://doi.org/10.1007/s00382-013-1713-0) [[2]](https://bankunderground.co.uk/2018/05/22/us-hurricane-clustering-a-new-reality/) [[3]](https://www.nature.com/articles/nature03906) [[4]](https://www.nytimes.com/2018/10/10/climate/hurricane-michael-climate-change.html). Thus while the total number of hurricanes per season may reduce, we notice an increase in the intensity of them.

# ## Does eye pressure decrease over time?
# Just like a high wind speed, lower atmospheric pressure increases the intensity of hurricanes. To analyze this, we produce a scatter grid of `min_pressure_merged` column and regress it against `min_season` column. We split this by basins.

# In[330]:


pgrid = sns.lmplot('min_season', 'min_pressure_merged', col='any_basin', 
                   data=hurricanes_major_basins_df, col_wrap=3,
                   sharex=False, sharey=False, line_kws={'color':'green'})


# In[333]:


pressure_corr_df = correlate_by_basin('min_season','min_pressure_merged')
print('Correlation coefficients for min_season vs min_pressure_merged')
pressure_corr_df


# Lower the atmospheric pressure, more intense is the hurricane. Hence we are looking for strong negative correlation between the pressure and season columns. From the charts and table above, we notice **South Pacific** basin once again tops the list with a mild negative correlation over time. The `p-value`s of **Western Pacific** and **Eastern Pacific** is larger than `0.05`, so we disregard their correlation coefficients. Over **North American** and **Indian** basins, we notice a weak positive correlation.

# ## Do hurricanes linger longer over time?
# While wind speed and atmospheric pressure measure two types of intensities, a neverending hurricane can also hurt the communities affected as it inundates the coast with rainfall and storm surge for longer periods of time. In this section we correlate the track duration in days against seasons.

# In[159]:


jgrid = sns.jointplot(x='min_season', y='track_duration_days', data=hurricanes_ipl,
             kind='hex', height=7, space=0.5)
j = jgrid.annotate(stats.pearsonr)

sns.regplot(x='min_season', y='track_duration_days', data=hurricanes_ipl, 
            ax=jgrid.ax_joint, color='green',scatter=False)

j = jgrid.ax_joint.set_title('Does hurricane duration increase over time?')


# At a global scale, we notice an increase in the duration of hurricanes. Below we split this up by basins to get a finer look.

# In[332]:


lgrid = sns.lmplot('min_season', 'track_duration_days', col='any_basin', 
                   data=hurricanes_major_basins_df, col_wrap=3,
                   sharex=False, sharey=False, line_kws={'color':'green'})


# In[334]:


linger_time_corr_df = correlate_by_basin('min_season','track_duration_days')
print('Correlation coefficients for min_season vs track_duration_days')
linger_time_corr_df


# At most basins, we notice a positive trend in hurricane track duration. An exception to this is the **North Atlantic** basin where the `p-value` is not significant enough to let us draw any conclusion.
# 
# The trend we notice here could be partly due to technological advancements that allow us to identify and track hurricanes at a very early stage. Hence, to complement this, we will analyze if hurricanes travel longer than usual once they make a landfall.

# ## Do hurricanes travel longer inland over time?
# Along the lines of track duration, it is relevant for us to investigate whether hurricanes travel longer inland over time. Thus, we correlate track length column of hurricanes that made landfall against seasons.

# In[341]:


jgrid = sns.jointplot(x='min_season', y='analysislength', data=landfall_tracks_df,
                      kind='reg', joint_kws={'line_kws':{'color':'green'}}, 
                      height=7, space=0.5, ylim=[0,2000])
j = jgrid.annotate(stats.pearsonr)
j = jgrid.ax_joint.set_title('Do hurricanes travel longer inland over time?')


# In[343]:


lgrid = sns.lmplot('min_season', 'analysislength', col='any_basin', 
                   data=landfall_tracks_df, col_wrap=3,
                   sharex=False, sharey=False, line_kws={'color':'green'})


# In[344]:


linger_distance_corr_df = correlate_by_basin('min_season','analysislength', df=landfall_tracks_df)
print('Correlation coefficients for min_season vs inland track length')
linger_distance_corr_df


# When we correlated inland track length over time, we were able to unravel an interesting observation. At basins where the correlation is statistically significant, it is negative (**North Indian, Western Pacific, South Pacific**). Thus while the duration of hurricanes continues to increase (due to reasons discussed previously), we notice hurricanes travel shorter distances inland. This could be problematic of communities affected as the hurricane could remain stagnant and produce stronger than usual storm surges and precipitation.

# ## Correlate observations over time
# Let us collect all trend analysis we performed so far into a single `DataFrame`.

# In[443]:


# clean data by dropping any NaN values
subset_cols = ['min_season', 'max_wind_merged',
                'min_pressure_merged', 'track_duration_days', 'category']

df_nona = hurricanes_major_basins_df.dropna(subset=subset_cols)

# loop through the basins
basins = list(df_nona['any_basin'].unique())
return_dict = {}
for basin in basins:
    subset_df =df_nona[df_nona['any_basin'] == basin]
    row_vector = []
    for col in subset_cols[1:]:
        r, p = stats.pearsonr(list(subset_df['min_season']), list(subset_df[col]))
        if p < 0.05:
            row_vector.append(round(r,4))
        else:
            row_vector.append(pd.np.NaN)

    return_dict[basin] = row_vector

# return as a DataFrame
return_df = pd.DataFrame.from_dict(return_dict, orient='index', columns=subset_cols[1:])
return_df


# We can visualize this correlation table as a heat map to appreciate how the hurricane severity indicators correlate over seasons for each basin.

# In[450]:


hm_plot = sns.heatmap(return_df, annot=True, linecolor='black', cmap='RdBu_r')
hm_plot.set_title('Heatmap of correlation coefficients of hurricane severity indicators')


# ## Conclusion
# In this notebook used the aggregated hurricane tracks produced in part 1 and the landfall tracks layer produced in part 2. We checked for missing sensor observations and imputed those records via interpolation. We then comprehensively analyzed how hurricane severity indicators such as **wind speed**, **atmospheric pressure**, **track duration**, **track length inland** and **category** correlate over time (seasons). We noticed the total number of hurricanes have decreased since the `1970`s globally and across all basins. While the number of hurricanes is less, we noticed their wind speed, track duration, and category **correlate positively**, while the atmospheric pressure **correlates negatively**, against seasons. This aligns with findings from major studies. Thus, we notice a reduction in the number of category 1 and 2, and an increase in the number of category 4 and 5 hurricanes. We noticed the **North Indian** basin to be highly correlated in this regard.
# 
# In this study, through these 3 part notebooks, we saw how to download hurricane data for the past `169` years from NOAA NEIC site over FTP, and how to interact with it initially using Pandas. Since this data is larger than memory for average computers, we learned how to aggregate the columns using the DASK distributed, delayed processing library. We input the result of DASK to the ArcGIS GeoAnalytics server to aggregate these point observations into hurricane tracks. The "reconstruct tracks" tool which performed this aggregation identified `12,362` individual hurricanes across the globe.
# 
# In the second notebook, we performed comprehensive visualization and exploratory analysis to understand the geography of hurricanes and the various basins they are categorized into. We observed a strong sinusoidal nature where hurricanes in the northern and southern hemisphere are offset by `6` months. We noticed certain names (`Irma`, `Florence`) are more popular than the rest. We overlaid the tracks over land boundary dataset to compute tracks traveled inland and identify places where landfall occur. Through density analysis of the landfall locations, we were able to identify the places that were most affected from a repeat landfall basis. By geo-enriching these places, we learned that **China**, **Hongkong** and **India** are home to the population that is most affected.
# 
# Many studies and articles [[5]](https://www.theguardian.com/environment/2018/oct/08/global-warming-must-not-exceed-15c-warns-landmark-un-report) [[6]](https://www.vox.com/2018/10/9/17951924/climate-change-global-warming-un-ipcc-report-takeaways) [[7]](https://www.vox.com/energy-and-environment/2018/10/3/17925470/hurricane-florence-2018-devastation-climate-change-flood-sea-level-rise) [[8]](https://fivethirtyeight.com/features/how-climate-change-made-our-hurricane-predictions-way-more-accurate/) shine light on anthropogenic influences on global Sea Surface Temperature (SST). Higher sea surface temperatures are observed to produce more intense storms; they are attributed to polar ice cap melting and rising sea levels. These combined with increased hurricane intensity can lead to severe storm surges causing increased flooding and damage of the coastal communities. Based on our density and geoenrichment analysis, we noticed these are places along the coast that are densely populated.
# 
# This study showcases how a comprehensive spatial data science project can be performed using ArcGIS and open source Python libraries on the ArcGIS Notebook server. The notebook medium makes it convenient to document code, narrative, and graphics in one place. This helps in making research reproducible and approachable. Thus, in this study, we were able to empirically validate the conclusions derived by other studies. In the future, when more hurricane datasets become available, this study can be repeated just by re-running these notebooks.

# #### References
#  - [1. Recent intense hurricane response to global climate change. Holland, G. & Bruyère, C.L. Clim Dyn (2014) 42: 617.](https://doi.org/10.1007/s00382-013-1713-0)
#  - [2. US Hurricane Clustering: A New Reality?](https://bankunderground.co.uk/2018/05/22/us-hurricane-clustering-a-new-reality/)
#  - [3. Increasing destructiveness of tropical cyclones over the past 30 years](https://www.nature.com/articles/nature03906)
#  - [4. The Hurricanes, and Climate-Change Questions, Keep Coming. Yes, They’re Linked. ](https://www.nytimes.com/2018/10/10/climate/hurricane-michael-climate-change.html)
#  - [5. We have 12 years to limit climate change catastrophe, warns UN ](https://www.theguardian.com/environment/2018/oct/08/global-warming-must-not-exceed-15c-warns-landmark-un-report)
#  - [6. Four big takeaways from the UN’s alarming climate change report](https://www.vox.com/2018/10/9/17951924/climate-change-global-warming-un-ipcc-report-takeaways)
#  - [7. Hurricane Florence caused up to 22 billion in damages. Climate change made the storm worse.](https://www.vox.com/energy-and-environment/2018/10/3/17925470/hurricane-florence-2018-devastation-climate-change-flood-sea-level-rise)
#  - [8. What Climate Change Taught Us About Hurricanes ](https://fivethirtyeight.com/features/how-climate-change-made-our-hurricane-predictions-way-more-accurate/)


# ====================
# predict_service_request_types.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Analyzing and predicting Service Request Types in DC

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Analyzing-and-predicting-Service-Request-Types-in-DC" data-toc-modified-id="Analyzing-and-predicting-Service-Request-Types-in-DC-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Analyzing and predicting Service Request Types in DC</a></span><ul class="toc-item"><li><span><a href="#Read-in-service-requests-for-2018" data-toc-modified-id="Read-in-service-requests-for-2018-1.1"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>Read in service requests for 2018</a></span></li><li><span><a href="#Read-in-Neighborhood-Clusters-dataset" data-toc-modified-id="Read-in-Neighborhood-Clusters-dataset-1.2"><span class="toc-item-num">1.2&nbsp;&nbsp;</span>Read in Neighborhood Clusters dataset</a></span></li><li><span><a href="#Construct-model-that-predicts-service-type" data-toc-modified-id="Construct-model-that-predicts-service-type-1.3"><span class="toc-item-num">1.3&nbsp;&nbsp;</span>Construct model that predicts service type</a></span><ul class="toc-item"><li><span><a href="#Data-preprocessing" data-toc-modified-id="Data-preprocessing-1.3.1"><span class="toc-item-num">1.3.1&nbsp;&nbsp;</span>Data preprocessing</a></span></li><li><span><a href="#Model-building" data-toc-modified-id="Model-building-1.3.2"><span class="toc-item-num">1.3.2&nbsp;&nbsp;</span>Model building</a></span></li><li><span><a href="#Alternate-model,-excluding-the-department-codes" data-toc-modified-id="Alternate-model,-excluding-the-department-codes-1.3.3"><span class="toc-item-num">1.3.3&nbsp;&nbsp;</span>Alternate model, excluding the department codes</a></span></li><li><span><a href="#How-many-requests-does-each-neighborhood-make?" data-toc-modified-id="How-many-requests-does-each-neighborhood-make?-1.3.4"><span class="toc-item-num">1.3.4&nbsp;&nbsp;</span>How many requests does each neighborhood make?</a></span></li><li><span><a href="#What-kind-of-requests-does-each-neighborhood-mostly-make?" data-toc-modified-id="What-kind-of-requests-does-each-neighborhood-mostly-make?-1.3.5"><span class="toc-item-num">1.3.5&nbsp;&nbsp;</span>What kind of requests does each neighborhood mostly make?</a></span></li></ul></li></ul></li></ul></div>

# The datasets used in this notebook are the 
# 1. __`City Service Requests in 2018`__
# 2. __`Neighborhood Clusters`__
# 
# These datasets can be found at [opendata.dc.gov](http://opendata.dc.gov/)
# 
# We start by importing the ArcGIS package to load the data using a service URL

# In[1]:


import arcgis
from arcgis.features import *


# ## Read in service requests for 2018

# [Link](http://opendata.dc.gov/datasets/city-service-requests-in-2018/geoservice?geometry=-77.49%2C38.811%2C-76.534%2C38.998) to Service Requests 2018 dataset

# In[2]:


requests_url = 'https://maps2.dcgis.dc.gov/dcgis/rest/services/DCGIS_DATA/ServiceRequests/MapServer/9'

requests_layer = FeatureLayer(requests_url)
requests_layer


# In[ ]:


# Extract all the data and display number of rows
requests_features = requests_layer.query()
print('Total number of rows in the dataset: ')
print(len(requests_features.features))


# This dataset updates on runtime, hence the number of rows could vary each time.

# In[ ]:


# store as dataframe
requests = requests_features.sdf

# View first 5 rows
requests.head()


# ## Read in Neighborhood Clusters dataset

# [Link](http://opendata.dc.gov/datasets/neighborhood-clusters) to this dataset

# In[ ]:


neighborhood_url = 'https://maps2.dcgis.dc.gov/dcgis/rest/services/DCGIS_DATA/Administrative_Other_Boundaries_WebMercator/MapServer/17'

neighborhood_layer = FeatureLayer(neighborhood_url)
neighborhood_layer


# In[ ]:


# Extract all the data and display number of rows
neighborhood_features = neighborhood_layer.query()
print('Total number of rows in the dataset: ')
print(len(neighborhood_features.features))


# In[ ]:


# store as dataframe
neighborhood = neighborhood_features.sdf

# View first 5 rows
neighborhood.head()


# We now __merge__ the two datasets

# In[ ]:


# Connect to the GIS
from arcgis.gis import GIS
gis = GIS('http://dcdev.maps.arcgis.com/', 'username', 'password')


# In[9]:


# Perform spatial join between CBG layer and the service areas created for all time durations
requests_with_neighborhood = arcgis.features.analysis.join_features(requests_url, neighborhood_url, spatial_relationship='Intersects', output_name='serviceRequests_Neighborhood_DC_1')


# In[10]:


requests_with_neighborhood.share(everyone=True)


# In[11]:


requests_with_neighborhood_url = str(requests_with_neighborhood.url)+'/0/'
layer = FeatureLayer(requests_with_neighborhood_url)
features = layer.query()
print('Total number of rows in the dataset: ')
print(len(features.features))


# In[12]:


merged = features.sdf
merged.head()


# ## Construct model that predicts service type
# 
# The variables used to build the model are:
# > 1. City Quadrant
# > 2. Neighborhood cluster
# > 3. Ward (Geographical unit)
# > 4. Organization acronym
# > 5. Status Code

# ### Data preprocessing

# In[13]:


quads = ['NE', 'NW', 'SE', 'SW']
def generate_quadrant(x):
    '''Function that extracts quadrant from street address'''
    try:
        temp = x[-2:]
        if temp in quads:
            return temp
        else:
            return 'NaN'
    except:
        return 'NaN'


# In[14]:


merged['QUADRANT'] = merged['STREETADDRESS'].apply(generate_quadrant)
merged['QUADRANT'].head()


# In[15]:


merged['QUADRANT'].unique()


# In[16]:


merged['CLUSTER'] = merged['NAME'].apply(lambda x: x[8:])
merged['CLUSTER'].head()


# In[17]:


merged['CLUSTER'] = merged['CLUSTER'].astype(int)


# In[18]:


merged['ORGANIZATIONACRONYM'].unique()


# In[19]:


merged['STATUS_CODE'].unique()


# Let's extract the number of possible outcomes, i.e. length of the target variable and also take a look at the values

# In[20]:


len(merged['SERVICETYPECODEDESCRIPTION'].unique())


# In[21]:


requests['SERVICETYPECODEDESCRIPTION'].unique()


# ### Model building

# In[22]:


# Import necessary packages
from sklearn.preprocessing import *
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score


# In[23]:


# Convert categorical (text) fields to numbers
number = LabelEncoder()
merged['SERVICETYPE_NUMBER'] = number.fit_transform(merged['SERVICETYPECODEDESCRIPTION'].astype('str'))
merged['STATUS_CODE_NUMBER'] = number.fit_transform(merged['STATUS_CODE'].astype('str'))


# In[24]:


# Extract desired fields
data = merged[['SERVICETYPECODEDESCRIPTION', 'SERVICETYPE_NUMBER', 'QUADRANT', 'CLUSTER', 'WARD', 'ORGANIZATIONACRONYM', 'STATUS_CODE', 'STATUS_CODE_NUMBER']]
data.reset_index(inplace=True)
data.head()


# Let's binarize values in fields `QUADRANT` (4) and `ORGANIZATIONACRONYM` (8)
# 
# Wonder why are not doing it for `CLUSTER`? Appropriate nomenclature of [adjacent clusters](http://opendata.dc.gov/datasets/neighborhood-clusters).

# In[25]:


import pandas as pd
data = pd.get_dummies(data=data, columns=['QUADRANT', 'ORGANIZATIONACRONYM'])
data.head()


# In[26]:


# Extract input dataframe
model_data = data.drop(['SERVICETYPECODEDESCRIPTION', 'SERVICETYPE_NUMBER', 'STATUS_CODE'], axis=1)
model_data.head()


# In[27]:


def handle_ward(x):
    accept = [range(0,8)]
    if x not in accept:
        return 0
    else:
        return x


# In[28]:


model_data['WARD'] = model_data['WARD'].apply(handle_ward)


# In[29]:


# Define independent and dependent variables
y = data['SERVICETYPE_NUMBER'].values
X = model_data.values


# In[30]:


# Split data into training and test samples of 70%-30%
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .3, random_state=522, stratify=y)


# In[31]:


# n_estimators = number of trees in the forest
# min_samples_leaf = minimum number of samples required to be at a leaf node for the tree
rf = RandomForestClassifier(n_estimators=2500, min_samples_leaf=5, random_state=522)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
print(y_pred)


# In[32]:


print('Accuracy: ', accuracy_score(y_test, y_pred))


# ### Alternate model, excluding the department codes

# In[33]:


data = merged[['SERVICETYPECODEDESCRIPTION', 'SERVICETYPE_NUMBER', 'QUADRANT', 'CLUSTER', 'WARD', 'ORGANIZATIONACRONYM', 'STATUS_CODE', 'STATUS_CODE_NUMBER']]
data.reset_index(inplace=True)
data.head()


# In[34]:


data1 = pd.get_dummies(data=data,columns=['QUADRANT'])
data1.head()


# In[35]:


model_data1 = data1.drop(['SERVICETYPECODEDESCRIPTION', 'SERVICETYPE_NUMBER', 'STATUS_CODE', 'ORGANIZATIONACRONYM'], axis=1)
model_data1.head()


# In[36]:


model_data1['WARD'] = model_data1['WARD'].apply(handle_ward)


# In[37]:


y = data['SERVICETYPE_NUMBER'].values
X = model_data1.values


# In[38]:


# Split data into training and test samples of 70%-30%
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .3, random_state=522, stratify=y)


# In[39]:


# n_estimators = number of trees in the forest
# min_samples_leaf = minimum number of samples required to be at a leaf node for the tree
rf = RandomForestClassifier(n_estimators=2500, min_samples_leaf=5, random_state=522)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
print(y_pred)


# In[40]:


print('Accuracy: ', accuracy_score(y_test, y_pred))


# A drop in accuracy from __68.39%__ to __48.78%__ demonstrates the importance of using the correct predictors.

# ### How many requests does each neighborhood make?

# In[41]:


# Count of service requests per cluster
cluster_count = merged.groupby('NAME').size().reset_index(name='counts')
cluster_count.head()


# In[42]:


# merge with original file
neighborhood = pd.merge(neighborhood, cluster_count, on='NAME')
neighborhood.head()


# In[43]:


temp = neighborhood.sort_values(['counts'], ascending=[False])
temp[['NAME', 'NBH_NAMES', 'counts']]


# In[44]:


# Viewing the map
search_result = gis.content.search("Neighborhood_Service_Requests")
search_result[0]


# ### What kind of requests does each neighborhood mostly make?

# In[45]:


import scipy.stats
merged.columns


# In[46]:


df = merged[['NAME', 'SERVICECODEDESCRIPTION']]


# In[ ]:


# Extract the most frequently occuring service request type, and its count
df1 = df.groupby('NAME').agg(lambda x: scipy.stats.mode(x)[0][0])
df2 = df.groupby('NAME').agg(lambda x: scipy.stats.mode(x)[1][0])


# In[48]:


df1.reset_index(inplace=True)
df2.reset_index(inplace=True)
df2 = df2.rename(columns={'SERVICECODEDESCRIPTION':'SERVICECODEDESCRIPTION_COUNT'})


# In[49]:


# merge the two datasets
final_df = pd.merge(df1, df2, on='NAME')
final_df.head()


# In[50]:


# merge it with neighborhood clusters
neighborhood_data = pd.merge(neighborhood, final_df, on='NAME')


# In[51]:


# view the map
search_result = gis.content.search("Neighborhood_Service_DC")
search_result[0]



# ====================
# predicting_enso.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Predicting El Niño–Southern Oscillation through correlation and time series analysis/deep learning
# 
# This example uses correlation analysis and time series analysis to predict El Niño–Southern Oscillation (ENSO) based on climate variables and indices. ENSO is an irregular periodical variation in winds and sea surface temperatures over the tropical eastern Pacific Ocean.

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Predicting-El-Niño–Southern-Oscillation-through-correlation-and-time-series-analysis/deep-learning" data-toc-modified-id="Predicting-El-Niño–Southern-Oscillation-through-correlation-and-time-series-analysis/deep-learning-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Predicting El Niño–Southern Oscillation through correlation and time series analysis/deep learning</a></span><ul class="toc-item"><li><span><a href="#Data-used" data-toc-modified-id="Data-used-1.1"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>Data used</a></span></li><li><span><a href="#Part-1.-Data-exploration" data-toc-modified-id="Part-1.-Data-exploration-1.2"><span class="toc-item-num">1.2&nbsp;&nbsp;</span>Part 1. Data exploration</a></span></li><li><span><a href="#Part-2.-Correlation-analysis" data-toc-modified-id="Part-2.-Correlation-analysis-1.3"><span class="toc-item-num">1.3&nbsp;&nbsp;</span>Part 2. Correlation analysis</a></span></li><li><span><a href="#Part-3.-Time-Series-Analysis/LSTM" data-toc-modified-id="Part-3.-Time-Series-Analysis/LSTM-1.4"><span class="toc-item-num">1.4&nbsp;&nbsp;</span>Part 3. Time Series Analysis/LSTM</a></span></li><li><span><a href="#Data-cleanup-and-transformation" data-toc-modified-id="Data-cleanup-and-transformation-1.5"><span class="toc-item-num">1.5&nbsp;&nbsp;</span>Data cleanup and transformation</a></span></li><li><span><a href="#Model-training" data-toc-modified-id="Model-training-1.6"><span class="toc-item-num">1.6&nbsp;&nbsp;</span>Model training</a></span></li><li><span><a href="#Prediction-and-evaluation" data-toc-modified-id="Prediction-and-evaluation-1.7"><span class="toc-item-num">1.7&nbsp;&nbsp;</span>Prediction and evaluation</a></span></li><li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-1.8"><span class="toc-item-num">1.8&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></li></ul></div>

# In[1]:


from IPython.display import YouTubeVideo
YouTubeVideo('d6s0T0m3F8s')


# ENSO can have tremendous potential impact such as droughts, floods, and tropical storms. You can look at a few story maps below to get a better understanding of the process.

# In[2]:


from arcgis.gis import GIS
agol = GIS()
story_maps = agol.content.search('What El Niño Means for Southern California')[0]
story_maps


# Accurate characterization of ENSO is critical for understanding the trends. In climate science, ENSO is characterized through [Southern Oscillation Index](https://www.ncdc.noaa.gov/teleconnections/enso/indicators/soi/) (SOI), a standardized index based on the observed sea level pressure differences between Tahiti and Darwin, Australia. If SOI exhibits warm (greater than 0.5) or cool phase conditions for at least five consecutive values, it officially becomes an El Niño or La Niña event. Therefore, predicting SOI is the first step of ENSO forecasting. This notebooks consists of 3 sections: (1) Data exploration (2) correlation analysis; (3) Time series analysis

# ## Data used
# The following four variables were used in this examplea as ENSO is beleived to be related to sea surface temperature, sea level pressure, precipitation, etc. 
# 1. Oceanic Nino Index (ONI), a climate index used for sea surface temperature (SST).
# 2. Eastern Tropical Pacific SST (Nino 3), another climate index used for SST focusing on a slightly different region.
# 3. Pacific North American Index (PNA). PNA is a closely related phenomena to ENSO.
# 4. Precipitation monthly mean. Historical global precipitation monthly mean in raster format.
# 
# **Note**: to run this sample, you need a few extra libraries in your conda environment. If you don't have the libraries, install them by running the following commands from cmd.exe or your shell
# 
# ```
# conda update conda
# conda install matplotlib
# conda install scikit-learn
# conda install -c conda-forge scikit-image
# conda install -c conda-forge keras 
# ```

# ## Part 1. Data exploration
# The first three variables along with SOI has been put into a .CSV file. To give you an overview, let's read it and look at the first few lines.

# In[3]:


get_ipython().run_line_magic('matplotlib', 'inline')

import os
import numpy as np
import pandas as pd
from pandas import read_csv
from pandas import datetime
from matplotlib import pyplot

def parser(x):
    if x.endswith('11') or x.endswith('12')or x.endswith('10'):
        return datetime.strptime(x, '%Y%m')
    else:
        return datetime.strptime(x, '%Y0%m')
enso_original_path = os.path.join('.', 'data', 'enso_original.csv')
df = read_csv(enso_original_path, header=0, parse_dates=[0], index_col=0, date_parser=parser)
start = 336 
df = df.iloc[start:]
df = (df - df.mean()) / df.std()
print(df.head())


# Let us examine how SOI changes over time.

# In[4]:


pyplot.figure(figsize=(15,5))
pyplot.plot(df.soi)
pyplot.title('How SOI changes over Time')
pyplot.xlabel('Time')


# Let us put all four variables together and see how they are distributed or if there is any visual relationship.

# In[5]:


i = 1
fig = pyplot.figure(figsize=(15,10))
for col in df.columns.tolist():
    fig.add_subplot(len(df.columns.tolist()), 1, i)
    df[col].plot()
    pyplot.title(col, y=0.8, loc='right')
    if i != len(df.columns.tolist()):
        pyplot.tick_params(
            axis='x',          # changes apply to the x-axis
            which='both',      # both major and minor ticks are affected
            bottom=False,      # ticks along the bottom edge are off
            top=False,         # ticks along the top edge are off
            labelbottom=False) # labels along the bottom edge are off
        pyplot.xlabel('')
    i += 1
pyplot.show()


# As we can see, there is some relationship between the input variables (i.e., oni, nino3, pna) and output variable, soi. For example, oni and soi look highly negatively correlated. We will cover how to model SOI using these variables in the time series analysis, but first let's look at how to bring in the last variable - precipitation.

# ## Part 2. Correlation analysis
# Global precipitation monthly mean is avaialble in raster format on ArcGIS online. In this part, we will cover how to indentify the most correlated (on time dimension) grid cell through lagged correlation analysis. There are two rationals behind this. 
# 1. In climate science, a phenomenon that is happening now could be a result of what has happend in the past. In other words, SOI of this month could be most correlated with the ONI of last month or the month before depending on the actual physical process.
# 2. ENOS is a type of teleconnection which refers to climate anomalies being related to each other at large distances.

# First, let us search and acess the precipitation data from ArcGIS Online.

# In[6]:


data = agol.content.search('Global Monthly Precipitation 1979-2017')[0]
data


# In[7]:


data_item = agol.content.get(data.id)


# Once the data is download to local, we can read it and visualize it through skimage and matplotlib to get a sense of what the data looks like. Because the spatial resolution is 2.5 by 2.5 degrees, there are 72 rows and 144 columns. The third dimension is the time dimension as the data contains monthly mean from 1970.1 to 2017.10, which is 466 months in total.

# In[8]:


from skimage import io
import math
from scipy.stats.stats import pearsonr
from numpy import unravel_index

precipitation_path = os.path.join('.', 'data', 'precipitation.tif')
precip_full = io.imread(precipitation_path)
precip_full = np.flip(precip_full, 0)

# print the dimension of the precip_full, (lat, lon, time)
print(precip_full.shape)
# let's print out the first month/band of this data
pyplot.figure(figsize=(15,6))
pyplot.imshow(precip_full[:,:,0], extent=[-180, +180, -90, 90])


# Before calculating the correlation, let's transform the precipitation monthly mean to precipitation anomaly, which a usually provides better signal than absolute precipitation. For short, anomaly means how much precipitation departs from what is normal for that time of year at a specific location. Let's say, we would like to know the anomaly of a specific grid in October this year. We need to compute the historial mean of October precipitation and then caculate its difference with value of this October.

# In[9]:


# calculate the historical mean for each month
a = np.zeros(shape = (precip_full.shape[0], precip_full.shape[1], math.ceil(precip_full.shape[2]/12)*12))
a[:,:,0:precip_full.shape[2]] = precip_full
a = a.reshape(a.shape[0], a.shape[1], int(a.shape[2]/12), 12)
monthly_sum = np.sum(a, axis=2)

montly_mean_1_10 = monthly_sum[:,:,0:10]/a.shape[2]
montly_mean_11_12 = monthly_sum[:,:,-2:]/(a.shape[2] - 1)
monthly_mean = np.append(montly_mean_1_10, montly_mean_11_12, axis=2)

# calculate the difference
for i in range(a.shape[3]):
    a[:,:,i,:] -=monthly_mean

# reshape the data back to its orginial dimension
a = a.reshape(a.shape[0], a.shape[1], a.shape[2]*a.shape[3])

# visualize the first month anomaly
pyplot.figure(figsize=(15,6))
pyplot.imshow(a[:,:,0], extent=[-180, +180, -90, 90])


# Now we have the anomaly. Let us calculate Pearson correlation coefficient between SOI and precipitation anomaly across all the grids based on a range of time lag from 0 to 5. The goal is the find the grid that has the greatest absolute Pearson correlation.

# In[10]:


df.shape


# In[11]:


lag = 5
fig, axs = pyplot.subplots(nrows=3, ncols=2, figsize=(14, 12))
fig.subplots_adjust(left=0.03, right=0.97, hspace=0.4, wspace=0.4)

for t in range(lag+1):
    soi = df.values[t:,0]
    soi = soi.reshape(soi.shape[0], 1)

    precip = a[:,:,0:-4-t]
    
    r2 = []
    for i in range(precip.shape[0]):
        for j in range(precip.shape[1]):
            r2_index = pearsonr(soi, precip[i,j,:].reshape(precip.shape[2], 1))[0]
            r2.append(r2_index)

    r2_map = np.array(r2).reshape(precip.shape[0], precip.shape[1])
    max_index = unravel_index(r2_map.argmax(), r2_map.shape)

    im = axs.flat[t-1].imshow(np.abs(r2_map), extent=[-180, +180, -90, 90])
    fig.colorbar(im, ax = axs[t//2, t%2])
    
    # we only care only absolute correlation in this case
    r2_map = np.abs(r2_map)
    max_index = unravel_index(r2_map.argmax(), r2_map.shape)
    axs.flat[t-1].set_title('lag=' + str(t) + '\n' + 'max_index=' + str(max_index) + '\n' + 'max_absolute_correlation=' + str(float("%.3f" % r2_map[max_index])))


# As can be seen, the largest correlation results from lag=0 at (row, column) = (34, 74). We will then take the precipitation anomaly of this grid and combine it with the other variables to predict SOI in another notebook.

# ## Part 3. Time Series Analysis/LSTM
# 
# The third part of ENSO prediction focuses on time seris analysis. Here our SOI time series prediction problem is formulated as a regression problem and the idea is to use prior time steps to predict the next time step(s). Specifically, this analysis consists of three sections:
# 1. Data cleanup and transformation: transform the raw data into format that supervised machine learning algorithms can take, and split out training and test datasets
# 2. Model training: train a [Long short-term memory](https://en.wikipedia.org/wiki/Long_short-term_memory) (LSTM) neural network.
# 3. Prediction and evaluation: evaluate the models on test dataset and plot the results.

# ## Data cleanup and transformation

# Time series data can be phrased as supervised learning if we think of by previous time steps as input variables and the next time step(s) as the output variable. Before we do any transformationm, let's first import all the necessary packages and take a look at what our data looks like.

# In[13]:


get_ipython().run_line_magic('matplotlib', 'inline')
import os.path
import warnings
import numpy as np
from math import sqrt
from matplotlib import pyplot
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
from pandas import datetime
from sklearn.metrics import mean_squared_error
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.models import load_model
warnings.filterwarnings('ignore')

def parser(x):
    if x.endswith('11') or x.endswith('12')or x.endswith('10'):
        return datetime.strptime(x, '%Y%m')
    else:
        return datetime.strptime(x, '%Y0%m')
enso_ready_path = os.path.join('.', 'data', 'enso_ready.csv')
df = read_csv(enso_ready_path, header=0, parse_dates=[0], index_col=0, date_parser=parser)
df.head()


# As precipitation data is not available until 1979, let's remove the first few rows, standardize each column by calcuting the z-score, and move soi (input variable) to the last column of the table.

# In[14]:


start = 336 
df = df.iloc[start:]
df = (df - df.mean()) / df.std()

cols = df.columns.tolist()
cols = cols[1:] + cols[:1]
df = df[cols]
df.head()


# Most supervised learning algorithms require all input variables to be on the same row with its corresponding output variable. In SOI prediction, the goal is to use the variables (i.e., oni, nino3, pna, precip, and soi) of the previous time steps (e.g. 12) to predict the SOI of the next time steps (e.g. 3). Formally, the use of prior time steps to predict the next time step is called the sliding window approach (aka window or lag method) in time series analysis/prediction. Therefore, let's define a method that transforms panda time series into format that supervised learning algorithms can take.

# In[15]:


"""
This method takes a time series and returns transformed data.
data: time series as pandas dataframe
n_in: number of previous time steps as input (X)
n_out: number of next time steps as output (y)
dropnan: whether or not to drop rows with NaN values
"""
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
    n_vars = 1 if type(data) is list else data.shape[1]
    df = DataFrame(data)
    cols, names = list(), list()
    # input sequence (t-n, ... t-1)
    for i in range(n_in, 0, -1):
        cols.append(df.shift(i))
        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
    # forecast sequence (t, t+1, ... t+n)
    for i in range(0, n_out):
        cols.append(df.shift(-i).iloc[:,-1])
        if i == 0:
            names += ['VAR(t)']
        else:
            names += ['VAR(t+%d)' % i]
    # put it all together
    agg = concat(cols, axis=1)
    agg.columns = names
    # drop rows with NaN values
    if dropnan:
        agg.dropna(inplace=True)
    return agg


# Note that after transformation, there are some edges cases where the input or output variable could be NaN. For example, as data before 1979 is not available, there is no way to form a row for SOI of 1979. That's why there is a dropnan option is the method.

# In[16]:


# specify the size of our sliding window and number of features
enso = df.values.astype('float32')
lag = 12
ahead = 3
n_features = 1
reframed = series_to_supervised(enso, lag, ahead)
reframed.head()


# ## Model training

# Now we have the data, let's define a method that trains a LSTM model. For the purpose of simplicity, we define a two layer neural network with one LSTM layer and one dense layer.

# In[17]:


"""
This method takes training data and returns a LSTM model
train: training data
n_lag: number of previous time steps
n_ahead: number of next time steps
nb_epoch: number of epochs
n_neurons: number of n_neurons in the first layer
"""
def fit_lstm(train, n_lag, n_ahead, n_batch, nb_epoch, n_neurons):
    # reshape training into [samples, timesteps, features]
    X, y = train[:, :-n_ahead], train[:, -n_ahead:]
    X = X.reshape(X.shape[0], n_lag, int(X.shape[1]/n_lag))

    # design neural network architecture. This is a simple LSTM just for demo purpose
    model = Sequential()
    model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))
    model.add(Dense(n_ahead))
    model.compile(loss='mean_squared_error', optimizer='adam')
    # fit the NN
    for i in range(nb_epoch):
        model.fit(X, y, epochs=1, batch_size=n_batch, verbose=2, shuffle=False)
        model.reset_states()
    return model


# Let's split our transformed data into training and test sets and feed the model training method. The first 80 precent will be used for training purpose and the last 20 percent will be using as test set. Note that in time sereis analysis, we don't do random shuffle because it's important to preserve time dependency/order.

# In[18]:


values = reframed.values
n_train = int(len(values) * 0.8)
train = values[:n_train, :]
test = values[n_train:, :]

# fit a LSTM model with the transformed data
# model fitting can be very time-consuming, therefore a pre-trained model is included in the data folder
model_path = os.path.join('.', 'data', 'my_model.h5')
if not os.path.exists(model_path):
    model = fit_lstm(train, lag, ahead, 1, 30, 30)
    model.save(model_path)
else:
    model = load_model(model_path)


# In[19]:


model.summary()


# ## Prediction and evaluation

# Now let's apply the model to the test set and evaluate the accuracy for each of those three next time steps.

# In[20]:


# predict the SOI values for next three time steps given a single input sample 
def forecast_lstm(model, X, n_batch, n_lag):
    # reshape input pattern to [samples, timesteps, features]
    X = X.reshape(1, n_lag, int(len(X)/n_lag))
    # make forecast
    forecast = model.predict(X, batch_size=n_batch)
    # convert to array
    return [x for x in forecast[0, :]]

# make prediciton for a list of input samples
def make_forecasts(model, n_batch, train, test, n_lag, n_ahead):
    forecasts = list()
    for i in range(len(test)):
        X = test[i, :-n_ahead]
        # make forecast
        forecast = forecast_lstm(model, X, n_batch, n_lag)
        # store the forecast
        forecasts.append(forecast)
    return forecasts


# In[21]:


forecasts = make_forecasts(model, 1, train, test, lag, ahead)
# pring out the output for the first input sample
forecasts[0]


# As mentioned in the very beginning, time series prediction is treated as a regression problem in our case, so let's compute mean square error (MSE) for each next time step.

# In[22]:


def evaluate_forecasts(y, forecasts, n_lag, n_seq):
    print('Evaluation results (RMSE) for each next tim step:')
    for i in range(n_seq):
        actual = [row[i] for row in y]
        predicted = [forecast[i] for forecast in forecasts]
        rmse = sqrt(mean_squared_error(actual, predicted))
        print('t+%d time step: %f' % ((i+1), rmse))
        
# evaluate forecasts
actual = [row[-ahead:] for row in test]
evaluate_forecasts(actual, forecasts, lag, ahead)


# To to a better understanding of the prediction result, let's plot it out and compare with with the original data.

# In[23]:


# plot the forecasts in the context of the original dataset, multiple segments
def plot_forecasts(series, forecasts, n_test, xlim, ylim, n_ahead, linestyle = None):
    pyplot.figure(figsize=(15,8))
    if linestyle==None:
        pyplot.plot(series, label='observed')
    else:
        pyplot.plot(series, linestyle, label='observed')
    pyplot.xlim(xlim, ylim)
    pyplot.legend(loc='upper right')
    # plot the forecasts in red
    for i in range(len(forecasts)):
        # this ensures not all segements are plotted, it is plotted every n_ahead
        if i%n_ahead ==0:
               off_s = len(series) - n_test + 2 + i - 1
               off_e = off_s + len(forecasts[i]) + 1
               xaxis = [x for x in range(off_s, off_e)]
               yaxis = [series[off_s]] + forecasts[i] 
               pyplot.plot(xaxis, yaxis, 'r')
    pyplot.show()
    
plot_forecasts(df['soi'].values, forecasts, test.shape[0] + ahead - 1, 0, 500, ahead)


# Here the blue line is the ogriginal time series, the red line is the prediction results. As we can see, it is doing a reasonable job. ENSO prediction is considered one of the most difficult task in climate science, but with more sophisicated modeling tuning and architecture desgin, we believe better results could be achieved.

# ## Conclusion

# In this notebook, we observed how ENSO prediction can be done with the aid of ArcGIS API for Python. We started with a correlation to find the most correlated grid in terms of precipitation. And then we performed time series analysis and LSTM to predict SOI based a few input variables including precipitation from prior time steps. With a basic LSTM example, we are able to acheive a reasonable accuracy as ENSO prediction is one of the most challenge tasks in climatology.

# In[ ]:






# ====================
# river_turbidity_estimation_using_sentinel2_data_.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # River Turbidity Estimation using Sentinel-2 data

# ## Table of Contents
# 
# * [Introduction](#1)
# * [Necessary imports](#2)
# * [Connect to your GIS](#3)
# * [Get the data for analysis](#4)
# * [Methodology](#5)
# * [Prepare data for analysis](#6)
#     * [Create geometry of AOI ](#7)
#     * [Filter out sentinel-2 tiles](#8)
#     * [Extract bands](#9)
# * [Generate water body mask](#10)
#     * [Create normalized difference water index raster](#11)
#     * [Create binary raster](#12)
#     * [Create water body mask](#13)
# * [Normalized difference turbidity index](#14)
#     * [Turbidity raster for water bodies ](#15)
# * [Visualize results](#16)
#     * [Get the results](#17)
#     * [Interpretation of results](#20)
# * [Conclusion](#21)
# * [literature resources](#22)

# ## Introduction <a class="anchor" id="1"></a>

# Turbidity represents the level of suspended sediments in water also indicating water clarity or how clear is the water. It is mainly caused by the presence of silt, algae in a water body, or industrial waste disposed in the rivers by mining activity, industrial operations, logging, etc.
# 
# Traditionally, turbidity is analyzed by evaluating water samples taken during field measurements. However, field studies are expensive, time and labor intensive, besides, during lockdown field surveys cannot be undertaken. Thus, a good alternative to field survey measurements is satellite remote sensing data, which can capture both spatial and temporal variations in river turbidity levels. Accordingly, Sentinel-2 multispectral data is used in the current study to evaluate the changes in river turbidity during COVID-19 lockdown, near the holy city of Allahabad, India.
# 
# Allahabad, situated in the northern part of India, is at the confluence of two major rivers: Ganga and its tributary river Yamuna. The point of confluence is a sacred place for Hindus where thousands of devotees gather everyday by the banks of the river to offer prayers. Millions of people depend on the waters from these rivers yet both rivers are heavily polluted by industrial waste, by sewage and even by the remains of the many bodies cremated on their banks. On March 25, 2020, India announced a lockdown for controlling the spread of COVID-19 leading to complete shutdown of industries and restricted human movement. The lockdown resulted in reduction of turbidity thereby improving water quality in rivers throughout the country. This notebook will elaborate the steps to measure this change in turbidity using ArcGIS API for Python.

# 

# ## Necessary Imports <a class="anchor" id="2"></a>

# In[1]:


import pandas as pd
from ipywidgets import HBox, VBox, Label, Layout


import arcgis
from arcgis.features import GeoAccessor, GeoSeriesAccessor
from arcgis.raster.analytics import convert_feature_to_raster, convert_raster_to_feature
from arcgis.raster.functions import extract_band, greater_than, clip, remap, colormap, stretch
from arcgis.features.analysis import dissolve_boundaries


# ## Connect to your GIS <a class="anchor" id="3"></a>

# In[2]:


from arcgis import GIS
agol_gis = GIS('Home')
ent_gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# ## Get the data for analysis <a class="anchor" id="4"></a>

# [Sentinel-2 Views](https://www.arcgis.com/home/item.html?id=fd61b9e0c69c4e14bebd50a9a968348c) is used in the analysis, this multitemporal layer consists 13 bands with 10, 20, and 60m spatial resolution. The imagery layer is rendered on-the-fly and available for visualization and analytics.  This imagery layer pulls directly from the Sentinel-2 on AWS collection and is updated daily with new imagery.

# In[3]:


s2 = agol_gis.content.get('fd61b9e0c69c4e14bebd50a9a968348c')
sentinel = s2.layers[0]
s2


# A feature layer was published which represents the extent of study area. This feature will be used for extracting Sentinel-2 tiles of study area.

# In[4]:


aoi1 = agol_gis.content.search('title:allahabad_aoi owner:api_data_owner', 'Feature Layer Collection')[0]
aoi1


# ## Methodology <a class="anchor" id="5"></a>

# 

# The above diagram encapsulates the overall methodology that has been followed in the estimation of the river turbidity.

# ## Prepare data for analysis <a class="anchor" id="6"></a>

# `Sentinel-2 Views` imagery layer consists data for the whole world and span different time periods. Thus, the first step is to filter out the data for the rivers in Allahabad region prior to and during the lockdown period.

# ### Create geometry of area of interest (AOI) <a class="anchor" id="7"></a>

# The geometry of AOI is created for filtering out the Sentinel-2 tiles for the study area.

# In[5]:


aoi_layer = aoi1.layers[0]
aoi_feature = aoi_layer.query(where='fid=1')
aoi_geom = aoi_feature.features[0].geometry
aoi_geom['spatialReference'] = {'wkid':3857}


# ### Filter out sentinel-2 tiles  <a class="anchor" id="8"></a>

# In[6]:


m = agol_gis.map('Allahabad, India', 11)
m


# 

# In[7]:


m.zoom_to_layer(aoi1)


# #### Before lockdown

# Sentinel-2 tiles were filtered out for before lockdown scenario using `query` function on the basis of `AcquistionDate` column. `Category` represents source of data, for the current analysis `Category=1` was used which means Sentinel-2 Level-1 data. Sentinel-2 tiles were filtered out for the before lockdown scenario from March 9, 2020 to March 11, 2020.

# In[8]:


from datetime import datetime
selected = sentinel.filter_by(where="(Category = 1)",
                             time=[datetime(2020, 3, 9), datetime(2020, 3, 11)],
                             geometry=arcgis.geometry.filters.intersects(aoi_geom))
df = selected.query(out_fields="AcquisitionDate, GroupName, CloudCover, DayOfYear",
                   order_by_fields="AcquisitionDate").sdf

df['AcquisitionDate'] = pd.to_datetime(df['acquisitiondate'], unit='ms')
df


# Sentinel-2 tile of `OBJECTID=10003775` was selected which represents before lockdown turbidity conditions. 

# In[9]:


tile1 = sentinel.filter_by('OBJECTID=10003775')
tile1.extent = m.extent
tile1.save('s2_allbd_20200309', gis=ent_gis)
tile1


# #### During lockdown
# 
# Sentinel-2 tiles were filtered out for during lockdown scenario using `query` function on the basis of `AcquistionDate` column. Sentinel-2 tiles were filtered out for the before lockdown scenario from April 9, 2020 to April 14, 2020.

# In[10]:


from datetime import datetime
selected = sentinel.filter_by(where="(Category = 1)",
                             time=[datetime(2020, 4, 9), datetime(2020, 4, 14)],
                             geometry=arcgis.geometry.filters.intersects(aoi_geom))
df = selected.query(out_fields="AcquisitionDate, GroupName, CloudCover, DayOfYear",
                   order_by_fields="AcquisitionDate").sdf

df['AcquisitionDate'] = pd.to_datetime(df['acquisitiondate'], unit='ms')
df


# Sentinel-2 tile of OBJECTID=10481214 was selected which represents during lockdown turbidity conditions.

# In[11]:


tile2 = sentinel.filter_by('OBJECTID=10481214')
tile2.extent = m.extent
tile2.save('s2_allbd_20200413', gis=ent_gis)
tile2


# In[12]:


tiles = ent_gis.content.search('title:s2_allbd_')
tiles


# In[13]:


timestamps=set([layer.title[-8:] for layer in tiles])
timestamps=[t.strftime('%Y%m%d') for t in sorted([datetime.strptime(t,'%Y%m%d') for t in timestamps])]


# ## Generate water body mask<a class="anchor" id="10"></a>

# ### Extract bands<a class="anchor" id="9"></a>

# Green (band 3), Red (band 4) and NIR (band 8) bands were extracted from the Sentinel-2 tiles for NDWI and NDTI calculation.

# In[14]:


def extract_green_band(tiles):
    print('extracting green band',end='\r')
    green_band = extract_band(tiles, [3])
    return green_band


# In[15]:


def extract_red_band(tiles):
    print('extracting red band',end='\r')
    red_band = extract_band(tiles, [4])
    return red_band


# In[16]:


def extract_nir_band(tiles):
    print('extracting nir band',end='\r')
    nir_band = extract_band(tiles, [8])
    return nir_band


# ### Create normalized difference water (NDWI) index raster<a class="anchor" id="11"></a>

# Normalized difference water index (NDWI) is a satellite based index used for mapping and detecting the surface water bodies. Water absorbs electromagnetic radiation in visible to infrared spectrum, that is why Green and Near Infrared bands are used to detect the water bodies. In the current study, band 3 (green) and band 8 (NIR) of Sentinel-2 are used for generating NDWI raster. Here, `t` represents timestamps.

# In[17]:


def create_ndwi_raster(green_band, nir_band, t, aoi_geom):
    print('creating ndwi raster',end='\r')
    ndwi = (green_band - nir_band)/(green_band + nir_band)
    c_ndwi = clip(ndwi, aoi_geom)
    ndwi_s = c_ndwi.save("ndwi_allbdt"+t, gis=ent_gis)
    ndwi_lyr = ndwi_s.layers[0]
    return ndwi_lyr


# ### Create binary raster<a class="anchor" id="12"></a>

# Binary raster is created from NDWI raster using a threshold value. The binary raster consists of two classes of water and non-water pixels where pixels with value greater than 0.03 are considered as water. Accordingly, this threshold value of 0.03 is used for creating the binary raster using the [greater_than](https://desktop.arcgis.com/en/arcmap/10.3/tools/spatial-analyst-toolbox/greater-than.htm) function. Here, `t` represents timestamps. 

# In[18]:


def create_binary_raster(ndwi_lyr, t):
    print('creating binary raster',end='\r')
    binary_raster = greater_than([ndwi_lyr, 0.03], 
                              extent_type='FirstOf', 
                              cellsize_type='FirstOf', 
                              astype='U16')
    binary_ras = binary_raster.save("binaryrast_"+t, gis=ent_gis)
    binaryras_lyr = binary_ras.layers[0]
    return binaryras_lyr


# ### Create water body mask<a class="anchor" id="13"></a>

# #### Convert binary raster to feature layer

# The binary rasters are converted to feature layer for extracting the boundaries of the water bodies. Here, `t` represents timestamps.

# In[19]:


def create_binary_poly(binaryras_lyr, t):
    print('creating binary feature layer',end='\r')
    binary_poly = convert_raster_to_feature(binaryras_lyr, 
                                          field='Value', 
                                          output_type='Polygon', 
                                          simplify=True, 
                                          output_name='binary_poly'+t, 
                                          gis=ent_gis)
    return binary_poly


# #### Extract water polygons

# In the feature layer returned above, 'gridcode=0' represents non water class and 'gridcode=1' represents water class. Thus, the water polygons with 'gridcode=1 are selected using the `query` function from the dataframe of the feature layer and saved into a new feature layer created using `gis.content.import` function consisting of these water polygons. Here, `t` represents timestamps.

# In[20]:


def extract_water_polygons(binary_poly, t):
    print('extracting water polygon',end='\r')
    dfm=binary_poly.layers[0].query('gridcode=1').sdf 
    water_poly=gis_ent.content.import_data(dfm, title='wpoly'+t)
    return water_poly


# The features of `water_poly` were dissolved using [dissolve_boundaries](https://pro.arcgis.com/en/pro-app/tool-reference/feature-analysis/dissolve-boundaries.htm) function. The features were dissolved on the basis of `gridcode` column.

# Finally the features in the layer `water_poly` are dissolved using [dissolve_boundaries](https://pro.arcgis.com/en/pro-app/tool-reference/feature-analysis/dissolve-boundaries.htm) function on the basis of gridcode column to obtain the water body mask in the following.

# In[21]:


def dissolve_water_features(water_poly, t):
    print('dissolving water features',end='\r')
    diss_f = dissolve_boundaries(water_poly,
                                 dissolve_fields=['gridcode'], 
                                 output_name='dissolve_poly'+t, 
                                 gis=ent_gis,  
                                 multi_part_features=True)
    return diss_f


# ###  Normalized difference turbidity index<a class="anchor" id="14"></a>

# The Normalize Difference Turbidity Index (NDTI) which is estimated using the spectral reflectance values of the water pixels is used to estimate the turbidity in water bodies. It uses the phenomenon that the electromagnetic reflectance is higher in green spectrum than the red spectrum for clear water. Hence, with increase in turbidity the reflectance of red spectrum also increases. Accordingly, in the current study, Sentinel-2 green (band 3) and red (band 4) bands are used to create the NDTI raster in the following. Here, t represents timestamps.

# In[22]:


def create_ndti_ras(red_band, green_band, t):
    print('creating ndti raster',end='\r')
    ndti = (red_band - green_band)/(red_band + green_band)
    ndti_s = ndti.save("ndti_allbd"+t, gis=ent_gis)
    ndti_lyr = ndti_s.layers[0]
    return ndti_lyr


# ### Turbidity raster for water bodies<a class="anchor" id="15"></a>

# Once the NDTI raster is prepared, its portion for the water body is extracted using the water body mask obtained earlier. To do so, fwe start by creating the geometry of `dissolve_f` as follows:

# In[23]:


def create_water_geom(diss_f):
    print('creating geometry',end='\r')
    aoi2_layer = diss_f.layers[0]
    aoi2_feature = aoi2_layer.query(where='gridcode=1')
    aoi2_geom = aoi2_feature.features[0].geometry
    aoi2_geom['spatialReference'] = {'wkid':3857}
    return aoi2_geom


# Second step is to [clip](https://developers.arcgis.com/python/api-reference/arcgis.raster.functions.html#clip) the NDTI rasters with the geometry of `dissolve_f` raster. Here, t represents timestamps.

# In[24]:


def clip_water_ndti_ras(ndti_lyr, aoi2_geom, t):
    print('clipping ndti raster with water boundary',end='\r')
    clip_ndti = clip(ndti_lyr, aoi2_geom)
    clip_ndti_ras = clip_ndti.save("cl_ndti"+t, gis=ent_gis)
    return clip_ndti_ras


# Third step is to apply [colormap](https://developers.arcgis.com/python/api-reference/arcgis.raster.functions.html?highlight=colormap#arcgis.raster.functions.colormap) for visualizing the results. Here, t represents timestamps.

# In[25]:


def stretch_ndti_ras(clip_ndti_ras, t):
    print('clipping stretch ndti raster',end='\r')
    stretch_rs = colormap(stretch(clip_ndti_ras.layers[0], 
                                  stretch_type='PercentClip', 
                                  min=0, 
                                  max=255),
                          colormap_name="Condition Number")
    ndti_cmap = stretch_rs.save("ndti_cmapr"+t, gis=ent_gis)
    return stretch_ras


# Finally, all of the above mentioned functions are used inside a loop and applied on the satellite imagery of the river from two different time periods: one before lockdown (March 9, 2020) and the second, during the lockdown period (April 13, 2020), to visualize and compare the difference in the turbidity levels.

# In[26]:


for t in timestamps:
    
    print(f'Processing layers for :{t}\n')
    tiles = ent_gis.content.search(f's2_allbd_{t}')[0].layers[0]
    green_band=extract_green_band(tiles=tiles)
    red_band=extract_red_band(tiles=tiles) 
    nir_band=extract_nir_band(tiles=tiles) 
    ndwi_lyr=create_ndwi_raster(green_band=green_band, nir_band=nir_band, t=t, aoi_geom=aoi_geom)
    binaryras_lyr=create_binary_raster(ndwi_lyr=ndwi_lyr, t=t)
    binary_poly=create_binary_poly(binaryras_lyr=binaryras_lyr, t=t)
    water_poly=extract_water_polygons(binary_poly=binary_poly, t=t)
    diss_f=dissolve_water_features(water_poly=water_poly, t=t)
    ndti_lyr=create_ndti_ras(green_band=green_band, red_band=red_band, t=t)
    aoi2_geom=create_water_geom(diss_f=diss_f)
    clip_ndti_ras=clip_water_ndti_ras(ndti_lyr=ndti_lyr, aoi2_geom=aoi2_geom, t=t)
    stretch_ras=stretch_ndti_ras(clip_ndti_ras=clip_ndti_ras, t=t)
    print(f'Processing completed for :{t}\n')


# ## Visualize results<a class="anchor" id="16"></a>

# ### Result Visualization<a class="anchor" id="17"></a>

# The NDTI rasters and their corresponding Sentinel-2 tiles are now accessed from the portal and used for visualization.

# In[27]:


ras1 = ent_gis.content.search('ndti_cmap20200309', 'Imagery Layer')[0]
ras2 = ent_gis.content.search('ndti_cmap20200413', 'Imagery Layer')[0]
ras3 = ent_gis.content.search('s2_allbd_20200309', 'Imagery Layer')[0]
ras4 = ent_gis.content.search('s2_allbd_20200413', 'Imagery Layer')[0]


# ### Create map widgets<a class="anchor" id="17"></a>

# 4 map wigets are created representing the NDTI raster and corresponding Sentinel-2 for 9th March (before lockdown) and 13th April (during lockdown).

# In[28]:


map1 = ent_gis.map("Allahabad, India", 12)
map1.basemap = 'national-geographic'
map1.add_layer(ras1)
map2 = ent_gis.map("Allahabad, India", 12)
map2.basemap = 'national-geographic'
map2.add_layer(ras2)
map3 = ent_gis.map("Allahabad, India", 12)
map3.add_layer(ras3)
map4 = ent_gis.map("Allahabad, India", 12)
map4.add_layer(ras4)


# ### Synchronize web maps<a class="anchor" id="18"></a>

# All the maps are synchronized with each other using [MapView.sync_navigation](https://developers.arcgis.com/python/api-reference/arcgis.widgets.html#arcgis.widgets.MapView.sync_navigation) functionality. It helps in comparing the river turbidity before and during lockdown. Detailed description about advanced map widget options can be referred [here](https://developers.arcgis.com/python/guide/advanced-map-widget-usage/)

# In[29]:


map1.sync_navigation(map2)
map2.sync_navigation(map3)
map3.sync_navigation(map4)


# ### Set the map layout<a class="anchor" id="19"></a>

# [Hbox and Vbox](https://developers.arcgis.com/python/api-reference/arcgis.widgets.html?highlight=hbox) were used to set the layout of map widgets.

# In[30]:


hbox_layout = Layout()
hbox_layout.justify_content = 'space-around'

hb1,hb2=HBox([Label('NDTI (March 9, 2020)'),Label('NDTI (April 13, 2020)')]),\
                HBox([Label('Sentinel-2 (March 9, 2020)'),Label('Sentinel-2 (April 13, 2020)')])
hb1.layout,hb2.layout=hbox_layout,hbox_layout


# ### Interpretation of results<a class="anchor" id="20"></a>

# In[31]:


VBox([hb1,HBox([map1,map2]),hb2, HBox([map3,map4])])


# 

# The maps above show the spatio-temporal variation in turbidity of the Ganga and Yamuna rivers due to Covid-19 lockdown. The satellite imageries used in the analysis are from before lockdown, dated March 9, 2020 (bottom-left), and during the lockdown, dated April 13, 2020 (botton-right).
# 
# It can be seen that turbidity declined in both the Ganga and Yamuna rivers during lockdown as indicated by the Red pixels representing high turbidity, turning to Orange, Yellow and finally Green, which is the lowest level of turbidity. Compared to Yamuna, the Ganga river had a significant impact on turbidity due to lockdown. The complete stretch of Ganga turned from red and yellow to mostly green pixels. The turbidity at the confluence point also declined followed by a decline in the stretch downstream.

# ## Conclusion<a class="anchor" id="21"></a>

# Ganga and Yamuna rivers are among the world's most polluted due to dumping of waste from industrial and religious activities as indicated by their highly turbid water. During the lockdown period, pollution levels fell dramatically in both rivers due to complete shutdown of the above-mentioned functions. This change was investigated using Sentinel-2 satellite imageries of the river stretches, before and during the lockdown periods, with results suggesting substantial decrease in turbidity levels thereby validating the same. The same methodology can be implemented to study the changes in river turbidity for other regions with Sentinel-2 data using ArcGIS platform.

# ## Literature resources<a class="anchor" id="22"></a>

# |Literature | Source | Link |
# | -| - |-|
# | Research Paper   | Changes in turbidity along Ganga River using Sentinel-2 satellite data during lockdown associated with COVID-19|https://www.tandfonline.com/doi/full/10.1080/19475705.2020.1782482|
# | Research Paper   |Water Turbidity Assessment in Part of Gomti River Using High Resolution Google Earth Quickbird satellite data|https://geospatialworldforum.org/2011/proceeding/pdf/Shivangifullpaper.pdf|


# ====================
# safe_streets_to_schools.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Safe Streets to Schools

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <br/>
# <div class="toc">
#     <ul class="toc-item">
#         <li><span><a href="#Introduction" data-toc-modified-id="Introduction-01">Introduction</a></span></li>
#         <li><span><a href="#Workflow" data-toc-modified-id="Workflow-02">Workflow</a></span></li>
#         <li><span><a href="#Necessary-Imports" data-toc-modified-id="Necessary-Imports-02">Necessary Imports</a></span></li>
#         <li><span><a href="#Connect-to-your-ArcGIS-Online-organization" data-toc-modified-id="Connect-to-your-ArcGIS-Online-organization-1">Connect to your ArcGIS online organization</a></span></li>
#         <li><span><a href="#Get-the-data-for-the-analysis." data-toc-modified-id="Get-the-data-for-the-analysis.-2">Get the data for the analysis.</a></span></li>
#         <li><span><a href="#Uncover-patterns" data-toc-modified-id="Uncover-patterns-3">Uncover patterns</a></span></li>
#         <li><span><a href="#Symbolize-by-category" data-toc-modified-id="Symbolize-by-category">Symbolize by category</a></span></li>
#         <li><span><a href="#Add-school-areas" data-toc-modified-id="Add-school-areas-5">Add school areas</a></span></li>           <li><span><a href="#Creating-areas-within-a-half-mile-walking-distance-of-schools" data-toc-modified-id="Creating areas-within-a-half-mile-walking-distance-of-schools-6">Creating areas within a half-mile walking distance of schools</a></span></li>
#         <li><span><a href="#Find-the-most-dangerous-school-areas" data-toc-modified-id="Find-the-most-dangerous-school-areas-7">Find the most dangerous school areas</a></span></li>
#         <li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-14">Conclusion</a></span></li>
#     </ul>
# </div>

# ### Introduction
# 

# Accidents near elementary schools, such as <a href="https://www.latimes.com/archives/la-xpm-2000-apr-30-me-25074-story.html">this one</a> has drawn the attention of city employees and civic-minded individuals to the topic of pedestrian and bicycle safety. 
# 
# In this notebook, we want to suggest policy actions to civic authorities of Pasadena, California that will reduce the likelihood of future accidents. We will map accident data regarding pedestrians and cyclists struck by vehicles. Then, we will determine the number of accidents that occurred within each school zone and identify the five most dangerous zones. We will present this analysis using a notebook that provides narrative context and helps stakeholders understand our methodology.
# 
# The sample uses ArcGIS API for Python to help city officials in improving pedestrian and bicycle safety near schools in the city. We will first uncover patterns in accident data using spatial analysis tool such as `calculate_density`, `find_hot_spots` and `HeatmapRenderer`. `HeatmapRenderer` highlights the capabilities of map widget to show areas where large number of accidents occurred. To visualize different category of accidents on the map, `create_symbol` tool plays a vital role along with functionality of renderers and arcade expressions.
# 
# As the visualizations are not enough to make policy decisions, we'll further use `create_drive_time_areas` and `summarize_within` tools to determine the number of accidents that have occurred within half mile walk distance from each school. 
# 
# Since the city does not have enough funds that encompasses all school zones, we will limit our analysis to the five most vulnerable areas. So, we apply filter to identify the five most dangerous zones in which a pedestrian or cyclist was injured or killed. City officials could then suggest measures such as adding more bike lanes in these areas or change street signs to reduce accidents near schools and increase safety. 
# 
# 

# ### Workflow

# 

# ### Necessary Imports

# In[1]:


get_ipython().run_line_magic('matplotlib', 'inline')

import pandas as pd
from datetime import datetime as dt
import matplotlib.pyplot as plt
from IPython.display import display

from arcgis.gis import GIS
from arcgis.features.analyze_patterns import calculate_density, find_hot_spots
from arcgis.mapping.symbol import create_symbol
from arcgis.features.use_proximity import create_drive_time_areas
from arcgis.features.summarize_data import summarize_within


# ### Connect to  your ArcGIS Online organization

# Connect to the GIS via an existing profile or creating a new connection by e.g. gis = GIS("https://www.arcgis.com", "arcgis_python", "P@ssword123")

# In[2]:


gis = GIS('home')


# ### Get the data for the analysis

# Search for the **Traffic Collisions** layer. You can specify the owner's name as "api_data_owner" to get more specific results.

# In[3]:


items = gis.content.search('title:Traffic Collisions, owner:api_data_owner', 'feature layer')


# In[4]:


items[0]


# In[5]:


item = items[0]


# The item shows all accidents that occurred in Pasadena during the past decade. 

# We can print the names of the layers in this item and assign them to variables that will be used in our analysis

# The code below prints name of layers.

# In[6]:


for lyr in item.layers:
    print(lyr.properties.name)


# Since the item is a Feature Layer Collection, accessing the layers property gives us a list of FeatureLayer objects. The collisions layer is the only layer in this item.

# In[7]:


collisions = item.layers[0]


# In[8]:


m1 = gis.map('Pasadena, California', 13)
m1


# The map shows all accidents that occurred in Pasadena during the past decade.

# In[9]:


m1.add_layer(collisions)


# In[10]:


m1.legend = True


# As the layer does not only contain information about location, but it also has other attributes that cannot be seen on map. So using `query` method, we can query the features on a feature layer. `sdf` property of `featureSet` class is a powerful tool to visualize all the features as Pandas `DataFrame`.

# In[11]:


collisions.query().sdf.columns


# In[12]:


collisions.query().sdf.InvWith.unique()


# Filter the layer to show only accidents that involved a pedestrian or cyclist.

# In[13]:


collisions.filter = "(InvWith = 'Bicycle') OR (InvWith = 'Pedestrian')"


# ### Uncover patterns

# We are able to visualize accidents on map. However, it does not reveal any patterns.  There can possibly be areas where particularly large number of accidents are occurring.
# 
# Different ways to find patterns in data includes point clustering, heat maps, hot spot analysis and [calculate density](https://pro.arcgis.com/en/pro-app/tool-reference/spatial-analyst/how-kernel-density-works.htm). These methods reveal where accidents are happening at abnormal rates.
# 
# First, we will apply `calculate_density` tool to get better insights on data.

# In[14]:


collision_density = calculate_density(input_layer=collisions,
                                      output_name='density_of_incidents' + str(dt.now().microsecond))


# In[15]:


collision_density


# In[16]:


m2 = gis.map('Pasadena, California', 13)
m2


# Areas with large number of accidents show up as more densely colored as compared to other areas.

# In[17]:


m2.add_layer(collision_density)


# In[18]:


m2.legend = True


# `find_hot_spots` method finds statistically significant spatial clusters of high values (hot spots) and low values (cold spots).

# In[19]:


collision_hot_spots = find_hot_spots(collisions,
                                     output_name='collision_hexagon_hot_spots' + str(dt.now().microsecond),
                                     shape_type='hexagon')


# In[20]:


collision_hot_spots


# In[21]:


m3 = gis.map('Pasadena, California', 13)
m3


# The red hex bins show areas of spatially significant clustering, while white hex bins show areas with no significant clustering. There are no blue areas on the map, but if there were, they would represent areas with low statistically significant clustering. The map indicates that accidents happen across the city, but with a statistically significant clustering in the downtown area.

# In[22]:


m3.add_layer(collision_hot_spots)


# In[23]:


m3.legend = True


# The `HeatmapRenderer` renders point data into a raster visualization that emphasizes areas of higher density or weighted values.

# We will use `HeatmapRenderer` to display generalized point pattern locations.

# In[24]:


m4 = gis.map('Pasadena, California', 13)
m4


# Like the `calculate_density`, the `HeatmapRenderer` indicates a high density of accidents in central Pasadena. However, the heat map particularly emphasizes the area south of the Foothill Freeway, corresponding to downtown.

# In[25]:


m4.add_layer({"type": "FeatureLayer",
              "url": collisions.url,   
              "renderer": "HeatmapRenderer",
              "opacity": 0.75})


# In[26]:


m4.legend = True


# We now have a better understanding of the data.

# ### Symbolize by category

# Next, we will change the symbols of accidents layer to show different categories of accidents. In particular, we want to distinguish fatal accidents from accidents with only injuries. We also want to distinguish pedestrian accidents from bicycle accidents. This information will add more detail to our findings and will help support policymaker decisions.

# In[27]:


df = collisions.query().sdf


# In[28]:


filtered_df = df[(df['InvWith'] == "Pedestrian") | (df['InvWith'] == "Bicycle")]


# In[29]:


filtered_df.head()


# To create these four symbol categories, we will use an [Arcade](https://developers.arcgis.com/arcade/guide/) expression. Arcade expressions use attribute information to determine symbology.

# We will first assign three variables that represent the four categories we want to symbolize. However, these variables aren't exclusive. Accidents involving pedestrians or cyclists likely also have injuries or fatalities. There are four combinations of the type, fatal, and injured variables:
# - Accidents involving a pedestrian and a fatality
# - Accidents involving a pedestrian and an injury
# - Accidents involving a cyclist and a fatality
# - Accidents involving a cyclist and an injury
# 
# To account for these combinations, we will create a 'When' function. A When function indicates that when certain conditions are met, a specific symbology category will be used.

# In[30]:


arcade_expression = """
var acc_type = $feature.InvWith;
var fatal = $feature.Nokilled;
var injured = $feature.NoInjured;
var result = When( 
    acc_type == 'Pedestrian' && fatal == '1'  , 'PedestrianFatality',
    acc_type == 'Pedestrian' && injured != '0', 'PedestrianInjury',
    acc_type == 'Bicycle'    && injured != '0', 'BicycleInjury',
    acc_type == 'Bicycle'    && fatal == '1',   'BicycleFatality',
    'null');
return result;
"""


# In[31]:


arcade_expression


# The code below cycles trough each category and asigns it a color, size and symbol.

# In[32]:


def get_symbol(color, size):
    return create_symbol(geometry_type='point',
                         symbol_type='simple',
                         symbol_style='o',
                         colors=color,
                         marker_size=size,
                         outline_style='s',
                         outline_color=[153,153,153,255], line_width=0.375)


def get_unique_values(color_list, values):
    return [ { "label": value, "symbol":get_symbol(color, size), "value": value } for color, size, value in zip(color_list, sizes, values) ]


# In[33]:


color_list = [ [255, 0, 0, 255], [0, 255, 0, 255], [0, 0, 255, 255], [0 , 255, 255, 255] ] 
values = ['PedestrianFatality', 'PedestrianInjury', 'BicycleInjury', 'BicycleFatality']
sizes = [15, 3, 3, 15]
uv = get_unique_values(color_list, values)


# In[34]:


m5 = gis.map('Pasadena, California', 13)
m5


# The fatalities receive a larger symbol size to indicate the difference in severity between injuries and fatalities.

# In[35]:


filtered_df.spatial.plot(map_widget=m5,
                         renderer_type='u-a',
                         unique_values=uv,
                         arcade_expression=arcade_expression)


# In[36]:


m5.legend = True


# The points are competing with the the basemap colors. So let's change the basemap to dark-gray.

# In[37]:


m6 = gis.map('Pasadena, California', 13)
m6


# Now, the accidents stand out because the points are not competing with the basemap colors. There is also a clear distinction between bicycle and pedestrian accidents.

# In[38]:


m6.basemap = 'dark-gray'


# In[39]:


filtered_df.spatial.plot(map_widget=m6,
                         renderer_type='u-a',
                         unique_values=uv,
                         arcade_expression=arcade_expression)


# ### Add school areas

# When it comes to policy decisions, hot spots and heat maps don't provide much context and make no explicit claims about the data. Because your subject is not only accidents but accidents near schools, we will add a layer of Pasadena Unified School District (PUSD) schools to our map. Then, we will find areas within a half-mile walking distance of each school.

# In[40]:


schools = gis.content.search('PUSD schools, owner:Learn_ArcGIS', 'Feature Layer',outside_org=True)[0]


# In[41]:


schools


# By default, the schools have red point symbols that can be difficult to distinguish from the accident points. We'll change the symbol so the schools stand out more. The square shape will be distinguishable from the circle shapes of the accidents. We'll also change the color to white so the symbols stand out against the dark basemap.

# In[42]:


m7 = gis.map('Pasadena, California', 13)
m7


# The layer of schools is added to the map. The schools now stand out and can be distinguished from the accidents.

# In[43]:


m7_renderer = {"renderer": "autocast",
                "type": "simple"}
def get_symbol(color):
    return create_symbol(geometry_type='point',
                         symbol_type='simple',
                         symbol_style='s',
                         colors=color,
                         marker_size=13,
                         outline_style='s',
                         outline_color=[153,153,153,255],
                         line_width=0.375)

symbol = get_symbol([153,153,153,255])
m7_renderer['symbol'] = symbol


# In[44]:


m7.basemap = 'dark-gray'


# In[45]:


filtered_df.spatial.plot(map_widget=m7,
                         renderer_type='u-a',
                         unique_values=uv,
                         arcade_expression=arcade_expression)


# In[46]:


m7.add_layer({ "type": "FeatureLayer",
               "url": schools.layers[0].url,
               "transparency": 75,
               "renderer": m7_renderer
                })


# In[47]:


m7.legend = True


# ### Creating areas within a half-mile walking distance of schools 

# Next, we'll create the walk-time areas around each school. This tool uses road network data to create areas that can be reached within a specific driving or walking distance or time. Creating areas within a half-mile walking distance of schools will show places where there are likely large numbers of student pedestrians.
# 
# We'll later be able to calculate the number of collisions near each school.

# In[48]:


walk_dist_from_schools = create_drive_time_areas(schools.layers[0],
                                                 break_values=[0.5],
                                                 break_units='Miles',
                                                 travel_mode='Walking Distance',
                                                 output_name='psud_schools_drivetime' + str(dt.now().microsecond))


# In[49]:


walk_dist_from_schools


# In[50]:


m8 = gis.map('Pasadena, California', 13)
m8


# Some school areas have no pedestrian or cyclist accidents, while others have a significant amount. At the same time, the area with the highest density of accidents is not within any school area. Adding more information beyond a heat map has provided essential context for policymakers.

# In[51]:


m8.basemap = 'dark-gray'


# In[52]:


filtered_df.spatial.plot(map_widget=m8,
                         renderer_type='u-a',
                         unique_values=uv,
                         arcade_expression=arcade_expression)


# In[53]:


m8.add_layer({ "type": "FeatureLayer",
               "url": schools.layers[0].url,
               "transparency": 75,
               "renderer": m7_renderer
                })


# In[54]:


m8.add_layer(walk_dist_from_schools)


# In[55]:


m8.legend = True


# We could stop your analysis here, and use this map as grounds to implement a policy that encompasses all school zones. Changing street signs and adding bicycle lanes in these areas may reduce accidents near schools.
# 
# However, sometimes a city does not have enough funds to enact new policy for every location. Many Pasadena school zones have few accidents, so policies in these areas may have little effect. Instead, policymakers want to focus their efforts on areas that need it most.
# 
# We'll calculate the number of accidents within each school zone and filter the layer to show only the five most dangerous zones. Then, policymakers can prioritize these zones over zones that are already relatively safe.

# ### Find the most dangerous school areas

# This tool summarizes the number of point features within a polygon feature.

# In[56]:


dangerous_areas = summarize_within(walk_dist_from_schools.layers[0],
                                   collisions,
                                   output_name="accident_count_within_school_zone" + str(dt.now().microsecond))


# In[57]:


dangerous_areas


# In[58]:


top_5_dangerous_areas = dangerous_areas.layers[0].query().sdf.sort_values(by='Point_Count', ascending=False)


# In[59]:


top_5_dangerous_areas[:5]


# The table is sorted so that school zones with more accidents are shown first. The first five school zones have accidents totaling 109, 106, 104, 63, and 62, respectively.
# 
# The sixth-highest school zone has 59 accidents, which is almost the same as the fifth highest. With this information, you might consider expanding policy to include this school zone as well. Alternatively, depending on the city's resources, we want to limit policy to focus only on the three most dangerous school zones, which have a much higher number of accidents than the fourth.
# 
# For this scenario, we'll continue to focus on the five most dangerous school zones. We'll filter the layer to show only these zones.

# In[60]:


m9 = gis.map('Pasadena, California', 13)
m9


# All five of the most dangerous school zones are relatively close to one another. Two of the most dangerous zones almost overlap entirely, which means the city can increase safety for both schools with many of the same policy decisions.

# In[61]:


m9.basemap = 'dark-gray'


# In[62]:


filtered_df.spatial.plot(map_widget=m9,
                         renderer_type='u-a',
                         unique_values=uv,
                         arcade_expression=arcade_expression)


# In[63]:


m9.add_layer({"type": "FeatureLayer", 
               "url": dangerous_areas.layers[0].url,
               "definition_expression" : "Point_Count>=61",
               "opacity": 0.7
              })


# In[64]:


m9.add_layer({ "type": "FeatureLayer",
                 "url": schools.layers[0].url,
                 "transparency": 75,
                 "renderer": m7_renderer
                })


# ### Conclusion

# We've created a map that highlights five school zones that would benefit from policy intervention. 
# The city officials can now identify the most dangerous school zones in order to reduce the likelihood of future accidents.


# ====================
# sar_to_rgb_image_translation_using_cyclegan.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # SAR to RGB image translation using CycleGAN

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Necessary imports](#Necessary-imports)
# * [Connect to your GIS](#Connect-to-your-GIS)
# * [Export training data](#Export-training-data)
# * [Train the model](#Train-the-model)
#  * [Prepare data](#Prepare-data)
#  * [Visualize training data](#Visualize-training-data)
#  * [Load model architecture](#Load-model-architecture)
#  * [Find an optimal learning rate](#Find-an-optimal-learning-rate)
#  * [Fit the model](#Fit-the-model)
#  * [Visualize results in validation set](#Visualize-results-in-validation-set)
#  * [Save the model](#Save-the-model)
# * [Model inference](#Model-inference)
# * [Results](#Results)
# * [Conclusion](#Conclusion)
# * [References](#References)

# ## Introduction

# The ability of SAR data to let us see through clouds make it more valuable specially in cloudy areas and bad weather. This is the time when earth observation can reap maximum benefits, but optical sensors prevent us doing that. Now a days a lot of organizations are investing in SAR data making it more available to users than before. The only disadvantage of SAR data is the unavailability of labelled data as it is more difficult for users to understand and label SAR data than optical imagery.
# 
# In this sample notebook, we will see how we can make use of benefits of SAR and optical imagery to perform all season earth observation. We will train a deep learning model to translate SAR imagery to RGB imagery, thereby making optical data (translated) available even in extreme weather days and cloudy areas. 
# 
# We will train a [CycleGAN](https://developers.arcgis.com/python/guide/how-cyclegan-works/) model for this case. It is important to note that the CycleGAN model expects unpaired data and it does not have any information on mapping SAR to RGB pixels, so it may map dark pixels in the source image to darker shaded pixels in the other image which may not be right always (especially in agricultural land areas). If this kind of problem is faced where results are mismatched because of wrong mapping, [Pix2Pix](https://developers.arcgis.com/python/guide/how-pix2pix-works/) model which expects paired data can be used.
# 

# ## Necessary imports

# In[1]:


import os, zipfile
from pathlib import Path

from arcgis.gis import GIS
from arcgis.learn import prepare_data, CycleGAN


# ## Connect to your GIS

# In[2]:


# Connect to GIS
gis = GIS('home') 


# ## Export training data 

# For this usecase, we have SAR imagery from Capella Space and world imagery in the form of RGB tiles near Rotterdam city in the Netherlands. We have exported that data in a new “CycleGAN” metadata format available in the [`Export Training Data For Deep Learning`](https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/export-training-data-for-deep-learning.htm) tool. This `Export Training Data For Deep Learning` tool available in [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview) as well as [ArcGIS Image Server](https://www.esri.com/en-us/arcgis/products/arcgis-image-server).
# 
# - `Input Raster`: SAR imagery tile
# - `Additional Raster`: RGB imagery
# - `Tile Size X & Tile Size Y`: 256
# - `Stride X & Stride Y`: 128
# - `Meta Data Format`: CycleGAN
# - `Environments`: Set optimum `Cell Size`, `Processing Extent`.

# 

# In the exported training data, 'A' and 'B' folders contain all the image tiles exported from SAR imagery and RGB imagery (world imagery cache), respectively. Each folder will also have other files like 'esri_accumulated_stats.json', 'esri_model_definition.emd', 'map.txt', 'stats.txt'. Now, we are ready to train the `CycleGAN` model.

# Alternatively, we have provided a subset of training data containing a few samples that follows the same directory structure mentioned above. You can use the data directly to run the experiments.

# In[3]:


training_data = gis.content.get('25ed4a30219e4ba7acb3633e1a75bae1')
training_data


# In[4]:


filepath = training_data.download(file_name=training_data.name)


# In[5]:


with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[6]:


output_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# ## Train the model

# We will train CycleGAN model [[1]](#References) that performs the task of Image-to-Image translation where it learns mapping between input and output images using unpaired dataset. This model is an extension of GAN architecture which involves simultaneous training of two generator models and two discriminator models. In GAN, we can generate images of domain Y from domain X, but in CycleGAN, we can also generate images of domain X from domain Y using the same model architecture.
# <br>
# <center>Figure 4. CycleGAN architecture</center>
# 
# It has two mapping functions: G : X → Y and F : Y → X, and associated adversarial discriminators Dy and Dx. G tries to generate images that look similar to images from domain Y, while Dy aims to distinguish between translated samples G(x) and real samples y. G aims to minimize this objective against an adversary D that tries to maximize it. The same process happens in generation of the images of domain X from domain Y using F as a generator and Dx as a discriminator.

# ### Prepare data

# We will specify the path to our training data and a few hyperparameters.
# 
# - `path`: path of the folder containing training data.
# - `batch_size`: Number of images your model will train on each step inside an epoch, it directly depends on the memory of your graphic card. 4 worked for us on a 11GB GPU.

# In[7]:


data = prepare_data(output_path, batch_size=8)


# ### Visualize training data

# To get a sense of what the training data looks like, `arcgis.learn.show_batch()` method randomly picks a few training chips and visualizes them.
# - `rows`: Number of rows to visualize

# In[8]:


data.show_batch()


# ### Load model architecture

# In[9]:


model = CycleGAN(data)


# ### Find an optimal learning rate

# [Learning rate](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) is one of the most important hyperparameters in model training. ArcGIS API for Python provides a learning rate finder that automatically chooses the optimal learning rate for you.

# In[10]:


lr = model.lr_find()


# ### Fit the model 

# We will train the model for a few epochs with the learning rate we have found. For the sake of time, we can start with 25 epochs. Unlike some other models, we train CycleGAN from scratch with a learning rate of 2e-04 for some initial epochs and then linearly decay the rate to zero over the next epochs.

# In[11]:


model.fit(25, lr)


# Here, with 25 epochs, we can see reasonable results — both training and validation losses have gone down considerably, indicating that the model is learning to translate SAR imagery to RGB and vice versa.

# ### Visualize results in validation set

# It is a good practice to see results of the model viz-a-viz ground truth. The code below picks random samples and shows us ground truth and model predictions, side by side. This enables us to preview the results of the model within the notebook.

# In[13]:


model.show_results(4)


# ### Save the model

# We will save the model which we trained as a 'Deep Learning Package' ('.dlpk' format). Deep Learning package is the standard format used to deploy deep learning models on the ArcGIS platform.
# 
# We will use the `save()` method to save the trained model. By default, it will be saved to the 'models' sub-folder within our training data folder.

# In[13]:


model.save("SAR_to_RGB_25e", publish=True)    


# ## Model inference

# We can translate SAR imagery to RGB and vice versa with the help of `predict()` method.
# 
# Using predict function, we can apply the trained model on the image which we want to translate.
# - `img_path`: path to the image file.
# - `convert_to`: 'a' or 'b' type of fake image we want to generate.

# In[23]:


#un-comment the cell to run predict over your desired image.
# model.predict(r"D:\CycleGAN\Data\exported_data_CycleGAN\A\images\000002800.tif", convert_to="b")


# In the above step, we are translating an image of `type a` i.e. SAR imagery to an image of `type b` i.e. RGB imagery. We can also perform `type b` to `type a` translation by changing the image file and `convert_to` parameter.

# In[22]:


#un-comment the cell to run predict over your desired image.
# model.predict(r"D:\CycleGAN\Data\exported_data_CycleGAN\B\images\000008007.tif", convert_to="a")


# Also, we can make use of `Classify Pixels Using Deep Learning` tool available in both ArcGIS Pro and ArcGIS Enterprise.
# 
# - `Input Raster`: The raster layer you want to classify.
# - `Model Definition`: It will be located inside the saved model in 'models' folder in '.emd' format.
# - `Padding`: The 'Input Raster' is tiled and the deep learning model classifies each individual tile separately before producing the final 'Output Classified Raster'. This may lead to unwanted artifacts along the edges of each tile as the model has little context to predict accurately. Padding as the name suggests allows us to supply some extra information along the tile edges, this helps the model to predict better.
# - `Cell Size`: Should be close to the size used to train the model. This was specified in the Export training data step.
# - `Processor Type`: This allows you to control whether the system's 'GPU' or 'CPU' will be used to classify pixels, by 'default GPU' will be used if available.

# 
# 

# ## Results

# The gif below was achieved with the model trained in this notebook and visualizes the generated RGB image over original RGB image near Rotterdam.
# 
# <p align="center"></p>

# ## Conclusion 

# In this notebook, we demonstrated how to use `CycleGAN` model using `ArcGIS API for Python` in order to translate imagery of one type to the other.

# ## References 

# [1] Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks; https://arxiv.org/abs/1703.10593.


# ====================
# shipwrecks_detection_using_bathymetric_data.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Shipwrecks detection using bathymetric data
# * 🔬 Data Science
# * 🥠 Deep Learning and Instance Segmentation

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Necessary imports](#Necessary-imports)
# * [Connect to your GIS](#Connect-to-your-GIS)
# * [Get the data for analysis](#Get-the-data-for-analysis)
# * [Preprocess bathymetric data](#Preprocess-bathymetric-data)
# * [Export training data](#Export-training-data)
# * [Train the model](#Train-the-model)
#  * [Prepare data](#Prepare-data)
#  * [Visualize a few samples from your training data](#Visualize-a-few-samples-from-your-training-data)
#  * [Load model architecture](#Load-model-architecture)
#  * [Find an optimal learning rate](#Find-an-optimal-learning-rate)
#  * [Fit the model](#Fit-the-model)
#  * [Accuracy Assessment](#Accuracy-Assessment)
#  * [Visualize results in validation set](#Visualize-results-in-validation-set)
#  * [Save the model](#Save-the-model)
# * [Model inference](#Model-inference)
# * [Conclusion](#Conclusion)

# ## Introduction

# In this notebook, we will use [bathymetry data](https://www.ngdc.noaa.gov/mgg/bathymetry/relief.html) provided by NOAA to detect shipwrecks from the Shell Bank Basin area located near New York City in United States. A Bathymetric Attributed Grid (BAG) is a two-band imagery where one of the bands is elevation and the other is uncertainty (define uncertainty of elevation value). We have applied deep learning methods after pre-processing the data (which is explained in [Preprocess bathymetric data](#Preprocess-bathymetric-data)) for the detection. 
# 
# One important step in pre-processing is applying [shaded relief](https://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/shaded-relief-function.htm) function provided in ArcGIS which is also used by NOAA in one of their BAG visualizations [here](https://maps.ngdc.noaa.gov/viewers/bathymetry/). Shaded Relief is a 3D representation of the terrain which differentiate the shipwrecks distinctly from the background and reveals them.  This is created by merging the Elevation-coded images and `Hillshade` method where a 3-band imagery is returned which is easy to interpret as compared to the raw bathymetry image. Subsequently, the images are exported as "RCNN Masks" to train a `MaskRCNN` model provided by ArcGIS API for Python for detecting the shipwrecks.
# 
# The notebook presents the use of deep learning methods to automate the identification of submerged shipwrecks which could be useful for hydrographic offices, archaeologists, historians who otherwise would spend a lot of time doing it manually.

# ## Necessary imports

# In[1]:


import os
from pathlib import Path
from datetime import datetime as dt

from arcgis.gis import GIS
from arcgis.raster.functions import RFT  
from arcgis.learn import prepare_data, MaskRCNN


# ## Connect to your GIS

# In[2]:


gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# ## Get the data for analysis

# In[3]:


bathymetry_img = gis.content.get('442df8b8b26d41c598e4e8953e9257ca')
bathymetry_img


# In[4]:


training_data_wrecks = gis.content.get('d9c02b5835b540e09e378c779ec3c17d')
training_data_wrecks


# ## Preprocess bathymetric data

# We are applying some preprocessing to the bathymetry data so that we can export the data for training a deep learning model. The preprocessing steps include mapping 'No Data' pixels value to '-1' and applying [Shaded Relief](https://pro.arcgis.com/en/pro-app/help/data/imagery/shaded-relief-function.htm) function to the output raster. The resultant raster after applying Shaded Relief function will be a 3-band imagery that we can use to export data using `Export Training Data for Deep Learning` tool in ArcGIS Pro 2.5, for training our deep learning model.

# All the preprocessing steps are recorded in the form of a [Raster function template](https://pro.arcgis.com/en/pro-app/help/data/imagery/raster-function-template.htm) which you can use in ArcGIS Pro to generate the processed raster. 

# In[5]:


shaded_relief_rft = gis.content.get('b9b7c724601c4bd1946c8e2bfe4d640d')
shaded_relief_rft


# In[6]:


shaded_relief_ob = RFT(shaded_relief_rft)


# In[8]:


# ! conda install -c anaconda graphviz -y


# In[1]:


# shaded_relief_ob.draw_graph()


# We need to add this custom raster function to ArcGIS Pro using Import functions option in the 'Custom' tab of 'Raster Functions'

# 
# 

# Once we apply the Raster function template on the bathymetry data, we will get the output image below. We will use this image to export training data for our deep learning model.

# In[7]:


shaded_relief = gis.content.get('1205289b53a24eda903d98760f81d352')
shaded_relief


# ## Export training data

# 
# Export training data using 'Export Training data for deep learning' tool, [click here](https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/export-training-data-for-deep-learning.htm) for detailed documentation: 
# 
# - Set 'shaded_relief' as `Input Raster`.
# - Set a location where you want to export the training data in `Output Folder` parameter, it can be an existing folder or the tool will create that for you.
# - Set the 'training_data_wrecks' as input to the `Input Feature Class Or Classified Raster` parameter.
# - Set `Class Field Value` as 'ecode'.
# - Set `Image Format` as 'TIFF format'
# - `Tile Size X` & `Tile Size Y` can be set to 256.
# - `Stride X` & `Stride Y` can be set to 50. 
# - Select 'RCNN Masks' as the `Meta Data Format` because we are training a 'MaskRCNN Model'.
# - In 'Environments' tab set an optimum `Cell Size`. For this example, as we have performing the analysis on the bathymetry data with 50 cm resolution, so, we used '0.5' as the cell size.

# 
# 

# ```Python
# arcpy.ia.ExportTrainingDataForDeepLearning(in_raster="shaded_Relief_CopyRaster",
# out_folder=r"\256X256_multiple_cellsize_stride50",
# in_class_data="training_data_wrecks", 
# image_chip_format="TIFF",
# tile_size_x=256,
# tile_size_y=256, 
# stride_x=50,
# stride_y=50,
# output_nofeature_tiles="ONLY_TILES_WITH_FEATURES",
# metadata_format="RCNN_Masks",
# start_index=0,
# class_value_field="ecode",
# buffer_radius=0,
# in_mask_polygons=None,
# rotation_angle=0, 
# reference_system="MAP_SPACE",
# processing_mode="PROCESS_AS_MOSAICKED_IMAGE",
# blacken_around_feature="NO_BLACKEN",
# crop_mode="FIXED_SIZE")``` 

# ## Train the model

# As we have already exported our training data, we will now train our model using ArcGIS API for Python. We will be using `arcgis.learn` module which contains tools and deep learning capabilities. Documentation is available [here to install and setup environment](https://developers.arcgis.com/python/guide/install-and-set-up/).

# ### Prepare data

# We can always apply multiple transformations to our training data when training a model that can help generalize the model better. Though, we do some standard data augmentations, we can enhance them further based on the data at hand, to increase data size, and avoid occurring.
# 
# Let us have look, how we can do it using Fastai's image transformation [library](https://docs.fast.ai/data.transforms.html).

# In[8]:


from fastai.vision.transform import crop, rotate, brightness, contrast, rand_zoom


# In[9]:


train_tfms = [rotate(degrees=30,                              # defining a transform using rotate with degrees fixed to
                     p=0.5),                                  # a value, but by passing an argument p.
              
              crop(size=224,                                  # crop of the image to return image of size 224. The position 
                   p=1.,                                      # is given by (col_pct, row_pct), with col_pct and row_pct
                   row_pct=(0, 1),                            # being normalized between 0 and 1.
                   col_pct=(0, 1)),                           
              
              brightness(change=(0.4, 0.6)),                  # Applying change in brightness of image.
              
              contrast(scale=(1.0, 1.5)),                     # Applying scale to contrast of image.
              
              rand_zoom(scale=(1.,1.2))]                      # Randomized version of zoom.

val_tfms = [crop(size=224,                                    # cropping the image to same size for validation datasets
                 p=1.0,                                       # as in training datasets.
                 row_pct=0.5, 
                 col_pct=0.5)]

transforms = (train_tfms, val_tfms)                           # tuple containing transformations for data augmentation 
                                                              # of training and validation datasets respectively.


# We would specify the path to our training data and a few hyper parameters.
# 
# - `path`: path of folder containing training data.
# - `batch_size`: No of images your model will train on each step inside an epoch, it directly depends on the memory of your graphic card.
# - `transforms`: tuple containing Fast.ai transforms for data augmentation of training and validation datasets respectively.
# 
# This function will return a fast.ai databunch, we will use this in the next step to train a model.

# In[2]:


gis = GIS('home')


# In[3]:


training_data = gis.content.get('91178e9303af49b0b9ae09c0d32ec164')
training_data


# In[4]:


filepath = training_data.download(file_name=training_data.name)


# In[5]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[6]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# In[ ]:


data = prepare_data(path=data_path, batch_size=8, transforms=transforms)


# ### Visualize a few samples from your training data

# To make sense of training data we will use the `show_batch()` method in `arcgis.learn`. This method randomly picks few samples from the training data and visualizes them.
# 
# `rows`: number of rows we want to see the results for.

# In[8]:


data.show_batch(rows=5)


# ### Load model architecture

# `arcgis.learn` provides the `MaskRCNN` model for instance segmentation tasks, which is based on a pretrained convnet, like ResNet that acts as the 'backbone'. More details about `MaskRCNN` can be found [here](https://github.com/Esri/arcgis-python-api/blob/master/guide/14-deep-learning/How_MaskRCNN_works.ipynb).

# In[15]:


model = MaskRCNN(data)


# ### Find an optimal learning rate

# Learning rate is one of the most important hyperparameters in model training. Here, we explore a range of learning rate to guide us to choose the best one. We will use the `lr_find()` method to find an optimum learning rate at which we can train a robust model.

# In[11]:


lr = model.lr_find()


# ### Fit the model

# To train the model, we use the `fit()` method. To start, we will train our model for 80 epochs. Epoch defines how many times model is exposed to entire training set. We have passes three parameters to `fit()` method:
# - `epochs`: Number of cycles of training on the data.
# - `lr`: Learning rate to be used for training the model.
# - `wd`: Weight decay to be used.

# In[10]:


model.fit(epochs=80, lr=lr, wd=0.1)


# As you can see, both the losses (valid_loss and train_loss) started from a higher value and ended up to a lower value, that tells our model has learnt well. Let us do an accuracy assessment to validate our observation.

# ### Accuracy Assessment

# We can compute the average precision score for the model we just trained in order to do the accuracy assessment. Average precision computes average precision on the validation set for each class. We can compute the Average Precision Score by calling `model.average_precision_score`. It takes the following parameters:
# - `detect_thresh`: The probability above which a detection will be considered for computing average precision.
# - `iou_thresh`: The intersection over union threshold with the ground truth labels, above which a predicted bounding box will be considered a true positive.
# - `mean`: If False, returns class-wise average precision otherwise returns mean average precision.

# In[13]:


model.average_precision_score(detect_thresh=0.3, iou_thresh=0.3, mean=False)


# The model has an average precision score of 0.94 which proves that the model has learnt well. Let us now see it's results on validation set.

# ### Visualize results in validation set

# The code below will pick a few random samples and show us ground truth and respective model predictions side by side. This allows us to validate the results of your model in the notebook itself. Once satisfied we can save the model and use it further in our workflow. The `model.show_results()` method can be used to display the detected ship wrecks. Each detection is visualized as a mask by default.

# In[11]:


model.show_results(rows=5, thresh=0.5)


# ### Save the model

# We would now save the model which we just trained as a 'Deep Learning Package' or '.dlpk' format. Deep Learning package is the standard format used to deploy deep learning models on the ArcGIS platform.
# 
# We will use the `save()` method to save the model and by default it will be saved to a folder 'models' inside our training data folder itself.

# In[ ]:


model.save('Shipwrecks_80e')


# The saved model can be downloaded from [here](https://pythonapi.playground.esri.com/portal/home/item.html?id=9e4a60fef8cd45c2a8aac34f145096e1) for inferencing purposes.

# ## Model inference

# The saved model can be used to detect shipwrecks masks using the `Detect Objects Using Deep Learning` tool available in [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview), or [ArcGIS Image Server](https://www.esri.com/en-us/arcgis/products/arcgis-image-server). For this sample, we will use the bathymetry data processed using the shaded relief raster function template to detect shipwrecks.

# 
# 

# 
# ```Python
# arcpy.ia.DetectObjectsUsingDeepLearning(in_raster="shaded_Relief_CopyRaster",
# out_detected_objects=r"\\ShipwrecksDetectObjects_80e",
# in_model_definition=r"\\models\Shipwrecks_80e\Shipwrecks_80e.emd",
# model_arguments ="padding 56;batch_size 4;threshold 0.3;return_bboxes False",
# run_nms="NMS",
# confidence_score_field="Confidence",
# class_value_field="Class",
# max_overlap_ratio=0,
# processing_mode="PROCESS_AS_MOSAICKED_IMAGE")
# ```

# The output of the model is a layer of detected shipwrecks which is shown below:

# 
# 

# <center>A subset of detected shipwrecks</center>

# To view the above results in a webmap [click here](https://pythonapi.playground.esri.com/portal/home/webmap/viewer.html?webmap=d2c89b32e1b242e79a35a3cb813b2fc5&amp;extent=-73.9327,40.5922,-73.9264,40.5948).

# ## Conclusion

# This notebook showcased how instance segmentation models like `MaskRCNN` can be used to automatically detect shipwrecks using bathymetry data. This notebook also showcased how custom transformations, irrespective of already present standard transformations, based on the data, can be added while preparing data in order to achieve better performance.


# ====================
# snow_avalanche_hazard_mapping_for_lake_tahoe.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Snow Avalanche Hazard Mapping for Lake Tahoe
# 

# ## Table of Contents
# 
# * [Introduction](#1)
# * [Necessary imports](#2)
# * [Connect to your GIS](#3)
# * [Get the data for analysis](#5)
# * [Clip and generate rasters from DEM](#11)
#     * [Create elevation raster](#12)
#     * [Create slope raster](#13)
#     * [Create aspect raster](#14)
#     * [Create curvature raster](#15)
# * [Land Use and Land Cover raster](#16)
# * [Snow avalanche vulnerability map](#18)
# * [Conclusion](#19)
# * [Data and literature resources](#20)

# ## Introduction <a class="anchor" id="1"></a>

# According to the [National Avalanche Center](https://avalanche.org/national-avalanche-center/), every year 25 to 30 people die due to snow avalanches in USA, interestingly termed as "White Death". Record shows that snow avalanches have killed more people in forests than any other natural disaster, while they were participating in recreational activities.
# 
# Recently on Jan 17, 2020, a snow avalanche caused several people missing at the Alpine Meadows ski area and near lake Tahoe. Other than human casualty, it also creates blockage on roads and railway tracks and can even destroy power supply lines. Avalanches could be triggered by various reasons such as heavy snowfall, deforestation, vibrations, change in wind direction & speed, etc.
# 
# In view of this destructive power of avalanches a study is undertaken near the Lake Tahoe area, California to identify vulnerable locations that could be hit by snow avalanches. Weighted Linear Combination (WLC) method based on combined GIS and Remote Sensing techniques is used in the sample to create a potential hazard map for avalanches. It consists of three steps:
# 
#    -  Reclassify all the datasets in the same range
# 
#    - Assign scores to the categories of each dataset
# 
#    - Weight each dataset based on relative importance and add them together

# ## Necessary imports <a class="anchor" id="2"></a>

# In[1]:


from arcgis.gis import GIS
import arcgis.raster.functions
from arcgis.raster.functions import aspect, clip, colormap, con, curvature, remap, slope       


# ## Connect to your GIS <a class="anchor" id="3"></a>

# In[2]:


gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# ##  Get the data for analysis <a class="anchor" id="5"></a>

# Avalanche is a dynamic hazard or in other words has a fast onset and offset period, with favorable conditions impacted by land use-landcover, elevation, slope, curvature, variations in weather. In this study, USA NLCD Land Cover data is used for land cover, and ASTER DEM is used for elevation, slope and curvature to create a potential snow avalanche hazard map for the Lake Tahoe area.

# In[3]:


# Aster DEM
dem = gis.content.search("DEM Lake Tahoe", 'imagery layer')[0]
dem


# In[4]:


lulc = gis.content.search("LULC Lake Tahoe", 'imagery layer')[0]
lulc


# In[5]:


aoi = gis.content.search("study area tahoe", 'feature layer')[0]
aoi


# Once the datasets are ready Weighted Linear Combination (WLC) is carried out. Here all the datasets were reclassified and scores were given to them so that all are in the same number scale. The  score ranges between 0 to 5, with high score implying extremely vulnerable area and low score indicating less vulnerable or safe areas.

# In[6]:


# Weighted Linear Combination (WLC) 
aoi_layer = aoi.layers[0]
aoi_feature = aoi_layer.query(where='OBJECTID=1')
aoi_feature.features[0].extent = dem.layers[0].extent
aoi_geom = aoi_feature.features[0].geometry
aoi_geom['spatialReference'] = {'wkid':3857}


# For further analysis, all the datasets were clipped with the AOI boundary.

# ## Clip and generate rasters from DEM <a class="anchor" id="11"></a>

# ASTER DEM imagery layer was used in the study, having a spatial resolution of 30m. The DEM was clipped with the AOI boundary, followed by creating classified rasters of DEM elevation, slope, curvature and aspect rasters from it.

# In[7]:


dem_clip = clip(dem.layers[0], aoi_geom)


# In[8]:


m = gis.map('Lake Tahoe, California')
m.add_layer(dem_clip)
m.legend = True
m


# 

# ### Create elevation raster <a class="anchor" id="12"></a>

# Elevation does not directly affect avalanche occurrences, but it impacts meteorological factors like temperature, wind speed, amount and type of precipitation which in turn affect the snowpack stability.
# 
# Generally, with an increase in elevation, the air temperature decreases while the wind speed increases. [Literature](http://www.indjst.org/index.php/indjst/article/view/105647) shows that most of the avalanches occur between 2700m to 6000m.
# 
# The study area DEM has elevation between 628m to 3292m. The elevation raster has been created by reclassifying the DEM. DEM was reclassified into 5 classes and scores were given to each class between 1 to 5 where 1 means not vulnerable and 5 means highly vulnerable to snow avalanche. The highest score of 5 was given to the elevation class 2345 to 3309 followed by 2002 to 2345 class with 4. A score of 1 was given to the elevation value between 612 to 1713 as the literature shows elevation below 2000m is less vulnerable for an avalanche.

# In[9]:


elevation = colormap(remap(dem_clip,
                           input_ranges=[612, 1433,
                                         1433, 1713,
                                         1713, 2002,
                                         2002, 2345,
                                         2345, 3309],
                           output_values=[1, 2, 2, 4, 5],
                           astype='u8'),
                     colormap=[[1, 255, 255, 204], [2, 0, 100, 0],
                               [4, 255, 127, 80], [5, 255, 0, 0]])


# In[10]:


m = gis.map('Lake Tahoe, California')
m.add_layer(elevation)
m.legend = True
m


# 

# ### Create slope raster<a class="anchor" id="13"></a>

# Slope is one of the important factors for avalanche occurrences. According to the past studies, most of the avalanches occurred on a slope between 28 to 45 degrees. Thus, while slope less than 25 degrees is not prone to avalanches at the same time too steep slopes also do not cause avalanches since it does not allow snow to accumulate. In view of this the slope map was created and reclassified into 5 classes, with appropriate label.

# In[11]:


slope = colormap(remap(slope(dem_clip),
                       input_ranges=[0,  12,  # Flat
                                     12, 28,  # Low
                                     28, 45,  # Highly vulnerable
                                     45, 55,  # Steep
                                     55, 90], # Very Steep
                       output_values=[1, 2, 5, 3, 2],
                       astype='u8'),
                 colormap=[[1, 141, 212, 0], [2, 255, 225, 204],
                           [5, 255, 0, 0]])


# In[12]:


m = gis.map('Lake Tahoe, California')
m.add_layer(slope)
m.legend = True
m


# 

# ### Create aspect raster<a class="anchor" id="14"></a>

# The snowpack stability is directly affected by the orientation of slope with respect to the sun. Snowpack on slopes facing the sun stabilizes faster in comparison to the snow on shaded slopes which remains unstable. Accordingly, the Aspect raster was reclassified into 5 classes and different scores were given to each class based on their orientation to the Sun.

# In[13]:


aspect = colormap(remap(aspect(dem_clip),
                     input_ranges=[-1, 0, #Flat
                                   0, 22.5,  # North
                                   22.5, 67.5,  # Northeast
                                   67.5, 112.5,  # East 
                                   112.5, 157.5, # Southeast 
                                   157.5, 202.5, # South
                                   202.5, 247.5, # Southwest 
                                   247.5, 292.5, # West 
                                   292.5, 337.5, # Northwest
                                   337.5, 360], # North
                        output_values=[1, 5, 5, 1, 2, 1, 1, 1, 4, 5],
                        astype='u8'),
                  colormap=[[1, 255, 255, 204],[2, 255, 255, 0],
                            [4, 255, 178, 102],[5, 255, 0, 0]])


# In[14]:


m = gis.map('Lake Tahoe, California')
m.add_layer(aspect)
m.legend = True
m


# 

# ### Create curvature raster<a class="anchor" id="15"></a>

# Curvature is also an important parameter for avalanche hazard prediction mapping. Snowpack over convex surfaces are more unstable than that on top of concave surfaces which usually can hold large amount of snow compared to convex surface.
# 
# Suitably, curvature raster was reclassified into 3 classes and convex slope was assigned the highest score of 5 as it was more vulnerable to avalanche, and 1 was assigned to concave surface as it was less vulnerable.

# In[15]:


curvature = curvature(dem_clip, curvature_type='standard', z_factor=1, astype='F32')


# In[16]:


curvature_s=colormap(remap(curvature,
                           input_ranges=[-60, 0, #concave slope
                                         0, 0, #flat slope 
                                         0.000001, 60], #convex slope
                             output_values=[1, 3, 5],
                             astype='F32'),
                       colormap=[[1, 255, 255, 204],
                                 [3, 255, 178, 102],
                                 [5, 255, 0, 0]])


# In[17]:


m = gis.map('Lake Tahoe, California')
m.add_layer(curvature_s)
m.legend = True
m


# 

# ## Land Use and Land Cover raster<a class="anchor" id="16"></a>

# In the study titled [USA NLCD Land Cover](https://www.arcgis.com/home/item.html?id=3ccf118ed80748909eb85c6d262b426f) ,[National Land Cover Database](https://www.mrlc.gov) was used from Living Atlas. Land Use and Land Cover(LULC) which represents the condition of the earth's surface is an important contributing factor for avalanches. For example areas with dense vegetation was less vulnerable than the areas with no vegetation and also snow and ice covered area were more prone to snow avalanches than barren land. 
# 
# This datatset has 20 land cover classes which includes vegetation type, development density, agricultural use, barren land, areas with water, snow and ice. The data was classified on the basis of modified Anderson Level II, and after clipping the LULC data with the AOI boundary, the data was resampled in 30m spatial resolution similar to the DEM data so that the overlay analysis can be done.  

# In[18]:


lulc_clip = clip(lulc.layers[0], aoi_geom)


# The study area has sixteen LULC classes. The raster was reclassified and scores were assigned to the LULC classes, with Snow & Ice class being highly vulnerable was scored as 5. Evergreen and mixed forests were less vulnerable to snow avalanche, so a low score was provided, while barren land, grassland and shrublands being moderately vulnerable, a score of 3 was assigned to them.

# In[19]:


lulc_c = colormap(remap(lulc_clip,
                        input_ranges=[1, 1, #Open Water
                                      2, 2, #Perennial Snow/Ice
                                      3, 3,  #Developed Open Space
                                      4, 4, #Developed Low Intensity
                                      5, 5, # Developed Medium Intensity
                                      6, 6, #Developed High Intensity
                                      7, 7, #Barrenland
                                      8, 8, #Deciduous Forest
                                      9, 9, #Evergreen Forest
                                      10, 10, #Mixed Forest
                                      11, 11, #Shrub/Scrub
                                      12, 12, #Grassland/ Herbaceous
                                      13, 13, #Pasture/ Hay
                                      14, 14, #Cultivated Crops
                                      15, 15, #Woody Wetlands
                                      16, 16], #Emergent Herbaceous Wetlands
                        output_values=[1, 5, 1, 1, 1, 1, 3, 2, 1, 2, 3, 3, 3, 2, 2, 2],
                        astype='u8'),
                  colormap=[[1, 0,100, 0], [2, 154, 205, 50],
                            [3, 255, 255, 0], [4, 255, 127, 80],
                            [5, 255, 0, 0]])


# In[20]:


m = gis.map('Lake Tahoe, California')
m.add_layer(lulc_c)
m.legend = True
m


# 

# ## Snow avalanche vulnerability map <a class="anchor" id="18"></a>

# The final risk map is created by again weighting and combining (WLC) all the above six parameters based on relative importance, with the total weight summing up to 1 after being added so that the output can be consistent. 

# In[21]:


risk_map = (0.4*elevation+0.1*slope+0.3*aspect+0.15*curvature_s+0.05*lulc_c)


# In[22]:


avalanche_risk_map = colormap(risk_map,
                         colormap=[[1, 0,100, 0], [2, 154, 205, 50],
                                   [3, 255, 255, 0,], [4, 255, 127, 80], 
                                   [5, 255, 0, 0]],
                         astype='u8')


# In[23]:


m = gis.map('Lake Tahoe, California')
m.add_layer(avalanche_risk_map)
m.legend = True
m


# 

# The final output potential avalanche hazard map has values from 1 to 5 with vulnerability increasing from 1 to 5. To make the numerical categories more meaningful the classes are labelled with suitable text description (done in ArcGIS Pro using `Raster Symbology`).

# In[24]:


m = gis.map('Lake Tahoe, California')
m.add_layer(avalanche_risk_map)
m.legend = True
m


# 

# Red pixels in the map here has the steepest convex slopes and shows area which are extremely vulnerable to avalanches. The Tahoe lake is moderately vulnerable. As discussed above, the areas above 2000m elevation are more vulnerable, dark green and green colour is showing areas less vulnerable to snow avalanche.

# ## Conclusion <a class="anchor" id="19"></a>

# Lake Tahoe experiences snow avalanches almost every year which causes deaths. In this sample, raster functions were used to create snow avalanche hazard map. Rasters were created from DEM, slope, aspect and curvature along with LULC map. WLC model is widely acceptable for natural disasters mapping, and is also able to handle and integrate large volume of data hence the model was incorporated with GIS to create the hazard map. The same workflow can be used to create avalanche hazard map for different regions.

# ## Data and literature resources<a class="anchor" id="20"></a>

# | Data and literature | Source | Link |
# | -| - |-|
# | ASTER DEM| Earthdata  |https://search.earthdata.nasa.gov/|
# | Land Cover data  |USA NLCD Land Cover data  |http://www.arcgis.com/home/item.html?id=3ccf118ed80748909eb85c6d262b426f
# | Research Paper   | Snow Avalanche Susceptibility Mapping using Remote Sensing and GIS in Nubra-Shyok Basin, Himalaya, India  |http://www.indjst.org/index.php/indjst/article/view/105647
# | Research Paper   |Potential Hazard Map for Snow Disaster Prevention Using GIS-Based Weighted Linear Combination Analysis and Remote Sensing Techniques: A Case Study in Northern Xinjiang, China  |http://www.scirp.org/journal/ars http://dx.doi.org/10.4236/ars.2014.34018|
# | Research Paper   |Landslide vulnerability mapping (LVM) using weighted linear combination (WLC) model through remote sensing and GIS techniques  |https://link.springer.com/article/10.1007/s40808-016-0141-7|


# ====================
# solar-energy-prediction-using-weather-variables.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Prediction of energy generation from Solar Photovoltaic Power Plants using weather variables  

# ## Table of Contents <a class="anchor" id="0"></a>
# * [Introduction](#1) 
# * [Imports](#2)
# * [Connecting to ArcGIS](#3)
# * [Accessing & Visualizing the datasets](#4)    
#     * [Training Set](#6)
#     * [Validation Set](#7)        
# * [Model Building](#9)
#     * [1 — FullyConnectedNetwork](#10)
#         * [Data Preprocessing](#11)  
#         * [Model Initialization ](#12)
#         * [Learning Rate Search ](#13)
#         * [Model Training ](#14) 
#         * [Solar Energy Generation Forecast & Validation](#15)  
#         * [Result Visualization](#16)
#     * [2 — MLModel](#17)
#         * [Data Preprocessing](#18)  
#         * [Model Initialization](#19)
#         * [Model Training ](#20)  
#         * [Explaining Predictor Importance of Solar Energy Generation](#26)
#         * [Solar Energy Generation Forecast & Validation](#21)  
#         * [Result Visualization](#22)      
# * [Conclusion](#23)
# * [Summary of methods used](#24)
# * [Data resources](#25)

# ## Introduction <a class="anchor" id="1"></a>

# Recently, there has been a great emphasis on reducing the carbon footprint of our cities by moving away from fossil fuels to renewable energy sources. City governments across the world, in this case the City of Calgary, Canada, are leading this change by becoming energy independent through solar power plants, either implemented on rooftops or within city utility sites.
# 
# This notebook aims to further aid this move to renewable solar energy by predicting the annual solar energy likely to be generated by a solar power station through local weather information and site characteristics. The hypothesis proposed by this notebook is that various weather parameters, such as temperature, wind speed, vapor pressure, solar radiation, day length, precipitation, snowfall, along with the altitude of a solar power station site, will impact the daily generation of solar energy.
# 
# Accordingly, these variables are used to train a model on actual solar power generated by solar power stations located in Calgary. This trained model will then be used to predict solar generation from potential solar power plant sites. Beyond the weather and altitude variables, the total energy generated from a solar station will also depend on the capacity of that station. For instance, a 100kwp solar plant will generate more energy than a 50kwp plant, and the final output will therefore also take into consideration the capacity of each solar power plant.

# ## Imports <a class="anchor" id="2"></a>

# In[1]:


get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib.pyplot as plt

import pandas as pd
from pandas import read_csv
from datetime import datetime
from IPython.display import Image, HTML

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import Normalizer
from sklearn.preprocessing import MinMaxScaler  
from sklearn.compose import make_column_transformer 
from sklearn.metrics import r2_score

import arcgis
from arcgis.gis import GIS
from arcgis.learn import FullyConnectedNetwork, MLModel, prepare_tabulardata


# ## Connecting to ArcGIS <a class="anchor" id="3"></a>

# In[2]:


gis = GIS('home')


# ## Accessing & Visualizing datasets  <a class="anchor" id="4"></a>

# The primary data used for this sample are as follows:

# Out of the several solar photovoltaic power plants in the City of Calgary, 11 were selected for the study. The dataset contains two components:
# 
# 1) Daily solar energy production for each power plant from September 2015 to December 2019.
# 
# 2) Corresponding daily weather measurements for the given sites. 
# 
# The datasets were obtained from multiple sources, as mentioned here ([Data resources](#25)), and preprocessed to obtain the main dataset used in this sample. Two feature layers were subsequently created from this dataset, one dataset for training and the other for validating.

# ### Training Set <a class="anchor" id="6"></a>
# 
# The training dataset consists of data from 10 solar sites for training the model. The feature layer containing the data is accessed here from Arcgis portal and visualized as follows:

# In[3]:


# Access Solar Dataset feature layer for Training, without the Southland Solar Plant which is hold out for validation
calgary_no_southland_solar = gis.content.get('adaead8cb3174ac6a89f0c14ae70aadd')
calgary_no_southland_solar


# In[4]:


# Access the layer from the feature layer
calgary_no_southland_solar_layer = calgary_no_southland_solar.layers[0]


# In[5]:


# Plot location of the 10 Solar sites in Calgary to be used for training
m1 = gis.map('calgary', zoomlevel=10)
m1.add_layer(calgary_no_southland_solar_layer)
m1


# The map above shows the 10 power plant locations that are used for collecting the training data.

# In[6]:


# Visualize the dataframe
calgary_no_southland_solar_layer_sdf = calgary_no_southland_solar_layer.query().sdf
calgary_no_southland_solar_layer_sdf=calgary_no_southland_solar_layer_sdf[['FID','date','ID','solar_plan','altitude_m',
                                                                           'latitude','longitude','wind_speed','dayl__s_',
                                                                           'prcp__mm_d','srad__W_m_','swe__kg_m_', 'tmax__deg',
                                                                           'tmin__deg','vp__Pa_','kWh_filled','capacity_f',
                                                                           'SHAPE']]
calgary_no_southland_solar_layer_sdf.head()


# In the table above, each row represents each individual day from September 2015 to December 2019, with the corresponding date shown in the field named **date** and the name of the solar site in the field named **solar_plan**.

# The primary information consists of the daily generation of energy in kilowatt-hour(KWh) given here in the field name **kWh_filled** for each of the selected 10 solar photovoltaic power plants in the City of Calgary. The field **capacity_f** indicates the capacity factor, which is obtained after normalizing the **kWh_filled** by the peak capacity of each solar photovoltaic sites, and will be used here as the dependent variable.   
# 
# In addition, it also contains data about weather variables for each day for the related solar plant, all of which, except wind speed, were obtained from [MODIS, Daymet observations](https://daac.ornl.gov/DAYMET/guides/Daymet_V3_CFMosaics.html). These variables are as follows:
# 
#  - <span style='background :lightgrey' >wind_speed</span> : wind speed(m/sec)
#  - <span style='background :lightgrey' >dayl__s_</span>   : Day length (sec/day)
#  - <span style='background :lightgrey' >prcp__mm_d</span> : Precipitation (mm/day)
#  - <span style='background :lightgrey' >srad__W_m_</span> : Shortwave radiation (W/m^2)
#  - <span style='background :lightgrey' >swe__kg_m_</span> : Snow water equivalent (kg/m^2)	
#  - <span style='background :lightgrey' >tmax__deg</span>  : Maximum air temperature (degrees C)
#  - <span style='background :lightgrey' >tmin__deg</span>  : Minimum air temperature (degrees C)
#  - <span style='background :lightgrey' >vp__Pa_</span>    : Water vapor pressure (Pa)

# To understand the distribution of the variables over the last few years and their respective relationship with the dependent variable of daily energy produced for a station, data from one of the stations is plotted as follows:

# In[7]:


# plot and Visualize the variables from the training set for one solar station - Hillhurst Sunnyside Community Association 
hillhurst_solar = calgary_no_southland_solar_layer_sdf[calgary_no_southland_solar_layer_sdf['solar_plan']=='Hillhurst Sunnyside Community Association'].copy()
hillhurst_datetime = hillhurst_solar.set_index(hillhurst_solar['date'])
hillhurst_datetime = hillhurst_datetime.sort_index() 
for i in range(7,hillhurst_datetime.shape[1]-1):
        plt.figure(figsize=(20,3))
        plt.title(hillhurst_datetime.columns[i])
        plt.plot(hillhurst_datetime[hillhurst_datetime.columns[i]])
        plt.show()


# In the plots above, it can be seen that each of the variables has a high seasonality, and it seems that there is a relationship between the dependent variable **kWh_filled** and the explanatory variables. As such, a correlation plot should be created to check the correlation between the variables. 

# In[8]:


# checking the correlation matrix between the predictors and the dependent variable of capacity_factor
corr_test = calgary_no_southland_solar_layer_sdf.drop(['FID','date','ID','latitude','longitude','solar_plan','kWh_filled'], axis=1)
corr = corr_test.corr()
corr.style.background_gradient(cmap='Greens').set_precision(2)


# The resulting correlation plot shows that the variable of shortwave radiation per meter square (**srad__W_m_**) has the largest correlation with the dependent variable of total solar energy produced expressed in terms of capacity factor(**capacity_f**). This is followed by the variable of day length(**dayl__s_**), as longer days are likely to produce more solar energy. These two are closely followed by max(**tmax__deg**) and min(**tmin__deg**) daily temperatures, and lastly the remaining variables with weaker correlation values.    

# ### Validation Set<a class="anchor" id="7"></a>
# 
# The validation set consists of daily solar generation data from September 2015 to December 2019 for one solar site, known as Southland Leisure Centre, and will be used to validate the trained model.

# In[9]:


# Access the Southland Solar Plant Dataset feature layer for validation
southland_solar = gis.content.get('af78423949b94c1783fa43d707df6d45')
southland_solar


# In[10]:


# Access the layer from the feature layer
southland_solar_layer = southland_solar.layers[0]


# In[11]:


#  Plot location of the Southalnd Solar site in Calgary to be used for validation
m1 = gis.map('calgary', zoomlevel=10)
m1.add_layer(southland_solar_layer)
m1


# In[12]:


# visualize the southland dataframe here
southland_solar_layer_sdf = southland_solar_layer.query().sdf
southland_solar_layer_sdf.head(2)


# In[13]:


# check the total number of samples
southland_solar_layer_sdf.shape


# ## Model Building <a class="anchor" id="9"></a>
# Once the training and the validation dataset have been processed and analyzed, they are ready to be used for modeling.
# 
# In this sample, two methods are used for modeling:
# 
# 1) `FullyConnectedNetwork` - First a deep learning framework called `FullyConnectedNetwork`, available in the `arcgis.learn` module in ArcGIS API for Python, is used.
# 
# 2) `MLModel` - In the second method, a regression model from scikit-learn is implemented via the `MLModel` framework in `arcgis.learn`. This framework can deploy any regression or classification model from the library by passing the name of the algorithm and its relevant parameters as keyword arguments.
# 
# 
# Finally, performance between the two methods will be compared in terms of model training and validation accuracy.
# 
# 
# 
# Further details on `FullyConnectedNetwork` & `MLModel` are available [here](https://developers.arcgis.com/python/guide/) in the Deep Learning with ArcGIS section.

# ### 1 — FullyConnectedNetwork <a class="anchor" id="10"></a>
# This is an Artificial Neural Network model from the `arcgis.learn` module, which is used here for modeling.

# ### Data Preprocessing <a class="anchor" id="11"></a>
# 
# First, a list is made that consists of the feature data that will be used for predicting daily solar energy generation. By default, it will receive continuous variables,  and in the case of categorical variables, the **True** value should be passed inside a tuple along with the variable. In this example, all of the variables are continuous.

# In[14]:


# Here a list is created naming all fields containing the predictors from the input feature layer
X = ['altitude_m', 'wind_speed', 'dayl__s_', 'prcp__mm_d','srad__W_m_','swe__kg_m_','tmax__deg','tmin__deg','vp__Pa_']


# In[15]:


# importing the libraries from arcgis.learn for data preprocessing
from arcgis.learn import prepare_tabulardata


# Once the explanatory variables are identified, the main preprocessing of the data is carried out by the `prepare_tabulardata` method from the `arcgis.learn` module in the ArcGIS API for Python. This function will take either a feature layer or a spatial dataframe containing the dataset as input and will return a TabularDataObject that can be fed into the model. 
# 
# The input parameters required for the tool are:
# 
# - <span style='background :lightgrey' >input_features</span> : feature layer or spatial dataframe containing the primary dataset
# - <span style='background :lightgrey' >variable_predict</span> : field name containing the y-variable from the input feature layer/dataframe
# - <span style='background :lightgrey' >explanatory_variables</span> : list of the field names as 2-sized tuples containing the explanatory variables as mentioned above

# In[16]:


# precrocessing data using prepare data method - it handles imputing missing values, normalization and train-test split
data = prepare_tabulardata(calgary_no_southland_solar_layer,
                           'capacity_f',
                           explanatory_variables=X)


# In[17]:


# visualizing the prepared data 
data.show_batch()


# ### Model Initialization <a class="anchor" id="12"></a>

# Once the data has been prepared by the `prepare_tabulardata` method, it is ready to be passed to the ANN for training. First, the ANN, known as `FullyConnectedNetwork`, is imported from `arcgis.learn` and initialized as follows:

# In[18]:


# importing the model from arcgis.learn
from arcgis.learn import FullyConnectedNetwork


# In[19]:


# Initialize the model with the data where the weights are randomly allocated
fcn = FullyConnectedNetwork(data)


# ### Learning Rate Search<a class="anchor" id="13"></a>

# In[20]:


# searching for an optimal learning rate using the lr_find for passing it to the final model fitting 
fcn.lr_find()


# Here the suggested learning rate by the `lr_find` method is around 0.0005754. The automatic lr_finder will take a conservative estimate of the learning rate, but some experts can interpret the graph more appropriately and find a better learning rate to be used for final training of the model.

# ### Model Training <a class="anchor" id="14"></a>
# 
# Finally, the model is now ready for training. To train the model, the `model.fit` is called and provided with the number of epochs for training and the estimated learning rate suggested by `lr_find` in the previous step:

# In[21]:


# the model is trained for 100 epochs 
fcn.fit(100,0.0005754399373371565)


# The train_loss and valid loss are plotted to check whether the model is over-fitting. The resulting plot shows that the model has been trained well and that the losses are gradually decreasing, but not significantly. 

# In[22]:


# the train vs valid losses is plotted to check quality of the trained model
fcn.plot_losses()


# Finally, the training results are printed to assess the prediction on the test set by the trained model.

# In[23]:


# the predicted values by the trained model is printed for the test set
fcn.show_results()


# In the table above, the values predicted by the model when applied to the test set, **prediction_results**, are similar to the actual values of the test set, **capacity_f**. 
# 
# As such, the model metrics of the trained model can be now estimated using the `model.score` function, which returns the r-squared metric of the fitted model.

# In[24]:


# the model.score method from the tabular learner returns r-square
r_Square_fcn_test = fcn.score() 
print('r_Square_fcn_test: ', round(r_Square_fcn_test,5))


# The high r-square value indicates that the model has been trained well 

# ### Solar Energy Generation Forecast & Validation <a class="anchor" id="15"></a>
# 
# The trained model(`FullyConnectedNetwork`) will now be used to predict the daily lifetime solar energy generation for the solar plant installed at the Southland Leisure Centre, since its installation in 2015. The aim is to validate the trained model and measure its performance of solar output estimation using only weather variables from the Southland Leisure Center. 
# 
# Accordingly, the `model.predict` method from `arcgis.learn` is used with the daily weather variables as input for the mentioned site, ranging from September 2015 to December 2019, to predict daily solar energy output in KWh for that same time period. The predictors are automatically chosen from the input feature layer of [southland_layer](#7) by the trained model without mentioning them explicitly, since their names are exactly same as those used to train the model.  

# In[25]:


# predicting using the predict function 
southland_solar_layer_predicted = fcn.predict(southland_solar_layer, output_layer_name='prediction_layer')


# In[26]:


# print the predicted layer
southland_solar_layer_predicted


# In[27]:


# Access & visualize the dataframe from the predicted layer 
test_pred_layer = southland_solar_layer_predicted.layers[0]
test_pred_layer_sdf = test_pred_layer.query().sdf
test_pred_layer_sdf.head()


# In[28]:


test_pred_layer_sdf.shape


# The table above returns the predicted values for the Southland photovoltaic power plant stored in the field called **prediction_results**, which holds the model estimated daily capacity factor of energy generation, whereas the actual capacity factor is in the field named **capacity_f**.
# 
# The capacity factor is a normalized value that will be rescaled back to the original unit of KWh by using the peak capacity of the Southland photovoltaic power plant of 153KWp.   

# In[29]:


# inverse scaling from capcacity factor to actual generation in KWh  - peak capcity of Southland Leisure Centre is 153KWp
test_pred_datetime = test_pred_layer_sdf[['field1','capacity_f','prediction']].copy()
test_pred_datetime = test_pred_datetime.rename(columns={'field1':'date'})
test_pred_datetime['date'] = pd.to_datetime(test_pred_datetime['date']) 
test_pred_datetime = test_pred_datetime.set_index(test_pred_datetime['date'])
test_pred_datetime['Actual_generation(KWh)'] = test_pred_datetime['capacity_f']*24*153
test_pred_datetime['predicted_generation(KWh)'] = test_pred_datetime['prediction']*24*153
test_pred_datetime = test_pred_datetime.drop(['date','capacity_f','prediction'], axis=1).sort_index() 
test_pred_datetime


# The table above shows the actual versus the model predicted daily solar energy generated for the Southland plant for the duration of late 2015 to the end of 2019. These values are now used to estimate the various model metrics to understand the prediction power of the model. 

# In[30]:


# estimate model metrics of r-square, rmse and mse for the actual and predicted values for daily energy generation
from sklearn.metrics import r2_score
r2_test = r2_score(test_pred_datetime['Actual_generation(KWh)'],test_pred_datetime['predicted_generation(KWh)'])
print('R-Square: ', round(r2_test, 2))


# The comparison returns a high r-square of 0.86, showing a high similarity between the actual and predicted values.

# In[31]:


# Comparison between the actual sum of the total energy generated to the total predicted values 
actual = (test_pred_datetime['Actual_generation(KWh)'].sum()/4/1000).round(2)  
predicted = (test_pred_datetime['predicted_generation(KWh)'].sum()/4/1000).round(2) 
print('Actual annual Solar Energy Generated by Southland Solar Station: {} MWh'.format(actual))
print('Predicted annual Solar Energy Generated by Southland Solar Stations: {} MWh'.format(predicted))


# Summarizing the values, the actual average annual energy generated by the solar plant is 170.03 MWh which is close to the predicted annual average generated energy of 170.34  MWh, indicating a high level high precision.

# ## Result Visualization<a class="anchor" id="16"></a>
# 
# Finally, the actual and predicted values are plotted to visualize their distribution across the entire lifetime of the power plant. 

# In[32]:


plt.figure(figsize=(30,6))
plt.plot(test_pred_datetime['Actual_generation(KWh)'],  linewidth=1, label= 'Actual') 
plt.plot(test_pred_datetime['predicted_generation(KWh)'], linewidth=1, label= 'Predicted')  
plt.ylabel('Solar Energy in KWh', fontsize=14)
plt.legend(fontsize=14,loc='upper right')
plt.title('Actual Vs Predicted Solar Energy Generated by Southland Solar-FulyConnectedNetwork Model', fontsize=14)
plt.grid()
plt.show() 


# In the plot above, the blue line represents the actual generation values, and the orange line represents the predicted generation values. The two show a high degree of overlap, indicating that the model has a high predictive capacity.

# ### MLModel <a class="anchor" id="17"></a>
# In the second method, a machine learning model is applied to model the same data using the `MLModel` framework from `arcgis.learn`. This framework can be used to import and apply any machine learning model from the scikit-learn library on the data returned by the `prepare_tabulardata` function from `arcgis.learn`.

# In[33]:


# importing the libraries from arcgis.learn for data preprocessing for the Machine Learning Model
from sklearn.preprocessing import MinMaxScaler


# ### Data Preprocessing<a class="anchor" id="18"></a>
# 
# Like the data preparation process for the neural network, first a list is made consisting of the feature data that will be used for predicting daily solar energy generation. By default, it will receive continuous variables, whereas for categorical variables, the **True** value should be passed inside a tuple along with the variables. These variables are then transformed by the RobustScaler function from scikit-learn by passing it, along with the variable list, into the column transformer function as follows: 

# In[34]:


# scaling the feature data using MinMaxScaler(), the default is Normalizer from scikit learn
X = ['altitude_m', 'wind_speed', 'dayl__s_', 'prcp__mm_d','srad__W_m_','swe__kg_m_','tmax__deg','tmin__deg','vp__Pa_']
preprocessors =  [('altitude_m', 'wind_speed', 'dayl__s_', 'prcp__mm_d','srad__W_m_','swe__kg_m_','tmax__deg',
                   'tmin__deg','vp__Pa_', MinMaxScaler())]


# Once the explanatory variables list is defined and the preprocessors are computed, they are now used as input for the `prepare_tabulardata` method in  `arcgis.learn`. The method takes a feature layer or a spatial dataframe containing the dataset and returns a TabularDataObject that can be fed into the model.
# 
# The input parameters required for the tool are similar to the ones mentioned previously:

# In[35]:


# importing the library from arcgis.learn for prepare data
from arcgis.learn import prepare_tabulardata


# In[36]:


# precrocessing data using prepare data method for MLModel
data = prepare_tabulardata(calgary_no_southland_solar_layer,
                           'capacity_f',
                           explanatory_variables=X,                           
                           preprocessors=preprocessors)


# In[37]:


# check the data that is being trained
data.show_batch()


# ### Model Initialization <a class="anchor" id="19"></a>
# 
# Once the data has been prepared by `prepare_tabulardata`method, it is ready to be passed to the selected machine learning model for training. Here, the GradientBoostingRegressor model from scikit-learn is used, which is passed into the `MLModel`function, along with its parameters, as follows:

# In[38]:


# importing the MLModel framework from arcgis.learn and the model from scikit learn 
from arcgis.learn import MLModel

# defining the model along with the parameters 
model = MLModel(data, 'sklearn.ensemble.GradientBoostingRegressor', n_estimators=100, random_state=43)


# ### Model Training <a class="anchor" id="20"></a>
# 
# Finally, the model is now ready for training, and the `model.fit` method is used for fitting the machine learning model with its defined parameters mentioned in the previous step.

# In[39]:


model.fit() 


# The training results are printed to compute some model metrics and assess the quality of the trained model.

# In[40]:


model.show_results()


# In the table above, the last column, **capacity_f_results**, returns the values predicted by the model, which are similar to the actual values in the target variable column, **capacity_f**. 
# 
# Subsequently, the model metrics of the trained model are now estimated using the `model.score()`  function, which currently returns the r-squared of the model fit as follows:

# In[41]:


# r-square is estimated using the inbuilt model.score() from the tabular learner
print('r_square_test_rf: ', round(model.score(), 5))


# The high R-squared value indicates that the model has been trained well.

# ### Explaining Predictor Importance of Solar Energy Generation <a class="anchor" id="26"></a>
# 
# Once the model has been fitted, it would be interesting to understand the explanability of the model, or the factors that are responsbile for predicting solar energy generation from the several varaible used in the model. The feature_importances method is used for the same as follows:

# In[42]:


import seaborn as sns


# In[43]:


feature_imp_RF = model.feature_importances_
rel_feature_imp = 100 * (feature_imp_RF / max(feature_imp_RF)) 
rel_feature_imp = pd.DataFrame({'features':list(X), 'rel_importance':rel_feature_imp })

rel_feature_imp = rel_feature_imp.sort_values('rel_importance', ascending=False)

plt.figure(figsize=[15,4])
plt.yticks(fontsize=10)
ax = sns.barplot(x="rel_importance", y="features", data=rel_feature_imp, palette="BrBG")

plt.xlabel("Relative Importance", fontsize=10)
plt.ylabel("Features", fontsize=10)
plt.show()


# The above graph shows the feature importances returned by the feature_importances method of the trained Gradient Boosting regressor algorithm implemented via the MLModel framework. Here the most important predictor for forecasting solar energy generation at the site is shown to be the amount of solar radiation received by the site in watts per square meter (srad__W_m_). This is followed by the length of day in seconds, maximum temperature and minimum temperature in the same order, which also matches the findings obtained from the correlation plot.     

# ### Solar Energy Generation Forecast & Validation<a class="anchor" id="21"></a>
# 
# The trained GradientBoostingRegressor model, implemented via the `MLModel`, will now be used to predict the daily lifetime solar energy generation for the solar plant installed at the Southland Leisure Centre, since its installation in 2015. The aim is to compare and validate its performance to the performance of the `FullyConnectedNetwork` model developed in earlier in this lesson.
# 
# To reiterate, the `model.predict` method from `arcgis.learn` is used with the daily weather variables as input for the mentioned site, ranging from September 2015 to December 2019, to predict daily solar energy output in KWh for the same time period. The predictors are automatically chosen from the input feature layer of [southland_layer](#7) by the trained model, without mentioning them explicitly, as their names are the same as those used for training the model.

# In[44]:


southland_solar_layer_predicted_rf = model.predict(southland_solar_layer, output_layer_name='prediction_layer_rf')


# In[45]:


# print the predicted layer
southland_solar_layer_predicted_rf


# In[46]:


# Access & visualize the dataframe from the predicted layer 
valid_pred_layer = southland_solar_layer_predicted_rf.layers[0]
valid_pred_layer_sdf = valid_pred_layer.query().sdf
valid_pred_layer_sdf.head()


# The table above returns the `MLModel` predicted values for the Southland plant stored in the field **prediction**, while the actual capacity factor is stored in the field named **capacity_f**.
# 
# The capacity factor is a normalized value that will be rescaled back to the original unit of KWh by using the peak capacity of the Southland photovoltaic power plant of 153KWp.

# In[47]:


# inverse scaling from capcacity factor to actual generation in KWh  - peak capcity of Southland Leisure Centre is 153KWp
valid_pred_datetime = valid_pred_layer_sdf[['field1','capacity_f','prediction']].copy()
valid_pred_datetime = valid_pred_datetime.rename(columns={'field1':'date'})
valid_pred_datetime['date'] = pd.to_datetime(valid_pred_datetime['date']) 
valid_pred_datetime = valid_pred_datetime.set_index(valid_pred_datetime['date'])
valid_pred_datetime['Actual_generation(KWh)'] = valid_pred_datetime['capacity_f']*24*153
valid_pred_datetime['predicted_generation(KWh)'] = valid_pred_datetime['prediction']*24*153
valid_pred_datetime = valid_pred_datetime.drop(['date','capacity_f','prediction'], axis=1)
valid_pred_datetime = valid_pred_datetime.sort_index() 
valid_pred_datetime.head()


# The table above shows the actual versus the `MLModel` predicted daily solar energy generated for the Southland plant for the duration of late 2015 to the end of 2019. These values are now used to estimate the various model metrics to understand the prediction power of the `MLModel`. 

# In[48]:


# estimate model metrics of r-square, rmse and mse for the actual and predicted values for daily energy generation
from sklearn.metrics import r2_score
r2_test = r2_score(valid_pred_datetime['Actual_generation(KWh)'],valid_pred_datetime['predicted_generation(KWh)'])
print('R-Square: ', round(r2_test, 2))


# The comparison returns a high R-squared of 0.84, indicating a high similarity between the actual and predicted values.

# In[49]:


# Comparison between the actual sum of the total energy generated to the total predicted values by the MLModel 
actual = (valid_pred_datetime['Actual_generation(KWh)'].sum()/4/1000).round(2)  
predicted = (valid_pred_datetime['predicted_generation(KWh)'].sum()/4/1000).round(2) 
print('Actual annual Solar Energy Generated by Southland Solar Station: {} MWh'.format(actual))
print('Predicted annual Solar Energy Generated by Southland Solar Stations: {} MWh'.format(predicted))


# Summarizing the values, the actual average annual energy generated by the solar plant is 170.03 MWh, which is close to the predicted annual average generated energy of 171.48 MWh. This indicates a high level of precision.

# ## Result Visualization<a class="anchor" id="22"></a>
# 
# Finally, the actual and predicted values are plotted to visualize their distribution across the entire lifetime of the power plant.

# In[50]:


plt.figure(figsize=(30,6))
plt.plot(valid_pred_datetime['Actual_generation(KWh)'],  linewidth=1, label= 'Actual') 
plt.plot(valid_pred_datetime['predicted_generation(KWh)'], linewidth=1, label= 'Predicted')  
plt.ylabel('Solar Energy in KWh', fontsize=14)
plt.legend(fontsize=14,loc='upper right')
plt.title('Actual Vs Predicted Solar Energy Generated by Southland Solar-FulyConnectedNetwork Model', fontsize=14)
plt.grid()
plt.show() 


# ## Conclusion<a class="anchor" id="23"></a>

# The goal of this project was to create a model that could predict the daily solar energy efficiency, thereby estimate the actual output of a photovoltaic solar energy plant at a location using the daily weather variables of the site as inputs. In the process it demonstrated the newly implemented artificial neural network, called `FullyConnectedNetwork`, and machine learning models, called `MLModel`, available in the `arcgis.learn` module in ArcGIS API for Python. 
# 
# Accordingly, data from 10 solar energy installation sites in the City of Calgary in Canada were used to train two different models — the first being the `FullyConnectedNetwork` model and second being the `MLModel` framework from the `arcgis.learn` module. These were eventually used to predict the daily solar output of a different solar plant in Calgary, which was withheld from the training set. The steps for implementing these models are elaborated in the notebook, and include the steps of data preprocessing, model training, and final inferencing.  
# 
# Comparison of the result shows that both models successfully predicted the solar energy output of the test solar plant with predicted values of 170.34 MWh and 171.48 MWh by the `FullyConnectedNetwork` and the `MLModel` algorithm respectively, compared to the actual value of average annual solar generation of 170.03 MWh for the station.
# 
# Finally, to expand on this model further in the furture, it would be interesting to apply this model to other solar generation plants located across different geographies and to record its performance to understand the generalizability of the model.

# ### Summary of methods used <a class="anchor" id="24"></a>

# | Method | Description | Examples |
# | -| - |-|
# | prepare_tabulardata| prepare data including imputation, normalization and train-test split  |prepare data ready for fitting a  MLModel or FullyConnectedNetwork model 
# | FullyConnectedNetwork()| set a fully connected neural network to a data  | initialize a FullyConnectedNetwork model with prepared data
# | model.lr_find()| find an optimal learning rate  | finalize a good learning rate for training the FullyConnectedNetwork model
# | MLModel() | select the ML algorithm to be used for fitting  | any supervised or unsupervised regression and classification model from scikit learn can be used
# | model.fit() | train a model with epochs & learning rate as input  | training the FullyConnectedNetwork model with sutiable input 
# | model.score() | find the model metric of R-squared of the trained model  | returns R-squared value after training the MLModel and FullyConnectedNetwork model 
# | model.predict() | predict on a test set | predict values using the trained models on test input 

# ### Data resources <a class="anchor" id="25"></a>

# | Dataset | Source | Link |
# | -| - |-|
# | Calgary solar energy| Calgary daily solar energy generation  |https://data.calgary.ca/Environment/Solar-Energy-Production/ytdn-2qsp|
# | Calgary Photovoltaic Sites| Location of Calgary Solar sites in Lat & Lon  |https://data.calgary.ca/dataset/City-of-Calgary-Solar-Photovoltaic-Sites/vrdj-ycb5|
# | Calgary Daily weather data| MODIS - Daily Surface Weather Data on a 1-km Grid for North America, Version 3  |https://daac.ornl.gov/DAYMET/guides/Daymet_V3_CFMosaics.html|  


# ====================
# spatial_and_temporal_trends_of_service_calls.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Spatial and temporal distribution of service calls using big data tools

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Introduction" data-toc-modified-id="Introduction-1">Introduction</a></span></li><li><span><a href="#Necessary-Imports" data-toc-modified-id="Necessary-Imports-2">Necessary Imports</a></span></li><li><span><a href="#Connect-to-your-ArcGIS-Enterprise-organization" data-toc-modified-id="Connect-to-your-ArcGIS-Enterprise-organization-3">Connect to your ArcGIS Enterprise organization</a></span></li><li><span><a href="#Ensure-your-GIS-supports-GeoAnalytics" data-toc-modified-id="Ensure-your-GIS-supports-GeoAnalytics-4">Ensure your GIS supports GeoAnalytics</a></span></li><li><span><a href="#Prepare-the-data" data-toc-modified-id="Prepare-the-data-5">Prepare the data</a></span></li><ul class="toc-item"><li><span><a href="#Create-a-big-data-file-share" data-toc-modified-id="Create-a-big-data-file-share-5.1">Create a big data file share</a></span></li><li><span><a href="#Edit-a-big-data-file-share" data-toc-modified-id="Edit-a-big-data-file-share-5.2">Edit a big data file share</a></span></li></ul><li><span><a href="#Get-data-for-analysis" data-toc-modified-id="Get-data-for-analysis-6">Get data for analysis</a></span></li><ul class="toc-item"><li><span><a href="#Search-for-big-data-file-shares" data-toc-modified-id="Search-for-big-data-file-shares-6.1">Search for big data file shares</a></span></li><li><span><a href="#Search-for-feature-layers" data-toc-modified-id="Search-for-feature-layers-6.2">Search for feature layers</a></span></li></ul><li><span><a href="#Describe-data" data-toc-modified-id="Describe-data-7">Describe data</a></span></li><li><span><a href="#Extract-features-within-New-Orleans-boundary" data-toc-modified-id="Extract-features-within-New-Orleans-boundary-8">Extract features within New Orleans boundary</a></span></li><li><span><a href="#Summarize-data" data-toc-modified-id="Summarize-data-9">Summarize data</a></span></li><li><span><a href="#Analyze-patterns" data-toc-modified-id="Analyze-patterns-10">Analyze patterns</a></span></li><li><span><a href="#Find-statistically-significant-hot-and-cold-spots" data-toc-modified-id="Find-statistically-significant-hot-and-cold-spots-11">Find statistically significant hot and cold spots</a></span></li><li><span><a href="#Visualize-other-aspects-of-data" data-toc-modified-id="Visualize-other-aspects-of-data-12">Visualize other aspects of data</a></span></li><li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-13">Conclusion</a></span></li></ul></div>

# ## Introduction

# The [arcgis.geoanalytics](https://developers.arcgis.com/python/api-reference/arcgis.geoanalytics.toc.html) module provides types and functions for distributed analysis of large datasets. These GeoAnalytics tools work with big data registered in the GIS datastores as well as with feature layers. 
# In this notebook, we will go through the steps for setting up data to create a [big data file share](https://enterprise.arcgis.com/en/server/latest/get-started/windows/what-is-a-big-data-file-share.htm). We will also edit big data file share manifest to set spatial reference of the dataset. Once the data gets registered, we will demonstrate the utility of a number of tools including `describe_dataset`, `aggregate_points`, `calculate_density`, `find_hot_spots`, `clip_layer`, and `run_python_script` in order to better understand our data.
# 
# The sample aims to find answers to some fundamental questions:
# - What is the spatial relationship between 911 calls?
# - Which block groups have the highest number of 911 calls reporting?
# - What is the most common reason for 911 calls?
# - How many 911 calls occur each month?
# - How many 911 calls occur each hour?
# 
# The data that will be used in this sample was originally obtained from [data.gov](https://www.data.gov) open data portal. You can obtain data by searching using keywords, for example: 'Calls for Service New Orleans'.
# This sample demonstrates ability of ArcGIS API for Python to perform big data analysis on your infrastructure.

# **Note:**
# 
# <font color='purple'>The ability to perform big data analysis is only available on ArcGIS Enterprise licensed with a GeoAnalytics server and not yet available on ArcGIS Online.

# ## Necessary Imports

# In[1]:


get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime as dt

import arcgis
import arcgis.geoanalytics
from arcgis.gis import GIS

from arcgis.geoanalytics.summarize_data import aggregate_points, describe_dataset
from arcgis.geoanalytics.analyze_patterns import calculate_density, find_hot_spots
from arcgis.geoanalytics.manage_data import clip_layer, run_python_script


# ## Connect to your ArcGIS Enterprise organization

# In[2]:


gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# ## Ensure your GIS supports GeoAnalytics

# After connecting to Enterprise portal, we need to ensure an ArcGIS Enterprise GIS is set up with a licensed GeoAnalytics server. To do so, we will call the ``is_supported()`` method. 

# In[3]:


arcgis.geoanalytics.is_supported()


# ## Prepare the data

# To register a file share or an HDFS, we need to format datasets as subfolders within a single parent folder and register the parent folder. This parent folder becomes a datastore, and each subfolder becomes a dataset. Our folder hierarchy would look like below:
|---FileShareFolder                 < -- The top-level folder is what is registered as a big data file share
   |---Calls                        < -- A folder "calls", composed of sub folders
      |---2011                      < -- A folder "2011", composed of 1 csv 
         |---Calls_for_Service_2011.csv                                                                                         
      |---2012                                                   
         |---Calls_for_Service_2012.csv
      |---2013
         |---Calls_for_Service_2013.csv
      |---2014
         |---Calls_for_Service_2014.csv
      |---2015
         |---Calls_for_Service_2015.csv
      |---2016
         |---Calls_for_Service_2016.csv     
      |---2017
         |---Calls_for_Service_2017.csv
      |---2018
         |---Calls_for_Service_2018.csv     
      |---2019
         |---Calls_for_Service_2019.csv        
# Learn more about preparing your big data file share datasets [here](https://enterprise.arcgis.com/en/server/latest/get-started/windows/what-is-a-big-data-file-share.htm)

# ### Create a big data file share

# The `get_datastores()` method of the geoanalytics module returns a `DatastoreManager` object that lets you search for and manage the big data file share items as Python API `Datastore` objects on your GeoAnalytics server.

# In[4]:


bigdata_datastore_manager = arcgis.geoanalytics.get_datastores()
bigdata_datastore_manager


# We will register service calls data as a big data file share using the `add_bigdata()` function on a `DatastoreManager` object. 
# 
# When we register a directory, all subdirectories under the specified folder are also registered with the server. Always register the parent folder (for example, \\machinename\mydatashare) that contains one or more individual dataset folders as the big data file share item. To learn more, see [register a big data file share](https://enterprise.arcgis.com/en/server/latest/manage-data/windows/registering-your-data-with-arcgis-server-using-manager.htm#ESRI_SECTION1_0D55682C9D6E48E7857852A9E2D5D189)
# 
# Note: 
# You cannot browse directories in ArcGIS Server Manager. You must provide the full path to the folder you want to register, for example, \\myserver\share\bigdata. Avoid using local paths, such as C:\bigdata, unless the same data folder is available on all nodes of the server site.

# In[5]:


# data_item = bigdata_datastore_manager.add_bigdata("ServiceCallsOrleans", r"\\machinename\datastore")


# In[5]:


bigdata_fileshares = bigdata_datastore_manager.search(id='cff51a1a-4f27-4955-a3ef-5fa23240ccf9')
bigdata_fileshares


# In[6]:


file_share_folder = bigdata_fileshares[0]


# Once a big data file share is created, the GeoAnalytics server samples the datasets to generate a [manifest](https://enterprise.arcgis.com/en/server/latest/get-started/windows/understanding-the-big-data-file-share-manifest.htm), which outlines the data schema and specifies any time and geometry fields. A query of the resulting manifest returns each dataset's schema.. This process can take a few minutes depending on the size of your data. Once processed, querying the manifest property returns the schema of the datasets in your big data file share.

# In[7]:


manifest = file_share_folder.manifest
manifest


# ### Edit a big data file share

# The spatial reference of the dataset is set to 4326, but we know this data is from New Orleans, Louisiana, and is actually stored in the [Louisiana State Plane Coordinate System](https://spatialreference.org/ref/esri/102682/html/). We need to edit the manifest with the correct spatial reference: {"wkid": 102682, "latestWkid": 3452}. Knowing the location where this data belongs to and the coordinate system which contains geospatial information of this dataset, we will edit our manifest. This will set the correct spatial reference.

# In[8]:


manifest['datasets'][0]['geometry']['spatialReference'] = { "wkid": 102682, "latestWkid": 3452 }


# In[9]:


file_share_folder.manifest = manifest


# In[10]:


file_share_folder.manifest


# ## Get data for analysis

# ### Search for big data file shares

# Adding a big data file share to the Geoanalytics server adds a corresponding [big data file share item](https://enterprise.arcgis.com/en/portal/latest/use/what-is-a-big-data-file-share.htm) on the portal. We can search for these types of items using the item_type parameter.

# In[11]:


search_result = gis.content.search("bigDataFileShares_ServiceCallsOrleans", item_type = "big data file share", max_items=40)
search_result


# In[12]:


data_item = search_result[0]


# In[13]:


data_item


# Querying the layers property of the [item](https://developers.arcgis.com/python/api-reference/arcgis.gis.toc.html#item) returns a featureLayer representing the data. The object is actually an API Layer object.

# In[14]:


data_item.layers


# In[15]:


calls = data_item.layers[0]


# In[16]:


calls.properties


# ### Search for feature layers

# In[17]:


block_grp_item = gis.content.get('9975b4dd3ca24d4bbe6177b85f9da7bb')


# In[18]:


block_grp_item


# We will use the first item for our analysis. Since the item is a Feature Layer Collection, accessing the layers property will give us a list of Feature layer objects.

# In[19]:


blk_grp_lyr = block_grp_item.layers[0]


# ## Describe data

# The [`describe_dataset`](https://developers.arcgis.com/python/api-reference/arcgis.geoanalytics.summarize_data.html#describe-dataset) method provides an overview of big data. By default, the tool outputs a [table](https://developers.arcgis.com/python/api-reference/arcgis.features.toc.html#table) layer containing calculated field statistics and a dict outlining geometry and time settings for the input layer.
# 
# Optionally, the tool can output a ``feature layer`` representing a sample set of features using the ``sample_size`` parameter, or a single polygon feature layer representing the input feature layers' extent by setting the ``extent_output`` parameter to True. 

# In[20]:


description = describe_dataset(input_layer=calls,
                               extent_output=True,
                               sample_size=1000,
                               output_name="Description of service calls" + str(dt.now().microsecond),
                               return_tuple=True)


# In[21]:


description.output_json


# In[22]:


description.sample_layer


# In[23]:


description.sample_layer.query().sdf


# We can see some records have missing or invalid attribute values, including in the fields the manifests defines as the time and geometry values. We will visualize a sample layer on the map to understand it better.

# In[24]:


m1 = gis.map()
m1


# In[25]:


m1.add_layer(description.sample_layer)


# The map shows that some data points have been located outside New Orleans because of missing or invalid geometries. We want to explore data points within New Orleans city limits. We will use [`clip_layer`](https://developers.arcgis.com/python/api-reference/arcgis.geoanalytics.manage_data.html#clip-layer) tool to extract only point features within the New Orleans boundary. This will remove data with missing or invalid geometries.

# ## Extract features within New Orleans boundary

# The ``clip_layer`` method extracts input point, line, or polygon features from an ``input_layer`` that fall within the boundaries of features in a ``clip_layer``. The output layer contains a subset of features from the input layer. We will clip our input **call** feature layer to the New Orleans **blk_grp_lyr** features.

# In[26]:


clip_result = clip_layer(calls, blk_grp_lyr, output_name="service calls in new Orleans" + str(dt.now().microsecond))


# In[27]:


clip_result


# In[28]:


orleans_calls = clip_result.layers[0]


# In[29]:


m2 = gis.map("New Orleans")
m2


# In[30]:


m2.add_layer(orleans_calls)


# ## Summarize data

# We can use the `aggregate_points` method in the `arcgis.geoanalytics.summarize_data` submodule to group call features into individual block group features. The output polygon feature layer summarizes attribute information for all calls that fall within each block group. If no calls fall within a block group, that block group will not appear in the output.

# The GeoAnalytics Tools use a [process spatial reference](https://developers.arcgis.com/rest/services-reference/process-spatial-reference.htm) during execution. Analyses with square or hexagon bins require a projected coordinate system. We'll use the World Cylindrical Equal Area projection (WKID 54034) below. All results are stored in the spatiotemporal datastore of the Enterprise in the WGS 84 Spatial Reference.
# 
# See the GeoAnalytics Documentation for a full explanation of [analysis environment settings](https://enterprise.arcgis.com/en/portal/latest/use/geoanalyticstool-useenvironmentsettings.htm).

# In[31]:


arcgis.env.process_spatial_reference = 54034


# In[32]:


agg_result = aggregate_points(orleans_calls, 
                              polygon_layer=blk_grp_lyr,
                              output_name="aggregate results of call" + str(dt.now().microsecond))


# In[33]:


agg_result


# In[34]:


m3 = gis.map("New Orleans")
m3


# In[35]:


m3.add_layer(agg_result)


# In[36]:


m3.legend = True


# ### Analyze patterns

# The [`calculate_density`](https://developers.arcgis.com/python/api-reference/arcgis.geoanalytics.analyze_patterns.html#calculate-density) method creates a density map from point features by spreading known quantities of some phenomenon (represented as attributes of the points) across the map. The result is a layer of areas classified from least dense to most dense. In this example, we will create density map by aggregating points within a bin of 1 kilometers. To learn more. please see [here](https://developers.arcgis.com/rest/services-reference/calculate-density-geoanalytics.htm)

# In[37]:


cal_density = calculate_density(orleans_calls,
                                weight='Uniform',
                                bin_type='Square',
                                bin_size=1,
                                bin_size_unit="Kilometers",
                                time_step_interval=1,
                                time_step_interval_unit="Years",
                                time_step_repeat_interval=1,
                                time_step_repeat_interval_unit="Months",
                                time_step_reference=dt(2011, 1, 1),
                                radius=1000,
                                radius_unit="Meters",
                                area_units='SquareKilometers',
                                output_name="calculate density of call" + str(dt.now().microsecond))


# In[38]:


cal_density


# In[39]:


m4 = gis.map("New Orleans")
m4


# In[40]:


m4.add_layer(cal_density)


# In[41]:


m4.legend = True


# ### Find statistically significant hot and cold spots

# The ``find_hot_spots`` tool analyzes point data and finds statistically significant spatial clustering of high (hot spots) and low (cold spots) numbers of incidents relative to the overall distribution of the data.
# 

# In[42]:


hot_spots = find_hot_spots(orleans_calls, 
                           bin_size=100,
                           bin_size_unit='Meters',
                           neighborhood_distance=250,
                           neighborhood_distance_unit='Meters',
                           output_name="get hot spot areas" + str(dt.now().microsecond))


# In[43]:


hot_spots


# In[44]:


m5 = gis.map("New Orleans")
m5


# In[45]:


m5.add_layer(hot_spots)


# In[46]:


m5.legend = True


# The darkest red features indicate areas where you can state with 99 percent confidence that the clustering of 911 call features is not the result of random chance but rather of some other variable that might be worth investigating. Similarly, the darkest blue features indicate that the lack of 911 calls is most likely not just random, but with 90% certainty you can state it is because of some variable in those locations. Features that are beige do not represent statistically significant clustering; the number of 911 calls could very likely be the result of random processes and random chance in those areas.

# ### Visualize other aspects of data

# The `run_python_script` method executes a Python script directly in an ArcGIS GeoAnalytics server site . The script can create an analysis pipeline by chaining together multiple GeoAnalytics tools without writing intermediate results to a data store. The tool can also distribute Python functionality across the GeoAnalytics server site.
# 
# Geoanalytics Server installs a Python 3.6 environment that this tool uses. The environment includes ``Spark 2.2.0``, the compute platform that distributes analysis across multiple cores of one or more machines in your GeoAnalytics Server site. The environment includes the ``pyspark module`` which provides a collection of distributed analysis tools for data management, clustering, regression, and more. The ``run_python_script`` task automatically imports the ``pyspark module`` so you can directly interact with it.
# 
# When using the ``geoanalytics`` and ``pyspark`` packages, most functions return analysis results as Spark DataFrame memory structures. You can write these data frames to a data store or process them in a script. This lets you chain multiple geoanalytics and pyspark tools while only writing out the final result, eliminating the need to create any bulky intermediate result layers.
# 
# 

# The **TextType** field contains the reason for the initial 911 call. Let's investigate the most frequent purposes for 911 calls in the New Orleans area by writing our own function:

# In[47]:


def groupby_texttype():
    from datetime import datetime as dt
    # Calls data is stored in a feature service and accessed as a DataFrame via the layers object
    df = layers[0]
    # group the dataframe by TextType field and count the number of calls for each call type. 
    out = df.groupBy('TypeText').count()
    # Write the final result to our datastore.
    out.write.format("webgis").save("groupby_texttype" + str(dt.now().microsecond))


# In the fnction above, calls layer containing 911 call points is converted to a DataFrame. pyspark can  used to group the dataframe by **TypeText** field to find the count of each category of call. The result can be saved as a feature service or other ArcGIS Enterprise layer type.

# In[48]:


run_python_script(code=groupby_texttype, layers=[calls])


# The result is saved as a feature layer. We can Search for the saved item using the `search()` method. Providing the search keyword same as the name we used for writing the result will retrieve the layer. 

# In[49]:


grp_cat = gis.content.search('groupby_texttype')[0]


# Accessing the `tables` property of the item will give us the tables object. We will then use `query()` method to read the table as spatially enabled dataframe.

# In[50]:


grp_cat_df = grp_cat.tables[0].query().sdf


# Sort the values in the decreasing order of the count field.

# In[51]:


grp_cat_df.sort_values(by='count', ascending=False, inplace=True)


# In[52]:


grp_cat_df.head(10).plot(x='TypeText', y='count', kind='barh')


# We can see that **COMPLAINT OTHER** is the most common category of call followed by **TRAFFIC INCIDENTS**. 

# Now we'll investigate 911 calls to investigate the most common reasons for calls within block addresses. We will define a new function to input to the ``run_python_script`` tool, this time grouping data by the **TextType** and **BLOCK_ADDRESS** attributes.

# In[53]:


def grpby_cat_blkadd():
    from datetime import datetime as dt
    # Load the big data file share layer into a DataFrame
    df = layers[0]
    out = df.groupBy('TypeText', 'BLOCK_ADDRESS').count()
    out.write.format("webgis").save("grpby_cat_blkadd" + str(dt.now().microsecond))


# In[54]:


run_python_script(code=grpby_cat_blkadd, layers=[calls])


# In[55]:


grp_cat_addr = gis.content.search('grpby_cat_blkadd')[0]


# In[56]:


grp_cat_addr_df = grp_cat_addr.tables[0].query().sdf


# In[57]:


grp_cat_addr_df.sort_values(by='count', ascending=False, inplace=True)


# In[58]:


grp_cat_addr_df.head(10).plot(x='BLOCK_ADDRESS', y='count', kind='barh')


# The chart shows that 22A block address has the highest number of incidents reported. So we want to further investigate the reason why this area had the most number of calls.

# In[59]:


blk_addr_high = grp_cat_addr_df[grp_cat_addr_df['BLOCK_ADDRESS'] == '22A']


# In[60]:


blk_addr_high.head()


# In[61]:


blk_addr_high.TypeText.sort_values(ascending=False).head()


# The result indicates the most common reason for a 911 call in the 22A Block in New Orleans is defined as **WARR STOP WITH RELEASE**.

# Now let's investigate the 911 call data for temporal trends. We saw in the manifest that the **TimeCreate** field holds specific time information for one instant (in UTC time zone) using a string format: **MM/dd/yyyy hh:mm:ss a**. We can parse these strings using Python's ``datetime module`` to extract year, month, day, hour, minute, and second to perform time analyses.

# Let's define a helper function to convert the **TimeCreate** attribute field string types into a date type.

# In[62]:


def calls_with_datetime():
    from datetime import datetime as dt
    # Load the big data file share layer into a DataFrame
    from pyspark.sql import functions as F
    df = layers[0]
    out = df.withColumn('datetime', F.unix_timestamp('TimeCreate', 'MM/dd/yyyy hh:mm:ss a').cast('timestamp'))
    out.write.format("webgis").save("calls_with_datetime" + str(dt.now().microsecond))


# In[63]:


run_python_script(code=calls_with_datetime, layers=[calls])


# In[64]:


calls_with_datetime = gis.content.search('calls_with_datetime')[0]


# In[65]:


calls_with_datetime


# In[66]:


calls_with_datetime_lyr = calls_with_datetime.layers[0]


# We will now split the date field into year, month and hour for studying temporal trends.

# In[67]:


def call_with_added_date_time_cols():
    from datetime import datetime as dt
    # Load the big data file share layer into a DataFrame
    from pyspark.sql.functions import year, month, hour
    df = layers[0]
    df = df.withColumn('year', year(df['datetime']))
    df = df.withColumn('month', month(df['datetime']))
    out = df.withColumn('hour', hour(df['datetime']))
    out.write.format("webgis").save("call_with_added_date_time_cols" + str(dt.now().microsecond))


# In[68]:


run_python_script(code=call_with_added_date_time_cols, layers=[calls_with_datetime_lyr])


# In[69]:


date_time_added_item = gis.content.search('call_with_added_date_time_cols')


# In[70]:


date_time_added_item[0]


# In[71]:


date_time_added_lyr = date_time_added_item[0].layers[0]


# In[72]:


def grp_calls_by_month():
    from datetime import datetime as dt
    # Load the big data file share layer into a DataFrame
    df = layers[0]
    out = df.groupBy('month').count()
    out.write.format("webgis").save("grp_calls_by_month" + str(dt.now().microsecond))


# In[73]:


run_python_script(code=grp_calls_by_month, layers=[date_time_added_lyr])


# In[74]:


month = gis.content.search('grp_calls_by_month')[0]


# In[75]:


grp_month = month.tables[0]


# In[76]:


df_month = grp_month.query().sdf


# In[77]:


df_month


# In[78]:


df_month.dropna().sort_values(by='month').plot(x='month', y='count', kind='bar')


# It shows that calls are most frequent in the earlier months of the year.

# In[79]:


def grp_calls_by_hour():
    from datetime import datetime as dt
    # Load the big data file share layer into a DataFrame
    df = layers[0]
    out = df.groupBy('hour').count()
    out.write.format("webgis").save("grp_calls_by_hour" + str(dt.now().microsecond))


# In[80]:


run_python_script(code=grp_calls_by_hour, layers=[date_time_added_lyr])


# In[81]:


hour = gis.content.search('grp_calls_by_hour')[0]


# In[82]:


grp_hour = hour.tables[0]


# In[83]:


df_hour = grp_hour.query().sdf


# In[84]:


df_hour.dropna().sort_values(by='hour').plot(x='hour', y='count', kind='bar')


# The chart above shows frequency of calls per hour of the day. We can see that there is a high frequency of calls during 4-5 p.m

# ## Conclusion

# We have shown spatial patterns in distribuition of emergency calls and the need of including temporal trends in the analysis. There is an obvious variation in the monthly, weekly, daily and even hourly distribution of service calls and this should be accounted for in further analysis or allocation of resources.


# ====================
# streams_extraction_using_deeplearning.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Streams extraction using deep learning
# * 🔬 Data Science
# * 🥠 Deep Learning and Pixel Classification

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Necessary imports](#Necessary-imports)
# * [Connect to your GIS](#Connect-to-your-GIS)
# * [Get the data for analysis](#Get-the-data-for-analysis)
# * [Export training data](#Export-training-data)
# * [Prepare data](#Prepare-data)
# * [Visualize a few samples from your training data](#Visualize-a-few-samples-from-your-training-data)
# * [Train the model](#Train-the-model)
#  * [Load model architecture](#Load-model-architecture)
#  * [Find an optimal learning rate](#Find-an-optimal-learning-rate)
#  * [Fit the model](#Fit-the-model)
#  * [Accuracy Assessment](#Accuracy-Assessment)
#  * [Visualize results in validation set](#Visualize-results-in-validation-set)
#  * [Save the model](#Save-the-model)
# * [Model inference](#Model-inference)
# * [Conclusion](#Conclusion)

# ## Introduction

# In this notebook, we will use 3 different geo-morphological characteristics derived from a 5 m resolution DEM in one of the watersheds of Alaska to extract streams. These 3 characteristics are Topographic position index derived from 3 cells, Geomorphon landform, and topographic wetness index.
# 
# We created a composite out of these 3 characteristic rasters and used Export raster tool to convert scale the pixels values to 8 bit unsigned. Subsequently, the images are exported as "Classified Tiles" to train a Multi-Task Road Extractor model provided by ArcGIS API for Python for extracting the streams.
# 
# Before proceeding through this notebook, it is advised to go through the API Reference for [Multi-Task Road Extractor](https://developers.arcgis.com/python/api-reference/arcgis.learn.toc.html#multitaskroadextractor). It will help in understanding the Multi-Task Road Extractor's workflow in detail.

# ## Necessary imports

# In[1]:


import os
import zipfile
from pathlib import Path

from arcgis.gis import GIS
from arcgis.learn import prepare_data, MultiTaskRoadExtractor, ConnectNet


# ## Connect to your GIS

# In[2]:


gis = GIS("home")
ent_gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# ## Get the data for analysis

# Here is the composite with 3 bands representing the 3 geo-morphological characteristics namely Topographic position index, Geomorphon landform, and Topographic wetness index.

# In[3]:


composite_raster = ent_gis.content.get('43c4824dd4bf41ee886be5042262f192')
composite_raster


# In[4]:


BeaverCreek_Flowlines = ent_gis.content.get('2b198075e53748b4ab84197bc6ddd478')
BeaverCreek_Flowlines


# ## Export training data

# 
# Export training data using 'Export Training data for deep learning' tool, [click here](https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/export-training-data-for-deep-learning.htm) for detailed documentation: 
# 
# - Set 'composite_3bands_6BC_8BC_8bitunsigned' as `Input Raster`.
# - Set a location where you want to export the training data in `Output Folder` parameter, it can be an existing folder or the tool will create that for you.
# - Set the 'BeaverCreek_Flowlines' as input to the `Input Feature Class Or Classified Raster` parameter.
# - Set `Class Field Value` as 'FClass'.
# - Set `Image Format` as 'TIFF format'
# - `Tile Size X` & `Tile Size Y` can be set to 256.
# - `Stride X` & `Stride Y` can be set to 128. 
# - Select 'Classified Tiles' as the `Meta Data Format`.
# - In 'Environments' tab set an optimum `Cell Size`. For this example, as we have performing the analysis on the geo-morphological characteristics with 5 m resolution, so, we used '5' as the cell size.

# 
# 

# `arcpy.ia.ExportTrainingDataForDeepLearning("composite_3bands_6BC_8BC_8bitunsigned.tif", r"D:\Stream Extraction\Exported_3bands_composite_8bit_unsigned", "BeaverCreek_Flowlines", "TIFF", 256, 256, 128, 128, "ONLY_TILES_WITH_FEATURES", "Classified_Tiles", 0, "FClass", 5, None, 0, "MAP_SPACE", "PROCESS_AS_MOSAICKED_IMAGE", "NO_BLACKEN", "FIXED_SIZE")`

# Alternatively, we have provided a subset of training data containing a few samples. You can use the data directly to run the experiments.

# In[5]:


training_data = gis.content.get('3a95fd7a25d54898bddabf1989eea87d')
training_data


# In[6]:


filepath = training_data.download(file_name=training_data.name)


# In[7]:


with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[8]:


output_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# ## Prepare data

# We will specify the path to our training data and a few hyperparameters.
# 
# - `path`: path of the folder containing training data.
# - `batch_size`: Number of images your model will train on each step inside an epoch, it directly depends on the memory of your graphic card. 8 worked for us on a 11GB GPU.

# In[45]:


data = prepare_data(output_path, chip_size=512, batch_size=4)


# ## Visualize a few samples from your training data

# To get a sense of what the training data looks like, `arcgis.learn.show_batch()` method randomly picks a few training chips and visualizes them.
# - `rows`: number of rows we want to see the results for.

# In[47]:


data.show_batch()


# ## Train the model

# ### Load model architecture

# `arcgis.learn` provides the MultiTaskRoadExtractor model for classifying linear features, which is based on multi-task learning mechanism. More details about multi-task learning can be found [here](https://developers.arcgis.com/python/guide/how-multi-task-road-extractor-works/#Multi-Task-learning).

# In[12]:


model = ConnectNet(data, mtl_model="hourglass")


# ### Find an optimal learning rate

# Learning rate is one of the most important hyperparameters in model training. Here, we explore a range of learning rate to guide us to choose the best one. We will use the `lr_find()` method to find an optimum learning rate at which we can train a robust model.

# In[13]:


lr=model.lr_find()


# ### Fit the model

# To train the model, we use the `fit()` method. To start, we will train our model for 50 epochs. Epoch defines how many times model is exposed to entire training set. We have passes three parameters to `fit()` method:
# - `epochs`: Number of cycles of training on the data.
# - `lr`: Learning rate to be used for training the model.
# - `wd`: Weight decay to be used.

# In[15]:


model.fit(50, lr)


# As you can see, both the losses (valid_loss and train_loss) started from a higher value and ended up to a lower value, that tells our model has learnt well. Let us do an accuracy assessment to validate our observation.

# ### Accuracy Assessment

# We can compute the mIOU (Mean Intersection Over Union) for the model we just trained in order to do the accuracy assessment. We can compute the mIOU by calling `model.mIOU`. It takes the following parameters:
# - `mean`: If False returns class-wise mean IOU, otherwise returns mean IOU of all classes combined.

# In[16]:


model.mIOU()


# The model has a mean IOU of 0.97 which proves that the model has learnt well. Let us now see it's results on validation set.

# ### Visualize results in validation set

# The code below will pick a few random samples and show us ground truth and respective model predictions side by side. This allows us to validate the results of your model in the notebook itself. Once satisfied we can save the model and use it further in our workflow. The `model.show_results()` method can be used to display the detected streams.

# In[17]:


model.show_results(rows=4)


# ### Save the model

# We would now save the model which we just trained as a 'Deep Learning Package' or '.dlpk' format. Deep Learning package is the standard format used to deploy deep learning models on the ArcGIS platform.
# 
# We will use the `save()` method to save the model and by default it will be saved to a folder 'models' inside our training data folder itself.

# In[49]:


model.save('stream_ext_50e', overwrite=True, publish=True, gis=gis)


# ## Model inference

# The saved model can be used to classify streams using the `Classify Pixels Using Deep Learning` tool available in ArcGIS Pro, or ArcGIS Enterprise.

# 
# 

# `arcpy.ia.ClassifyPixelsUsingDeepLearning("composite_3bands_6BC_8BC_8bitunsigned.tif", r"D:/stream_ext_20e/stream_ext_20e.dlpk", "padding 64;batch_size 4;predict_background True;return_probability_raster False;threshold 0.5", "PROCESS_AS_MOSAICKED_IMAGE", None); out_classified_raster.save(r"\Documents\ArcGIS\Packages\Stream Extraction demo_9ee52d\p20\stream_identification.gdb\detected_streams")`

# The output of the model is a layer of detected streams which is shown below:

# In[19]:


extracted_streams = ent_gis.content.get('389772c5dbb745b79953dc7ee0dc5876')
extracted_streams


# In[20]:


map1 = gis.map()
map1.add_layer(extracted_streams)
map1


# ![image.png](attachment:image.png)

# 

# In[21]:


map1.zoom_to_layer(extracted_streams)


# ## Conclusion

# The notebook presents the workflow showing how easily you can combine traditional morphological landform characteristics with deep learning methodologies using `arcgis.learn` to detect streams.


# ====================
# tabular_data_supervised_learning_using_automl.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Supervised learning of tabular data using AutoML

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Prepare tabular data](#prepare)
# * [Train model using AutoML](#AutoML)
# * [Reload trained model for prediction](#Reload)

# ## Introduction
# <a id='Introduction'></a> 

# Pipeline of activities in a typical machine learning project involves data preprocessing, exploratory data analysis, feature selection/feature engineering, model selection, hyper parameter tuning, generating model explanation and model selection/evaluation. This is an iterative process and data scientists spend a lot of time going through multiple iterations of this pipeline before they are able to identify the best model. AutoML aims to automates this workflow. 

# `arcgis.learn` users will now be able to use AutoML for supervised learning classification or regression problems involving tabular data. The AutoML implementation in `arcgis.learn` builds upon the implementation from MLJar (https://github.com/mljar/mljar-supervised) 

# ## Prepare tabular data
# <a id='prepare'></a>

# Data can be [feature layer](https://developers.arcgis.com/python/guide/working-with-feature-layers-and-features/), [spatially enabled dataframe](https://developers.arcgis.com/python/guide/introduction-to-the-spatially-enabled-dataframe/) with/without rasters or just a simple dataframe. The data for AutoML is prepared the same way it is prepared for [supervised learning ML Models](https://developers.arcgis.com/python/guide/ml-and-dl-on-tabular-data/).
# 

# In[1]:


get_ipython().run_line_magic('matplotlib', 'inline')

from IPython.display import Image, HTML
import arcgis
from arcgis.gis import GIS
from arcgis.learn import prepare_tabulardata,AutoML
from sklearn.preprocessing import MinMaxScaler,RobustScaler


# Here we will be taking a feature layer hosted on ArcGIS Online, convert it to a spatially enabled dataframe and prepare the data using prepare_tabulardata method from `arcgis.learn`. More details about data preparation for ML Models can be found [here](https://developers.arcgis.com/python/guide/ml-and-dl-on-tabular-data/)

# In[2]:


gis=GIS('home')
calgary_no_southland_solar = gis.content.search('calgary_no_southland_solar owner:api_data_owner', 'feature layer')[0]


# In[3]:


calgary_no_southland_solar_layer = calgary_no_southland_solar.layers[0]
calgary_no_southland_solar_layer_sdf = calgary_no_southland_solar_layer.query().sdf
calgary_no_southland_solar_layer_sdf=calgary_no_southland_solar_layer_sdf[['FID','date','ID','solar_plan','altitude_m',
                                                                           'latitude','longitude','wind_speed','dayl__s_',
                                                                           'prcp__mm_d','srad__W_m_','swe__kg_m_', 'tmax__deg',
                                                                           'tmin__deg','vp__Pa_','kWh_filled','capacity_f',
                                                                           'SHAPE']]
calgary_no_southland_solar_layer_sdf.head()


# In[4]:


X = ['altitude_m', 'wind_speed', 'dayl__s_', 'prcp__mm_d','srad__W_m_','swe__kg_m_','tmax__deg','tmin__deg','vp__Pa_']
preprocessors =  [('altitude_m', 'wind_speed', 'dayl__s_', 'prcp__mm_d','srad__W_m_','swe__kg_m_','tmax__deg',
                   'tmin__deg','vp__Pa_', RobustScaler())]


# In[5]:


data = prepare_tabulardata(calgary_no_southland_solar_layer,
                           'capacity_f',
                           explanatory_variables=X,                           
                           preprocessors=preprocessors)


# ## Train model using AutoML
# <a id='AutoML'></a>

# In[6]:


from arcgis.learn import AutoML


# AutoML class accepts the following paramters:
# - data (Required Paramter): Returned data object from `prepare_tabulardata` function in the previous step.
#     
# - total_time_limit (Optional parameter): It is the total time in seconds that must be used for AutoML training. Default set is 3600 (1 Hr). At the completion of total_time_limit, the training of AutoML completes and the best model trained until then is used. 
#     
# - mode (Optional Parameter): Model can be either Explain. Perform or Compete. Default is Explain.
# 
# - algorithms (Optional Parameter): This parameter takes in list of algorithms as input. The algorithms could be subset of the following: Linear,Decision Tree,Random Forest,Extra Trees,LightGBM,Xgboost,Neural Network.
# 
# - eval_metric (Optional Parameter): The metric to be used to compare models. 

# ### AutoML modes

# - Explain : To to be used when you want to explain and understand the data.
#                                       Uses 75%/25% train/test split.
#                                       Uses the following models: Baseline, Linear, Decision Tree,
#                                       Random Forest, XGBoost, Neural Network, and Ensemble.
#                                       Has full explanations in reports: learning curves, importance
#                                       plots, and SHAP plots.
# - Perform : To be used when you want to train a model that will be used in real-life use cases.
#                                       Uses 5-fold CV (Cross-Validation).
#                                       Uses the following models: Linear, Random Forest, LightGBM,
#                                       XGBoost,Neural Network, and Ensemble.
#                                       Has learning curves and importance plots in reports.
# - Compete : To be used for machine learning competitions (maximum performance).
#                                       Uses 10-fold CV (Cross-Validation).
#                                       Uses the following models: Decision Tree, Random Forest, Extra Trees,
#                                       XGBoost, Neural Network, Nearest Neighbors, Ensemble,
#                                       and Stacking.It has only learning curves in the reports.
#  

# In[7]:


AutoML_class_obj = AutoML(data=data)


# After creating the AutoML object by passing the data obtained from `prepare_tabulardata` and using default values for other parameters, now we proceed to train the model using AutoML. This is done by calling the `fit` method as shown below. New folder will be created and all the models and their varients are saved in that folder. 

# In[8]:


AutoML_class_obj.fit()


# Once the best model is identified after the completion of `fit` method, the model is then saved by calling the `save` method. The transforms and the encoders used on the training data, along with the Esri Model Definition (EMD) file and the dlpk is then saved in the path specified by the user. 

# In[ ]:


AutoML_class_obj.save('<Pass the path>')


# We can get the score of the best model, visualize the results on validation dataset and also get predictions on new data using the corresponding methods shown below. 

# In[9]:


AutoML_class_obj.score()


# In[10]:


AutoML_class_obj.show_results()


# In[11]:


AutoML_class_obj.predict(data._dataframe.iloc[:100][X],prediction_type="dataframe")


# As in the case of `MLModel` and `FullyConnectedNetwork`, predictions can also be obtained in the form of feature class and rasters.

# Additionally it is also possible to generate and view the report of all the models trained by the AutoML, the performance and the hyperparamters used in each of the model variant. The report can be generated using the report method as shown below. The reports, also show the learning curves and feature importance charts for each of the model evaluated during the training.  

# In[ ]:


AutoML_class_obj.report()


# ![report.PNG](attachment:report.PNG)

# ## Reload trained model for prediction
# <a id='Reload'></a>

# The trained AutoML can be reloaded from the disk to get the predictions on new data. This is done using `from_model` method. This method takes in the path to the emd file as the input. The best model that was identified using the training phase will automatically be picked up and the prediction can be done on the new data using this model by calling the predict method. 

# In[1]:


from arcgis.learn import AutoML
AutoML_test_reload=AutoML.from_model(r'<Path to the emd file created after saving the model>')


# In[14]:


AutoML_test_reload.predict(data._dataframe.iloc[:100],prediction_type="dataframe")



# ====================
# temperature_forecast_using_time_series_data.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Temperature forecast using time series data

# ## Table of Contents <a class="anchor" id="0"></a>
# * [Introduction](#1)
# * [Prerequisites](#10)
# * [Necessary Imports](#2)
# * [Connect to your GIS](#3)
# * [Obtain and Visualize England's Data for Analysis](#4) 
#     * [Boundary](#5)
#     * [Weather Stations](#6)
#     * [Historic Temperature Data](#7) 
# * [Convert to Timeseries Data](#8)
# * [Model Building](#9)      
#     * [LSTM](#11) 
#         * [Temperature forecast by LSTM model](#12) 
#     * [Support Vector Machine(SVR)](#13) 
#         * [Temperature forecast by SVR model](#14)
# * [Temperature Interpolation for England](#15)
#     * [Result Visualization](#16)
# * [Conclusion](#17)
# * [Summary of methods used](#18)
# * [Data resources](#19)

# ## Introduction <a class="anchor" id="1"></a>
# 
# Weather forecasting has been a significant area for application of advanced deep learning and machine learning methodologies over traditional methods to improve weather prediction. These new methods are appropriate for processing large chunks of data where massive quantity of historic weather datasets could be utilized for forecasting. This sample showcases two autoregressive methods: one using a deep learning and another using a machine learning framework to predict temperature of England.
# 
# Historic temperature data from various weather stations across England is collected from [here](https://rp5.ru/Weather_in_the_world). The data consists of daily temperature measurements ranging from February 2005 till September 2019 which are auto regressed to predict daily temperature for each of the identified stations for October 2019. The forecasted temperature obtained for the stations is then spatially interpolated using ArcGIS spatial interpolation tool to produce a temperature prediction surface for the entire country. Here is a schematic flow chart of the operation:

# 

# ## Prerequisites<a class="anchor" id="10"></a>
# 
# Some required libraries for this sample are NumPy for processing arrays, pandas for operating with DataFrame, ArcGIS for geoprocessing, scikit-learn=0.22.1 for machine learning, tensorflow=2.0.0 and keras=2.2.4 for deep learning. 

# ## Necessary Imports <a class="anchor" id="2"></a>

# In[3]:


get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib.pyplot as plt

import numpy as np
import pandas as pd
import math
from datetime import datetime as dt
from IPython.display import Image, HTML

from sklearn.svm import SVR
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Activation, Dropout
from tensorflow.keras.optimizers import Adam
import tensorflow.keras.backend as K

from arcgis.gis import GIS
from arcgis.features import SpatialDataFrame
from arcgis.features.analysis import interpolate_points


# ## Connect to your GIS <a class="anchor" id="3"></a>

# In[5]:


gis = GIS('home')


# ## Obtain and Visualize Data for Analysis<a class="anchor" id="4"></a>
# 
# The primary data used for this sample is as follows:

# ### Data 1— England Boundary <a class="anchor" id="5"></a>
# First the boundary of England shapefile is accessed. This will be used to interpolate temperature within this particular area.

# In[6]:


# Access England Boundary
england_border = gis.content.get('0856d38fea9149a48227cdc2f1e4f4f6')
england_border


# In[7]:


# Get the feature layer
england_boundary_layer = england_border.layers[0]


# In[77]:


# Plot England boundary
england_map = gis.map('England', zoomlevel=6)
england_map.add_layer(england_boundary_layer)
england_map


# ### Data 2 — England Weather Stations  <a class="anchor" id="6"></a>
#  
# There are several weather stations in England that record a variety of weather data. Here 29 weather stations are strategically selected such that they are well distributed across England and can be used to forecast temperature which will cover the entire country. These include stations located at prominent English cities such as London, Birmingham, Cardiff, Exeter, Nottingham, Plymouth and others, as shown in the map below. 

# In[8]:


# Access England Weather Stations
england_weather_stations = gis.content.get('fd3ecbd95b7148b8a7cbcc866cedd514')
england_weather_stations


# In[9]:


england_weather_stations_layer = england_weather_stations.layers[0]


# In[78]:


# England weather stations
england_weather_stations_map = gis.map('England', zoomlevel=6)
england_weather_stations_map.add_layer(england_weather_stations_layer)
england_weather_stations_map


# The locations of the weather stations are mapped here which are uniformly distributed throughout England. This is necessary to create a well interpolated prediction surface. The more the number of weather stations, more precise would be the interpolated result.      

# In[11]:


# Access spatial dataframe
england_weather_stations_layer_sdf = pd.DataFrame.spatial.from_layer(england_weather_stations_layer)
england_weather_stations_layer_sdf.head()


# The table above shows the latitude (Y) and longitude (X) values of the 29 weather station used in this study.  

# ### Data 3 —  Historic Temperature  <a class="anchor" id="7"></a>
# Daily Mean temperature in degree Celsius ranging from February, 2005 till September,2019 is accessed from the above mentioned weather stations. One issue with timeseries datasets that needed to be addressed was missing data. Thus, weather stations with the least amount of missing data were selected for the study.

# In[12]:


# Access historic temperature data of England
table = gis.content.get('d15eba5e9fe54a968e272c32d8e58e1f')


# In[13]:


temp_history = table.tables[0]


# In[14]:


# Visualize as pandas dataframe
all_station_temp_history = temp_history.query().sdf


# In[15]:


all_station_temp_history.tail()


# In[16]:


all_station_temp_history.shape


# In[17]:


england_temp = all_station_temp_history[all_station_temp_history.columns[0:30]]


# In[18]:


england_temp.tail()


# The table above shows the historic temperature data in degree Celsius of all the weather stations starting from 2005 to September 2019. The first column is the Date field which is the day of the recorded temperature and rest of the columns are weather stations. 

# ## Convert to Timeseries format <a class="anchor" id="8"></a>
# 
# This temperature dataset is now transformed into a timeseries data format where the Date column is set as the index of the dataset.

# In[19]:


# Change to datetime format
england_temp_new = england_temp.copy() 
england_temp_new[england_temp_new.columns[0]] = pd.to_datetime(england_temp_new[england_temp_new.columns[0]], format='%d-%m-%Y')
england_temp_new = england_temp_new.set_index(england_temp_new.columns[0])
england_temp_new = england_temp_new.sort_index()
all_station_temp = england_temp_new.astype('float')
all_station_temp.tail()


# ## Model Building <a class="anchor" id="9"></a>
# Once the dataset is transformed into a timeseries dataset, it is ready to be used for modelling. In this sample two types of methodology are used for modelling: 
# 
# 1) LSTM - First a deep learning framework of LSTM is used which is appropriate for handling time series data. 
# 
# 2) Support Vector Machine  - In the second option the machine learning algorithm of Support Vector Regression(SVR) is used to compare the performance between the two methods in terms of accuracy and computation time.

# ### LSTM <a class="anchor" id="11"></a>
# 
# LSTM (Long short-term memory) first proposed by [Hochreiter & Schmidhuber](http://www.bioinf.jku.at/publications/older/2604.pdf), is a type of Recurrent Neural Network(RNN). RNN could be defined as a special kind of neural network which can retain information from past inputs which is not possible for traditional neural networks. This makes it suitable for forecasting timeseries data wherein prediction is done based on past data. 
# LSTM is built of units, each consisting of four neural networks, which are used to update its cell state using information from new inputs and past outputs.
# 
# A function is created here which encapsulates the steps for processing and predicting from the timeseries data.
# 
# First an empty datetime DataFrame is created for the number of days the temperature is to be forecasted, where future predicted values will be stored.

# In[25]:


# create future forecast dates
def create_dates(start,days):
    v = pd.date_range(start=start, periods=days+1, freq='D', closed='right')
    seven_day_forecast = pd.DataFrame(index=v) 
    return seven_day_forecast


# This next method accesses the station name from the input data and the related values for that station. 

# In[26]:


# get values, station name and drop null values
def get_value_name(all_station_temp,i):
    station_value = all_station_temp[[all_station_temp.columns[i]]].dropna()
    station_name = all_station_temp.columns[i]
    return station_value, station_name 


# Sequence of a timeseries is very important, hence while splitting the values into train and test set, the order is to be retained. This method takes the above accessed values and divides it into user input ratio before and after a certain date.

# In[27]:


# train-test split for a user input ratio
def train_test_split(value, name, ratio):
    nrow = len(value)
    print(name+' total samples: ',nrow)
    split_row = int((nrow)*ratio)
    print('Training samples: ',split_row)
    print('Testing samples: ',nrow-split_row)
    train = value.iloc[:split_row]
    test = value.iloc[split_row:]
    return train, test, split_row     


# Data scaling is essential before feeding it to a LSTM, which helps it train better compared to raw unscaled data. This method scales the train and test data using a minmax scaler from sci-kit learn.

# In[22]:


# data transformation
def data_transformation(train_tract1,test_tract1):
    scaler = MinMaxScaler()
    train_tract1_scaled = scaler.fit_transform(train_tract1)
    test_tract1_scaled = scaler.fit_transform(test_tract1)          
    train_tract1_scaled_df = pd.DataFrame(train_tract1_scaled, index = train_tract1.index, columns=[train_tract1.columns[0]])
    test_tract1_scaled_df = pd.DataFrame(test_tract1_scaled,
                                         index = test_tract1.index, columns=[test_tract1.columns[0]])
    return train_tract1_scaled_df, test_tract1_scaled_df, scaler     


# Finally one more transformation of feature engineering is required, which is to create new features using lagged values of the time series data itself. Here the number of lag terms could be specified and the function would create lag number of new features using the lagged terms.  

# In[38]:


# feature builder - This section creates feature set with lag number of predictors--Creating features using lagged data
def timeseries_feature_builder(df, lag):
    df_copy = df.copy()
    for i in range(1,lag):
        df_copy['lag'+str(i)] = df.shift(i) 
    return df_copy
    df_copy = df.copy()


# Null values resulting from the above feature creation are dropped followed by converting the train and test values to arrays, which is the input data type for LSTM.

# In[40]:


# preprocessing -- drop null values and make arrays 
def make_arrays(train_tract1,test_tract1):
    X_train_tract1_array = train_tract1.dropna().drop(train_tract1.columns[0], axis=1).values
    y_train_tract1_array = train_tract1.dropna()[train_tract1.columns[0]].values
    X_test_tract1_array = test_tract1.dropna().drop(test_tract1.columns[0], axis=1).values
    y_test_tract1_array = test_tract1.dropna()[test_tract1.columns[0]].values    
    return X_train_tract1_array, y_train_tract1_array, X_test_tract1_array, y_test_tract1_array


# LSTM model with three hidden layer is created each having a user input number of LSTM memory units, with a dropout rate of 20% for each layer, and a final output dense layer predicting a single value. 

# In[47]:


# Define LSTM model
def lstm_model(units, trainX, testX, y_train_tract1_array, y_test_tract1_array):
    model = Sequential()
    model.add(LSTM(units,return_sequences=True, input_shape=(trainX.shape[1],trainX.shape[2]),kernel_initializer='lecun_uniform'))
    model.add(Dropout(0.2))    
    model.add(LSTM(units, return_sequences=True))
    model.add(Dropout(0.2))    
    model.add(LSTM(units))
    model.add(Dropout(0.2))
    model.add(Dense(1))        
    model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')
    
    model.fit(trainX, y_train_tract1_array, batch_size=120, epochs=100, validation_data=(testX, y_test_tract1_array), verbose=0)
    return model


# In the validation method, the fitted model is used here to predict on the test set and the results are added to a column called Forecast for visualization. The accuracy of the predicted result is measured by r-square method, to check its similarity with the actual temperature readings, which is intuitive to interpret. 

# In[42]:


# validation result 
def valid_result(model, testX, y_test_tract1_array, scaler, station_value, split_row, lag):    
    testPredict = model.predict(testX)
    rSquare_test = r2_score(y_test_tract1_array, testPredict)
    print('Test R-squared is: %f'%rSquare_test)    
    testPredict = scaler.inverse_transform(testPredict)        
    new_test_tract1 = station_value.iloc[split_row:]       
    test_tract1_pred = new_test_tract1.iloc[lag:].copy()
    test_tract1_pred['Forecast'] = testPredict
    return test_tract1_pred 


# Once the results are validated, the model is used to forecast temperature for the next 31 days or the month of October, 2019 by a walk forward of each day. Here the past lag number of days are used to predict the temperature for October 1st. This predicted value is then included as a predictor for forecasting the next days value, and input into the fitted model along with past days temperatures, and so on till all the days are predicted. This is repeated for each weather station. 

# In[43]:


# multi step future forecast for next days number of days. 
def forecast(model, testX, test_tract1, lag, scaler, days):
    seven_days = []
    new0 = testX[-1]        
    last = test_tract1.iloc[-1]
    new_predict = last[0]        
    new_array = np.insert(new0, 0, new_predict)        
    new_array = np.delete(new_array, -1)
    new_array_reshape = np.reshape(new_array, (-1,1,lag))       
    new_predict = model.predict(new_array_reshape)
    temp_predict = scaler.inverse_transform(new_predict) 
    seven_days.append(temp_predict[0][0].round(2))
    
    for i in range(1,days):
        new_array = np.insert(new_array, 0, new_predict)             
        new_array = np.delete(new_array, -1)
        new_array_reshape = np.reshape(new_array, (-1,1,lag))            
        new_predict = model.predict(new_array_reshape)
        temp_predict = scaler.inverse_transform(new_predict) 
        seven_days.append(temp_predict[0][0].round(2))
    return seven_days         


# Finally the main function is created which calls the above modules for predicting the monthly forecast. This consists of first accessing time series data for each station, processing them into appropriate input format and fitting a LSTM model on 90% of the data as training set. This is followed by validating the trained model on the rest 10% of the data and final forecasting for the next 31 days using the trained model, which is then repeated for all the 29 station.

# In[45]:


def england_temp_lstm(all_station_temp, lag, days):    
    
    seven_day_forecast_lstm = create_dates('2019-09-30',days) 
    
    for i in range(len(all_station_temp.columns)):
        
        # preprocessing
        station_value, station_name = get_value_name(all_station_temp,i)        
        train_tract1, test_tract1, split_row = train_test_split(station_value, station_name, 0.90)        
        train_tract1_scaled_df, test_tract1_scaled_df, scaler = data_transformation(train_tract1,test_tract1) 
        train_tract1 = timeseries_feature_builder(train_tract1_scaled_df, lag+1) 
        test_tract1 = timeseries_feature_builder(test_tract1_scaled_df, lag+1)               
        X_train_tract1_array, y_train_tract1_array, X_test_tract1_array, y_test_tract1_array = make_arrays(train_tract1, 
                                                                                                           test_tract1)        
        trainX = np.reshape(X_train_tract1_array, (X_train_tract1_array.shape[0],1,X_train_tract1_array.shape[1]))
        testX = np.reshape(X_test_tract1_array, (X_test_tract1_array.shape[0],1,X_test_tract1_array.shape[1]))                
        
        # LSTM modelling & forecast
        model = lstm_model(30, trainX, testX, y_train_tract1_array, y_test_tract1_array)             
        test_tract1_pred = valid_result(model, testX, y_test_tract1_array, scaler, station_value, split_row, lag)        
        seven_days = forecast(model, testX, test_tract1, lag, scaler, days)       
        seven_day_forecast_lstm[station_name] = np.array(seven_days)       
        
        # plot result
        plt.figure(figsize=(20,5))
        plt.plot(test_tract1_pred)        
        plt.plot(seven_day_forecast_lstm[station_name], color='red', label='forecast')         
        plt.ylabel('Temperature(°C)')
        plt.legend(loc='upper right')
        plt.title(station_name + '- October 2019 Temperature Forecast')
        plt.show()        
        
    return(seven_day_forecast_lstm)


# Once the main function is ready it is called on the weather station dataset consisting of past temperature data from the selected weather stations. It is given three input: the data table, number of past day's data to be used for forecasting and the number of days for which the temperature is to be predicted.

# In[48]:


get_ipython().run_cell_magic('time', '', '# Fitting and forecast using LSTM  -- output of train loss and valid loss is turned off\nlstm_prediction = england_temp_lstm(all_station_temp,120,31)\n')


# The result above returns the model metric of R-squared for the test data for each weather station followed by visualization of the actual past observations and the output predicted by the model for the same observation and the monthly forecast of October,2019. Here the past observed temperatures is in Blue, validation is in Orange and forecast is in Red. It is observed that the data is fitted reasonably with high accuracy for most of the stations. 

# #### Temperature forecasted by LSTM model <a class="anchor" id="12"></a>
# 
# The forecasted temperature for the weather stations by the model is as follows:   

# In[49]:


# 30 days of forecast for October,2019 obtained from the LSTM model for each weather stations
lstm_prediction.head()


# The table above gives the daily forecast for the month of October for each of the 29-location using LSTM. The columns indicate the location of the weather stations and the rows are the forecasted temperature in degree Celsius for each day of the month starting from 2019-10-01.

# ### Support Vector Machine <a class="anchor" id="13"></a>
# 
# Support Vector Machine was first proposed by Vladimir Vapnik as a binary linear classifier in 1963 which was further developed in 1992 to a non linear classifier. This algorithm classifies data by creating hyperplanes between different classes using the maximal margin method. The maximum margin represented as epsilon(ε) is the maximum separation distance(2ε) that can be achieved between the nearest data points of the two classes. These data points which are critical for the hyperplane are known as support vectors. 
# 
# This study being a regression problem, here its regression variant also known as [Support Vector Regression (SVR)](https://papers.nips.cc/paper/1238-support-vector-regression-machines.pdf) is used, which was proposed in 1996 by Vapnik .et.al, suitable for regression in high dimensionality space. The algorithm uses the same maximal margin principle but instead of separating classes it creates a tube with a radius of epsilon(ε) to include the data points. The primary parameters for SVR are the kernel function and its coefficient required to map the data points to a higher dimension space, epsilon(ε) or tube radius, and C or cost as penalty.          
# 
# In the second method this machine learning algorithm of Support Vector Regression (SVR) is applied to compare performances by the deep learning framework, since it is suitable for fitting high dimensional data with comparatively fewer samples.
# 

# Accordingly a function is first created which would first receive the time series data, followed by fitting it using the ML model and forecasting daily for the month of October, 2019. 
# 
# The support vector regression here uses a radial basis function or rbf as the kernel, a moderate cost value of C=10 and an epsilon of 0.001. These are the critical parameters for the svr model, which can be further tuned for better result. This is then fitted on the train set and predicted on the test set, with accuracy of the fit measured in terms of R-squared.

# In[55]:


# fitting & Validating using SVR
def fit_svr(X_train_tract1_array, y_train_tract1_array, X_test_tract1_array, y_test_tract1_array):
    model_svr = SVR(kernel='rbf', gamma='auto', tol=0.001, C=10.0, epsilon=0.001)
    model_svr.fit(X_train_tract1_array,y_train_tract1_array)
    y_pred_train_tract1 = model_svr.predict(X_train_tract1_array)
    y_pred_test_tract1 = model_svr.predict(X_test_tract1_array)        
    print('r-square_SVR_Test: ', round(model_svr.score(X_test_tract1_array,y_test_tract1_array),2))
    return model_svr, y_pred_test_tract1   


# The forecasted value on the test set using the fitted model is estimated and included with the actual observed temperatures set for visualization

# In[56]:


# validation result  
def valid_result_svr(scaler, y_pred_test_tract1, station_value, split_row, lag):
    new_test_tract1 = station_value.iloc[split_row:]
    test_tract1_pred = new_test_tract1.iloc[lag:].copy()
    y_pred_test_tract1_transformed = scaler.inverse_transform([y_pred_test_tract1])
    y_pred_test_tract1_transformed_reshaped = np.reshape(y_pred_test_tract1_transformed,(y_pred_test_tract1_transformed.shape[1],-1))
    test_tract1_pred['Forecast'] = np.array(y_pred_test_tract1_transformed_reshaped)
    return test_tract1_pred


# Once the model is validated the fitted model is used to forecast temperature for user input days using past data. This is also estimated using one day walk forward method as mentioned in case of LSTM.

# In[57]:


# multi-step future forecast
def forecast_svr(X_test_tract1_array, days ,model_svr, lag, scaler):
    last_test_sample = X_test_tract1_array[-1]        
    X_last_test_sample = np.reshape(last_test_sample,(-1,X_test_tract1_array.shape[1]))        
    y_pred_last_sample = model_svr.predict(X_last_test_sample)                
    new_array = X_last_test_sample
    new_predict = y_pred_last_sample
    new_array = X_last_test_sample
    new_predict = y_pred_last_sample

    seven_days_svr=[]
    for i in range(0,days):               
            new_array = np.insert(new_array, 0, new_predict)                
            new_array = np.delete(new_array, -1)
            new_array_reshape = np.reshape(new_array, (-1,lag))                
            new_predict = model_svr.predict(new_array_reshape)
            temp_predict = scaler.inverse_transform([new_predict])
            seven_days_svr.append(temp_predict[0][0].round(2))
            
    return seven_days_svr 


# All the above methods are finally included in the main function which will take three input of the historical temperature data, number of lag data to be used and the number of days to be forecasted. 

# In[58]:


def england_temp_svr(all_station_temp, lag, days):     
    
    seven_day_forecast_svr = create_dates('2019-09-30',days)
    
    for i in range(len(all_station_temp.columns)):
        
        # preprocessing
        station_value, station_name = get_value_name(all_station_temp,i)       
        train_tract1, test_tract1, split_row = train_test_split(station_value, station_name, 0.80)              
        train_tract1_scaled_df, test_tract1_scaled_df, scaler = data_transformation(train_tract1,test_tract1)        
        train_tract1 = timeseries_feature_builder(train_tract1_scaled_df,lag+1)
        test_tract1 = timeseries_feature_builder(test_tract1_scaled_df, lag+1)        
        X_train_tract1_array, y_train_tract1_array, X_test_tract1_array, y_test_tract1_array = make_arrays(train_tract1,
                                                                                                           test_tract1)

        # SVR modeling
        model_svr, y_pred_test_tract1 = fit_svr(X_train_tract1_array, y_train_tract1_array,
                                                X_test_tract1_array, y_test_tract1_array)                       
        test_tract1_pred = valid_result_svr(scaler, y_pred_test_tract1, station_value, split_row, lag)        
        seven_days_svr = forecast_svr(X_test_tract1_array, days, model_svr, lag, scaler)            
        seven_day_forecast_svr[station_name] = np.array(seven_days_svr)        
        
        # plot result
        plt.figure(figsize=(20,5))
        plt.plot(test_tract1_pred)
        plt.plot(seven_day_forecast_svr[station_name], color='red', label='forecast') 
        plt.ylabel('Temperature(°C)')
        plt.legend(loc='upper right')
        plt.title(station_name + '- October 2019 Temperature Forecast')
        plt.show()    
        
    return(seven_day_forecast_svr)


# The function is now called on the same temperature dataset and outputs are recorded. It is given three input: first the historic temperature dataset, the number of lagged terms to be used for prediction and finally the number of days of forecasting.   

# In[60]:


get_ipython().run_cell_magic('time', '', '# Fitting and forecast using SVM\nsvr_prediction = england_temp_svr(all_station_temp, 365, 30)\n')


# In the above section the SVR model is trained on past time series data followed by forecasting temperature for the month of October,2019 for each of the stations. The results are plotted for each station with past observed temperatures in Blue, validation in Orange and forecast in Red. The validation closely matches the observed daily temperatures in most of the cases as also given by the test metrics of r-square with the highest values reaching up to 0.87 for the SVR model.    

# #### Temperature forecasted by SVR model <a class="anchor" id="14"></a>
# The output obtained by the above SVR model is visualized here which will now be processed to interpolate temperature for the whole of England. 

# In[61]:


# Daily forecast obtained from the SVR model for each of the weather stations
svr_prediction.head()


# ## Temperature Interpolation for England <a class="anchor" id="15"></a>

# Now the predictions obtained by the SVM method would be used for spatially mapping and estimating temperatures for the entire country for a certain day. Hence location of each weather station needs to be added to this table for the spatial operation. This is done in the following.    

# In[62]:


# Change station names to index
england_temp_forecast = svr_prediction.transpose()
# Make station names to a column
england_temp_forecast_station = england_temp_forecast.reset_index()
england_temp_forecast_station = england_temp_forecast_station.rename(columns={'index':'Station'})
# Join forecast temperature data to weather station location using the Staion column
predicted_temp_by_station = pd.merge(england_temp_forecast_station, england_weather_stations_layer_sdf, left_on='Station', right_on='Station', how='left')
predicted_temp_by_station.head()


# ### Result Visualization <a class="anchor" id="16"></a>

# Now the table above gives the daily temperature forecast for the month of October,2019 for all the 29 weather stations in England, with location of each station. Out of these predictions the first day of October is selected for creating a probable temperature surface for the entire country.

# In[63]:


# Select the first day out 
oct1st_temp = predicted_temp_by_station.iloc[:, [0,1,33,32]]
oct1st_temp = oct1st_temp.rename(columns={oct1st_temp.columns[1]:'temp_pred'})
oct1st_temp.head()


# The data is now converted into a spatial data frame in the following. 

# In[64]:


# convert dataframe to a spatial dataframe
sdf = oct1st_temp.spatial.from_xy(df=oct1st_temp, sr=4326, x_column='X', y_column='Y')
# create feature layer from the spatial dataframe
oct1st_temp_points = gis.content.import_data(sdf, title='eng_temp_points')


# In[65]:


oct1st_temp_points


# The [interpolate_points](https://developers.arcgis.com/python/api-reference/arcgis.features.analysis.html?highlight=interpolate%20points#arcgis.features.analysis.interpolate_points) method from the ArcGIS API for Python allows us to predict values at new locations based on measurements from a collection of points. The method takes point data with values at each point and returns areas classified by predicted values. 
# 
# For example, we are using interpolate_points tool to estimate temperature of England.
# The input parameters required for the tool are:
# - `input_layer`: feature layer having the predicted temperature for all the weather stations
# - `interpolate_options` specify the speed v/s accuracy on a range of 1 to 9 with 9 as the maximum accuracy
# - `field` indicates the column name in the feature layer which has the predicted temperatures
# - `bounding_polygon_layer` here is the England boundary shapefile which sets the boundary within which the spatial interpolation would be estimated
# - `output_name` the name of the resulting output interpolated surface.  
# 

# In[66]:


# Interpolate the predicted temperature data to create forecast surface for England
oct1st_temp_surface = interpolate_points(oct1st_temp_points, 
                                         interpolate_option=9,                                         
                                         field='temp_pred', 
                                         bounding_polygon_layer=england_boundary_layer,
                                         output_name='Interpolated Temperature'+ str(dt.now().microsecond))


# In[67]:


oct1st_temp_surface


# In[79]:


# Plot the interpolated temperature surface
eng_interpolated_temp = gis.map('England', zoomlevel=6)
eng_interpolated_temp.add_layer(oct1st_temp_surface)
eng_interpolated_temp.legend = True
eng_interpolated_temp


# The interpolated forecast temperature surface above for England shows that temperature gradually increases from the northern to the southern part of the country, and ranges from a minimum of 9.5 degrees to a maximum of 17.43 degree Celsius respectively, with two distinct zones dividing the country. Lower temperatures are prevalent in the interior part of the country compared to the coastal belts, with London falling in the maximum temperature zone. This is expected since coastal regions are usually warmer than inland areas due to different climatic conditions.  

# ## Conclusion<a class="anchor" id="17"></a>

# In this notebook, a timeseries temperature dataset was used to predict daily temperature for England for the month of October using historic data from 29 weather stations. Two methods were used for the study — first a deep learning framework of LSTM was used followed by a machine learning method of Support Vector Regression. Both the models were able to model the data relatively well as evident from the high accuracy obtained for both train and test set. However, the SVM model was consistently performing better than the LSTM model with higher accuracy. Some instances of low accuracy for both the models were caused for certain stations due to less training data points available. 
# 
# Similarly, interesting differences were observed while forecasting daily temperatures using the two models. Here the SVR model was able to capture the daily fluctuations in greater detail which looked more real compared to the forecast returned by the LSTM model, as observed from the forecast plots. Besides, the total run time for the SVR model for fitting the data was only  a few minutes and was far less than the time taken by the LSTM model, which was training for 100 epochs for each station. 
# 
# Finally, the sample shows how deep learning or machine learning models could be combined with spatial tools in ArcGIS to process timeseries data and produce meaningful results that could be valuable for various industry verticals. 

# ## Summary of methods used <a class="anchor" id="18"></a>

# | Method | Question | Examples |
# | -| - |-|
# | interpolate_points| What is the value of any points located between other given points?  |Predicting temperature at any intermediate location based on measurement from surrounding station |
# 

# ## Data resources <a class="anchor" id="19"></a>

# | Shapefile | Source | Link |
# | -| - |-|
# | england_border| Counties and Unitary Authorities (December 2016) Full Clipped Boundaries in England and Wales  |https://data.gov.uk|
# | england_weather_stations| Weather for 243 countries of the world  |https://rp5.ru/Weather_in_the_world|
# | table(england historic weather data)| Weather for 243 countries of the world  |https://rp5.ru/Weather_in_the_world|   
# 
# | Articles | Source | Link |
# | -| - |-|
# | LONG SHORT-TERM MEMORY| Neural Computation 9(8):1735{1780, 1997  |http://www.bioinf.jku.at/publications/older/2604.pdf|
# | Support Vector Regression Machines | Bell Labs and Monmouth University Department of Electronic Engineering |https://papers.nips.cc/paper/1238-support-vector-regression-machines.pdf|
# 
# 


# ====================
# tonga_volcano_eruption_2022.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Tonga Volcano Eruption - SO2 Analysis

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Read the event from feeds](#Read-the-event-from-feeds)
# * [Visualize the extent of damage](#Visualize-the-extent-of-damage)
# * [Remote Sensing and Image Processing](#Remote-Sensing-and-Image-Processing)
#   * [Visual Assessment (Select before and after rasters)](#Visual-Assessment-(Select-before-and-after-rasters)
# * [SO2 Analysis](#SO2-Analysis)
#   * [Getting the data](#Getting-the-data)
#   * [Time Series SO2 Plot](#Time-Series-SO2-plot)
#   * [Visualize with Scatterplots](#Visualized-with-Scatterplots)
#   * [Visualize with Heatmap](#Visualize-with-a-heatmap)
# * [Summary](#Summary)
# * [References](#References)

# ## Introduction

# From December 20th, 2021, a volcanic eruption began on `Hunga Tonga–Hunga Ha'apai`, a submarine volcano in the Tongan archipelago in the southern Pacific Ocean. Nearly four weeks later, the eruption reached a very large and powerful climax (at around January 15th, 2022). Location-wise, `Hunga Tonga–Hunga Ha'apai` is 65 km (~40 mi) north of Tongatapu, the country's main island, and is part of the highly active `Tonga–Kermadec Islands volcanic arc`, a subduction zone extending from New Zealand north-northeast to Fiji. The eruption caused tsunamis in Tonga, Fiji, American Samoa, Vanuatu, and along the Pacific rim, including damaging tsunamis in New Zealand, Japan, the United States, the Russian Far East, Chile, and Peru. Named as "hundreds of times more powerful" than the atomic bomb dropped on Hiroshima by NASA [[1]](./tonga_volcano_eruption_2022.ipynb#Reference), the event was probably the largest volcanic eruption in the 21st century, and the largest recorded since 1991. 
# 
# A satellite animation from [Wikipedia (Image collected by Himawari-8)](https://en.wikipedia.org/wiki/2022_Hunga_Tonga%E2%80%93Hunga_Ha%27apai_eruption_and_tsunami) showed a massive explosion with a lot of smoke.
# 
# ![Satellite animation of the eruption on Hunga Tonga–Hunga Ha'apai on Jan 15, 2022. Image from Wikipedia](https://miro.medium.com/max/960/1*evpCJ-6NT8SKcbZM5KqxGQ.gif)

# ## Read the event from feeds
# 
# First, we need to run a set of commands to parse the `NASA's EONET Feeds` [[2]](./tonga_volcano_eruption_2022.ipynb#Reference), in order to grasp volcano eruptions happening worldwide, and the detailed information for the Tonga Volcano eruption.

# In[1]:


import json
from arcgis.geometry import Geometry
import requests
import pandas as pd
from arcgis.features import Feature, FeatureSet, FeatureCollection, GeoAccessor
from arcgis.gis import GIS
from arcgis.mapping import MapImageLayer
from arcgis.geometry import Point
from arcgis.geometry.functions import buffer


# In[2]:


""" read the response from HTTP request, and load as JSON object;
    all "events" in original JSON will be stored as "features"
"""
response = requests.get("https://eonet.gsfc.nasa.gov/api/v3/events/geojson?category=Volcanoes,earthquakes&start=2019-12-01&end=2022-11-30&status=closed")  
obj = json.loads(response.text)


# In[3]:


fset = FeatureSet.from_geojson(obj)
fc = FeatureCollection.from_featureset(fset, symbol=None, 
                                       name="Natural Disaster Feed Events Feature Collection")
fc.query()


# In[4]:


df = fc.query().sdf
# drop unnecessary columns
cols = [col for col in df.columns if col not in ['OBJECTID', 'SHAPE', 'link', 'index', 'sources']]

df3 = df[cols]
df3 = df3[df3['title'].str.contains("Tonga") == True]


# In[5]:


df3


# The most relevant search result appears to be the `47th` entry of the truncated DataFrame. Now let's locate this entry from the DataFrame with complete information. We can tell the link to the event is at `"https://eonet.gsfc.nasa.gov/api/v2.1/events/EONET_5985"`, while the source url appears to be `"https://volcano.si.edu/volcano.cfm?vn=243040"`.

# In[20]:


df_entry = df[df['title'].str.contains("Tonga") == True].loc[47]
df_entry


# In[6]:


map_g = GIS().map("Tonga")


# In[11]:


# Global volcano eruptions from end of year 2019 till now
map_g.clear_graphics()
map_g.remove_layers()
map_g


# Per information provided in the EONet Feed, Tonga Volcano Eruption site is located as a point geometry in the [feed entry](https://eonet.gsfc.nasa.gov/api/v2.1/events/EONET_5985). We can now create a Point geometry, and draw the buffered area in the map.

# In[8]:


pt = Point({"x" : -175.382, "y" : -20.536, 
            "spatialReference" : {"wkid" : 4326}})


# In[9]:


pt_buffered = pt.buffer(distance=10)
map_g.draw(pt_buffered)


# Up to this point, we should be seeing a buffered area around the quake center on the map widget, run the cell below to add the volcano and earthquake events from the recent three years to the map as well. Remember to zoom in to the buffered zone to get a clearer picture of nearby quakes.

# In[10]:


# Filter based on disaster type, and each type is assigned a different symbology
for ea in fc.query():
    
    title = ea.get_value('title')
    obj_id = ea.attributes['OBJECTID']
    obj_type = ea.geometry_type
    
    if "Volcano" in title and obj_type == 'Point':
        df_sel = df[df['OBJECTID']==obj_id]
        df_sel.spatial.plot(map_widget= map_g,
                            name=title,
                            renderer_type='s',
                            symbol_type='simple',
                            symbol_style='d', # d - for diamonds
                            colors='Reds_r',
                            cstep=50,
                            outline_color='Blues',
                            marker_size=10)
    elif "quake" in title and obj_type == 'Point':
        df_sel = df[df['OBJECTID']==obj_id]
        df_sel.spatial.plot(map_widget= map_g,
                            name=title,
                            renderer_type='s',
                            symbol_type='simple',
                            symbol_style='+', # + - for crosses
                            colors='khaki',
                            cstep=50,
                            outline_color='Blues',
                            marker_size=10)


# ## Visualize the extent of damage
# 
# Next, we can further look into the Tonga Volcanic Eruption with different informational layers, e.g. the `Magnitudes and Shake Intensities of Recent Earthquakes`, `Significant Global Volcanic Eruptions`, and `Sea Floor Crustal Age`.

# In[21]:


gis = GIS(profile="your_online_profile")


# In[22]:


from arcgis.mapping import WebMap
webmap = gis.content.get("e0abd2028f4043d89a4a907674d270ff")


# In[113]:


wm_obj = WebMap(webmap)
wm_obj


# Details about layers being shown below:
# 
# (1) Recent Earthquakes
#  - This service presents recent earthquake information from the USGS Prompt Assessment of Global Earthquakes for Response (PAGER) program.
#  - In addition to displaying earthquakes by magnitude, this service also provide earthquake impact details. Impact is measured by population as well as models for economic and fatality loss. For more details, see: [PAGER Alerts](https://earthquake.usgs.gov/earthquakes/pager).
# 
#   - Events are updated as frequently as every 5 minutes and are available up to 30 days with the following exceptions:
#   - Events with a Magnitude LESS than 3.0 are retained for 3 days
#   - Events with a Magnitude LESS than 4.5 are retained for 7 days
#   
#  - In addition to event points, ShakeMaps are also provided. These have been dissolved by Shake Intensity to reduce the Layer Complexity.
#  - The specific layers provided in this service have been Time Enabled and include:
#   - Events by Magnitude: The event’s seismic magnitude value.
#     - Contains PAGER Alert Level: USGS PAGER (Prompt Assessment of Global Earthquakes for Response) system provides an automated impact level assignment that estimates fatality and economic loss.
#     - Contains Significance Level: An event’s significance is determined by factors like magnitude, max MMI, ‘felt’ reports, and estimated impact.
#   - Shake Intensity: The Instrumental Intensity or Modified Mercalli Intensity (MMI) for available events.
# 
# 
# (2) Significant Global Volcanic Eruptions
#  - This feature layer, utilizing data from the National Oceanic and Atmospheric Administration (NOAA), displays global locations of significant volcanic eruptions. A significant eruption is classified as one that meets at least one of the following criteria: 
#   - Caused fatalities
#   - Caused moderate damage (approximately 1 million or more)
#   - Has a Volcanic Explosivity Index (VEI) of 6 or larger 
#   - Caused a tsunami
#   - Was associated with a major earthquake
#   
#   
# (3) Sea Floor Crustal Age:
#  - Scientists use the magnetic polarity of the sea floor to determine the age. Very little of the sea floor is older than 150 million years. This is because the oldest sea floor is subducted under other plates and replaces by new surfaces. The tectonic plates are constantly in motion and new crust is always being created. This continual motion is evidenced by the occurrence of earthquakes and volcanoes. Data accessed from [here](https://www.ngdc.noaa.gov/mgg/ocean_age/ocean_age_2008.html).

# In[23]:


# Recent Earthquakes
quake = gis.content.get("9e2f2b544c954fda9cd13b7f3e6eebce")
# Significant Global Volcanic Eruptions
erupt = gis.content.get("3318aafcd304414bb8da481d173c551d")
# sea floor crustal age
seafloor = gis.content.get("aa1c10c58c894467a1ade5a43ed70c19")


# Use the `side_by_side` method defined below to display these four informational maps regarding Tonga Volcano eruption, and note that, `sync_navigation` is used here to display the four maps in a synchronous manner reacting to zoom in/out and drag/drop.

# In[24]:


from ipywidgets import *
from arcgis.geocoding import geocode

def side_by_side(address, item2, item3, item4, item1="satellite", 
                 label2="Recent earthquakes", label3="Significant eruptions",
                 label4="Sea Floor Crustal Age"):
    location = geocode(address)[0]

    satmap1 = gis.map(location)
    satmap1.basemap = item1

    satmap2 = gis.map(location)
    satmap2.add_layer(item2)

    satmap1.layout=Layout(flex='1 1', padding='6px', height='450px')
    satmap2.layout=Layout(flex='1 1', padding='6px', height='450px')

    label1 = HBox([Label("Satellite"), Label(label2)])
    box1 = HBox([satmap1, satmap2])
    
    # set hbox layout preferences
    hbox_layout = Layout()
    hbox_layout.justify_content = 'space-around'
    label1.layout = hbox_layout
    
    # sync left and right maps
    satmap1.sync_navigation(satmap2)
    
    satmap3 = gis.map(location)
    satmap3.add_layer(item3)

    satmap4 = gis.map(location)
    satmap4.add_layer(item4)

    satmap3.layout=Layout(flex='1 1', padding='6px', height='450px')
    satmap4.layout=Layout(flex='1 1', padding='6px', height='450px')

    label2 = HBox([Label(label3), Label(label4)])
    box2 = HBox([satmap3, satmap4])
    label2.layout = hbox_layout
    
    # sync left and right maps
    satmap1.sync_navigation(satmap3)
    satmap3.sync_navigation(satmap4)
    
    return VBox([label1, box1, label2, box2])


# In[26]:


side_by_side("Hunga Tonga–Hunga Ha'apai", quake.layers[0], erupt, seafloor)


# ## Remote Sensing and Image Processing

# ### Visual Assessment (Select before and after rasters)
# 
# A way to visually assess the damage caused by the volcano eruption is from comparing the before- and after-eruption images side by side. However, the high-resolution imagery services covering the area are not easily accessible publicly. Just for demonstration purposes, we will now look at two sample imagery provided by `Maxar`'s `Open Data Program` (To support the humanitarian community, [Maxar](https://blog.maxar.com/for-a-better-world/2022/open-data-response-to-the-volcano-eruption-in-tonga?utm_source=blog&utm_medium=organic&utm_campaign=tonga-water-analysis) publicly releases data of the affected areas to support response efforts as part of their [OPEN DATA PROGRAM](https://www.maxar.com/open-data/tonga-volcano?utm_source=blog&utm_medium=organic&utm_campaign=odp-tonga-volcano)).

# Follow the instructions in [OPEN DATA PROGRAM](https://www.maxar.com/open-data/tonga-volcano?utm_source=blog&utm_medium=organic&utm_campaign=odp-tonga-volcano) to download the before- and after- disaster images into your local download path, e.g. `"<Your download folder>"`.

# In[5]:


l_path = r"<Your download folder>\10_homes-and-buildings-before-main-eruption_29dec2021_wv2.jpg"
r_path = r"<Your download folder>\11_ash-covered-homes-and-buildings-after-main-eruption_18jan2022_wv2.jpg"


# Then use the `side_by_side2` method defined below to initialize the raster images into a `Raster` data object, and set the two images up for display side-by-side.

# In[3]:


import numpy
import matplotlib.pyplot as plt
from arcgis.raster import Raster


# In[4]:


def side_by_side2(address, raster1, raster2, r_extent, opa=1):
    location = geocode(address)[0]

    satmap1 = gis.map(location)
    item1 = Raster(raster1, extent=r_extent, opacity = opa)
    satmap1.add_layer(item1)

    satmap2 = gis.map(location)
    item2 = Raster(raster2, extent=r_extent, opacity = opa)
    satmap2.add_layer(item2)

    satmap1.layout=Layout(flex='1 1', padding='6px', height='450px')
    satmap2.layout=Layout(flex='1 1', padding='6px', height='450px')

    box = HBox([satmap1, satmap2])
    
    return box


# In[10]:


extent = {"xmin":-175.38255,
          "ymin":-20.536216,
          "xmax":-175.38244,
          "ymax":-20.536941,
          "spatialReference":{"wkid":4326}
         }
side_by_side2("Hunga Tonga–Hunga Ha'apai", 
              l_path,
              r_path,
              r_extent=extent)


# ## SO2 Analysis
# 
# Tonga's `Hunga-Tonga-Hunga-Ha'apai volcano` erupted on Saturday, January 15, 2022, with a huge plume of ash, gas and steam that spewed up to 13 miles into the atmosphere. The mushroom-like cloud covered the entire South Pacific Island kingdom and the tsunami that coastal areas experienced was frightening for many but rose 3 feet (80 centimeters), allowing most to escape. The smoke people saw naturally consists of water vapor, carbon dioxide (CO2), sulfur dioxide (SO2), and ash.
# 
# After being released, SO2 is converted to sulfate aerosols at a high altitude. The aerosols are able to reflect sunlight and cool the Earth’s climate, and they are also a cause of ozone depletion. With massive eruptions, SO2 can be injected more than 10 km into the atmosphere. However, the released SO2 is hard to observe due to its invisibility. Now, let's explore how we can visualize SO2 movement after the eruption with a scatter plot and heat map.
# 
# <b>Note:</b> The follow procedures require the use of `seaborn`, `h5py` and `imageio` packages. If not installed previously, please run the following lines to start the installation:

# In[ ]:


get_ipython().system('pip install seaborn')


# In[ ]:


get_ipython().system('pip install h5py')


# In[ ]:


get_ipython().system('pip install imageio')


# In[46]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import h5py

get_ipython().run_line_magic('matplotlib', 'inline')


# ### Getting the data
# 
# `NASA`'s Goddard Earth Sciences Data and Information Services Center ([GES DISC](https://disc.gsfc.nasa.gov/)) has provided free access (and downloads) of the world-wide atmospheric composition data, including SO2, at its [portal](https://disc.gsfc.nasa.gov/data-access). <b>Note:</b> the data is only downloadable after user sign-up with an Earthdata account [[2]](./tonga_volcano_eruption_2022.ipynb#Reference). After creating the Earthdata account, user needs to link the [GES DISC](https://disc.gsfc.nasa.gov/) with the account and verify if downloading is then allowed by downloading the example data file from [URL](https://acdisc.gesdisc.eosdis.nasa.gov/data//Aqua_AIRS_Level3/AIRX3STD.006/2006/AIRS.2006.12.31.L3.RetStd001.v6.0.9.0.G13155192744.hdf).
# 
# We are going to derive the SO2 data from the [OMPS_NPP_NMSO2_PCA_L2 data product](https://disc.gsfc.nasa.gov/datasets/OMPS_NPP_NMSO2_PCA_L2_2/summary), which is a Level 2 orbital-track volcanic and anthropogenic sulfur dioxide product for the Ozone Mapping and Profiler Suite (OMPS) [Nadir Mapper (NM)](https://ozoneaq.gsfc.nasa.gov/omps/) onboard the NASA/NOAA Suomi National Polar-orbiting Partnership (SNPP) satellite [[5, 7]](./tonga_volcano_eruption_2022.ipynb#Reference).
# 
# When making temporal selections to the search, make sure to choose from <b>January 12th to 23rd</b> since the eruption of `Hunga Tonga–Hunga Ha'apai` reached its climax on January 15, 2022, and normally the decay period is around 10 days. When getting data, we will be instructed by the portal to download a text file (.txt), which can not only be used in the scripted downloading process, but it will also help us obtain a list of the file names in the next cell. The step-by-step instructions on how to use `wget` to download data into one's Windows and `curl` to Mac or Linux environment can be found in the [NASA GSFC's instruction page](https://disc.gsfc.nasa.gov/data-access).
# 
# Now, assuming that we have followed the instructions to register for a new account, link the `GES DISC` with the account, select required data for downloads, and have the download script (.txt) ready as in `"<Your download folder>\5608382643-download.txt"`, let's get to analysis and visualizaiton:

# In[47]:


f = open(r"<Your download folder>\5608382643-download.txt", 'r')
files = [i.split('/')[-1] for i in f.read().split('\n')[1:]]
files[0:5]


# ##### Using XArray
# 
# <b>Note:</b> The follow procedures require the use of `xarray`, `netCDF4` and `h5netcdf` packages. If not installed previously, please run the following lines to start the installation:

# In[ ]:


get_ipython().system('pip install xarray')


# In[ ]:


get_ipython().system('pip install netCDF4')


# In[ ]:


get_ipython().system('pip install h5netcdf')


# In[1]:


import xarray as xr
import netCDF4


# In[21]:


location_data = [xr.open_dataset(r'<Your download folder>' + '/' + file, 
                                 group="GEOLOCATION_DATA", engine="h5netcdf") for file in files]
location_data[0]


# In[38]:


location_data[0].groupby("Time").mean()


# In[20]:


science_data = [xr.open_dataset(r'<Your download folder>' + '/' + file, 
                                group="SCIENCE_DATA", engine="h5netcdf") for file in files]
science_data[0]


# In[39]:


science_data[0].groupby("ColumnAmountSO2").mean()


# ##### Using h5py and imageio

# In[48]:


dataset = []
for file in files:
    try:
        h5_read = h5py.File(r'<Your download folder>' + '/' + file, 'r')
        dataset.append(h5_read)
    except:
        print(file)


# In[49]:


list(dataset[0].keys())


# In[50]:


list(dataset[0]['GEOLOCATION_DATA'].keys()) 


# In[51]:


list(dataset[0]['SCIENCE_DATA'].keys())


# In[52]:


def exact_h5(h5_file,key):
    result = list(h5_file[key])
    return result


# In[53]:


# Latitude
get_lat = [exact_h5(i,'GEOLOCATION_DATA/Latitude') for i in dataset]
lat_ = [sum([list(ii) for ii in list(i)],[]) for i in get_lat]
lat = sum(lat_,[])

# Longitude
get_lon = [exact_h5(i,'GEOLOCATION_DATA/Longitude') for i in dataset]
lon_ = [sum([list(ii) for ii in list(i)],[]) for i in get_lon]
lon = sum(lon_,[])

# SO2 in Dobson Units (1 DU = 2.69 ∙1016 molecules/cm2)
get_so2 = [exact_h5(i,'SCIENCE_DATA/ColumnAmountSO2') for i in dataset]
so2_ = [sum([list(ii) for ii in list(i)],[]) for i in get_so2]
so2 = sum(so2_,[])

# Date
get_date = [exact_h5(i,'GEOLOCATION_DATA/UTC_CCSDS_A') for i in dataset]
date_ = [[ii.decode('UTF-8').split('T')[0] for ii in i] for i in get_date] 
date = sum([list(np.repeat(i,36)) for i in date_],[])


# What's worth pointing out, is that we have to convert the longitude range. Because the longitude range of a typical map is between -180 and 180, and if we directly plot the location of Hunga Tonga–Hunga Ha'apai, which is located at longitude -175.4, then the movement of SO2 will be cut due to locating close to the range's limit (-180). In the first line of the cell below, we are modifying the longitude rage.

# In[54]:


### convert longitude range by making the 180th meridian a center.
### sum with 360 if the longitude is negative.

lon_mod = [360+i if i <0 else i for i in lon]

df = pd.DataFrame(zip(lat, lon_mod, so2, date),
                  columns=['Lat', 'Long', 'SO2', 'Date'])
df.head()


# In[55]:


df.tail()


# ### Time-Series SO2 plot
# 
# Next, in order to see the daily amount of SO2 surrounding the volcano, we need to create a filtered DataFrame selecting only the amount of SO2 within a square area of 15 degrees of latitude and longitude from the eruption point. We will also select only rows with positive SO2 values.

# In[56]:


#Hunga Tonga–Hunga Haʻapai: latitude -20.55, longitude -175.39 (converted to 184.61) 

df_filtered = df[(df['Lat'] >= -35.55) & (df['Lat'] <= -5.45) &
                 (df['Long'] >= 169.61) & (df['Long'] <= 199.61)&
                 (df['SO2'] > 0)
                ]
df_filtered.head()


# In order to preserve the DataFrame, we can now export it to a FeatureClass (either saved in a local shapefile, or a CSV). Then if the notebook gets restarted accidentally, we can start from here (instead of revisiting the download and parsing processes).

# In[25]:


df_filtered.spatial.to_featureclass(location="./tonga_volcano_so2.shp")


# In[60]:


df_filtered.to_csv("./tonga_volcano_so2.csv")


# With the intermediate results saved into a local CSV file, we can always start from this checkpoint without re-doing the previous steps - just import the table via:
# ```
# import pandas as pd
# df_filtered = pd.DataFrame.spatial.from_table(filename='/arcgis/home/tonga_volcano_so2.csv')
# ```

# In[12]:


df_groupdate = df_filtered[['SO2','Date']].groupby('Date').sum()
df_groupdate = df_groupdate.reset_index()
df_groupdate


# The table above shows that the daily amount of SO2 reached the maximum point on January 16, which is the same as below, when the graph shows that SO2 surrounding the island had increased drastically from 15 to January 16, when the explosion occurred.

# In[13]:


sns.set_style('darkgrid')
df_groupdate.plot(x='Date', y='SO2', figsize = (11,6), lw=4)

plt.legend(labels= ['SO2'])
plt.ylabel('Dobson Units')
plt.show()


# ### Visualize with Scatterplots
# 
# With latitude, longitude, and SO2 columns, the DataFrame can be plotted as dots with colors in accordance with location and density. Let's start with creating a list of dates to filter the DataFrame. Only rows with SO2 more than 0.5 DU are selected to avoid the excessiveness of data when we plot them.

# In[14]:


df = df_filtered

dates = list(set(list(df['Date'])))
dates.sort()
dates = dates[1:]    #remove January 11

df_plot = [df[(df['Date']==i) & (df['SO2'] > 0.5)] for i in dates]


# Take the single day of Jan 17th as an example, let us make a scatter plot of the amount of SO2, a few days after the eruption.

# In[15]:


sns.set_style('darkgrid')
plt.figure(figsize=(8,6))

#scatter plot
g = sns.scatterplot(data=df_plot[5], x='Long', y='Lat',
                    palette='coolwarm', hue='SO2', linewidth=0.01,
                    hue_norm=(0.5,5.5))
g.set(xlim=(65, 270))
g.set(ylim=(-55, 36))

#plot Hunga Tonga–Hunga Haʻapai' location
plt.scatter(184.615, -20.55, color='gray', marker = '^')

#Hunga Tonga–Hunga Haʻapai' text
plt.text( 191, -21.5, '<= Hunga Tonga–Hunga Haʻapai', horizontalalignment='left',
         size=9, color='Black')

#date text
plt.text( 80, -50, '2022-01-17', horizontalalignment='left',
         size=10, color='Black')

plt.legend('')
plt.xlabel('')
plt.ylabel('Latitude')
plt.xticks([])
plt.show()


# Now, we can successfully visualize the scatter plot of a single day, let's plot every daily scatter plot and store them in a list, then combine these daily plots into a GIF file to see the progress.

# In[17]:


#create a list of numbers for use as saving name
save_order = [i+1 for i in list(range(len(df_plot)))]

for i,d,so in zip(df_plot, dates, save_order):
    sns.set_style('darkgrid')
    plt.figure(figsize=(8,6))
    
    #scatter plot
    g = sns.scatterplot(data=i, x='Long', y='Lat',
                        palette='coolwarm', hue='SO2', linewidth=0.01,
                        hue_norm=(0.5,5.5))
    g.set(xlim=(65, 270))
    g.set(ylim=(-55, 36))
    plt.legend('')
    
    #plot Hunga Tonga–Hunga Haʻapai' location
    plt.scatter(184.615, -20.55, color='gray', marker = '^')

    #Hunga Tonga–Hunga Haʻapai' text
    plt.text( 191, -21.5, '<= Hunga Tonga–Hunga Haʻapai', horizontalalignment='left',
             size=9, color='Black')

    #date text
    plt.text( 80, -50, d, horizontalalignment='left',
             size=10, color='Black')
    
    plt.title('')
    plt.xlabel('')
    plt.ylabel('Latitude')
    plt.xticks([])
    
    #export as PNG files, output location can be changed
    plt.savefig(str(so) + '.png', bbox_inches='tight',pad_inches = 0)
    plt.show()


# In[18]:


from PIL import Image
import imageio

# read PNG files
img = []
for i in save_order[0:-1]:           
    myImage = Image.open(str(i) + ".png")
    img.append(myImage)

#export the GIF file, output location can be changed
imageio.mimsave('so2_2.gif', img, duration=0.3)


# In[23]:


from IPython.display import Image
Image(data=open('so2_2.gif','rb').read(), format='png')


# Now we have created local snapshots of the SO2 expansion over the area from Jan 13th to Jan 20th, 2022, let's explore further into how we can preserve the maps and data across organization. The ArcGIS API for Python also faciliates your publishing and storage of Web Layers and Web Maps onto ArcGIS Online organization or Enterprise.

# ### Visualize with a heatmap

# In[75]:


m1 = GIS().map(mode="2D")
m1.extent = {'spatialReference': {'latestWkid': 3857, 'wkid': 102100},
             'xmin': -20601969.898812853,
             'ymin': -3220149.6465940434,
             'xmax': -18195120.752169862,
             'ymax': -2241755.684544048}
m1


# A `Spatially Enabled DataFrame` in ArcGIS API for Python can be plotted in the map widget with the renderer set as `HeatMap`. Now we will use `SeDF.spatial.plot(...)` to render the SO2 amount as heat map, for the first day of the list.

# In[117]:


this_frame = df_filtered[df_filtered['Date']=='2022-01-12']
this_frame.head()


# In[118]:


m1.clear_graphics()
this_frame.spatial.plot(map_widget = m1,
                        renderer_type='h', # specify the heatmap renderer using its notation 'h'
                        visual_variables={"blur_radius":30, "field":"SO2", "ratio":0.1},
                        alpha=0.75
                       )


# Now rendering the 1st day is done, let's go through all days in the list and have the map widget render these maps in a sequence.

# In[ ]:


import time

for i in dates:
    time.sleep(3)
    m1.clear_graphics()
    m1.remove_layers()
    new_frame = df_filtered[df_filtered['Date']==i]
    new_frame.spatial.plot( map_widget = m1,
                            renderer_type='h', # specify the heatmap renderer using its notation 'h'
                            visual_variables={"blur_radius":30, "field":"SO2", "ratio":0.1},
                            alpha=0.75
                           )
    print('Tonga_SO2_' + str(i))


# An alternative approach is to add the FeatureLayer from remote GIS to local notebook, and view the time-specific heatmap display of the SO2 values via a time slider:

# In[66]:


from datetime import datetime
m1.time_slider = True
m1.set_time_extent(start_time=datetime(2022,1,13), end_time=datetime(2022,1,21), interval=1, unit='days')


# In[72]:


fl_item = gis.content.search("Tonga Volcano type:Feature Service")[0]
m1.remove_layers()
m1.add_layer(fl_item, {"renderer":"HeatmapRenderer",
                       "field": "SO2",
                       "opacity":0.75})


# Last but not least, we can also view from the Web Map item being saved on our GIS instance:

# In[76]:


from arcgis.mapping import WebMap
wm_item = gis.content.search("Tonga Volcano Eruption")[0]
wm = WebMap(wm_item)
wm


# ## Summary
# 
# When facing natural disasters, human beings sometimes seem to be really vulnerable and helpless. The GIS and Science communities, though always trying to help with the monitoring and mitigation, are lack of real-time support and resources, e.g. in this case, in lack of real-time data to help monitor/predict the event, and later observe the CO2 and SO2 movement. 
# 
# This notebook has indicated, though SO2 is hard to observe with bare eyes, there are still ways to fetch information, visualize and analyze the volcano eruption events and SO2 releases with NASA and Maxar's data, and ArcGIS API for Python, seaborn, h5py, etc. as libraries.

# ## References
# 
# [1] CNN News Report, https://www.cnn.com/2022/01/24/asia/tonga-hiroshima-bomb-volcano-intl-hnk-scn-scli
# 
# [2] Earth Observatory Natural Event Tracker (EONET), https://eonet.gsfc.nasa.gov/what-is-eonet
# 
# [3] Bhanot, K. (2020, August 12). Getting NASA data for your next geo-project. Medium. Retrieved April 12, 2022, from https://towardsdatascience.com/getting-nasa-data-for-your-next-geo-project-9d621243b8f3
# 
# [4] Python-visualization. (2020). Folium. Retrieved from https://python-visualization.github.io/folium
# 
# [5] Agency for Toxic Substances and Disease Registry (ATSDR). 1998. Toxicological profile for Sulfur Dioxide. Atlanta, GA: U.S. Department of Health and Human Services, Public Health Service.
# 
# [6] Boriharn K, https://towardsdatascience.com/visualize-the-invisible-so2-with-nasa-data-and-python-2619f8ed4ea1
# 
# [7] Can Li, Nickolay A. Krotkov, Peter Leonard, Joanna Joiner (2020), OMPS/NPP PCA SO2 Total Column 1-Orbit L2 Swath 50x50km V2, Greenbelt, MD, USA, Goddard Earth Sciences Data and Information Services Center (GES DISC), Accessed: Mar 19, 2022, 10.5067/MEASURES/SO2/DATA205


# ====================
# track_river_pollutants.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Network Analysis: Investigate Chennai Floods
# 
# * 👟 Ready To Run!
# * 🔬 Data Science
# 
# __Requirements__
# * 🗄️ Utility Service Configuration: Geocoding
# * 🗄️ Utility Service Configuration: Hydrology
# 
# On December 1–2, 2015, the Indian city of Chennai received more rainfall in 24 hours than it had seen on any day since 1901. The deluge followed a month of persistent monsoon rains that were already well above normal for the Indian state of Tamil Nadu. At least 250 people had died, several hundred had been critically injured, and thousands had been affected or displaced by the flooding that ensued.

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Network-Analysis:-Investigate-Chennai-floods" data-toc-modified-id="Network-Analysis:-Investigate-Chennai-floods-1">Network Analysis: Investigate Chennai floods</a></span><ul class="toc-item"><li><span><a href="#Summary-of-this-sample" data-toc-modified-id="Summary-of-this-sample-1.1">Summary of this sample</a></span></li><li><span><a href="#Chennai-Floods-Explained" data-toc-modified-id="Chennai-Floods-Explained-1.2">Chennai Floods Explained</a></span></li><li><span><a href="#How-much-rain-and-where?" data-toc-modified-id="How-much-rain-and-where?-1.3">How much rain and where?</a></span></li><li><span><a href="#Spatial-Analysis" data-toc-modified-id="Spatial-Analysis-1.4">Spatial Analysis</a></span></li><li><span><a href="#What-caused-the-flooding-in-Chennai?" data-toc-modified-id="What-caused-the-flooding-in-Chennai?-1.5">What caused the flooding in Chennai?</a></span><ul class="toc-item"><li><span><a href="#A-wrong-call-that-sank-Chennai" data-toc-modified-id="A-wrong-call-that-sank-Chennai-1.5.1">A wrong call that sank Chennai</a></span></li></ul></li><li><span><a href="#Flood-Relief-Camps" data-toc-modified-id="Flood-Relief-Camps-1.6">Flood Relief Camps</a></span><ul class="toc-item"><li><span><a href="#Routing-Emergency-Supplies-to-Relief-Camps" data-toc-modified-id="Routing-Emergency-Supplies-to-Relief-Camps-1.6.1">Routing Emergency Supplies to Relief Camps</a></span></li></ul></li></ul></li></ul></div>

# 

# The image above provides satellite-based estimates of rainfall over southeastern India on December 1–2, accumulating in 30–minute intervals. The rainfall data is acquired from the Integrated Multi-Satellite Retrievals for GPM (IMERG), a product of the [Global Precipitation Measurement](http://www.nasa.gov/mission_pages/GPM/main/index.html) mission. The brightest shades on the map represent rainfall totals approaching 400 millimeters (16 inches) during the 48-hour period. These regional, remotely-sensed estimates may differ from the totals measured by ground-based weather stations. According to Hal Pierce, a scientist on the GPM team at NASA’s Goddard Space Flight Center, the highest rainfall totals exceeded 500 mm (20 inches) in an area just off the southeastern coast.
# 
# [Source: NASA http://earthobservatory.nasa.gov/IOTD/view.php?id=87131]

# ## Summary of this sample
# This sample showcases not just the analysis and visualization capabilities of your GIS, but also the ability to store illustrative text, graphics and live code in a Jupyter notebook.
# 
# The sample starts off reporting the devastating effects of the flood. We plot the locations of rainfall guages and **interpolate** the data to create a continuous surface representing the amount of rainfall throughout the state.
# 
# Next, we plot the locations of major lakes and **trace downstream** the path floodwaters would take. We create a **buffer** around this path to show at-risk areas.
# 
# In the second part of the sample, we take a look at **time series** satellite imagery and observe the human impacts on natural reservoirs over a period of two decades.
# 
# We then vizualize the locations of relief camps and analyze their capacity using **pandas** and **matplotlib**. We **aggregate** the camps by district to understand which ones have the largest number of refugees.
# 
# In the last section, we perform a **routing** analysis to figure out the best path to route emergency supplies from storage to the relief camps.
# 
# First, let's import all the necessary libraries and connect to our GIS.

# In[1]:


from datetime import datetime as dt
from IPython.display import YouTubeVideo

from arcgis.gis import GIS
from arcgis.geocoding import geocode
from arcgis import features


# In[2]:


gis = GIS("home")


# 
# ## Chennai Floods Explained

# In[3]:


YouTubeVideo('x4dNIfx6HVs')


# The catastrophic flooding in Chennai is the result of the heaviest rain in several decades, which forced authorities to release a massive 30,000 cusecs (cubic feet per second) from the Chembarambakkam reservoir into the Adyar river over two days. This caused the river to flood its banks and submerge neighborhoods on both sides. It did not help that the Adyar’s stream is not very deep or wide, and its banks have been heavily encroached upon over the years.
# Similar flooding triggers were in action at Poondi and Puzhal reservoirs, and the Cooum river that winds its way through the city.
# While Chief Minister J Jayalalithaa said that damage was “inevitable” during the earlier phase of heavy rain, the fact remains that the mindless development of Chennai over the last two decades — <b>the filling up of lowlands and choking of stormwater drains and other exits for water — has played a major part in the escalation of the crisis.</b>
# 
# [Source: Indian Express http://indianexpress.com/article/explained/why-is-chennai-under-water/#sthash.LlhnqM4B.dpuf]

# ## How much rain and where?

# To get started with our analysis, we bring in a map of the affected region.

# In[4]:


map_chennai = gis.map("Chennai")
map_chennai


# We can search for content in our GIS and add layers to our map that can be used for visualization or analysis:

# In[5]:


data_groups = gis.groups.search('"Esri Sample Notebooks Data" owner:esri_notebook',
                                outside_org=True)
group_query = f"group: {data_groups[0].id}" if data_groups else ""
chennai_waste = gis.content.search(f"Solid Waste Management {group_query}",
                                   item_type="feature service",
                                   outside_org=True)[0]
chennai_waste


# In[6]:


map_chennai.add_layer(chennai_waste)


# In[7]:


map_chennai.take_screenshot()


# To get a sense of how much it rained and where, let's use rainfall data for December 2nd 2015, obtained from the Regional Meteorological Center in Chennai. Tabular data is hard to visualize, so let's bring in a map from our GIS to visualize the data:

# In[8]:


rainfall = gis.content.search(f"Chennai_precipitation {group_query}",
                              item_type="feature service",
                              outside_org=True)[0]
rainfall


# In[9]:


map_rainfall = gis.map("Tamil Nadu, India")
map_rainfall


# We then add this layer to our map to see the locations of the weather stations from which the rainfall data was collected:

# In[10]:


map_rainfall.add_layer(rainfall, {"renderer": "ClassedSizeRenderer",
                                  "field_name": "RAINFALL"})


# In[11]:


map_rainfall.take_screenshot()


# Here we used the **smart mapping** capability of the GIS to automatically render the data with proportional symbols.

# ## Spatial Analysis
# Rainfall is a continuous phenonmenon that affects the whole region, not just the locations of the weather stations. Based on the observed rainfall at the monitoring stations and their locations, we can interpolate and deduce the approximate rainfall across the whole region. We use the **Interpolate Points** tool from the GIS's spatial analysis service for this.
# 
# The Interpolate Points tool uses <a href="http://desktop.arcgis.com/en/desktop/latest/guide-books/extensions/geostatistical-analyst/what-is-empirical-bayesian-kriging-.htm">empirical Bayesian kriging</a> to perform the interpolation.

# In[12]:


interpolated_rf = features.analyze_patterns.interpolate_points(
    rainfall, field='RAINFALL')


# Let us create another map of Tamil Nadu state and render the output from Interpolate Points tool

# In[13]:


map_int = gis.map("Tamil Nadu")
map_int


# In[14]:


map_int.add_layer(interpolated_rf['result_layer'])


# In[15]:


map_int.take_screenshot()


# We see that rainfall was most severe in and around Chennai as well some parts of central Tamil Nadu.

# ## What caused the flooding in Chennai?

# ### A wrong call that sank Chennai
# Much of the flooding and subsequent waterlogging was a consequence of the outflows from major reservoirs into swollen rivers and into the city following heavy rains. The <b>release of waters from the Chembarambakkam reservoir</b> in particular has received much attention. [Source: The Hindu, http://www.thehindu.com/news/cities/chennai/chennai-floods-a-wrong-call-that-sank-the-city/article7967371.ece]

# In[16]:


map_water = gis.map("Chennai")
map_water


# Let's have a look at the major lakes and water reservoirs that were filled to the brim in Chennai due to the rains. We'll plot the locations of some of the reservoirs that had a large outflow during the rains.
# 
# To plot the locations, we use geocoding tools from the `tools` module. Your GIS can have more than one geocoding service, but for simplicity, the sample below chooses the first available geocoder to perform an address search.

# In[17]:


map_water.draw(geocode("Chembarambakkam, Tamil Nadu")[0],
               {"title": "Chembarambakkam", "content": "Water reservoir"})
map_water.draw(geocode("Puzhal Lake, Tamil Nadu")[0],
               {"title": "Puzhal", "content": "Water reservoir"})
map_water.draw(geocode("Kannampettai, Tamil Nadu")[0],
               {"title": "Poondi Lake ", "content": "Water reservoir"})


# To identify the flood prone areas, let's trace the path that the water would take when released from the lakes. To do this, we first bring in a layer of lakes in Chennai:

# In[18]:


chennai_lakes = gis.content.search(f"chennai lakes {group_query}",
                                   item_type="feature service",
                                   outside_org=True)[0]
chennai_lakes


# Now, let's call the **`Trace Downstream`** analysis tool from the GIS:

# In[19]:


downstream = features.find_locations.trace_downstream(chennai_lakes)
downstream.query()


# The areas surrounding the trace paths are most prone to flooding and waterlogging. To identify the areas that were at risk, we buffer the traced flow paths by one mile in each direction and visualize it on the map. We see that large areas of the city of Chennai were susceptible to flooding and waterlogging.

# In[20]:


floodprone_buffer = features.use_proximity.create_buffers(
    downstream, [1], units='Miles')


# In[21]:


map_water.add_layer(floodprone_buffer)


# In[22]:


map_water.take_screenshot()


# ## Flood Relief Camps
# 
# To provide emergency assistance, the Tamil Nadu government has set up several flood relief camps in the flood affected areas. They provide food, shelter and the basic necessities to thousands of people displaced by the floods. The locations of the flood relief camps was obtained from http://cleanchennai.com/floodrelief/2015/12/09/relief-centers-as-on-8-dec-2015/ and published to the GIS as a layer, that is visualized below:

# In[23]:


relief_centers = gis.content.search(
    f"Chennai Relief Centers {group_query}",
    item_type="Feature Service",
    outside_org=True)[0]


# In[24]:


map_relief = gis.map("Chennai")
map_relief


# In[25]:


map_relief.add_layer(chennai_waste)


# In[26]:


map_relief.add_layer(relief_centers)


# Let us read the relief center layer as a pandas dataframe to analyze the data further

# In[27]:


relief_data = relief_centers.layers[0].query().sdf
relief_data.head()


# In[28]:


relief_data.columns = relief_data.columns.str.lower()


# In[29]:


relief_data['no_of_pers'].sum()


# In[30]:


relief_data['no_of_pers'].describe()


# In[31]:


relief_data['no_of_pers'].hist()


# In our dataset, each row represents a relief camp location. To quickly get the dimensions (rows & columns) of our data frame, we use the `shape` property.

# In[32]:


relief_data.shape


# As of 8th December, 2015, there were 31,478 people in the 136 relief camps. Let's aggregate them by the district the camp is located in. To accomplish this, we use the `aggregate_points` tool.

# In[33]:


chennai_waste_featurelayer = chennai_waste.layers[0]


# In[34]:


res = features.summarize_data.aggregate_points(
    relief_centers,
    chennai_waste_featurelayer,
    False,
    ["no_of_pers Sum"])


# In[35]:


aggr_lyr = res['aggregated_layer']


# In[36]:


map_relief.add_layer(aggr_lyr, {"renderer": "ClassedSizeRenderer",
                                "field_name": "sum_no_of_pers"})


# In[37]:


map_relief.take_screenshot()


# In[38]:


df = aggr_lyr.query().sdf
df.head()


# Let's represent the aggregate result as a table:

# In[39]:


df = aggr_lyr.query().sdf

df2 = df[['district_n', 'sum_no_of_pers']]
df2.set_index('district_n', inplace=True)
df2


# In[40]:


df2.plot(kind='bar')


# ### Routing Emergency Supplies to Relief Camps

# A centralized location was established at Nehru Stadium to organize the relief materials collected from various organizations and volunteers. From there, the relief material was distributed to people affected by the flood.
# 
# ArcGIS routing tools can help plan routes for relief trucks from the center stadium to relief camps:

# In[41]:


map_route = gis.map("Chennai")
map_route


# In[42]:


nehru_stadium = geocode('Jawaharlal Nehru Stadium, Chennai',
                        max_locations=1,
                        as_featureset=True
                        )

map_route.add_layer(nehru_stadium)


# In[43]:


start_time = dt(2015, 12, 13, 9, 0)


# In[44]:


routes = features.use_proximity.plan_routes(
    stops_layer=relief_centers,
    route_count=10,
    max_stops_per_route=15,
    route_start_time=start_time,
    start_layer=nehru_stadium.to_dict(),
    travel_mode='Driving Time',
    stop_service_time=30)

map_route.add_layer(routes['routes_layer'])


# In[45]:


map_route.add_layer(routes['assigned_stops_layer'])


# In[46]:


map_route.take_screenshot()


# Once the routes have been generated, they can be given to drivers, and used to ensure that relief material is promptly delivered to those in need and help alleviate the suffering they are going through.


# ====================
# traffic-light-detection-on-oriented-imagery-triangulation.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Traffic Light Detection In Oriented Imagery Using Triangulation
# 
# > * 🔬 Data Science
# > * 🥠 Deep Learning and Object Classification

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Necessary imports](#Necessary-imports)
# * [Download & setup data](#Download-&-setup-data)
# * [Model training](#Model-training)
# * [Model inferencing](#Model-inferencing)
# * [Extract location of traffic lights on map using Triangulation](#Extract-location-of-traffic-lights-on-map-using-Triangulation)
# * [Results](#Results)
# * [Exporting the output](#Exporting-the-output)
# * [Conclusion](#Conclusion)
# * [References](#References)

# ## Introduction

# Generally, object detection is applied on images taken looking straight down at the ground, like in traditional satellite imagery, predictions from which can be visualized on a map and incorporated into your GIS. Other imagery, however, is more difficult to visualize and incorporate into your GIS. Such non-nadir oriented imagery includes oblique, bubble, 360-degree, street-side, and inspection imagery, among others. Through this sample, we will demonstrate the utility of an object detection model for detecting objects in oriented imagery using `ArcGIS API for Python`.
# 
# The `arcgis.learn` module supports a number of object detection models, such as `SingleShotDetector`, `RetinaNet`, `FasterRCNN`, `YoloV3`, and more. In this notebook, we will be using the YoloV3 model for detecting traffic lights in oriented imagery. The biggest advantage of `YOLOv3` in `arcgis.learn`, is that it comes preloaded with weights pretrained on the [COCO dataset](https://cocodataset.org/#home). This makes it ready-to-use for the 80 common objects (car, truck, person, etc.) that are part of the COCO dataset. Using this model, we will detect traffic lights in oriented imagery.

# ## Necessary imports

# In[1]:


import os, json, cv2
from math import *
import numpy as np
import itertools
import pandas as pd
import zipfile
from pathlib import Path


# In[2]:


from arcgis import GIS
from arcgis.geometry import Point,Geometry
from arcgis.learn import YOLOv3


# ## Download & setup data

# For this notebook, we will use sample oriented imagery and oriented imagery meta data files, available on ArcGIS Online, for inferencing and plotting points.

# In[4]:


gis = GIS(
    "home"
)


# In[5]:


# 96c6401f5eb94a899c85ab044ddcb8fb item id is for sample imagery
# Sample data can be directly downloded by clickng on the link below
oriented_imagery_data = gis.content.get("96c6401f5eb94a899c85ab044ddcb8fb") 
oriented_imagery_data


# In[6]:


filepath = oriented_imagery_data.download(save_path = os.getcwd(), file_name=oriented_imagery_data.name)


# In[7]:


with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# After extracting the zip file, we will set the path of the items in the zip file.
# - `data_path`: Folder containing all the oriented imagery.
# - `image_meta_data` : File containing meta data for all the oriented images in the `data_path`.
# 

# In[7]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]), "street_view_data")
image_meta_data = Path(os.path.join(os.path.splitext(filepath)[0]), "oriented_imagery_meta_data.csv")


# In[8]:


image_path_list = [os.path.join(data_path, image) for image in os.listdir(data_path)]


# ## Model training

# Since we are using the pretrained `YOLOv3` model, we will pass the `pretrained_backbone` attribute as `True`. This will download the pre trained weights of the `YOLOv3` model with COCO dataset while the `YOLOv3` model is initializing. We will use these weights later to detect traffic lights.

# In[9]:


yolo = YOLOv3(pretrained_backbone=True)


# ## Model inferencing

# Once the model is loaded and ready for inferencing, we will create a function named `traffic_light_finder` that will take oriented imagery as input and will return the following:
# - `Json` containing traffic lights coordinates
# - Traffic lights annotated image
# 
# We will save all the traffic lights annotated image into a folder named <b>traffic_light_marked</b> and save all the annotations in a combined `json` file on the disk.

# In[10]:


def traffic_light_finder(oriented_image_path):
    flag = 0
    coordlist = []
    temp_list = {}
    out = yolo.predict(
        oriented_image_path, threshold=0.5, batch_size=4
    )  # Depending upon your GPU capability, batch_size number can be changed.
    test_img = cv2.imread(oriented_image_path)
    if len(out[0]) == 0:
        temp_list["object"] = False
    else:
        for index, (value, label, confidence) in enumerate(zip(out[0], out[1], out[2])):
            if label == "traffic light":
                flag = 1
                coordlist.append(
                    [int(value[0]), int(value[1]), int(value[2]), int(value[3])]
                )
                test_img = cv2.rectangle(
                    test_img,
                    (int(value[0]), int(value[1]), int(value[2]), int(value[3])),
                    (0, 0, 255),
                    10,
                )
                textvalue = label + "_" + str(confidence)
                cv2.putText(
                    test_img,
                    textvalue,
                    (int(value[0]), int(value[1]) - 10),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    1.5,
                    (0, 0, 255),
                    2,
                )
        if flag == 1:
            temp_list["object"] = True
            temp_list["coords"] = coordlist
            temp_list["assetname"] = "traffic light"
    return temp_list, test_img


# Here we will create a folder named <b>traffic_light_marked</b> that will contain the images with detected traffic lights. We can use these images to check the output of the model, and later for our use case.

# In[7]:


marked_image_saved_folder = os.path.join(os.getcwd(), "traffic_light_marked")
os.makedirs(marked_image_saved_folder, exist_ok=True)
print("Path created for saving the images with traffic light detected on them : - ", marked_image_saved_folder)


# In[ ]:


detections = {}
for e, image in enumerate(image_path_list):
    try:
        val_dict, out_image = traffic_light_finder(image)
        if bool(val_dict):
            detections[os.path.basename(image)] = val_dict
            cv2.imwrite(os.path.join(marked_image_saved_folder, os.path.basename(image)), out_image)
    except Exception as e:
        print(e)


# Here, we are also saving the coordinates of the traffic lights in a `json` file. We can use these coordinates to create a web map or for other use cases.

# In[14]:


with open("traffic_light_data_sample.json", "w") as f:
    json.dump(detections, f)


# Below are some of the images showcasing how the pretrained `YOLOv3` model performs on the oriented imagery. 

# 

# ## Extract location of traffic lights on map using Triangulation

# We have successfully ran the `YOLOv3` pretrained model on the oriented imagery and generated the coordinates of the detected traffic lights.
# 
# We also created an oriented image meta data CSV file (downloaded above) that contains the meta data of the oriented imagery, such as the coordinates of the image, <b>AvgHtAG</b>, <b>CamHeading</b>, <b>CamOri</b>, <b>HFOV</b>, <b>VFOV</b> etc. You can learn more about these data points from this [document](https://www.esri.com/content/dam/esrisites/en-us/about/events/media/UC-2019/technical-workshops/tw-5765-872.pdf).
# 
# Using this data, we will now try to find the exact location of the traffic lights on the map.

# In[26]:


camera_df = pd.read_csv(image_meta_data)
camera_df.head()


# In[27]:


dets = list(detections.keys())


# In[28]:


def find_intersection(
    x1,
    y1,
    x2,
    y2,
    x3,
    y3,
    x4,
    y4,
):
    px = ((x1 * y2 - y1 * x2) * (x3 - x4) - (x1 - x2) * (x3 * y4 - y3 * x4)) / (
        (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4)
    )
    py = ((x1 * y2 - y1 * x2) * (y3 - y4) - (y1 - y2) * (x3 * y4 - y3 * x4)) / (
        (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4)
    )
    return [px, py]


def ccw(A, B, C):
    return (C.y - A.y) * (B.x - A.x) > (B.y - A.y) * (C.x - A.x)


def intersect(A, B, C, D):
    return ccw(A, C, D) != ccw(B, C, D) and ccw(A, B, C) != ccw(A, B, D)


class dotdict(dict):
    """dot.notation access to dictionary attributes"""

    __getattr__ = dict.get
    __setattr__ = dict.__setitem__
    __delattr__ = dict.__delitem__


def process(input_list, threshold=(10, 15)):
    combos = itertools.combinations(input_list, 2)
    points_to_remove = [
        point2
        for (point1, point2) in combos
        if abs(point1[0] - point2[0]) <= threshold[0]
        and abs(point1[1] - point2[1]) <= threshold[1]
    ]
    points_to_keep = [point for point in input_list if point not in points_to_remove]
    return points_to_keep


# In[ ]:


(H, W, _) = cv2.imread(image_path_list[0]).shape
points = []
meta_data= []

for i in range(len(dets) - 1):  # check coordinates of two consecutive images
    # load data of image1
    img1 = (dets[i])[:-4]
    cam1 = camera_df[camera_df["Name"] == img1].to_dict("records")[0]
    bboxes1 = detections[img1 + ".jpg"]["coords"]

    # load data of image2

    img2 = (dets[i + 1])[:-4]
    cam2 = camera_df[camera_df["Name"] == img2].to_dict("records")[0]
    bboxes2 = detections[img2 + ".jpg"]["coords"]

    DIST = cam1["FarDist"]

    for bbox1 in bboxes1:  # loop over all the bbox in image1
        if bbox1[3] > 50:  # ignore small bboxes
            
            x1_0 = eval(cam1["SHAPE"])["x"]
            y1_0 = eval(cam1["SHAPE"])["y"]
            
            # calculate the angle of the object in image1
            direction_angle1 = cam1["CamHeading"] + cam1["HFOV"] / 2.0 * (
                (bbox1[0] + bbox1[2] / 2) - W / 2.0
            ) / (W / 2.0)
            angle_subtended_by_object1 = cam1["VFOV"] * bbox1[3] / H
            
            # calculate the distance where the object is based on angle
            x1_1 = eval(cam1["SHAPE"])["x"] + DIST * cos(
                pi / 2 - radians(direction_angle1)
            )
            y1_1 = eval(cam1["SHAPE"])["y"] + DIST * sin(
                pi / 2 - radians(direction_angle1)
            )

            for bbox2 in bboxes2:  # loop over all the bbox in image2
                if bbox2[3] > 50:  # ignore small bboxes

                    x2_0 = eval(cam2["SHAPE"])["x"]
                    y2_0 = eval(cam2["SHAPE"])["y"]
                    
                    # calculate the angle of the object in image2
                    direction_angle2 = cam2["CamHeading"] + cam2["HFOV"] / 2.0 * (
                        bbox2[0] + bbox2[2] / 2 - W / 2.0
                    ) / (W / 2.0)
                    angle_subtended_by_object2 = cam2["VFOV"] * bbox2[3] / H
                    
                    # calculate the distance where the object is based on angle
                    x2_1 = eval(cam2["SHAPE"])["x"] + DIST * cos(
                        pi / 2 - radians(direction_angle2)
                    )
                    y2_1 = eval(cam2["SHAPE"])["y"] + DIST * sin(
                        pi / 2 - radians(direction_angle2)
                    )
                    
                    # find if the line intersects
                    val = intersect(
                        dotdict({"x": x1_0, "y": y1_0}),
                        dotdict({"x": x1_1, "y": y1_1}),
                        dotdict({"x": x2_0, "y": y2_0}),
                        dotdict({"x": x2_1, "y": y2_1}),
                    )
                    xmin, ymin, xmax, ymax = (
                        bbox2[0],
                        bbox2[1],
                        bbox2[0] + bbox2[2],
                        bbox2[1] + bbox2[3],
                    )
                    
                    # find the point where line from image1 and image2 intersect
                    if val:
                        midpoint = find_intersection(
                            x1_0, y1_0, x1_1, y1_1, x2_0, y2_0, x2_1, y2_1
                        )
                        points.append(midpoint)
                        meta_data.append(
                            {
                                "image1": img1,
                                "image2": img2,
                                "points": midpoint,
                                "coords": [xmin, ymin, xmax, ymax],
                                "x": midpoint[0],
                                "y": midpoint[1],
                            }
                        )


# After the above mentioned process we have got some coordinates where there will be traffic lights but as one traffic light can be detected in multiple images therefore we will further cluster the data and take only one traffic light from a cluster.
# 
# In this way we will remove the redundant traffic light near a point.

# In[36]:


print 'Number of traffic lights extracted - {}'.format(len(points))
outpoints = process(points)
print 'Number of traffic lights extracted after clustering and removing redundant traffic light - {}'.format(len(outpoints))


# ## Results 

# Next, we will load a map and draw the final selected traffic light coordinates on it.

# In[3]:


m = gis.map('Vilnius City')
m


# In[ ]:


m.center = {'x': 25.28489583988743, 'y': 54.70681816057357,
            'spatialReference': {'wkid': 4326, 'latestWkid': 4326}}
m.zoom = 19
m.basemap = 'satellite'


# In[ ]:


for point in outpoints:
    intpoint = {'x': point[0], 'y': point[1],
                'spatialReference': {'wkid': 102100,
                'latestWkid': 3857}}
    m.draw(arcgis.geometry.Point(intpoint), symbol={
        'type': 'simple-marker',
        'style': 'square',
        'color': 'red',
        'size': '8px',
        })


# ## Exporting the output 

# In[ ]:


out_meta_data = []
for e,i in enumerate(points):
    if i in outpoints:
        out_meta_data.append(meta_data[e])


# In[ ]:


# creating a spatial dataframe and exporting as feature class
spatial_df = []
for e, i in enumerate(out_meta_data):
    tempdict = {}
    tempdict["X"] = i["x"]
    tempdict["Y"] = i["y"]
    tempdict["Z"] = 100
    tempdict["ImgUrn"] = str(
        i["image2"][1:]
        + "|VilniusCity_ExposurePoints|"
        + str(
            camera_df[camera_df["Name"] == i["image2"]].to_dict("records")[0][
                "OBJECTID"
            ]
        )
    )
    tempdict["ImgGeom"] = json.dumps(
        {
            "xmin": i["coords"][0],
            "ymin": i["coords"][1],
            "xmax": i["coords"][2],
            "ymax": i["coords"][3],
            "pos": "BC",
        }
    )
    tempdict["Labels"] = "traffic lights"
    tempdict["SHAPE"] = Geometry(
        {
            "x": i["x"],
            "y": i["y"],
            "spatialReference": {"wkid": 3857, "latestWkid": 102100},
        }
    )
    spatial_df.append(tempdict)

    
df = pd.DataFrame(data=spatial_df)
df.spatial.set_geometry("SHAPE")

# exporting the layer on ArcGIS Online org
exported_layer = df.spatial.to_featurelayer(r"exported_traffic_points", sanitize_columns=False,gis=gis)


# ## Conclusion 

# In this notebook, we performed object detection on oriented imagery. We used the `YoloV3` model with pretrained weights for detecting traffic lights, located these on the gis map using `ArcGIS API for Python`, and exported the output as a feature class.

# ## References 

# [1] [Managing and visualizing oriented imagery](https://doc.arcgis.com/en/imagery/workflows/resources/managing-and-visualizing-oriented-imagery.htm)
# 
# [2] [YOLOv3 Object Detector](https://developers.arcgis.com/python/guide/yolov3-object-detector/)
# 
# [3] [Working with Oriented Imagery](https://www.esri.com/content/dam/esrisites/en-us/about/events/media/UC-2019/technical-workshops/tw-5765-872.pdf)


# ====================
# traffic-light-detection-on-oriented-imagery.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Traffic Light Detection In Oriented Imagery Using ArcGIS Pretrained Model
# 
# > * 🔬 Data Science
# > * 🥠 Deep Learning and Object classification

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Necessary imports](#Necessary-imports)
# * [Download & setup data](#Download-&-setup-data)
# * [Model training](#Model-training)
# * [Model inferencing](#Model-inferencing)
# * [Relative depth estimation model](#Relative-depth-estimation-model)
# * [Extract location of traffic lights on map](#Extract-location-of-traffic-lights-on-map)
# * [Results](#Results)
# * [Conclusion](#Conclusion)
# * [References](#References)

# ## Introduction

# We have generally applied object detection on images taken looking straight down at the ground, like traditional satellite imagery, predictions from which can be visualized on a map and incorporated into your GIS. Other imagery, however, is more difficult to visualize and incorporate into your GIS. Such non-nadir oriented imagery includes oblique, bubble, 360-degree, street-side, and inspection imagery, among others. Through this sample, we will demonstrate the utility of an object detection model for detecting objects in an oriented imagery using `ArcGIS API for Python`.
# 
# The `arcgis.learn` module supports number of object detection models such as `SingleShotDetector`, `RetinaNet`, `FasterRCNN`, `YoloV3` and even more. In the notebook, we will be using `YoloV3` model for detecting traffic lights in the oriented imagery. The biggest advantage of `YOLOv3` in `arcgis.learn` is that it comes preloaded with weights pretrained on the [COCO dataset](https://cocodataset.org/#home). This makes it ready-to-use for the 80 common objects (car, truck, person, etc.) that are part of the COCO dataset. Using this model, we will try to detect traffic light in the oriented imagery.

# ## Necessary imports

# In[1]:


import os, json, cv2
from math import *
import numpy as np
import itertools
import pandas as pd
import zipfile
from pathlib import Path


# In[2]:


import arcgis, arcpy
from arcgis import GIS
from arcgis.geometry import Point
from arcgis.learn import YOLOv3


# ## Download & setup data

# We will need oriented imagery and oriented imagery meta data file so that we can use that for inferencing and plotting the points. We have sample images uploaded on the ArcGIS Online org. We will download those items below and use those for our workflow.

# In[4]:


gis = GIS(
    "home"
)


# In[5]:


# 96c6401f5eb94a899c85ab044ddcb8fb item id is for sample imagery
# Sample data can be directly downloded by clickng on the link below
oriented_imagery_data = gis.content.get("96c6401f5eb94a899c85ab044ddcb8fb") 
oriented_imagery_data


# In[6]:


filepath = oriented_imagery_data.download(save_path = os.getcwd(), file_name=oriented_imagery_data.name)


# In[7]:


with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# After the extraction of the zip file, we will set the path of the items which are there in the zip file which we will use in this workflow.
# - `data_path`: Folder containing all the oriented imagery.
# - `image_meta_data` : File containing meta data for all the oriented images in the data_path.
# - `depth_image_path` : Folder containing all the relative estimated depth image of oriented imagery.
# 

# In[7]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]), "street_view_data")
image_meta_data = Path(os.path.join(os.path.splitext(filepath)[0]), "oriented_imagery_meta_data.csv")
depth_image_path = Path(os.path.join(os.path.splitext(filepath)[0]), "saved_depth_image")


# In[8]:


image_path_list = [os.path.join(data_path, image) for image in os.listdir(data_path)]


# ## Model training

# Since we will be using the pretrained `YOLOv3` model so we will pass `pretrained_backbone` as `True`. In this way while initializing the `YOLOv3` model the pre trained weights of the `YOLOv3` model with COCO dataset will be downloaded. We will later be using these weights to detect traffic lights.

# In[9]:


yolo = YOLOv3(pretrained_backbone=True)


# ## Model inferencing

# Once we have the model loaded and ready for inferencing, we will be create a function named `traffic_light_finder` that will take oriented image as input and will return 2 things. 
# - Json containing traffic lights coordinates
# - Traffic lights annotated image
# 
# We will save all the traffic lights annotated image into a folder named <b>traffic_light_marked</b> and save all the annotations in a combined json file on the disk.

# In[10]:


def traffic_light_finder(oriented_image_path):
    flag = 0
    coordlist = []
    temp_list = {}
    out = yolo.predict(oriented_image_path, threshold=0.5, batch_size = 4) # Depending upon your GPU capability, batch_size number can be changed.
    test_img = cv2.imread(oriented_image_path)
    if len(out[0]) == 0:
        temp_list["object"] = False
    else:
        for index, (value, label, confidence) in enumerate(zip(out[0], out[1], out[2])):
            if label == "traffic light":
                flag = 1
                coordlist.append(
                    [int(value[0]), int(value[1]), int(value[2]), int(value[3])]
                )
                test_img = cv2.rectangle(
                    test_img,
                    (int(value[0]), int(value[1]), int(value[2]), int(value[3])),
                    (0, 0, 255),
                    10,
                )
                textvalue = label + "_" + str(confidence)
                cv2.putText(
                    test_img,
                    textvalue,
                    (int(value[0]), int(value[1]) - 10),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    1.5,
                    (0, 0, 255),
                    2,
                )
        if flag == 1:
            temp_list["object"] = True
            temp_list["coords"] = coordlist
            temp_list["assetname"] = "traffic light"
    return temp_list, test_img


# Here we will create a folder named <b>traffic_light_marked</b> which will contain all the images with traffic lights detected on them. We can use these images to check the output of the model. Later we can use them for our use case.

# In[7]:


marked_image_saved_folder = os.path.join(os.getcwd(), "traffic_light_marked")
os.makedirs(marked_image_saved_folder, exist_ok=True)
print("Path created for saving the images with traffic light detected on them : - ", marked_image_saved_folder)


# In[ ]:


detections = {}
for e, image in enumerate(image_path_list):
    try:
        val_dict, out_image = traffic_light_finder(image)
        if bool(val_dict):
            detections[os.path.basename(image)] = val_dict
            cv2.imwrite(os.path.join(marked_image_saved_folder, os.path.basename(image)), out_image)
    except Exception as e:
        print(e)


# Here we are also saving the coordinates of the traffic lights in a json file. We can use these coordinates to create a webmap or in any of the other use cases.

# In[14]:


with open("traffic_light_data_sample.json", "w") as f:
    json.dump(detections, f)


# Below are some of the images showcasing how the pretrained `YOLOv3` model performs on the oriented imagery. 

# 

# ## Relative depth estimation model

# We now have run the `YOLOv3` pretrained model on all the oriented images and got the coordinates of detected traffic lights in them.
# 
# We will now calculate the relative estimated depth of the objects in the oriented imagery. For that we have a pretrainded model available from the Open-source [Bts-PyTorch](https://github.com/ErenBalatkan/Bts-PyTorch) based on [From Big to Small: Multi-Scale Local Planar Guidance for Monocular Depth Estimation](https://arxiv.org/pdf/1907.10326.pdf).
# 
# We have packagaed the model as a dlpk file which we can use with ArcGIS Pro to calculate the relative estimated depth of the objects in the oriented imagery.
# 
# For this sample notebook, we have already provided the output of this model on all the sample images in the folder <b>saved_depth_image</b> with the sample data downloded in the [Oriented Imagery Sample Data](https://www.arcgis.com/home/item.html?id=96c6401f5eb94a899c85ab044ddcb8fb).

# In[11]:


depth_model_item = gis.content.get("c19f7ce733cd4811b5609566fa4cf5bb")
depth_model_item


# Once we have downloaded the dlpk file we will use it for calculating the estimated depth of the oriented images.

# 

# In[ ]:


with arcpy.EnvManager(processorType='cpu'):
    out_classified_raster = \
        arcpy.ia.ClassifyPixelsUsingDeepLearning(image_path_list[0],
            r"D:\sample\relative_depth_estimation.dlpk", None,
            'PROCESS_AS_MOSAICKED_IMAGE', None)
    out_classified_raster.save(r"D:\sample\samplename.png")


# Below is the image showcasing how the pretrained relative depth estimation model performs on the oriented imagery.

# 

# ## Extract location of traffic lights on map

# We now have run the `YOLOv3` pretrained model on all the oriented images and got the coordinates of detected traffic lights in them. We now also have the relative estimated depth of objects in oriented imagery.
# 
# We also have an oriented image meta data file in csv format (downloaded above) which contains the meta data of the oriented imagery like the coordinate at which the image was taken, <b>AvgHtAG</b>, <b>CamHeading</b>, <b>CamOri</b>, <b>HFOV</b>, <b>VFOV</b> etc. You can understand more about these data points from this [document](https://www.esri.com/content/dam/esrisites/en-us/about/events/media/UC-2019/technical-workshops/tw-5765-872.pdf).
# 
# Using these data, now we will now try to find the exact location of the traffic lights on the map.

# In[26]:


camera_df = pd.read_csv(image_meta_data)
camera_df.head()


# In[27]:


dets = list(detections.keys())


# In[28]:


def find_intersection(
    x1,
    y1,
    x2,
    y2,
    x3,
    y3,
    x4,
    y4,
    ):
    px = ((x1 * y2 - y1 * x2) * (x3 - x4) - (x1 - x2) * (x3 * y4 - y3
          * x4)) / ((x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4))
    py = ((x1 * y2 - y1 * x2) * (y3 - y4) - (y1 - y2) * (x3 * y4 - y3
          * x4)) / ((x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4))
    return [px, py]


def process(input_list, threshold=(10, 10)):
    combos = itertools.combinations(input_list, 2)
    points_to_remove = [point2 for (point1, point2) in combos
                        if abs(point1[0] - point2[0]) <= threshold[0]
                        and abs(point1[1] - point2[1]) <= threshold[1]]
    points_to_keep = [point for point in input_list if point
                      not in points_to_remove]
    return points_to_keep


# In[ ]:


OBJECT_HEIGHT_IN_WORLD = 0.9
(H, W, _) = cv2.imread(image_path_list[0]).shape
points = []

for i in range(len(dets) - 1):  # check coordinates of two consecutive images

    # load data of image1

    img1 = (dets[i])[:-4]
    cam1 = camera_df[camera_df['Name'] == img1].to_dict('records')[0]
    bboxes1 = detections[img1 + '.jpg']['coords']

    # load data of image2

    img2 = (dets[i + 1])[:-4]
    cam2 = camera_df[camera_df['Name'] == img2].to_dict('records')[0]
    bboxes2 = detections[img2 + '.jpg']['coords']

    for bbox1 in bboxes1:  # loop over all the bbox in image1
        if bbox1[3] > 50:  # ignore small bboxes

            # calculate the anngle of the object in image1

            direction_angle1 = cam1['CamHeading'] + cam1['HFOV'] / 2. \
                * (bbox1[0] + bbox1[2] / 2 - W / 2.) / (W / 2.)
            angle_subtended_by_object1 = cam1['VFOV'] * bbox1[3] / H

            # calculale the distance of object in image1 from center

            dist1 = OBJECT_HEIGHT_IN_WORLD \
                / tan(radians(angle_subtended_by_object1))
            dist1 = dist1 * pi

            # find coordinate of object in image1

            x12 = Point(eval(cam1['SHAPE']))['x'] + dist1 * cos(pi / 2
                    - radians(direction_angle1))
            y12 = Point(eval(cam1['SHAPE']))['y'] + dist1 * sin(pi / 2
                    - radians(direction_angle1))
            x11 = Point(eval(cam1['SHAPE']))['x']
            y11 = Point(eval(cam1['SHAPE']))['y']

            for bbox2 in bboxes2:  # loop over all the bbox in image2
                if bbox2[3] > 50:  # ignore small bboxes

                    # calculate the anngle of the object in image2

                    direction_angle2 = cam2['CamHeading'] + cam2['HFOV'
                            ] / 2. * (bbox2[0] + bbox2[2] / 2 - W / 2.) \
                        / (W / 2.)
                    angle_subtended_by_object2 = cam2['VFOV'] \
                        * bbox2[3] / H

                    # calculale the distance of object in image2 from center

                    dist2 = OBJECT_HEIGHT_IN_WORLD \
                        / tan(radians(angle_subtended_by_object2))
                    dist2 = dist2 * pi

                    # find coordinate of object in image2

                    x22 = Point(eval(cam2['SHAPE']))['x'] + dist2 \
                        * cos(pi / 2 - radians(direction_angle2))
                    y22 = Point(eval(cam2['SHAPE']))['y'] + dist2 \
                        * sin(pi / 2 - radians(direction_angle2))
                    x21 = Point(eval(cam2['SHAPE']))['x']
                    y21 = Point(eval(cam2['SHAPE']))['y']

                    # fin the point where coordinate from image1 and image2 intersect

                    pointval = find_intersection(
                        x11,
                        y11,
                        x12,
                        y12,
                        x21,
                        y21,
                        x22,
                        y22,
                        )

                    # load estimated depth image and select the mininum depth from the area where object is identified

                    (xmin, ymin, xmax, ymax) = (bbox2[0], bbox2[1],
                            bbox2[0] + bbox2[2], bbox2[1] + bbox2[3])
                    depth_image = \
                        cv2.imread(os.path.join(depth_image_path, img2
                                   + '.jpg'))
                    cropped_depth_image = depth_image[ymin:ymax, xmin:
                            xmax]

                    # take the estimated depth as distance from the center

                    DIST = np.min(cropped_depth_image[:, :, 0])
                    DIST = DIST + 7

                    # find coordinate of object using estimated depth as distance

                    x22_1 = Point(eval(cam2['SHAPE']))['x'] + DIST \
                        * cos(pi / 2 - radians(direction_angle2))
                    y22_1 = Point(eval(cam2['SHAPE']))['y'] + DIST \
                        * sin(pi / 2 - radians(direction_angle2))

                    point0 = np.array([float(pointval[0]),
                            float(pointval[1])])
                    point1 = np.array([float(x22_1), float(y22_1)])

                    # calculate euclidian distance between the point where coordinate from image1 and image2 intersect and  point calcuated using estimated depth

                    dist_points = np.linalg.norm(point0 - point1)

                    # if distance is less than 5 then take the point

                    if dist_points < 5:
                        points.append(pointval)


# After the above mentioned process we have got some coordinates where there will be traffic lights but as one traffic light can be detected in multiple images therefore we will further cluster the data and take only one traffic light from a cluster.
# 
# In this way we will remove the redundant traffic light near a point.

# In[36]:


print 'Number of traffic lights extracted - {}'.format(len(points))
outpoints = process(points)
print 'Number of traffic lights extracted after clustering and removing redundant traffic light - {}'.format(len(outpoints))


# ## Results 

# We will load a map and draw the final selected coordinates on it. These coordinates are the places where there are traffic lights.

# In[3]:


m = gis.map('Vilnius City')
m


# In[ ]:


m.center = {'x': 25.28489583988743, 'y': 54.70681816057357,
            'spatialReference': {'wkid': 4326, 'latestWkid': 4326}}
m.zoom = 19
m.basemap = 'satellite'


# In[ ]:


for point in outpoints:
    intpoint = {'x': point[0], 'y': point[1],
                'spatialReference': {'wkid': 102100,
                'latestWkid': 3857}}
    m.draw(arcgis.geometry.Point(intpoint), symbol={
        'type': 'simple-marker',
        'style': 'square',
        'color': 'red',
        'size': '8px',
        })


# ## Conclusion 

# In this notebook, we have performed object detection on imagery taken at any angle naming oriented imagery. We use `YoloV3` model with pretrained weights for detecting traffic lights and located these on the gis map using `ArcGIS API for Python`.

# ## References 

# [1] [Managing and visualizing oriented imagery](https://doc.arcgis.com/en/imagery/workflows/resources/managing-and-visualizing-oriented-imagery.htm)
# 
# [2] [YOLOv3 Object Detector](https://developers.arcgis.com/python/guide/yolov3-object-detector/)
# 
# [3] [Working with Oriented Imagery](https://www.esri.com/content/dam/esrisites/en-us/about/events/media/UC-2019/technical-workshops/tw-5765-872.pdf)


# ====================
# train_a_tensorflow-lite_model_for_identifying_plant_species.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Plant species identification using a TensorFlow-Lite model within mobile devices

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Get the data for analysis](#Get-the-data-for-analysis)
# * [Train an image classification model](#Train-an-image-classification-model)
#     * [Necessary imports](#Necessary-imports)
#     * [Download Dataset](#Download-Dataset)
#     * [Filter out non RGB Images](#Filter-out-non-RGB-Images)
#     * [Prepare data](#Prepare-data)
#     * [Visualize a few samples from your training data](#Visualize-a-few-samples-from-your-training-data)
#     * [Load model architecture](#Load-model-architecture)
#     * [Find an optimal learning rate](#Find-an-optimal-learning-rate)
#     * [Fit the model](#Fit-the-model)
#     * [Visualize results in validation set](#Visualize-results-in-validation-set)
#     * [Save the model](#Save-the-model)
# * [Deploy model](#Deploy-model)
# * [References](#References)

# ## Introduction

# Deep Learning models are huge and requires high computation for inferencing. Can we train Deep Learning models which require less computation power, are smaller in size and can be deployed on mobile phones? Well, the answer is 'yes'. With the integration of capability to train [TensorFlow lite](https://www.tensorflow.org/lite) models with ArcGIS API for Python, we can now train DL models that can be deployed on mobile devices and are smaller in size.
# 
# Where can we use them? We can use them up to train multiple DL models to perform classification tasks specifically for mobile devices. One such integration we did is in the ["Survey123"](https://survey123.arcgis.com/) application which is a simple and intuitive form-centric data gathering solution being used by multiple surveyors while performing ground surveys, where we integrated a tf-lite model to classify different plant species while clicking it's picture in the app.
# 
# This notebook intends to showcase this capability to train a deep learning model that can be used in mobile applications for a real time inferencing using TensorFlow Lite framework. As an example, we will train the same plant species classification model which was discussed earlier but with a smaller dataset.
# <br>
# <center>A snapshot of plant classifier in Survey123 application</center>

# ## Get the data for analysis

# [PlantCLEF](https://www.imageclef.org/lifeclef/2017/plant) data is available in three sets:
# 
# - a “trusted” training set based on the online collaborative Encyclopedia Of Life (EoL) [[1]](#References).
# - A ”noisy” training set (obtained from Google and Bing image search results, including mislabeled or irrelevant images [[2]](#References).
# - The previous years (2015-2016) images depicting only a subset of the species [[3]](#References).
# 
# For this notebook, we have taken a subset from the "trusted" training set based on the online collaborative Encyclopedia Of Life [[1]](#References) with 39,354 images belonging to 100 plant species and changed their specie numbers with specie names, as an example specie number '42' is changed to 'Acanthus mollis'. The information about the specie name is present in the "xml" file present along with each image file. We wrote a script to perform the specie name and specie number mapping. To know how we have done this, please have a look at the script [here](https://geosaurus.maps.arcgis.com/home/item.html?id=1867f7011d57438f89c6f91c2aeacc93).

# Use the following command to run the downloaded script. It requires three arguments to be passed:
# 
# - path to downloaded PlantCLEF data
# - path of the destination folder
python changing_specie_name_with_number.py data/path dest/path
# ## Train an image classification model

# We will train our model using `arcgis.learn` module within ArcGIS API for Python. `arcgis.learn` contains tools and deep learning capabilities required for this study. A detailed documentation to install and setup the environment is available [here](https://developers.arcgis.com/python/guide/install-and-set-up/).

# ### Necessary imports

# Firstly, we need to set the environment variable for ArcGIS to enable TensorFlow as backend. To perform this, we can set `ARCGIS_ENABLE_TF_BACKEND` parameter's value to 1 as shown below.

# In[1]:


get_ipython().run_line_magic('env', 'ARCGIS_ENABLE_TF_BACKEND=1')


# In[2]:


import os
from pathlib import Path

from arcgis.gis import GIS
from arcgis.learn import prepare_data, FeatureClassifier


# ### Download Dataset

# In[2]:


gis = GIS('home')


# In[3]:


training_data = gis.content.get('81932a51f77b4d2d964218a7c5a4af17')
training_data


# In[4]:


filepath = training_data.download(file_name=training_data.name)


# In[5]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[6]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# ### Filter out non RGB Images

# In[7]:


from glob import glob
from PIL import Image


# In[8]:


for image_filepath in glob(os.path.join(data_path, 'images', '**','*.jpg')):
    if Image.open(image_filepath).mode != 'RGB':
        os.remove(image_filepath)


# ### Prepare data

# We will now use the `prepare_data()` function to apply various types of transformations and augmentations on the training data. These augmentations enable us to train a better model with limited data and also prevent the model from overfitting. 
# 
# Here, we are passing 3 parameters to the `prepare_data()` function.
#  - `path`: path of folder containing training data.
#  - `chip_size`: Same as per specified while exporting training data.
#  - `batch_size`: No. of images your model will train on each step inside an epoch, it directly depends on the memory of your graphic card and the type of model which you are working with. For this sample, a batch size of 64 worked for us on a GPU with 11GB memory.

# In[9]:


data = prepare_data(
    path=data_path,
    dataset_type='Imagenet',
    batch_size=64,
    chip_size=300
)


# ### Visualize a few samples from your training data

# To make sense of training data we will use the `show_batch()` method in arcgis.learn. `show_batch()` randomly picks a few samples from the training data and visualizes them.
# 
#  - `rows`: No of rows we want to see the results for.

# In[10]:


data.show_batch(rows=2)


# ### Load model architecture

# `arcgis.learn` provides capabilities to determine class of each feature in the form of `FeatureClassifier` model. To have an in-depth information about it's working and usage, have a look at this [link](https://developers.arcgis.com/python/guide/how-feature-categorization-works/).  

# As we are training a model to be deployed on mobile phones, we must define the model with "tensorflow" backend. In order to do that we can set the parameter `backend` to "tensorflow". 

# In[12]:


model = FeatureClassifier(data, backbone='MobileNetV2', backend='tensorflow')


# ### Find an optimal learning rate

# Learning rate is one of the most important hyperparameters in model training. Here, we explore a range of learning rates to guide us to choose the best one. `arcgis.learn` leverages fast.ai’s learning rate finder to find an optimum learning rate for training models. We can use the `lr_find()` method to find the optimum learning rate at which can train a robust model fast enough.

# In[13]:


lr = model.lr_find()


# Based on the learning rate plot above, we can see that the learning rate suggested by `lr_find()` for our training data is 0.000691831. We can use it to train our model. In the latest release of `arcgis.learn` we can train models without even specifying a learning rate. That internally uses the learning rate finder to find an optimal learning rate and uses it.

# ### Fit the model 

# To train the model, we use the `fit()` method. To start, we will use 25 epochs to train our model. Epoch defines how many times model is exposed to entire training set.

# In[14]:


model.fit(25, lr=lr)


# ### Visualize results in validation set 

# The code below will pick a few random samples and show us ground truth and respective model predictions side by side. This allows us to validate the results of your model in the notebook itself. Once satisfied, we can save the model and use it further in our workflow.

# In[15]:


model.show_results(rows=4, thresh=0.2)


# Here a subset of ground truth from training data is visualized along with the predictions from the model. As we can see, our model is performing well and the predictions are comparable to the ground truth.

# ### Save the model

# We will save the model which we trained in a tf-lite format.
# 
# We will use the `save()` method to save the trained model. By default, it will be saved to the 'models' sub-folder within our training data folder.

# In[ ]:


model.save('Plant-identification-25-tflite', framework="tflite")


# ## Deploy model

# The tf-lite model can now be deployed on mobile devices. Survey123 for ArcGIS has an upcoming feature that integrates such tf-lite models. To learn more on deploying this model in Survey123, join the [Early Adopter Community](https://www.esri.com/en-us/early-adopter) to access the Survey123 private beta.

# ## References 

# [1] http://otmedia.lirmm.fr/LifeCLEF/PlantCLEF2017/TrainPackages/PlantCLEF2017Train1EOL.tar.gz<br>
# [2] http://otmedia.lirmm.fr/LifeCLEF/PlantCLEF2017/TrainPackages/PlantCLEF2017Train2Web.txt<br>
# [3] http://otmedia.lirmm.fr/LifeCLEF/PlantCLEF2015/Packages/TrainingPackage/PlantCLEF2015TrainingData.tar.gz   


# ====================
# training_a_wind_turbine_detection_model_using_large_volume_of_training_data.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Training a wind turbine detection model using large volumes of training data

# * 🔬 Data Science
# * 🥠 Deep Learning and Object Detection

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Export training data](#Export-Training-Data) or [Download sample training data](#Download-sample-training-data-optional)
# * [Model training](#Model-Training)
#  * [Executing model training script](#Executing-model-training-script)
#  * [Monitor model training](#Monitor-model-training)
# * [Model inference](#Model-inference)
# * [Conclusion](#Conclusion)

# ## Introduction

# When training robust deep learning models, large amounts of training data is usually required. Unfortunately, large volumes of data can often be difficult to manage and process. To reduce the effort required to export training data and train a model, we can distribute the workload to different processes, or even different machines altogether.

# In this workflow, we will perform three broad steps.
# 
# - Exporting or downloading the training data.
# - Training the model.
# - Deploying the model and extracting its footprints.

# This workflow requires deep learning dependencies to be installed. Documentation is available [here](https://developers.arcgis.com/python/guide/install-and-set-up/) for installing and setting up an appropriate environment.

# ## Export Training Data

# We will be using the ['Export Training Data for Deep Learning'](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-analyst/export-training-data-for-deep-learning.htm) tool to export the training data.

# Imagery and Data Sources:
# - Imagery: NAIP Imagery downloaded [here](https://nrcs.app.box.com/v/naip).
# - labels: The U.S. Wind Turbine Database downloaded [here](https://eerscmap.usgs.gov/uswtdb/assets/data/uswtdbSHP.zip).

# Here, we have multiple rasters and a label file. We will prepare a list of arguments and a map with `arcpy.ia.ExportTrainingDataForDeepLearning`. In this sample, we are distributing processing per image. However, in cases with larger rasters, we can further split the rasters to parts and distribute the processing across various windows on the raster.

# ```python
# # Imports
# import arcpy
# from glob import glob
# from multiprocessing.pool import Pool
# import os
# 
# # Prepare a list of Images
# rasters = glob(r'c:\data\imagery\**\*.sid')
# print(f"-- Found {len(rasters)} rasters")
# 
# # other parameters
# lbl_shp = r'c:\data\labels\uswtdbSHP\uswtdb_v4_0_20210409_buffer_25m.shp'
# tile_size = 400
# cell_size = 0.6
# output_path = r"c:\data\exported_training_data"
# 
# # Prepare arguments list
# arglist = []
# for idx, raster in enumerate(rasters):
#     outpath = os.path.join(output_path, os.path.basename(raster)[:4])
#     arglist.append((
#         raster,
#         outpath,
#         lbl_shp,
#         "TIFF",
#         tile_size,
#         tile_size,
#         0,
#         0,
#         "ONLY_TILES_WITH_FEATURES",
#         "PASCAL_VOC_rectangles",
#         0,
#         None,
#         0,
#         None,
#         0,
#         "MAP_SPACE",
#         "PROCESS_AS_MOSAICKED_IMAGE",
#         "NO_BLACKEN",
#         "FIXED_SIZE"
#     ))
#     
# # Export training data using multiprocessing
# nprocesses = os.cpu_count() - 1
# print(f"-- using {nprocesses} Processes")
# my_pool = Pool(nprocesses)
# res = my_pool.imap_unordered(arcpy.ia.ExportTrainingDataForDeepLearning, arglist)
# for i, val in enumerate(res):
#     print(f"--Progress: {i}/{len(arglist)} Rasters | {(i*100/len(arglist)):0.2f} % |             ", end='\r')
# print("-- Done")
# my_pool.close()
# my_pool.join()
# del my_pool
# ```

# This will create all of the files necessary for the next step in the 'Output Path'. These files will serve as our training data.

# **You can either create a python script with the content above or download a sample using the ArcGIS Python API, as shown below.**

# In[1]:


from arcgis.gis import GIS
gis = GIS('home')


# In[2]:


exporting_script = gis.content.get('e105f94a25604b798b96495b89c6877f')
exporting_script


# In[3]:


import zipfile
from pathlib import Path
zpath = exporting_script.download(file_name=exporting_script.name)
with zipfile.ZipFile(zpath, 'r') as zip_ref:
    zip_ref.extractall('.')


# ## Download sample training data optional
# 
# Alternatively, you can also directly get a sample of exported training data, as shown below.

# In[5]:


from arcgis.gis import GIS
gis = GIS('home')


# In[4]:


training_data = gis.content.get('bc846c5999cd44efaeafcdce3ae1ddf5')
training_data


# In[5]:


filepath = training_data.download(file_name=training_data.name)


# In[8]:


import zipfile
from pathlib import Path
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)
#
output_path = str(Path(filepath).parent / 'wind_turbines')


# In[15]:


print(output_path)


# ## Model Training

# When working with large volumes of training data, it is recommended that you use distributed training if multiple GPUs are available to you. This process can be done using a python script, as shown below.

# ```python
# # Imports
# from arcgis.learn import prepare_data, FasterRCNN
# from glob import glob
# import os
# 
# # Load training data
# training_data_path = r"c:\data\exported_training_data"
# data_folders = glob(os.path.join(training_data_path, '**'))
# data = prepare_data(
#     data_folders, 
#     batch_size=16, 
#     chip_size=400)
# 
# # Train model
# model = FasterRCNN(data)
# model.fit(10, .001, tensorboard=True)
# model.save('wind_turbine_model_10epochs')
# ```

# **You can either create a python script with the content above or download a sample using the ArcGIS Python API, as shown below.**

# In[5]:


from arcgis.gis import GIS
gis = GIS('home')


# In[6]:


training_script = gis.content.get('3a0e7754a29c4bf19eadffc3d08e5f23')
training_script


# In[6]:


import zipfile
from pathlib import Path
zpath = training_script.download(file_name=training_script.name)
with zipfile.ZipFile(zpath, 'r') as zip_ref:
    zip_ref.extractall('.')


# ### Executing model training script

# Once you have the script ready, you can execute it as documented below.

# ```bash
# python -m torch.distributed.launch --nproc_per_node=<number-of-GPUs> train_wind_turbine_model.py
# ```

#  **Important!** Replace `<number-of-GPUs>` with an integer value representing the optimal number of GPU's you want to utilize.
# 
# Learn more about the torch distributed launch module in the pytorch documentation [here](https://pytorch.org/docs/1.8.1/distributed.html#launch-utility)

# ### Monitor model training

# While the model is training, we can monitor the performance of the model using tensorboard. 
# 
# To start monitoring with tensorboard, we need to execute the related command printed by the model training script.

# 

# ```bash
# tensorboard --host=test-multigpu-w --logdir="C:\data\training_log"
# ```
# We will execute this command in a separate anaconda prompt.

# 
# 

# Once executed, we need to Launch the tensorboard monitor in a browser by visiting the printed URL. The tensorboard monitor will look like the following image:

# 

# For more information about tensorboard support in ArcGIS Python API, you can follow [this guide](https://developers.arcgis.com/python/guide/monitor-model-training-with-tensorboard/).

# ## Model inference

# We will find the model saved in the 'models' folder. The saved model can be used to detect wind turbines using the [Detect Objects Using Deep Learning](https://pro.arcgis.com/en/pro-app/latest/tool-reference/image-analyst/detect-objects-using-deep-learning.htm) tool, available in both [ArcGIS Pro](https://www.esri.in/en-in/products/arcgis-pro/overview) and [ArcGIS Enterprise](https://enterprise.arcgis.com/en/).

# 

# 

# You can also achieve this using arcpy.

# ```python
# with arcpy.EnvManager(
#     extent="685195.489825479 3593238.45671656 688035.481538387 3595205.11376178", 
#     cellSize=0.6, 
#     processorType="GPU"
# ):
#     arcpy.ia.DetectObjectsUsingDeepLearning(
#         "ortho_1-1_hn_s_tx003_2020_1.sid", 
#         r"C:\data\wind_turbines.shp", 
#         r"C:\data\wind_turbine_model_10epochs.dlpk", 
#         "padding 100;threshold 0.5;nms_overlap 0.1;batch_size 64;exclude_pad_detections True", 
#         "NMS", 
#         "Confidence", 
#         "Class", 
#         0, 
#         "PROCESS_AS_MOSAICKED_IMAGE"
#     )
# ```

# The output of this model is a layer of detected wind turbines, as show below.

# 

# <center>A subset of detected wind turbines.

# 

# <center>A single wind turbine.

# ## Conclusion

# This notebook has demonstrated a workflow to train a deep learning model that detects features representing wind turbines. This notebook uses multiprocessing to reduce the time required to export training data. In this notebook, we also leveraged multiple GPUs to reduce the time for model training. We can follow a similar approach to classify other objects of interest, like trees, buildings, structures, etc.

#  


# ====================
# untitled.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# In[ ]:


import nbformat
from nbconvert import PythonExporter
import os
import re

def remove_images_from_markdown(cell):
    if cell['cell_type'] == 'markdown':
        # Regular expression to find <img> tags with base64 encoded images
        pattern = r"<img src='data:image\/png;base64,[^']+'(.*?)>"
        cell['source'] = re.sub(pattern, '', cell['source'])
    return cell

def process_notebook(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        notebook = nbformat.read(file, as_version=4)

    # Process each cell
    processed_cells = [remove_images_from_markdown(cell) for cell in notebook['cells']]
    notebook['cells'] = processed_cells

    # Convert to Python file
    exporter = PythonExporter()
    python_code, _ = exporter.from_notebook_node(notebook)

    return python_code

def main():
    directory = '/path/to/your/notebooks'
    for file in os.listdir(directory):
        if file.endswith('.ipynb'):
            file_path = os.path.join(directory, file)
            python_code = process_notebook(file_path)
            # Write the Python code to a .py file
            with open(file_path.replace('.ipynb', '.py'), 'w', encoding='utf-8') as py_file:
                py_file.write(python_code)

if __name__ == "__main__":
    main()



# ====================
# vehicle_detection_and_tracking.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Vehicle detection and tracking using deep learning

# > * 🔬 Data Science
# * 🥠 Deep Learning and Object Detection
# * 🛤️ Tracking

# ## Table of Contents
# * [Introduction and objective](#Introduction-and-objective)
# * [Necessary imports](#Necessary-imports)
# * [Prepare data that will be used for training](#Prepare-data-that-will-be-used-for-training)
# * [Model training](#Model-training)
#  * [Visualize training data](#Visualize-training-data)
#  * [Load model architecture](#Load-model-architecture)
#  * [Train the model](#Train-the-model)
#  * [Visualize results on validation set](#Visualize-results-on-validation-set)
#  * [Save the model](#Save-the-model) 
# * [Inference and tracking](#Inference-and-tracking)
# * [Conclusion](#Conclusion)

# ## Introduction and objective

# Vehicle detection and tracking is a common problem with multiple use cases.
# Government authorities and private establishment might want to understand the traffic flowing through a place to better develop its infrastructure for the ease and convenience of everyone. A road widening project, timing the traffic signals and construction of parking spaces are a few examples where analysing the traffic is integral to the project.
# 
# Traditionally, identification and tracking has been carried out manually. A person will stand at a point and note the count of the vehicles and their types. Recently, sensors have been put into use, but they only solve the counting problem. Sensors will not be able to detect the type of vehicle.
# 
# In this notebook, we'll demonstrate how we can use deep learning to detect vehicles and then track them in a video. We'll use a short [video](https://youtu.be/e8cxjNE-4_I) taken from live traffic camera feed.

# ## Necessary imports

# In[1]:


import os
import pandas as pd
from pathlib import Path

from arcgis.gis import GIS
from arcgis.learn import RetinaNet, prepare_data


# In[2]:


gis = GIS('home')


# ## Prepare data that will be used for training

# You can download vehicle training data from [here](https://www.arcgis.com/home/item.html?id=ccaa060897e24b379a4ed2cfd263c15f). Extract the downloaded file to get your training data.

# ## Model training

# Let's set a path to the folder that contains training images and their corresponding labels.

# In[3]:


training_data = gis.content.get('ccaa060897e24b379a4ed2cfd263c15f')
training_data


# In[4]:


filepath = training_data.download(file_name=training_data.name)


# In[5]:


import zipfile
with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[6]:


data_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# We'll use the `prepare_data` function to create a fastai databunch with the necessary parameters such as `batch_size`, and `chip_size`. A complete list of parameters can be found in the [API reference](https://developers.arcgis.com/python/api-reference/arcgis.learn.toc.html#prepare-data).
# 
# The given dataset has 235 images of size 854x480 pixels. We will define a `chip_size` of 480 pixels which will create random crops of 480x480 from the given images. This way we will maintain the aspect ratios of the objects but can miss out on objects when training the model for fewer epochs. To avoid cropping, we can set `resize_to`=480 so that every chip is an entire frame and doesn't miss any object, but there is a risk of poor detection with smaller sized object.

# In[8]:


data = prepare_data(data_path, 
                    batch_size=4, 
                    dataset_type="PASCAL_VOC_rectangles", 
                    chip_size=480)


# We see the warning above because there are a few images in our dataset with missing corresponding label files. These images will be ignored while loading the data. If it is a significant number, we might want to fix this issue by adding the label files for those images or removing those images.

# We can use the `classes` attribute of the data object to get information about the number of classes.

# In[4]:


data.classes


# ### Visualize training data

# To visualize and get a sense of the training data, we can use the `data.show_batch` method.

# In[14]:


data.show_batch()


# In the previous cell, we see a sample of the dataset. We can observe, in the given chips, that the most common vehicles are cars and bicycles. It can also be noticed that the different instance of the vehicles have varying scales.

# ### Load model architecture

# `arcgis.learn` provides us object detection models which are based on pretrained convnets, such as ResNet, that act as the backbones. We will use `RetinaNet` with the default parameters to create our vehicle detection model. For more details on `RetinaNet` check out [How RetinaNet works?]() and the [API reference](https://developers.arcgis.com/python/api-reference/arcgis.learn.toc.html#retinanet).

# In[6]:


retinanet = RetinaNet(data)


# We will use the `lr_find()` method to find an optimum learning rate. It is important to set a learning rate at which we can train a model with good accuracy and speed.

# In[7]:


lr = retinanet.lr_find()


# ### Train the model

# We will now train the `RetinaNet` model using the suggested learning rate from the previous step. We can specify how many epochs we want to train for. Let's train the model for 100 epochs. Also, we can turn `tensorboard` True if we want to visualize the training process in tensorboard.

# In[8]:


retinanet.fit(100, lr=lr, tensorboard=True) 


# After the training is complete, we can view the plot with training and validation losses.

# In[21]:


retinanet.learn.recorder.plot_losses()


# ### Visualize results on validation set

# To see sample results we can use the `show_results` method. This method displays the chips from the validation dataset with ground truth (left) and predictions (right). We can also specify the threshold to view predictions at different confidence levels. This visual analysis helps in assessing the qualitative results of the trained model.

# In[18]:


retinanet.show_results(thresh=0.4)


# To see the quantitative results of our model we will use the `average_precision_score` method.

# In[19]:


retinanet.average_precision_score(detect_thresh=0.4)


# We can see the average precision for each class in the validation dataset. Note that while car and bicycle have a good score, van doesn't, and a few have a score of 0. Remember when we visualized the data using `show_batch` we noted that the cars and bicycles were the most common objects. It means, the scores could be correlated with the number of examples of these objects we have in our training dataset.
# 
# Let's look at the number of instances of each class in the training data and it should explain.

# In[22]:


all_classes = []
for i, bb in enumerate(data.train_ds.y):
    all_classes += bb.data[1].tolist()
    
df = pd.value_counts(all_classes, sort=False)
df.index = [data.classes[i] for i in df.index] 
df


# It is evident that the classes that have a score of 0.0 have extremely low number of examples in the training dataset.

# ### Save the model

# Let's save the model by giving it a name and calling the `save` method, so that we can `load` it later whenever required. The model is saved by default in a directory called `models` in the `data_path` initialized earlier, but a custom path can be provided.

# In[11]:


retinanet.save('vehicle_det_ep100_defaults')


# ## Inference and tracking

# 
# Multiple-object tracking can be performed using `predict_video` function of the `arcgis.learn` module. To enable tracking, set the `track` parameter in the `predict_video` function as `track=True`.
# 
# The following options/parameters are available in the predict video function for the user to decide:-
# 
# * `vanish_frames`: The number of frames the object remains absent from the frame to be considered as vanished.
# 
# * `detect_frames`: The number of frames an object remains present in the frame to start tracking.
# 
# * `assignment_iou_thrd`: There might be multiple trackers detecting and tracking objects. The Intersection over Union (iou) threshold can be set to assign a tracker with the mentioned threshold value.
# 

# In[9]:


video_data = gis.content.get('1801dc029fed467ba67d6e39113202af')
video_data


# In[10]:


videopath = video_data.download(file_name=video_data.name)


# In[11]:


import zipfile
with zipfile.ZipFile(videopath, 'r') as zip_ref:
    zip_ref.extractall(Path(videopath).parent)


# In[16]:


video_file = os.path.join(os.path.splitext(videopath)[0], 'test.mp4')


# In[ ]:


retinanet.predict_video(input_video_path=video_file, 
                        metadata_file='test.csv',
                        track=True, 
                        visualize=True, 
                        threshold=0.5,
                        resize=True)


# <video width="100%" height="450" loop="loop" controls src="../../static/video/test_predictions.mp4" />

# We can count the number of vehicles per unit of time and update a feature layer with the live count of cars, buses, trucks etc. When this process is done for multiple intersections within the city, an ArcGIS dashboard can be created. It queries the continually updated feature layers and displays the results using a dashboard such the following:

# <video width="100%" height="450" loop="loop" controls src="../../static/video/video_dc_parade.mp4" />

# ## Conclusion

# In this notebook, we have learnt how to automate multi-object tracking and counting system. This will not only help in  intelligent traffic management but can be found useful in wide variety of applications.


# ====================
# visualize_monthly_changes_in_hirakund_reservoir_using_video.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Visualize monthly changes in Hirakund reservoir using video

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Necessary Imports](#Necessary-Imports)
# * [Connect to your GIS](#Connect-to-your-GIS)
# * [Get the data for analysis](#Get-the-data-for-analysis)
# * [Function to create collection of images with desired time intervals](#Function-to-create-collection-of-images-with-desired-time-intervals)
# * [Make video from image collection](#Make-video-from-image-collection)
# * [Conclusion](#Conclusion)

# ## Introduction

# The World is changing daily. Major changes like shrinking of lakes, river path shifts, construction of megastructures can be seen directly from the satellite images. This notebook creates a movie to visualize monthly changes in Hirakund reservoir, Odisha.

# First, let's import all the necessary libraries and connect to our GIS via an existing profile or creating a new connection by e.g. ```gis = GIS("https://www.arcgis.com", "username", "Password").```
# 
# **Note**: to run this sample, you need a few extra libraries in your conda environment. If you don't have the libraries, install them by running the following commands from cmd.exe or your shell
# 
# ```
# conda install -c anaconda pillow
# conda install -c conda-forge imageio
# ```

# ## Necessary Imports

# In[1]:


get_ipython().system('conda install -c anaconda pillow -y')


# In[2]:


get_ipython().system('conda install -c conda-forge imageio=2.7 -y')


# In[3]:


import os
import imageio
import pandas as pd
import datetime as dt
from platform import system
from PIL import Image, ImageFont, ImageDraw

import arcgis
from arcgis.gis import GIS
from arcgis import geometry, geocode
from arcgis.raster.functions import apply


# ## Connect to your GIS

# In[4]:


ent_gis = GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# ## Get the data for analysis

# Search for Multispectral Landsat layer in ArcGIS Online.

# In[5]:


landsat_item = ent_gis.content.get('d9b466d6a9e647ce8d1dd5fe12eb434b')
landsat = landsat_item.layers[0]
landsat_item


# Applying Natural color to the filtered Landsat collection using predefined `apply` function

# In[6]:


rgb_collection = apply(landsat, 'Natural Color with DRA')


# In[14]:


g = geocode('Hirakund reservoir, Odisha', out_sr=3857)[0]
extent = g.get('extent')


# In[7]:


m = ent_gis.map('Hirakund reservoir, Odisha, India')
m.basemap = 'satellite'
m


# 

# ## Function to create collection of images with desired time intervals

# The function below creates an array of images with the desired time intervals. If a user specifies 'm' then the images in the selected collection will be consolidated on a monthly basis i.e. all the images of the specified extent will be mosaicked monthly and if the user specifies 'y' as the interval then the images in the selected collection will be consolidated on yearly basis.

# In[8]:


from functools import lru_cache

@lru_cache(maxsize=50)
def load_font():
    try:
        if system()=='Windows':
            return ImageFont.truetype("arial.ttf", 30)
        elif system()=='Linux':
            return ImageFont.truetype("~/.fonts/truetype/dejavu/DejaVuSans.ttf", 30)
        else:
            return ImageFont.truetype("Arial.ttf", 30)
    except:
        return ImageFont.load_default()


# In[9]:


def collection(df, interval, start, end, height, width):
    images=[]
    if(interval=='m'):                                                                                     # monthly
        for i in range(int(start.split('-')[0]), int(end.split('-')[0])+1):
            for j in range(1,13):
                selected = df[(df['AcquisitionDate'].dt.year == i) & (df['AcquisitionDate'].dt.month == j)]
                id = selected['OBJECTID'].values.tolist()
                if(len(id)>0):
                    rgb_collection.mosaic_by(method="LockRaster",lock_rasters=id)
                    img_name = 'img_'+str(i)+"-"+str(j)+".jpg"
                    rgb_collection.export_image(bbox=extent, size=[height,width], f='image', 
                                                  save_folder='.', 
                                                  save_file=img_name)
                    img = Image.open(img_name).convert('RGB')
                    font = load_font()
                    draw = ImageDraw.Draw(img)
                    draw.text((550, 0),str(j)+"-"+str(i),(255,255,255),font=font)
                    images.append(img)
                    os.remove(img_name)
                    
    elif(interval=='y'):                                                                                  # yearly
        for i in range(int(start.split('-')[0]), int(end.split('-')[0])+1):
            selected = df[df['AcquisitionDate'].dt.year == i]
            id = selected['OBJECTID'].values.tolist()
            if(len(id)>0):
                rgb_collection.mosaic_by(method="LockRaster",lock_rasters=id)
                img_name = 'img_'+str(i)+".jpg"
                rgb_collection.export_image(bbox=extent, size=[height,width], f='image', 
                                              save_folder='.', 
                                              save_file=img_name)
                img = Image.open(img_name).convert('RGB')
                font = load_font()
                draw = ImageDraw.Draw(img)
                draw.text((550, 0),str(i),(255,255,255),font=font)    
                images.append(img)
                os.remove(img_name)
    
    return images


# ## Make video from image collection

# The function below will generate a movie (gif) from the collection saved from the above step.

# In[10]:


def create_movie(target, interval, start, end, height, width, extent, duration):
    start_date = dt.datetime.strptime(start, '%Y-%m-%d')
    end_date = dt.datetime.strptime(end, '%Y-%m-%d')
    selected = target.filter_by(where="(Category = 1) AND (CloudCover <=0.5)",
                             time=[start_date, end_date],
                             geometry=arcgis.geometry.filters.intersects(extent))

    df = selected.query(out_fields="AcquisitionDate, GroupName, CloudCover, DayOfYear", 
                        order_by_fields="AcquisitionDate").sdf
    df['AcquisitionDate'] = pd.to_datetime(df['AcquisitionDate'], unit='ms')
    frames = collection(df, interval, start, end, height, width)
    imageio.mimsave('/arcgis/home/movie'+'_'+interval+'.gif', frames, duration=duration)
    print("Movie Created")


# In[15]:


create_movie(rgb_collection,'m' ,'2019-01-01','2019-12-31', 1250, 450, extent, 0.4)          # calling create_movie function


# The movie (gif) will be created in the same directory where your notebook exists. The gif below is generated using the code above which shows how the Hirakund reservoir in Odisha, India changed monthly in the year 2019.

# ![SegmentLocal](../../static/img/movie_m.gif "segment")

# ## Conclusion

# The sample notebook shows how you can animate an Imagery Layer over time to get a visual detail of the change that has happened on any given extent, either monthly or annually. You can bring any image collection that you have or use the image service provided by Esri Living Atlas and run this notebook against it. This can be repeated for any other location and for any other time interval by changing the `extent` variable and the `time` interval when calling the `create_movie()` function.


# ====================
# which_areas_are_good_cougar_habitat.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Which areas are good cougar habitat?

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# 
# <div class="toc">
#     <ul class="toc-item">
#         <li><span><a href="#Introduction" data-toc-modified-id="Introduction-1">Introduction</a></span></li>
#         <li><span><a href="#Workflow" data-toc-modified-id="Workflow-2">Workflow</a></span></li>
#         <li><span><a href="#Necessary-Imports" data-toc-modified-id="Necessary-Imports-3">Necessary Imports</a></span></li>
#         <li><span><a href="#Connect-to-your-GIS" data-toc-modified-id="Connect-to-your-GIS-4">Connect to your GIS</a></span></li>
#         <li><span><a href="#Get-the-data-for-analysis" data-toc-modified-id="Get-the-data-for-analysis-5">Get the data for analysis</a></span></li>
#         <li><span><a href="#Defining-the-project-boundary" data-toc-modified-id="Defining-the-project-boundary-6">Defining the project boundary</a></span></li>
#         <ul class="toc-item">
#             <li><span><a href="#Identify-the-area-within-three-miles-around-the-state-park" data-toc-modified-id="Identify-the-area-within-three-miles-around-the-state-park-6.1">Identify the area within three miles around the state park.</a></span></li>
#             <li><span><a href="#Create-the-study-area-boundary" data-toc-modified-id="Create-the-study-area-boundary-6.2">Create the study area boundary</a></span></li>   
#         <li><span><a href="#Create-a-layer-of-highway-features-within-the-study-area" data-toc-modified-id="Create-a-layer-of-highway-features-within-the-study-area-6.3">Create a layer of highway features within the study area</a></span></li>
#         </ul> 
#         <li><span><a href="#What-is-suitable-cougar-habitat?" data-toc-modified-id="What-is-suitable-cougar-habitat?-7">What is suitable cougar habitat?</a></span></li>
#         <ul class="toc-item">
#             <li><span><a href="#Identify-the-area-that-are-suitable-cougar-habitat-using-the-criteria-defined-by-the-experts-from-the-state-park" data-toc-modified-id="Identify-the-area-that-are-suitable-cougar-habitat-using-the-criteria-defined-by-the-experts-from-the-state-park-7.1">Identify the area that are suitable cougar habitat using the criteria defined by the experts from the state park</a></span></li>
#             <li><span><a href="#Identify-areas-that-are-suitable-cougar-habitat-using-the-criteria-defined-by-the-experts-from-DFW" data-toc-modified-id="Identify-areas-that-are-suitable-cougar-habitat-using-the-criteria-defined-by-the-experts-from-DFW-7.2">Identify areas that are suitable cougar habitat using the criteria defined by the experts from DFW</a></span></li>
#         </ul>    
#         <li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-11">Conclusion</a></span></li>
#         
# </div>

# ## Introduction

# Cascadia state park is a recreational paradise for camping, picnicking and hiking in the state of Oregon, US. Many visitors usually hike from the park to Soda Creek Falls on trails or old logging roads that continue into the Willamette national forest. It is about a mile and half from park. Research by <a href="https://www.fs.fed.us/">US forest service</a> indicates that the national forest is a habitat of approximate 6,400 cougar's population. With increase in cougar population, there is a tremendous increase in sightings by people. Park officials are therefore concerned about the safety of their visitors. On the other hand, they don't want to alarm potential visitors as this would mean reduced funding for the park in next year's state budget.
# 
# In view of this, a technical committee is set up, with wildlife experts from three agencies (State Park, National Forest, state's Department of Fish and Wildlife) to undertake a study of cougar populations in and around the park.

# This sample presents an approach to using ArcGIS API for Python to help committee identify potential cougar habitat. The process uses suitability analysis to identify areas that meet the specified criteria.
# Through this notebook, we will demonstrate the utility of a number of spatial analysis tools including `create_buffer`, `extract_data`, `dissolve_boundaries`, and `derive_new_locations`. 
# 
# Further, based on the results of the analysis, the committee can identify large habitat areas for various management goals - conservation, management of wildlife, recreation, and so on. 

# ## Workflow

# 

# ## Necessary Imports

# In[1]:


get_ipython().run_line_magic('matplotlib', 'inline')

import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import display
from datetime import datetime as dt

from arcgis.gis import GIS
from arcgis.features.use_proximity import create_buffers
from arcgis.features.manage_data import dissolve_boundaries
from arcgis.features.manage_data import extract_data
from arcgis.features.find_locations import derive_new_locations


# ## Connect to your GIS

# In[2]:


gis = GIS("home")


# ### Get the data for analysis

# Search the GIS for feature layer collections by specifying the item type as 'Feature Layer Collection' or 'Feature Layer'. We will specify our search by specific tag as **cougar_habitat_case_study**.

# In[3]:


items = gis.content.search('title:cougar_habitat_case_study AND owner:api_data_owner', 'Feature Layer')


# In[4]:


for item in items:
    display(item)


# We will use the first item for our analysis. Since the item is a Feature Layer Collection, accessing the layers property will give us a list of FeatureLayer objects.

# In[5]:


cougar_item = items[0]


# The code below cycles through the layers and prints their names.

# In[6]:


for lyr in cougar_item.layers:
    print(lyr.properties.name)


# Let us assign a variable to each of these feature layers.

# In[7]:


state_park = cougar_item.layers[0]
vegetation = cougar_item.layers[1]
slope = cougar_item.layers[2]
sub_watershed = cougar_item.layers[3]
highways = cougar_item.layers[4]
streams = cougar_item.layers[5]


# Let's plot the state park on map for visualization.

# In[8]:


m1 = gis.map('oregon')
m1.basemap = 'arcgis-light-gray'
m1


# ![image](https://user-images.githubusercontent.com/13968196/232156315-12eda133-14d8-49a3-b5c7-9503e45943c6.png)

# In[9]:


m1.zoom_to_layer(state_park)
m1.add_layer(state_park)


# ## Defining the project boundary

# Park officials are mainly concerned with the area in and around the park where visitors might encounter a cougar. Their surveys have found that many people hike, at most, about three miles from the park (a six-mile round trip), an area that encompasses the trails leading into the national forest. 

# ### Identify the area within three miles around the state park

# To create the buffer layer, we will use `create_buffers` tool and specify the distance of 3 miles. This will create proposed area boundary for park officials.

# In[10]:


buffer_park = create_buffers(state_park, 
                             dissolve_type='Dissolve', 
                             distances=[3],
                             ring_type='Rings', 
                             units='Miles', 
                             output_name="BufferPark" + str(dt.now().microsecond))


# In[11]:


buffer_park


# In[12]:


m2 = gis.map('oregon')
m2.basemap = 'arcgis-light-gray'
m2


# 
# ![image](https://user-images.githubusercontent.com/13968196/232156588-bd5be8bc-00ba-4a95-9e15-e2e9a2818647.png)

# In[13]:


m2.zoom_to_layer(buffer_park.layers[0])
m2.add_layer(buffer_park.layers[0])


# However, since DFW has several cougar habitat mapping projects in progress, they have been mapping wildlife habitat watershed-by-watershed in other parts of the state. Therefore, they suggest major watersheds that includes the park and the western portions of the forest as the study area. Given the available funding, the committee members decide to limit the study area to three sub watersheds in the southeast portion of the major watershed. This area encompasses the state park and a portion of the national forest.

# ### Create the study area boundary

# We will apply a filter on sub watersheds layer to select the ones that cover state park as well as major portion of national forest.

# In[14]:


sub_watershed.filter = '(HUC5_ID = 550) OR (HUC5_ID = 556) OR (HUC5_ID = 569)'


# Then we will use the `dissolve_boundaries` tool to erase the boundaries between the filtered sub-watersheds, creating a new layer containing the study area boundary.

# In[15]:


state_area_boundary = dissolve_boundaries(sub_watershed, 
                                          output_name='DissolveBoundaries' + str(dt.now().microsecond))


# In[16]:


state_area_boundary


# In[17]:


m3 = gis.map('oregon')
m3.basemap = 'arcgis-light-gray'
m3


# ![image](https://user-images.githubusercontent.com/13968196/232156867-d1abee6c-dbb7-44ca-a904-9da00281204b.png)
# 
# 

# In[18]:


m3.zoom_to_layer(state_area_boundary.layers[0])
m3.add_layer(state_area_boundary.layers[0])


# ### Create a layer of highway features within the study area

# The `extract_data` tool is used to extract data from one or more layers within a given extent. The extracted data format can be a file geodatabase, shapefiles, csv, or kml. File geodatabases and shapefiles are added to a zip file that can be downloaded. for more details click [here](https://developers.arcgis.com/python/api-reference/arcgis.features.manage_data.html?highlight=extract_data#arcgis.features.manage_data.extract_data).

# To clip features using another layer, we will extract the features to a file, download the file to local device, and then add the file containing the features back into layer.

# In[19]:


ext_state_highway = extract_data(input_layers=[highways],
                                 extent=state_area_boundary.layers[0],
                                 clip=True,
                                 data_format='shapefile',
                                 output_name='ext_state_highway' + str(dt.now().microsecond))


# In[20]:


ext_state_highway


# The shapefile can now be published as feature layer collection by using publish method.

# In[21]:


clipped_highway_lyr = ext_state_highway.publish()


# In[22]:


display(clipped_highway_lyr)


# In[23]:


m4 = gis.map('oregon')
m4.basemap = 'arcgis-light-gray'
m4


# ![image](https://user-images.githubusercontent.com/13968196/232159170-49275dee-911f-435c-8d0d-4ce3346dd0d5.png)
# 

# In[25]:


m4.zoom_to_layer(clipped_highway_lyr)
m4.add_layer(state_area_boundary)
m4.add_layer(clipped_highway_lyr)


# ## What is suitable cougar habitat?

# In order to find out suitable cougar habitat, the method that DFW has been using for its other habitat studies is suitability analysis, and, for consistency, the committee members decide to stick with this approach for the current study.
# 
# In suitability analysis, criteria are specified for what makes an area suitable for a particular use such as a housing subdivision, a wind farm, or cougar habitat. The criteria are often based on firsthand experience, expert knowledge (including published studies), or industry standards.
# 
# The committee discuss the following criteria for the study area:
# 1.	slope: The first criterion is terrain. Earlier studies by DFW indicate that cougars are generally found on steep slopes. Hence, it is a preferred habitat.
# 2.	Vegetation: Studies by wildlife experts indicate that cougars are likely to be found in forested areas that provide cover for hunting. Three forest types within the study area clearly constitute suitable habitat i.e. True Fir-Hemlock Montane Forest (code 34), Douglas Fir-Western Hemlock-Red Cedar Forest (code 49), and Mixed Conifer/Mixed Deciduous Forest (code 67). State park officials considers including Regenerating young forest (code 121) as one of the preferred areas of cougar habitat.
# 3.	Streams: The DFW experts present research showing that cougars have a wide home area and can easily find water, although the presence of streams might provide better habitat. The research has found that the area within 2,500 feet of a stream can be considered preferred habitat. The experts from the state park counter with a study by a local college showing that cougars often use streams and riparian areas as corridors to move around their territory. They believe preferred habitat should be limited to areas within 500 feet of a stream.
# 4.	Highways: The experts from DFW and the national forest cite research showing that in areas where there are many roads there may be fewer prey (deer and elk), which can make the area less desirable habitat for cougars. But there is only one major road in the study area. They believe that cougars might stay at most 500 feet away from the highway, but even then they would likely approach or cross the highway in search of prey. The experts from the state park believe that the presence of cars and people on the roads within the park would tend to make the immediate area less desirable for cougars. They suggest excluding as habitat the area within 1,500 feet of the highway, which would cover the roads inside the park.

# The state park officials are mainly interested in identifying cougar habitat in the vicinity of the park, to ensure the safety of their visitors. The table below lists general criteria and specific values for both approaches.
# 

# 

# ### Identify the area that are suitable cougar habitat using the criteria defined by the experts from the state park

# The `derive_new_locations` tool allows you to combine attribute and spatial criteria in a single statement by adding a set of expressions, one at a time. A new layer of areas that meet the specified criteria is created.

# Before running `derive_new_locations`, we will filter the vegetation layer to select three vegetation types to be included: codes 34, 49, 67. We can actually specify the vegetation criteria in the Derive New Locations tool, but using the filter first will create a simpler selection statement. It will also streamline the process when we run the analysis again using the criteria defined by the Department of Fish and Wildlife.)

# In[25]:


vegetation.filter = '(VEG_CODE = 34) OR (VEG_CODE = 49) OR (VEG_CODE = 67)'


# Criteria defined by park officials for identifying suitable cougar habitat.
# - Slope where GRIDCODE is 1
# - Slope intersects Vegetation
# - Slope within a distance of 500 feet from Stream
# - Slope not within a distance of 1500 feet from Highway

# In[26]:


potential_cougar_habitat_A = derive_new_locations(input_layers=[slope, vegetation, streams, highways],
                                           expressions=[{"operator":"","layer":0,"selectingLayer":1,"spatialRel":"intersects"},
                                                        {"operator":"and","layer":0,"selectingLayer":2,"spatialRel":"withinDistance","distance":500,"units":"Feet"},
                                                        {"operator":"and","layer":0,"selectingLayer":3,"spatialRel":"notWithinDistance","distance":1500,"units":"Feet"},
                                                        {"operator":"and","layer":0,"where":"GRIDCODE = 1"}],
                                           output_name='derive_new_loactions' + str(dt.now().microsecond))


# In[27]:


m5 = gis.map('oregon')
m5.basemap = 'arcgis-light-gray'
m5


# ![image](https://user-images.githubusercontent.com/13968196/232159694-12d915bc-8679-4b04-ac76-79e5db51799a.png)
# 

# In[28]:


m5.zoom_to_layer(potential_cougar_habitat_A.layers[0])
m5.add_layer(potential_cougar_habitat_A)


# ### Identify areas that are suitable cougar habitat using the criteria defined by the experts from DFW

# First, we will edit the Vegetation filter to add the timber harvest areas. Add a fourth expression and specify VEG_CODE is 121

# In[29]:


vegetation.filter = '(VEG_CODE = 121)'


# Criteria defined by DFW for identifying suitable cougar habitat.
# - Slope where GRIDCODE is 1
# - Slope intersects Vegetation
# - Slope within a distance of 2500 feet from Stream
# - Slope not within a distance of 500 feet from Highway

# In[30]:


potential_cougar_habitat_B = derive_new_locations(input_layers=[slope, vegetation, streams, highways],
                                           expressions=[{"operator":"","layer":0,"selectingLayer":1,"spatialRel":"intersects"},
                                                        {"operator":"and","layer":0,"selectingLayer":2,"spatialRel":"withinDistance","distance":2500,"units":"Feet"},
                                                        {"operator":"and","layer":0,"selectingLayer":3,"spatialRel":"notWithinDistance","distance":500,"units":"Feet"},
                                                        {"operator":"and","layer":0,"where":"GRIDCODE = 1"}],
                                           output_name='derive_new_loactions' + str(dt.now().microsecond))


# In[31]:


m6 = gis.map('oregon')
m6.basemap = 'arcgis-light-gray'
m6


# ![image](https://user-images.githubusercontent.com/13968196/232160248-0a4da153-0cb4-4695-bb37-3f538b3fda57.png)
# 

# In[32]:


m6.zoom_to_layer(potential_cougar_habitat_B.layers[0])
m6.add_layer(potential_cougar_habitat_B)


# ### Conclusion

# The state park officials can see that their criteria resulted in a very limited cougar habitat area. The staff from DFW and the national forest, however, are confident that the results created using their criteria are a good first attempt at identifying potential cougar habitat.
# 
# Thus, scientists from DFW and the national forest decide to work together to field check the results, looking for evidence of cougars inside, as well as outside, the potential habitat areas. 

# In[ ]:






# ====================
# which_college_district_has_the_fewest_low_income_families.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Which college district has the fewest low-income families?

# A pilot program was run by a local cable operator in the county to provide low-cost computers and Internet access to low-income families with kids in high school. This showed a marked improvement in school performance for these kids, and the program has brought the company a fair amount of positive publicity and goodwill in the community.
# 
# Company officials now want to set up a similar program for community college students. The company provides Internet access to the five community college districts in the county, and officials are aware that the colleges are under a lot of pressure - they are facing funding cuts at the same time as increased demand for enrollment. To try to improve the situation the colleges are turning more and more to distance learning, primarily via the Internet. By providing computers and Internet access, the cable company can enable more low-income students to take advantage of online classes.
# 
# This case study uses ArcGIS API for Python to find districts that have the fewest low income families in order to empower these students.
# 
# We will use ``summarize_within`` tool to get the number of low-income families within each community district. We will also visualize this using the map widget.

# 

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Connect-to-your-ArcGIS-Online-organization" data-toc-modified-id="Connect-to-your-ArcGIS-Online-organization-2">Connect to your ArcGIS Online organization</a></span></li><li><span><a href="#Get-data-for-analysis" data-toc-modified-id="Get-data-for-analysis-3">Get data for analysis</a></span></li><li><span><a href="#Find-the-community-college-district-with-the-fewest-low-income-families" data-toc-modified-id="Find-the-community-college-district-with-the-fewest-low-income-families-4">Find the community college district with the fewest low-income families</a></span></li><li><span><a href="#Get-the-number-of-low-income-households-in-each-district" data-toc-modified-id="Get-the-number-of-low-income-households-in-each-district-5">Get the number of low-income households in each district</a></span></li><li><span><a href="#Visualization-to-show-district-with-fewest-households" data-toc-modified-id="Visualization-to-show-district-with-fewest-households-6">Visualization to show district with fewest households</a></span></li><li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-7">Conclusion</a></span></li></ul></div>

# ### Connect to your ArcGIS Online organization
# 
# 
# We first establish a connection to our organization which could be an ArcGIS Online organization or an ArcGIS Enterprise. To be able to run the code using ArcGIS API for Python, we will need to provide credentials of a user within an ArcGIS Online organization.

# In[1]:


from arcgis.gis import GIS
import pandas as pd


# Please sign-in into your organization to continue to execute this notebook.

# In[3]:


gis = GIS('home')


# ## Get data for analysis

# In[3]:


san_diego_data = gis.content.search('title:CommunityCollege_CensusTracts owner:api_data_owner', 
                                 'Feature layer',
                                  outside_org=True)


# In[4]:


san_diego_data


# In[5]:


from IPython.display import display

for item in san_diego_data:
    display(item)


# In[6]:


san_diego_item = san_diego_data[0] # get first item from the list of items


# In[7]:


for lyr in san_diego_item.layers:
    print(lyr.properties.name)


# Since the item is a Feature Layer Collection, accessing the layers property will give us a list of Feature Layers.

# In[8]:


census_tract_income = san_diego_item.layers[0]


# In[9]:


community_college_dist = san_diego_item.layers[1] 


# In[17]:


m1 = gis.map('San Diego')
m1


# In[16]:


m1.add_layer(community_college_dist)


# In[20]:


m2 = gis.map('San Diego')
m2


# ### Find the community college district with the fewest low income families

# Convert the layer into pandas dataframe to calculate the number of households in each tract with income less than $30,000.

# In[21]:


sdf = pd.DataFrame.spatial.from_layer(census_tract_income)


# In[22]:


sdf.columns


# In[23]:


sdf.head()


# The census tract layer contains the number of households in each of several income categories, such as less than \$10,000, \$10,000 to \$15,000, \$15,000 to \$20,000, and so on.
# 
# The aim of the project is to provide support to families with an annual income less than \$30,000.
# 
# We will add a field to the census tract dataframe and sum the number of households in each tract with income less than \$30,000.

# In[24]:


sdf['income_lt_30k'] = sdf['INCOME_LES'] + sdf['INCOME_10K'] + sdf['INCOME_15K'] + sdf['INCOME_20K'] + sdf['INCOME_25K']


# In[25]:


sdf.income_lt_30k.head()


# In[26]:


sdf.head()


# In[27]:


sdf.shape


# We will import the spatially enabled dataframe back into the GIS and create a feature layer. 

# In[28]:


census_tract = gis.content.import_data(sdf,
                                       title='CensusTract',
                                       tags='datascience')


# In[22]:


census_tract


# ## Get the number of low-income households in each district

# We will summarize census tracts by community college districts to find the total number of low-income households in each district. If a tract falls in two or more districts, the value for that tract will be split proportionally between the districts (based on the area of the tract in each district).

# In[29]:


from arcgis.features.summarize_data import summarize_within
from datetime import datetime as dt


# In[30]:


tracts_within_boundary = summarize_within(community_college_dist,
                                          census_tract,
                                          summary_fields=["income_lt_ SUM"],
                                          shape_units='SquareMiles',
                                          output_name='TractsWithinBoundary' + str(dt.now().microsecond))


# In[31]:


tracts_within_boundary


# In[34]:


m3 = gis.map('San Diego')
m3


# In[33]:


m3.add_layer(tracts_within_boundary)


# The map displays the census tracts color-coded by the number of households in each census tract with income less than $30,000 per year.

# In[35]:


tracts_within_boundary_lyr = tracts_within_boundary.layers[0]


# In[36]:


sdf = pd.DataFrame.spatial.from_layer(tracts_within_boundary_lyr)


# In[37]:


sdf.columns


# In[38]:


sdf.sort_values(['sum_income_lt_'], inplace=True)


# In[39]:


sdf.head()


# ### Visualization to show district with fewest households

# In[42]:


m4 = gis.map('San Diego')
m4


# In[41]:


m4.add_layer(tracts_within_boundary, {"renderer":"ClassedSizeRenderer",
                                      "field_name": "sum_income_lt_"})


# It's clear that the Mira Costa district has by far the fewest low-income households. That's where the pilot program could be set up.

# ## Conclusion

# We have successfully located a district with the fewest low income families.
# We can assess the success of the project for the next 6 months and give recommendations to expand the program across other areas in the country.


# ====================
# wildfire_analysis_using_sentinel-2_imagery.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# #  Pawnee Fire Analysis
# 
# > * 🔬 Data Science
# * 🖥️ Requires RasterAnalytics Portal Configuration
# * 🖥️ Requires GeoEnrichment Portal Configuration
# * 🖥️ Requires GeoAnalytics Portal Configuration
# 
# The Pawnee Fire was a large wildfire that burned in Lake County, California. The fire started on June 23, 2018 and burned a total of 15,185 acres (61 km2) before it was fully contained on July 8, 2018.

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Pawnee-Fire-analysis" data-toc-modified-id="Pawnee-Fire-analysis-1">Pawnee Fire Analysis</a></span><ul class="toc-item"><li><span><a href="#Remote-Sensing-using-Sentinel-2-data" data-toc-modified-id="Remote-Sensing-using-Sentinel-2-data-1.1">Remote Sensing using Sentinel-2 data</a></span></li><li><span><a href="#Data-Preparation" data-toc-modified-id="Data-Preparation-1.2">Data Preparation</a></span></li><li><span><a href="#Visual-Assessment" data-toc-modified-id="Visual-Assessment-1.3">Visual Assessment</a></span></li><li><span><a href="#Quantitative-Assessment" data-toc-modified-id="Quantitative-Assessment-1.4">Quantitative Assessment</a></span></li><li><span><a href="#Impact-Assessment" data-toc-modified-id="Impact-Assessment-1.5">Impact Assessment</a></span></ul></li>
# <li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-2">Conclusion</a></span></li></ul></div>

# ## Remote Sensing using Sentinel-2 data

# In[1]:


from datetime import datetime
import warnings

from IPython.display import HTML
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import arcgis
from arcgis import GIS
from arcgis.raster.functions import *
from arcgis.geoanalytics.use_proximity import create_buffers
from arcgis.geoenrichment import enrich
from arcgis.features import SpatialDataFrame
from arcgis.raster.analytics import create_image_collection
from arcgis.raster.analytics import list_datastore_content

gis= GIS('https://pythonapi.playground.esri.com/portal', 'arcgis_python', 'amazing_arcgis_123')


# ### Data Preparation

# In this analysis, we will be using Sentinel-2 data.
# 
# Sentinel-2 is an Earth observation mission developed by ESA as part of the Copernicus Programme to perform terrestrial observations in support of services such as forest monitoring, land cover change detection, and natural disaster management.
# 
# In this analysis data downloaded from https://earthexplorer.usgs.gov/ is used for creating hosted image service. 
# We add the data to the datastore and we then run the create_image_collection function which creates a collection with the input_rasters specified and publishes the collection as an image service. 
# 

# We use list_datasore_content() in order to see the contents in the rasterstore.

# In[2]:


list_datastore_content("/rasterStores/LocalRasterStore")


# In[3]:


sentinel_collection = create_image_collection(image_collection="pawnee_fire_multispectral",
                      input_rasters=["/rasterStores/LocalRasterStore/S2A_MSIL1C_20180624T184921_N0206_R113_T10SEJ_20180624T234856.SAFE",
                                     "/rasterStores/LocalRasterStore/S2B_MSIL1C_20180622T185919_N0206_R013_T10SEJ_20180622T205930.SAFE"],
                      raster_type_name="Sentinel-2", 
                      raster_type_params={"productType":"All","processingTemplate":"Multispectral"},
                      context={"image_collection_properties":{"imageCollectionType":"Satellite"},"byref":True}, gis = gis)


# In[9]:


sentinel = sentinel_collection.layers[0]


# ### Select before and after rasters

# In[10]:


aoi = {'spatialReference': {'latestWkid': 3857, 'wkid': 102100},
 'xmax': -13643017.100720055,
 'xmin': -13652113.10708598,
 'ymax': 4739654.477447927,
 'ymin': 4731284.622850712}
arcgis.env.analysis_extent = aoi
sentinel.extent = aoi


# In[11]:


selected = sentinel.filter_by(where="acquisitiondate BETWEEN timestamp '2018-06-15 00:00:00' AND timestamp '2018-06-24 19:59:59'",
                              geometry=arcgis.geometry.filters.intersects(aoi))

df = selected.query(out_fields="*", order_by_fields="OBJECTID ASC").df
df['AcquisitionDate'] = pd.to_datetime(df['AcquisitionDate'], unit='ms')
df.tail(40)


# In[12]:


prefire = sentinel.filter_by('OBJECTID=2') 
midfire = sentinel.filter_by('OBJECTID=1')


# ## Visual Assessment

# In[13]:


truecolor = extract_band(midfire, [4,3,2])
truecolor


# ### Visualize Burn Scars

# We extract the [13, 12, 4] bands to improve visibility of fire and burn scars. This band combination pushes further into the SWIR range of the electromagnetic spectrum, where there is less susceptibility to smoke and haze generated by a burning fire.

# In[9]:


extract_band(midfire, [13,12,4])


# For comparison, the same area before the fire started shows no burn scar.

# In[10]:


extract_band(prefire, [13,12,4])


# ## Quantitative Assessment

# The **Normalized Burn Ratio (NBR)** can be used to delineate the burnt areas and identify the severity of the fire. 
# 
# The formula for the NBR is very similar to that of NDVI except that it uses near-infrared band 9 and the short-wave infrared band 13:
# \begin{align}
# {\mathbf{NBR}} = \frac{\mathbf{B9} - \mathbf{B13}}{\mathbf{B9} + \mathbf{B13} + \mathbf{WS}} \\   
# \end{align}
# 
# The NBR equation was designed to be calcualted from reflectance, but it can be calculated from radiance and digital_number_(dn) with changes to the burn severity table below. The WS parameter is used for water suppression, and is typically 2000. 
# 
# For a given area, NBR is calculated from an image just prior to the burn and a second NBR is calculated for an image immediately following the burn. Burn extent and severity is judged by taking the difference between these two index layers:
# 
# \begin{align}
# {\Delta \mathbf{NBR}} = \mathbf{NBR_{prefire}} - \mathbf{NBR_{postfire}} \\   
# \end{align}
# 
# The meaning of the ∆NBR values can vary by scene, and interpretation in specific instances should always be based on some field assessment. However, the following table from the USGS FireMon program can be useful as a first approximation for interpreting the NBR difference:
# 
# 
# | \begin{align}{\Delta \mathbf{NBR}}  \end{align}      | Burn Severity |
# | ------------- |:-------------:|
# | 0.1 to 0.27   | Low severity burn |
# | 0.27 to 0.44  | Medium severity burn |
# | 0.44 to 0.66 | Moderate severity burn |
# | > 0.66 | High severity burn |
# 
# [Source: http://wiki.landscapetoolbox.org/doku.php/remote_sensing_methods:normalized_burn_ratio]

# ### Use Band Arithmetic and Map Algebra 

# In order to perform raster analysis on raw pixel value, we filter out the scenes from the sentinel image service again and create new layers

# In[11]:


nbr_prefire  = band_arithmetic(prefire, "(b9 - b13) / (b9 + b13 + 2000)")
nbr_postfire = band_arithmetic(midfire, "(b9 - b13) / (b9 + b13 + 2000)")

nbr_diff = nbr_prefire - nbr_postfire


# In[12]:


burnt_areas = colormap(remap(nbr_diff, 
                             input_ranges=[0.1,  0.27,  # low severity 
                                           0.27, 0.44,  # medium severity
                                           0.44, 0.66,  # moderate severity
                                           0.66, 1.00], # high severity burn
                             output_values=[1, 2, 3, 4],                    
                             no_data_ranges=[-1, 0.1], astype='u8'), 
                       colormap=[[4, 0xFF, 0xC3, 0], [3, 0xFA, 0x8E, 0], [2, 0xF2, 0x55, 0], [1, 0xE6, 0,    0]])


# In[13]:


# Visualize burnt areas
burnt_areas


# With this, we have computed the NBR on scenes from before and after the burn, and computed the NBR difference to identify places that have been affected by the fire. We've also normalized the values to match a burn severity index, and applied a color map that brings out the extent of fire damage.
# 

# ### Area calculation

# In[14]:


pixx = (aoi['xmax'] - aoi['xmin']) / 1200.0
pixy = (aoi['ymax'] - aoi['ymin']) / 450.0

res = burnt_areas.compute_histograms(aoi, pixel_size={'x':pixx, 'y':pixy})

numpix = 0
histogram = res['histograms'][0]['counts'][1:]
for i in histogram:
    numpix += i


# ### Report burnt area

# In[15]:


sqmarea = numpix * pixx * pixy # in sq. m
acres = 0.00024711 * sqmarea   # in acres

HTML('<h3>Fire has consumed <font color="red">{:,} acres</font>  till {}</h3>.' \
     .format(int(acres), df.iloc[-1]['AcquisitionDate'].date()))


# In[16]:


get_ipython().run_line_magic('matplotlib', 'inline')

plt.title('Distribution by severity', y=-0.1)
plt.pie(histogram, labels=['Low Severity', 'Medium Severity', 'Moderate Severity', 'High Severity']);
plt.axis('equal');


# ### Visualize burnt areas

# In[35]:


firemap = gis.map()
firemap.extent = aoi
firemap.add_layer([truecolor, burnt_areas])

firemap


# ### Persist the burnt areas layer in the GIS

# If required, using the save(), we can persist the output in the gis as a new layer. This uses distributed raster analysis to perform the analysis at the source resolution.

# In[18]:


burnt_areas = burnt_areas.save()


# ## Raster to Feature layer conversion

# Use Raster Analytics and Geoanalytics to convert the burnt area raster to a feature layer. The `to_features()` method converts the raster to a feature layer and `create_buffers()` fills holes in the features and dissolves them to output one feature that covers the extent of the Pawnee Fire.

# In[19]:


burnt_areas = burnt_areas.layers[0]
fire_item = burnt_areas.to_features(output_name='Pawnee_Fire_Feature_Layer', gis=gis)
fire_layer = fire_item.layers[0]


# In[3]:


fire = create_buffers(fire_layer, 100, 'Meters', dissolve_option='All', multipart=True, output_name='PawneeFireArea_Buffer')
fire = fire.layers[0]


# ## Visualize Feature Layer

# In[40]:


vectormap = gis.map()
vectormap.basemap = 'dark-gray'
vectormap.extent  = aoi

vectormap.add_layer(fire)
vectormap


# ## Impact Assessment

# ### Assess Human Impact

# In[24]:


from arcgis import geometry 
 
sdf = SpatialDataFrame.from_layer(fire)

fire_geometry = sdf.iloc[0].SHAPE
sa_filter = geometry.filters.intersects(geometry=fire_geometry, sr=4326)

def age_pyramid(df):
    get_ipython().run_line_magic('matplotlib', 'inline')
    warnings.simplefilter(action='ignore', category=FutureWarning)
    pd.options.mode.chained_assignment = None 
    plt.style.use('ggplot')

    df = df[[x for x in impacted_people.columns if 'MALE' in x or 'FEM' in x]]
    sf = pd.DataFrame(df.sum())
    age = sf.index.str.extract('(\d+)').astype('int64')
    f = sf[sf.index.str.startswith('FEM')]
    m = sf[sf.index.str.startswith('MALE')]
    sf = sf.reset_index(drop = True)
    f = f.reset_index(drop = True)
    m = m.reset_index(drop = True)
    sf['age'] = age
    f["age"] = age
    m["age"] = age
    f = f.sort_values(by='age', ascending=False).set_index('age')
    m = m.sort_values(by='age', ascending=False).set_index('age')
    

    popdf = pd.concat([f, m], axis=1)
    popdf.columns = ['F', 'M']
    popdf['agelabel'] = popdf.index.map(str) + ' - ' + (popdf.index+4).map(str)
    popdf.M = -popdf.M
    
    sns.barplot(x="F", y="agelabel", color="#CC6699", label="Female", data=popdf, edgecolor='none')
    sns.barplot(x="M",  y="agelabel", color="#008AB8", label="Male",   data=popdf,  edgecolor='none')
    plt.ylabel('Age group')
    plt.xlabel('Number of people');
    return plt;


# ### Age Pyramid of Affected Population

# In[25]:


impacted_people = enrich(sdf, 'Age')
age_pyramid(impacted_people);


# # Conclusion
# 
# In this notebook example, we used Sentinel-2 data in order to perform remote sensing. For this we filtered out pre and post fire scenes. Using extract_band() we carried out visual assessment of the burnt area. We then computed the NBR on these scenes and computed the NBR difference to identify places that have been affected by the fire, using raster functions. We also normalized the values to match the burn severity index, applied a color map raster function that brings out the extent of fire damage and calculated the burnt area. Finally, we carried out a human impact assessment by plotting the age pyramid of affected population 


# ====================
# wildlife_species_identification_in_camera_trap_images.py
# ====================

#!/usr/bin/env python
# coding: utf-8

# # Wildlife Species Identification in Camera Trap Images
# > * 🔬 Data Science
# > * 🥠 Deep Learning and Image Classification

# ## Table of Contents
# * [Introduction](#Introduction)
# * [Necessary imports](#Necessary-imports)
# * [Connect to your GIS](#Connect-to-your-GIS)
# * [Get the data for analysis](#Get-the-data-for-analysis)
#  * [Data selection](#Data-selection)
#  * [Prepare data](#Prepare-data)
#  * [Visualize a few samples from your training data](#Visualize-a-few-samples-from-your-training-data)
# * [Train the model](#Train-the-model)
#  * [Load model architecture](#Load-model-architecture)
#  * [Find an optimal learning rate](#Find-an-optimal-learning-rate)
#  * [Fit the model](#Fit-the-model)
#  * [Visualize results in validation set](#Visualize-results-in-validation-set)
#  * [Save the model](#Save-the-model)
# * [Model Inference](#Model-inference)
#  * [Data preparation](#Data-preparation)
#  * [Inferencing](#Inferencing) 
# * [Conclusion](#Conclusion)

# ## Introduction 

# Automatic animal identification can improve biology missions that require identifying species and counting individuals, such as animal monitoring and management, examining biodiversity, and population estimation.
# 
# This notebook will showcase a workflow to classify animal species in camera trap images. The notebook has two main sections:
# 1. Training a deep learning model that can classify animal species  
# 2. Using the trained model to classify animal species against each camera location
# 
# We have used a subset of a community licensed open source dataset for camera trap images, provided under the [LILA BC](http://lila.science/) (Labeled Information Library of Alexandria: Biology and Conservation) repository, to train our deep learning model, which is further detailed in [this](#Get-the-data-for-analysis) section. For inferencing, we have taken 5 fictional camera locations in Kruger National Park, South Africa, and to each of those points, we have attached some images. This feature layer simulates a scenario where there are multiple cameras at different locations that have captured images that need to be classified for animal species. The whole workflow enabling this is explained in [this](#Model-inference) section.

# Note: This notebook is supported in ArcGIS Enterprise 10.9 and later.

# ## Necessary imports 

# In[1]:


import os
import json
import zipfile
import pandas as pd
from pathlib import Path
from fastai.vision import ImageDataBunch, get_transforms, imagenet_stats

from arcgis.gis import GIS
from arcgis.learn import prepare_data, FeatureClassifier, classify_objects, Model


# ## Connect to your GIS 

# In[2]:


# connect to web GIS
gis = GIS("Your_enterprise_profile")                               # ArcGIS Enterprise 10.9 or later


# ## Get the data for analysis

# In this notebook, we have used the "WCS Camera Traps" dataset, made publicly available by the [Wildlife Conservation Society](https://www.wcs.org/) under the [LILA BC repository](http://lila.science/). This dataset contains approximately 1.4M camera trap images representing different species from 12 countries, making it one of the most diverse camera trap data sets publicly available. The dataset can be further explored and downloaded from [this link](http://lila.science/datasets/wcscameratraps). This data set is released under the [Community Data License Agreement (permissive variant)](https://cdla.io/permissive-1-0/).

# ### Data selection

# This dataset has 1.4M images related to 675 different species, of which we have chosen 11 species whose conservation status was either "Endangered", "Near Threatened", or "Vulnerable". These species include Jaguars, African elephants, Lions, Thomson's gazelles, East African oryxs, Gerenuks, Asian elephants, Tigers, Ocellated turkeys, and Great curassows.

# The json file (wcs_camera_traps.json) that comes with the data contains metadata we require.
# The cell below converts the downloaded data into required format to be used by `prepare_data()`.

# In[3]:


# # list containing corrupted filenames from the data downloaded
# corrupted_files=['animals/0402/0905.jpg', 'animals/0407/1026.jpg', 'animals/0464/0880.jpg',  
#                  'animals/0464/0881.jpg', 'animals/0464/0882.jpg', 'animals/0464/0884.jpg',
#                  'animals/0464/0888.jpg', 'animals/0464/0889.jpg', 'animals/0645/0531.jpg',
#                  'animals/0645/0532.jpg', 'animals/0656/0208.jpg', 'animals/0009/0215.jpg']

# # list with selected 11 species ids
# retained_specie_list = [7,24,90,100,119,127,128,149,154,372,374]  

# with open('path_to: wcs_camera_traps.json') as f:
#     metadata = json.load(f)
# annotation_df = pd.DataFrame(metadata['annotations'])                 # load the annotations and images into a dataframe
# images_df = pd.DataFrame(metadata['images'])
# img_ann_df = pd.merge(images_df,
#                       annotation_df,
#                       left_on='id',
#                       right_on='image_id',
#                       how='left').drop('image_id', axis=1)

# train_df = img_ann_df[['file_name','category_id']]                    # selecting required columns from the merged dataframe.
# train_df = train_df[~train_df['file_name'].isin(corrupted_files)]     # removing corrupted files from the dataframe
# train_df = train_df[~img_ann_df['file_name'].str.contains("avi")]

# # A 'category_id' of 0 indicates an image that does not contain an animal. 
# # To reduce the class imbalance, we will only retain
# # ~50% of the empty images in our training dataset.
# new_train_df = train_df[train_df['category_id']==0].sample(frac=0.5,
#                                                     random_state=42)
# new_train_df = new_train_df.append(train_df[train_df['category_id']
#                                             .isin(retained_specie_list)])


# Alternatively, we have provided a subset of training data containing a few samples from the 11 species mentioned above.

# In[3]:


agol_gis = GIS("home")


# In[4]:


training_data = agol_gis.content.get('677f0d853c85430784169ce7a4a54037')
training_data


# In[5]:


filepath = training_data.download(file_name=training_data.name)


# In[6]:


with zipfile.ZipFile(filepath, 'r') as zip_ref:
    zip_ref.extractall(Path(filepath).parent)


# In[7]:


output_path = Path(os.path.join(os.path.splitext(filepath)[0]))


# In[8]:


new_train_df = pd.read_csv(str(output_path)+'/retained_data_subset.csv')


# In[9]:


new_train_df


# ### Prepare data 

# Here, we are not using the `prepare_data()` function provided by `arcgis.learn` to prepare the data for analysis. Instead, we will use the `ImageDataBunch.from_df` method provided by fast.ai to read the necessary information from a dataframe and convert it into a 'DataBunch' object, which will then be used for training. We will use the standard set of transforms for data augmentation and use 20% of our training dataset as a validation dataset. It is important to note that we are also normalizing our inputs.

# In[10]:


Path_df=Path(output_path)                                           # path to the downloaded data
data = ImageDataBunch.from_df(path=Path_df,
                              folder='',
                              df=new_train_df,
                              fn_col=0,
                              label_col=1,
                              valid_pct=0.2,
                              seed=42,
                              bs=16,
                              size=224,
                              num_workers=2).normalize(imagenet_stats)


# ### Visualize a few samples from your training data

# When working with large sets of jpeg images, it is possible that some images will only be available as a stream coming from an image and will not be complete. If these image streams are present in the dataset, they could potentially break the training process. To ensure that the training flow does not break, we set the LOAD_TRUNCATED_IMAGES parameter to True to indicate that the model should train on whatever image stream is available.

# In[11]:


from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True


# In[12]:


data.show_batch(5)


# ## Train the model 

# ### Load model architecture

# The `arcgis.learn` module provides the ability to determine the class of each feature in the form of the `FeatureClassifier` model. For more in-depth information about it's working and usage, see this [link](https://developers.arcgis.com/python/guide/how-feature-categorization-works/). For our training, we will use a model with the 34 layered 'ResNet' architecture.

# In[13]:


model = FeatureClassifier(data)


# ### Find an optimal learning rate

# Learning rate is one of the most important hyperparameters in model training. Here, we explore a range of learning rates to guide us to choosing the best one. `arcgis.learn` leverages fast.ai’s learning rate finder to find an optimum learning rate for training models. We will use the `lr_find()` method to find the optimum learning rate at which we can train a robust model, while still considering the speed and efficiency of the model.

# In[14]:


lr = model.lr_find()


# In[15]:


lr


# Based on the learning rate plot above, we can see that the learning rate suggested by `lr_find()` for our training data is 0.002. We can now use this learning rate to train our model. However, in the latest release of `arcgis.learn`, we can now train models without even specifying a learning rate. This new functionality will use the learning rate finder to determine and implement an optimal learning rate.

# ### Fit the model

# To train the model, we use the `fit()` method. To start, we will use 5 epochs to train our model. The number of epochs specified will define how many times the model is exposed to the entire training set.
# 
# Note: The results displayed below are obtained after training on the complete dataset belonging to the 11 species specified above. However, the dummy training data provided may not produce similar results.

# In[16]:


model.fit(5, lr=lr)


# ### Visualize results in validation set

# Notice that after training our model for 5 epochs, we are already seeing ~95% accuracy on our validation dataset. We can further validate this by selecting a few random samples and displaying the ground truth and respective model predictions side by side. This allows us to validate the results of the model in the notebook itself. Once satisfied, we can save the model and use it further in our workflow.

# In[17]:


model.show_results(rows=8, thresh=0.2)


# Here, a subset of ground truths from the training data is visualized alongside the predictions from the model. As we can see, our model is performing well, and the predictions are comparable to the ground truth.

# ### Save the model

# We will use the `save()` method to save the trained model. By default, it will be saved to the 'models' sub-folder within our training data folder.

# In[ ]:


model.save('Wildlife_identification_model', publish=True, overwrite=True)


# ## Model inference

# This section demonstrates the application of our trained animal species classification model on the images stored as feature attachments. 

# ### Data preparation 

# To start, we have a feature class named "CameraTraps" that contains 5 features that are hosted on our enterprise account. We will consider these features to be camera locations. We also have multiple locally stored images for each of these camera locations that we want to use against our model.

# In[20]:


# Accessing the feature layer on the enterprise.

search_result = gis.content.search("title: CameraTraps owner:portaladmin", 
                                 "Feature Layer")
cameratraps = search_result[0]
cameratraps


# In[21]:


# Cloning the item to local enterprise account

gis.content.clone_items([cameratraps])


# In[22]:


search_result = gis.content.search("title: CameraTraps owner:portaladmin", 
                                 "Feature Layer")
cameratraps_clone = search_result[0]
cameratraps_clone


# In[23]:


cr_lyr = cameratraps_clone.layers[0]


# In order to attach images to each of these camera locations, we need to enable attachments to the feature layer.

# In[24]:


cr_lyr.manager.update_definition({"hasAttachments": True})


# In[25]:


cr_lyr.query().sdf


# Now, we can specify the path on the local drive where images are stored against each camera location. One important point to note here, is that the directory structure of the data folder path should have separate folders with the 'objectid' of the camera location points as their names and that contain all of the photos captured by that camera.
# 
# In our case, as we have 5 camera locations, we had a data folder containing 5 different folders with the names 1, 2, 3, 4, and 5 as the 'objectid' of the points (see the spatial dataframe table above). Each of these folders had multiple images captured by the cameras. Below is a diagram of the folder structure mentioned:
# 

# In[26]:


# path to images captured by each of those camera locations.

path = r'./Camera Traps/data'


# As there could be multiple images captured at each point, we need to create a separate feature for each image, and then attach the image to the feature added.

# In[27]:


for folder in os.listdir(path):
    count=1
    oid=folder
    for img in os.listdir(os.path.join(path,folder)):
        if count > 1:
            cr_lyr.edit_features(adds=[cr_lyr.query(where='objectId='+folder).features[0]])
            oid = cr_lyr.query().features[-1].attributes.get('objectid')
        print("Adding "+str(img)+" captured by camera number "+str(folder)+" to feature id number "+str(oid))
        cr_lyr.attachments.add(oid, os.path.join(path, folder, img))
        count+=1


# Note that there are separate entries in the feature class for each image captured.

# In[28]:


cr_lyr.query().sdf


# In[29]:


# To check attachment at feature id - 1

cr_lyr.attachments.get_list(1)


# ### Inferencing 

# Next, we will access the published model from the gis server. This model is trained on the entire dataset containing samples belonging to 11 species.

# In[30]:


fc_model_package = gis.content.search("title: Wildlife_identification_model owner:portaladmin",
                                      item_type='Deep Learning Package')[0]
fc_model_package


# In[31]:


fc_model = Model(fc_model_package)


# In[32]:


fc_model.install()


# In[33]:


fc_model.query_info()


# Now that our data is prepared and the model is installed, we can make use of the `classify_objects` tool provided by `ArcGIS API for Python` to classify each of the image attachments using the model we trained earlier in [this step](#Train-the-model).

# In[34]:


inferenced_item = classify_objects(input_raster=cr_lyr,
                 model = fc_model,
                 model_arguments={'batch_size': 4},
                 output_name="inferenced_cameratraps",
                 class_value_field='ClassLabel',
                 context={'processorType':'GPU'},
                 gis=gis)
inferenced_item


# ## Conclusion 

# In this notebook, we saw how we can train a deep learning model to classify camera trap images. We have used fast.ai's 'ImageDataBunch.from_df' function to prepare our data from the dataframe we created and used the `FeatureClassifier` model provided by `arcgis.learn` to train a feature classifier model. We also worked with feature layer attachments to attach multiple points to features and classify the attachment images linked to them.
# 
# This notebook is a clear example showcasing how easily the functions provided by `arcgis.learn` can be used interchangeably with the functions provided by other vision APIs. 
